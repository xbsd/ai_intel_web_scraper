[
  {
    "id": "kx-official_docs-785b2ad91af9",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/home.htm",
    "title": "About",
    "text": "You are here:\nkdb Insights Enterprise\nDocumentation\nHome\n\n## About\n\nkdb Insights Enterprise\nis a turnkey solution that allows you to capture and analyze your high-volume, high-velocity, machine-generated, time-series data.\n\n### Key features\n\n- Data discovery and exploration\n- Scalability\n- Security and access\n- Available and fault tolerant data\n- Diagnostics\n\n## Start here\n\nTo begin your journey, follow the sections below:\nIntroduction\nIncludes everything you need to begin your journey with\nkdb Insights Enterprise\n.\nInstall kdb Insights Enterprise\nSign up for free trial\nDeploy on Azure Marketplace\nDeploy to a Kubernetes cluster\nInstall KXI CLI Interface\nThe Basics\nExplore the core concepts of our technology.\nAboutkdb Insights Enterprise\nInterfaces\nManage Packages\nConfigure Database\nStore Data\nIngest & Transform Data\nQuery Data\nAnalyze Data\nVisualize Data\nDiagnose Deployments\nReference\nExplore our rich set of APIs.\nRest API\nPackage Object Reference\nPackaging API\nStream Processor API\nMachine Learning API\nLanguage Interfaces\nExamples\nRead guides on database creation, data ingestion and transformation,  querying, and visualization.\nCreate a Database\nIngest Object Storage Data\nStream Data from Kafka\nIngest SQL Data\nQuery Ingested Data\nView Ingested Data\n....\nand many more\n\n## Latest News and Releases\n\nRelease notes\nLatest\nPrevious\nAzure Latest\nAzure Previous",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 202,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-25be00debffd",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/query.htm",
    "title": "Query Ingested Data",
    "text": "\n# Query Ingested Data\n\nThis page contains a walkthrough which guides you through the steps to query stored data on an active databases.\nThis walkthrough makes reference to data ingested through pipelines created in the other walkthroughs and deployed to the\ninsights-demo\ndatabase. Therefore, before you can create the queries in this walkthrough, ensure the\ninsights-demo\ndatabase is created, as described in the\ndatabase\ndocumentation.\nYou must also build the pipelines to ingest the data. Details on these are provided on these pages:\n- Weather\n- Crime\n- Subway\n- Health\n\n## Create a query\n\nClick\nCreate new\nunder\nQueries\non the\nOverview page\n.\nThere is more than one method to query data from a database. Let's start with the\nBasic\nquery in the Query Builder section of the screen, shown below. Click the\nSQL\nor\nq\ntab for details on how to run a query using SQL or q.\nPreview\nBasic\nSQL\nq\nThe Preview tab is a lightweight query option to fetch small samples of a table using minimal time and resources and requires no coding experience.\n- Select one of the available tables;weather,crime,subway,health,taxifor theTable Namefield.\n- Define anOutput variable. This is generally optional but we use one in this example.\n- ClickRun Queryto return data.\nExplore your data without any no coding experience but allows you to apply filtering and other query options.\n- Select one of the available tables;weather,crime,subway,health,taxifor theTable Namefield.\n- Define anOutput variable. This is generally optional but we use one in this example.\n- ClickRun Queryto return data.\nTo query the\ninsight-demo\ndatabase with\nSQL\n:\n- Enter one of the sql queries from the following table, in the SQL tab.DataSQLqueryweatherselect * from weathercrimeselect * from crimesubwayselect * from subwayFor example, to see the number of events generated in thesubwaypipeline:textCopy`select * from subway`\n- Define anOutput Variable. This is generally optional but we use one in this example.\n- ClickRun Queryto generate results.\nNote\nThis option only works when Query Environment(s) is enabled. Refer to\nSystem Information\nfor details on how to check the status.\nTo query the\ninsight-demo\ndatabase with\nq\n:\n- Enter a query such as the following:Dataqqueryweatherselect from weathercrimeselect from crimesubwayselect from subway\n- Selectinsights-demofrom the list ofDatabasesand select aninstance.\n- Define anOutput Variable. This is generally optional but we use one in this example.\n- ClickRun Queryto generate results.\nNote\nYou always need to define an\nOutput Variable\nif you wish to use the results in the\nscratchpad\nand is valid for the current session.\n\n### Ad-hoc queries using Scratchpad\n\nYou have the option to run additional data investigations in the\nScratchpad\nwith\nq\nor\nPython\n. Scratchpad queries can reference the\nOutput Variable\ndefined in the\nprevious section\n. The following scratchpad queries reference the\nOutput Variable\ndefined in the previous section.\nFor example, to check the weather for the borough of\nQueens\n, where the basic query has been stored in the Output Variable\nw\n, enter the following query in the Scratchpad:\nSQL\nCopy\n\n```\nselect from w where borough like \"Queens\"\n```\n\n- PressCtrl + D(Windows) orâD(Mac) to evaluate the current line.WarningIf you are usingpython, only the first line of code is processed.NoteThere is no requirement to use the Scratchpad to view data. The Scratchpad is for ad hoc analysis of the data and is done afterRun Queryis run.\n\n### View results\n\nOnce you have run queries either by clicking\nRun Query\nor\nRun Scratchpad\nthe results are returned to lower portion of the screen in three tabs.  Right-click inside the console window to clear results. For more information on each of these tabs see:\n- Console\n- Table\n- Visual\nNote\nThe console shows results of the most recent query irrespective of the selected tab.  When switching between tabs, re-run\nRun Query\nor\nRun Scratchpad\nto repopulate results for the selected tab.\n\n## Query subway data\n\nThe following sections provide instructions and code examples to help you query the\nKafka subway\ndata.\n\n### Event count\n\nTo count the number of subway journeys in the table, do the following.\n- Enter the following query in theSQLtab:SQLCopySELECTCOUNT(*)FROMsubway\n- Define theOutput Variableass.\n- ClickRun Queryto execute the query. Rerun the query to get an updated value.\n\n### Filtering and visualizing\n\nTo get a subset of the data and perform further analysis using the Scratchpad:\n- Enter the following query in theSQLtab.SQLCopyselect*fromsubwaywherearrival_time=current_date\n- Define theOutput Variableass.\n- ClickRun Query.\nAdditional analysis, of todayâs train data, can be performed against the\ns\nvariable using the\nscratchpad\n. Querying in the scratchpad is more efficient than direct querying of the database and supports both\nq\nand\npython\n.\n- Enter the following query, in the Scratchpad and clickRun All.qCopyselectfroms where trip_id like\"AFA21GEN-5108-Weekday-00_091350_5..N71R\"This pulls data for a selectedtrip_id. Change the value of thetrip_idto another if this example returns no results.\n- Results are displayed in the tabs;Console,TableorVisualtabs.\n- In theVisualtab, set they-axisto usestop_sequenceandx-axistoarrival_timeto return a plot of journey times between each of the stops.\nNote\nEach time you change the results tab you must rerun the query.\n\n### Calculate average time between stops\n\nYou can calculate the average time between stops, as a baseline to determine percentage lateness.\n- In the scratchpad enter the following code, replacing thetrip-idwith the value you used in the previous section.qCopy`arrival_time`time_between_stops xcolsupdate time_between_stops:0^`second$arrival_time[i]-arrival_time[i-1]fromselectfromswhere trip_id like\"AFA21GEN-1091-Weekday-00_138900_1..N03R\"The following screenshot shows this query in the scratchpad and the results in the Console.NoteUnderstanding the q queryIn the above query, the followingqelements are used:ElementDescriptionxcolsTo reorder table columns^To replace nulls with zeros$To cast back toseconddatatypex[i]-x[i-1]To subtract each column from the previous oneIf you run into an error on execution, check to ensure the correct code indentation is applied fors3.InformationUse theTabletab to filter the results. For example, doing a column sort by clicking on the column header for the newly created variabletime_between_stopstoggles the longest and shortest stop for the selected trip.\nNote\nEach time you change the results tab you must rerun the query\n\n### Calculating percentage lateness for all trains\n\nYou can discover which trains were most frequently on time.\n- In the Scratchpad, replace any existing code with the following and clickRun Scratchpad.qCopy// Getting the length of each train journeyandnum of stopss3:select start_time:first arrival_time,journey_time:`second$last arrival_time-first arrival_time,first_stop:first stop_name,last_stop:last stop_name,numstops:count stop_sequenceby route_short_name,direction_id,trip_idfroms;// Filtering only trains that fully completed their routes4:selectfroms3 where numstops=(max;numstops) fby route_short_name;// Calculating the average journey time per sroutes5:update avg_time:`second$avg journey_time by route_short_namefroms4;// Calculating the % difference between actual journey timeandaverage time per routes6:update avg_vs_actual_pc:100*(journey_time-avg_time)%avg_timefroms5NoteUnderstanding the q query?In the above query, the followingqelements are used:ElementDescriptionfirst, lastTo get first or last record-To subtract one column from anothercountTo return the number of recordsbyTo group the results of table by column/s - similar to excel pivotfbyTo filter results by a newly calculated field without needing to add it to tablex=(max;x)To filter any records that equal themaximumvalue for that column$To cast back toseconddatatype*To perform multiplication%To perform division.If you run into an error on execution, check to ensure the correct code indentation is applied.\n\n### Most punctual train\n\nTo calculate which train was most punctual:\n- In the Scratchpad, replace any existing code with the following and clickRun ScratchpadqCopyselectfroms6 where avg_vs_actual_pc=min avg_vs_actual_pcUsingmin, the 04:53 route E train was the most punctual (your result will differ).\n\n### Visualize journey time\n\nIn the\nVisual\ntab, create a visualization for a single route.\n- In the Scratchpad, replace any existing code with the following and clickRun Scratchpad.qCopy// Filteringforat only inbound trains on Route1selectfroms6 where route_short_name=`1,direction_id=`inbound\n- Switch to a bar chart.  Set they-axistoavg_vs_actual_pcand thex-axistostart_timeto view results as shown below.\n\n### Distribution of journey times between stations\n\nTo assess the distribution of journey times between stations.\n- In the Scratchpad, replace any existing code with the following and clickRun Scratchpad.qCopy{count each group1xbar x}1e-9*\"j\"$razeexec1_deltas arrival_time by trip_idfroms\n- From the histogram, you can see that the most common journey time between stations is 90 seconds.\n\n## Next steps\n\n- Read about how tovisualize your data.\n\n## Further reading\n\nUse the following links to learn more about specific topics mentioned in this page:\n- Query Builder\n- Scratchpad\n- q language Reference\n- q data types\n- SQL queries",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1339,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-871b47356afe",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/ingest-and-transform-overview.htm",
    "title": "Ingest and Transform Overview",
    "text": "\n# Ingest and Transform Overview\n\nThis page provides an overview of data ingestion in\nkdb Insights Enterprise\nexplaining how both streaming and batch data can be brought into the platform using shared components.\nData ingestion brings external data into\nkdb Insights Enterprise\n. Data ingestion can come in two forms, streaming data or batch data. Both cases of data ingestion use the same building blocks as batch ingestion is just a bounded case of streaming ingestion. Data ingestion and transformation is powered by the\nkdb Insights Stream Processor\n. Users can take advantage of three main methods for building ingestion or transformation pipelines. Click on the links below to learn more:\nUsing the\nkdb Insights Enterprise\nWeb Interface\nUsing APIs\nThe\nimport wizard\nlets you connect external data sources with a kdb Insights database.\nPipelines\nallow you to connect to data sources to data sinks, transform data and analyze data in a drag and drop UI.\nPipelines\ncan be written using the\npipeline API\nand submitted using the\nkdb Insights CLI\nas a package.\nWarning\nIf entitlements are enabled, but you donât have entitlements to a database, any pipeline you deploy that tries to read from that database will fail.\nBefore deploying a pipeline, ensure you have entitlements to all the databases that pipeline needs to access, especially if you're using a Database Reader.\n\n## Examples\n\nSee below for a list of pipeline examples to get up and running:\n- S3 Ingestion- Import data from an S3 bucket\n- Kafka- Import data from a Kafka stream\n- PostgreSQL- Query data from PostgreSQL",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 262,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-e64b2d526453",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Interfaces/kxi-interfaces.htm",
    "title": "Overview",
    "text": "\n# kdb Insights EnterpriseInterfaces\n\nThis page provides an overview of the interfaces you can use to run and manage\nkdb Insights Enterprise\n:\n- KXI Command Line Interface\n- Web Interface\n- VSCode Extension\n- Developer Libraries\n\n## KXI Command Line Interface (CLI)\n\nThe\nKXI Command Line Interface (CLI)\nis a development interface for administrators and data developers to manage the\nkdb Insights Enterprise\napplication.\nUse the CLI to:\n- Manage installation, upgrade, and uninstall ofkdb Insights Enterprisedeployments.\n- Programmatically create, manage, and tear down assemblies and packages that allow you to manage the life cycle of deployments and custom logic.\n- Configure the system to meet your requirements using thekxi install setupcommand to simplify the process and set up the required parameters. Apply more advanced configurations using thevaluesfile it generates.\n- Manage access to the system with user and service level authentication and authorization, role management, single sign-on (SSO), and identify brokering.\nNote\nThe deployment of assembly\nyaml\nfiles using the\nkxi assembly\ncommand in the kdb Insights CLI has been deprecated and may be removed from future releases. Use packages instead, which you can deploy and manage using the kdb Insights CLI. For more information about packages, refer to\npackages\n.\n\n## Web Interface\n\nThe\nweb interface\nis an interactive visual browser-based tool for data developers and analysts to create and build databases for the purpose of querying and visualizing data into dashboards.\n- Build and configure your database with query, write down, schema, stream, and other advanced settings. Assign resources based on your data requirements.\n- Use the pipeline builder or import wizard to guide you through importing data from several different sources, including cloud and relational data services.\n- Explore and query your data usingq, SQL or Python.\n- Use Views, the embedded version ofKX Dashboards, a drag and drop interface that helps to analyze and visualize your data by building Views. Views include feature rich components such as charts, maps, and data grids for deeper insights.\n\n## Visual Code Studio Extension\n\nThe kdb Visual Studio Code extension is a development tool that provides features for developers working with the kdb+ database and the q programming language. This extension is particularly useful for those who need to manage and interact with various kdb environments efficiently. It integrates seamlessly with\nkdb Insights Enterprise\nwhen using a shared kdb process.\nUse the kdb VS Code extension to:\n- Installq. The extension integrates seamlessly withq.\n- Writeqcode with support for syntax highlighting, code prediction, and autocomplete.\n- Write and executeqfrom a single line of code, code block, orqfile.\n- Write and execute bothqand Python code againstkdb Insights Enterprise.\n- Connect to multipleqprocesses orkdb Insights Enterprisedeployments. Open multiple connections simultaneously, making it easy to develop and test with both q and Python across different kdb andkdb Insights Enterprisesetups.\n- Use a KX data source to choose a connection, specify the parameters and run API requests using SQL or qSQL. KX data source files allow you to build queries within VS Code, associate them with a connection, and run them against thekdb Insights EnterpriseAPI endpoints.\n- Use a KX workbook to choose a connection and runqor Python code against any connection. Workbooks simplify prototyping and executingqand Python code on aqprocess, utilizing the variables from akdb Insights Enterprisedeployment.\n- View results from your queries in two different windows. TheOutputwindow displays results as they are received by the kdb VS Code extension. Thekdb Resultswindow displays the kdb returned data in a table.\nTo access this extension and detailed documentation, refer to the\nVisual Studio Marketplace\n.\n\n## Developer Libraries\n\nLibraries are available in various languages, designed for developers to integrate with:\n- kdb Insights Enterprise- allows access from outside your cluster using load balancers, or from inside your cluster without the need for load balancers.\n- kdb Insights Reliable Transport- a microarchitecture that ensures reliable streaming of messages. It is designed to meet both high availability and high-performance requirements, and it is built upon the Raft consensus algorithm.\nRefer to the following resources to learn more about publishing, subscribing, and querying data.\n- Get started\n- Use the C interface\n- Use the Java interface\n- Use the Python interface\n- Internal q publisher and subscriber",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 703,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-771c5dfa5494",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/views.htm",
    "title": "View Ingested Data",
    "text": "\n# View Ingested Data\n\nThis section guides you through the steps to create Views in the\nkdb Insights Enterprise\nWeb Interface.\nYou can build rich data visualizations in\nkdb Insights Enterprise\nusing over 40 components including tables, maps, charts and more.\n\n## Create a View\n\nThe following steps explain how to create a View containing a Data Grid. This Data Grid can then be customized to display data from the\nwalkthroughs created earlier\n.\n- Theinsights-demodatabase must be created and added to theinsights-demopackage. Refer to thecreate a package and database walkthroughfor details.\n- ClickCreate NewunderViewson theOverviewpage.\n- In the Create View dialog set the following values:SettingValueView Nameinsights-demo-viewSelect a Packageinsights-demo\n- ClickCreate.Note that to add the new view to theinsights-demopackage the package must not be deployed. If it is you musttear it downto proceed.\n- Click-and-drag aData Grid(first component on the list), from the component menu into the central workspace.\n- With the component selected,Click to populate Data Source.or clickData Sourcein theBasicsproperties for the selected component.InformationWhen a component is selected in the workspace it displays a light-blue border.\n- ClickNewto add a new Data Source.\n- Enter a name for the Data Source.\n- ClickAPIto configure the Data Source. This walkthrough usesAPIto setup the Data Source. For further information on the otherData Sources see here.\n- Select the relevant tab in thenext sectionto customize the Data Source forweather,crime,subwayorhealthdata sets.\n\n## Data Grid\n\nA Data Grid tabulates data and offers a range of formatting options for tables, including highlight rules and custom templates.\nOnce you have completed the step in the previous section\nto create your view with the data grid\n, select the tab below that corresponds to the data set you have already ingested. Then complete the configuration to create a data grid for that tables data.\nweather\nsubway\nhealth\ncrime\nName the new Data Source\nweather\nand configure the following API properties.\n\n| Property | Value |\n| --- | --- |\n| table | weather |\n| startTS | 2022-07-28T00:00:00.000000000 |\n| endTS | 2022-07-28T23:59:00.000000000 |\n\nClick\nExecute\n,\nApply\nand then\nSelect Item\nto apply the data to the\nData Grid\n.  This step closes the query editor.\nNote\nDefining table in the query editor\nWhen creating a new data source, the\ntable\nis the name of the data table as hosted on the database, it's not the\nOutput variable\nas defined when\nexploring\nthe data.\n\n### Basics\n\nIn the\nBasics\nproperties enable\nCustom Layout\nfor end-user control of the columns to be displayed.\n\n| Property | Value |\n| --- | --- |\n| Filtering | Column Filters |\n\n\n### Custom Filters\n\nIn the\nCustom Filters\nproperties you can add a greater level of filter control with the following custom filters:\n\n| Property | Value |\n| --- | --- |\n| timestamp | DateTime |\n| sensor | Selection |\n| airtemp | Number |\n| name | Selection |\n| borough | Selection |\n\n\n### Columns\n\nUse the\nColumn\nproperties to apply column configurations which style content and widths for the display of data in the table.\n\n| Column | Display Name | Min Width | Text Align | Format | Precision | Date Format | Time Format | Range Color | Template | Hidden |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| timestamp | Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| sensor | Sensor | 100 | Left | General | - | - | - | - | - | - |\n| airtemp | Temperature | 100 | Center | Formatted Number | 2 | - | - | - | - | - |\n| name | Name | 150 | Left | General | - | - | - | - | - | - |\n| borough | Borough | 150 | Left | General | - | - | - | - | - | - |\n| longitude | longitude | - | - | - | - | - | - | - | - | Yes |\n| latitude | latitude | - | - | - | - | - | - | - | - | Yes |\n| color | color | - | - | - | - | - | - | - | - | Yes |\n\nYou can re-arrange the column order with a click-and-drag to move columns. For example, select the\nTemperature\ncolumn and reposition it below the\nBorough Column\nin the Data Grid\nColumns\nproperties.\n\n### Highlight Rules\n\nSet\nHighlight Rules\nproperties to create a color gradient for temperature. Use a\nGradient\nhighlight rule to give temperature definition in the table, using the following properties.\n\n| Property | Value |\n| --- | --- |\n| Name | Temperature Gradient |\n| Target | airtemp |\n| Rule Min | -120 |\n| Rule Max | 120 |\n| Apply Palette To | background |\n| COLOR PALETTE | #2bc2f0,#c2e3ed,#ffffff,#fab23f,#ff0000 |\n\n\n### Selection & Routing\n\nThe\nSelection & Routing\nproperties are used to pull value(s) from a table and set them to a\nview state parameter\n. A view state parameter is a stored value accessible to all components and queries.\nChange\nSelection Mode\nto\nSingle Row\nto enable the selection control.  Actions are used to map cell values from columns to view state parameters, enabled by a click inside the data grid. Set the\nTrigger Column\nto use wildcard,\n*\n.\nCreate actions to map:\n\n| Source column | View state parameter | Type | Default | Value |\n| --- | --- | --- | --- | --- |\n| sensor | sensor | symbol | Q-JC_04 | Q-JC_04 |\n| borough | borough | symbol | - | - |\n| name | name | symbol | - | - |\n\nName the new Data Source\nsubway\nand configure the following API properties.\n\n| Property | Value |\n| --- | --- |\n| table | subway |\n| startTS | yyyy-mm-dd 00:00:00.000000000 |\n| endTS | yyyy-mm-dd 23:59:00.000000000 |\n\nNote\nstartTS and endTS\nSet\nstartTS\nas\n00:00\nand\nendTS\nas\n23:59\nof the deployment date; use a\nSQL query\nto find the deployment date. Ensure both the\nsubway\npipeline and\ninsights-demo\ndatabase are running and active to access data in your View.\n- ClickExecute.\n- ClickApply.\n- ClickSelect Itemto apply the data to theData Grid.  This step closes the query editor.\n\n### Basics\n\nSubway\nis an informational data set it is recommended to use a simpler\nData Grid\n, so the following properties are recommended:\n\n| Property | Value |\n| --- | --- |\n| Filtering | Column Filters |\n| Enable Grouping | Unchecked |\n\n\n### Custom Filters\n\nCustom Filters\nproperties help find additional information for given routes. Columns with no custom filter use default text search. Add the following:\n\n| Name | Type |\n| --- | --- |\n| route_short_name | Selection |\n| trip_headsign | Selection |\n| route_long_name | Selection |\n| direction_id | Selection |\n| arrival_time | DateTime |\n\nColumns\nAdd the following\nColumn\nconfigurations style content and widths for the display of data in the table under the\nColumns\nproperties.\n\n| Column | Display Name | Min Width | Text Align | Format | Precision | Date Format | Time Format | Range Color | Template | Hidden |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| trip_id | Trip | - | - | - | - | - | - | - | - | Yes |\n| arrival_time | Arrival Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| stop_id | Stop | 100 | Left | General | - | - | - | - | - | - |\n| stop_sequence | Sequence | 100 | Left | General | - | - | - | - | - | - |\n| stop_name | Stop Name | 250 | Left | General | - | - | - | - | - | - |\n| stop_lat | Stop longitude | - | - | - | - | - | - | - | - | Yes |\n| stop_lon | Stop latitude | - | - | - | - | - | - | - | - | Yes |\n| route_id | Route ID | - | - | - | - | - | - | - | - | Yes |\n| trip_headsign | Route Header | 200 |  | General | - | - | - | - | - | - |\n| direction_id | Direction | 100 | - | General | - | - | - | - | - | - |\n| route_short_name | Route Number | 100 | - | Number | - | - | - | - | see below | - |\n| route_long_name | Route Name | 200 | - | General | - | - | - | - | - | - |\n| route_desc | Description | - | - | - | - | - | - | - | - | Yes |\n| route_type | Type | - | - | - | - | - | - | - | - | Yes |\n| route_url | Route URL link | - | - | - | - | - | - | - | - | Yes |\n| route_color | Route color | - | - | - | - | - | - | - | - | Yes |\n\nA feature of\ncolumns\nis custom\nTemplate\n.\n- Template adds custom styling to the data grid, but requires a little coding to get the most out of it.\n- Template usesHandlebarsto apply column formats.\n- Forsubway, use theroute_colorcolumn to apply color to the text of theroute_short_namecolumn; add to theAdvanced modeof Template:\nJavaScript\nCopy\n\n```\n<font color=\"{{route_color}}\">{{route_short_name}}</font>\n```\n\n\n### File Export\n\nIn the\nFile Export\nproperties uncheck\nShow Export CSV button\nand\nShow Export Excel Button\nto hide unnecessary file export.\nName the new Data Source\nhealth\nand configure the following API properties.\n\n| Property | Value |\n| --- | --- |\n| table | health |\n| startTS | yyyy-mm-dd 00:00:00.000000000 |\n| endTS | yyyy-mm-dd 23:59:00.000000000 |\n\nNote\nstartTS and endTS\nSet\nstartTS\nas 00:00 and\nendTS\nas 23:59 of the deployment date; use a\nSQL query\nto find the deployment date. Ensure both the\nhealth\npipeline and\ninsights-demo\ndatabase are running and active to access data in your View.\nClick\nExecute\n, then click\nApply\n. Next, click\nSelect Item\nto apply the data to the\nData Grid\n. This closes the query editor.\n\n### Basics\n\nIn the\nBasics\nproperties enable\nColumn Filtering\n.\n\n### Custom Filters\n\nNumeric columns filter with\n>\n,\n<\nor\n> <\nfor a range.\n\n| Name | Type |\n| --- | --- |\n| timestamp | DateTime |\n| name | Selection |\n| airquality | Number |\n| trafficcongestion | Number |\n| bikeacccnt | Number |\n| caracccnt | Number |\n\n\n### Columns\n\nColumn configurations style content and widths for the display of data in the table.\n- UseRange Colorto add color gradients to numeric columns.\n- CheckInvert Range Colorto give high values a darker shade.\n\n| Column | Display Name | Min Width | Text Align | Format | Precision | Date Format | Time Format | Range Color | Template | Hidden |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| name | Name | 250 | Left | General | - | - | - | - | - | - |\n| lat | Latitude | - | - | Number | - | - | - | - | - | Yes |\n| long | Longitude | - | - | Number | - | - | - | - | - | Yes |\n| neighborhood | Neighborhood | 250 | Left | General | - | - | - | - | - | Yes |\n| airquality | Air Quality | 250 | Left | Number | 2 | - | - | #d1e3ff | - | - |\n| trafficcongestion | Traffic Congestion | 100 | Left | Number | 2 | - | - | #ffe999 | - | - |\n| bikeacccnt | Bike Accidents | 100 | Left | Number | 0 | - | - | - | - | - |\n| caracccnt | Car Accidents | 100 | Left | Number | 0 | - | - | - | - | - |\n| timestamp | Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n\nChange the column order to re-position the Timestamp column as the first column.\n\n### Highlight Rules\n\nUnder the\nHighlight Rules\nproperties, add a color highlight for number of bike and car accidents.\n\n| Value | Bikeacccnt | Caracccnt |\n| --- | --- | --- |\n| Name | Bike Accidents | Car Accidents |\n| Target | bikeacccnt | caracccnt |\n| Rule Min | 0 | 0 |\n| Rule Max | 120 | 12 |\n| Apply Palette To | background | background |\n| COLOR PALETTE | #ffffff, #b71c1c | #ffffff, #b71c1c |\n\n\n### Selection & Routing\n\nSelection & Routing\nproperties are used to pull value(s) from a table and set it to a\nview state parameter\n. A view state parameter is a stored value accessible to all components and queries. Create a view state for neighborhood to use in other (optional) queries, for example a chart of accidents, congestion and air quality.\n- Change theSelection Modeto aSingle Row.\n- Define an action to map the neighborhood name to a view state parameter. This action occurs on a click inside theData Grid; set theTrigger Columnto use the wildcard,*.\nACTIONS\n\n| Property | Value |\n| --- | --- |\n| Trigger Column | * |\n| Trigger Action | Click |\n| Source Column | name |\n| > | view state parameter:name |\n\nName the new Data Source\ncrime\nand configure the following API properties.\n\n| Property | Value |\n| --- | --- |\n| table | crime |\n| startTS | 2022-03-31 00:00:00.000000000 |\n| endTS | 2022-03-31 23:59:00.000000000 |\n\nClick\nExecute\n, then click\nApply\n. Next, click\nSelect Item\nto apply the data to the\nData Grid\n.  This closes the query editor.\nNote\nRequires an active database and running pipeline\nTo build a view, the\ncrime\nmust be\nrunning\nand the\ninsights-demo\nis active.\n\n### Basics\n\nSet the following\nBasics\nproperties:\n\n| Property | Value |\n| --- | --- |\n| Filtering | Column Filters |\n| Enable Grouping | Checked |\n| Custom Layout | Checked |\n\n\n### Custom Filters\n\nIn the\nCustom Filter\nproperties, use filters of\nType\n=\nSelection\nfor descriptive columns and\nType\n=\nDateTime\nfor timestamps. Set the following filters:\n\n| Filter | Setting |\n| --- | --- |\n| incident_time | DateTime |\n| nypd_precinct | Selection |\n| borough | Selection |\n| patrol_borough | Selection |\n| radio_code | Selection |\n| crime_in_progress | Selection |\n| call_timestamp | DateTime |\n| displatch_timestamp | DateTime |\n| arrival_timestamp | DateTime |\n| closing_timestamp | DateTime |\n\n\n### Columns\n\nSet\nColumn\nproperties to ensure descriptive columns have sufficient width to display their information.\n\n| Column | Display Name | Min Width | Text Align | Format | Precision | Date Format | Time Format | Range Color | Template | Hidden |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| event_id | Event | 80 | Left | General | - | - | - | - | - | - |\n| incident_time | Time of Incident | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| nypd_precinct | Precinct | 60 | Left | Number | 0 | - | - | - | - | - |\n| borough | Borough | 100 | Left | General | - | - | - | - | - | - |\n| patrol_borough | Patrol Borough | 200 | Left | Number | - | - | - | - | - | - |\n| call_x_geo | Call x-cord. | - | - | Number | - | - |  | - | - | Yes |\n| call_y_geo | Call y-cord. | - | - | Number | - | - | - | - | - | Yes |\n| radio_code | Radio Code | 80 | Left | Number | - | - | - | - | - | - |\n| description | Crime Description | 300 | Left | General | - | - | - | - | - | - |\n| crime_in_progress | Crime in Progress? | 100 | Left | General | - | - |  | - | - | - |\n| call_timestamp | Call Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| dispatch_timestamp | Displatch Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| arrival_timestamp | Arrival Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| closing_timestamp | Close Time | 100 | Left | Time | - | - | HH:mm:ss | - | - | - |\n| latitude | Latitude | - | - | Number | - | - | - | - | - | Yes |\n| longitude | Longitude | - | - | Number | - | - | - | - | - | Yes |\n\n\n### Highlight Rules\n\nAdd color highlights to give context to the seriousness of reported crime.  Use wildcard,\n*\nto apply a highlight to the entire row.\n\n| Value | Critical | Serious | Non-critical |\n| --- | --- | --- | --- |\n| Name | Critical | Serious | Non Critical |\n| Target | * | * | * |\n| Condition Source | crime_in_progress | crime_in_progress | crime_in_progress |\n| Condition Operator | == | == | == |\n| Condition Value | Critical | Serious | Non Critical |\n| Background Color | #ffa1a1 | #ffd27d | #fcefd6 |\n\nTo learn more about highlight rules click\nhere\n\n## Data Form with View State\n\nThis section guides you on how to create custom queries with the\nData Form\n. A\nData Form\nexposes\nview state parameters\nto change in the View; for example,\nstartTS\nand\nendTS\nparameters.\n- Click-and-drag aData Forminto the View.\n- Click on theParameterproperty underViewstate Parametersto open the view state parameter dialog.\n- To create view states for startTS and endTS:ClickNewto create view states forStart TimeandEnd Timewith the following properties:PropertyValueTypeSet both totimestamp.DefaultUse the deployment date as the default value for bothstartTSandendTS.  Use aSQL queryto find the deployment date forweather,subwayorhealthdata sets. These values are shown in the tabs below.\nStart Time\nweather, subway and health\ncrime\n\n| Property | Value |\n| --- | --- |\n| Type | timestamp |\n| Default | 2022-07-28T00:00:00.000000000 |\n| Value | 2022-07-28T23:59:00.000000000 |\n\nNote\nUse the deployment date for\nDefault\nand\nValue\nyyyy-mm-dd\n.\n\n| Property | Value |\n| --- | --- |\n| Type | timestamp |\n| Default | 2022-03-31 00:00:00.000000000 |\n| Value | 2022-03-31 00:00:00.000000000 |\n\nEnd Time\nweather, subway and health\ncrime\n\n| Property | Value |\n| --- | --- |\n| Type | timestamp |\n| Default | yyyy-mm-dd 23:59:00.000000000 |\n| Value | yyyy-mm-dd 23:59:00.000000000 |\n\nNote\nUse the deployment date for\nDefault\nand\nValue\nyyyy-mm-dd\n.\n\n| Property | Value |\n| --- | --- |\n| Type | timestamp |\n| Default | 2022-03-31 23:59:00.000000000 |\n| Value | 2022-03-31 23:59:00.000000000 |\n\n\n## Use View State Parameters in Data Source\n\nTo use the View State parameters,\ncreated above\n:\n- Replace thestartTSandendTSvalues, set in the Data Grid, with their corresponding View State parameters in the data editor as shown below.\nNote\nDate Picker\nDate Picker is a time-specific input; map\nStart Time\nor\nEnd Time\nview states to the\nSelected Date\nproperty under the list of\nBasics\noptions.\n\n### Filter\n\nUse the\nFilter\nproperty of the data editor.\n- Create a view state of typeList(namedfilter) and assign to theFilterproperty of data editor inData Grid.\n- Add aData Form.\n- Map the createdfilterview state toViewstateParameterof theData Form.\n- SwitchParameter TypetoDropdown\n- AddItemscorresponding to the selected values.\nThere is a special case for defining the Value for\nItems\nwhich uses the following structure as determined by the\ngetData API\n.\nq\nCopy\n\n```\n=;column_name;value\n```\n\n\n| item | setting |\n| --- | --- |\n| = | matches numeric values, uselikefor text values. |\n| column_name | data column name referenced in the database. |\n| value | filter value; numeric for=, text forlike. |\n\nFilter\nvalues\nby data set:\nsubway\ncrime\nhealth\n\n| Property | Value |\n| --- | --- |\n| Parameter | Directionview state parameter |\n| Display Name | Direction |\n\nSet the following\nItems\n:\n\n| Item | Value |\n| --- | --- |\n| like;direction_id;inbound | Inbound |\n| like;direction_id;outbound | Outbound |\n\n\n| Property | Value |\n| --- | --- |\n| Parameter | CIPview state parameter |\n| Display Name | Crime In Progress |\n\nSet the following\nItems\n:\n\n| Items | Value |\n| --- | --- |\n| like;crime_in_progress;Critical | Critical |\n| like;crime_in_progress;Non CIP | Not in Progress |\n| like;crime_in_progress;Non Critical | Not Critical |\n| like;crime_in_progress;Serious | Serious |\n\n\n| Property | Value |\n| --- | --- |\n| Parameter | Car Accidentview state parameter |\n| Display Name | Number of Car Accidents |\n\nSet the following\nItems\n:\n\n| Item | Value |\n| --- | --- |\n| =;caracccnt;0 | None |\n| =;caracccnt;1 | One |\n| =;caracccnt;2 | Two |\n| =;caracccnt;3 | Three |\n| =;caracccnt;4 | Four |\n| =;caracccnt;5 | Five |\n\nNote\nDrop Down List\nis a self-contained dropdown component.  Assign the\nfilter\nview state parameter to\nSelected Value\nproperty listed under\nBasics\n. Define\nItems\nas for data form.\n\n## Next steps\n\n- Add a map to your weather view\n- Vizualise streaming data in real-time\n\n## Further reading\n\nClick a link below to learn more about the following:\n- View (Dashboards) components\n- Data Grids\n- Data Forms\n- Date Pickers\n- Filtering\n- Highlight rules\n- Actions\n- Custom templating\n- Drop downs",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3913,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-22c5e46f194f",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/views-overview.htm",
    "title": "Views Overview",
    "text": "\n# Views Overview\n\nThis page provides an overview of\nkdb Insights Enterprise\nViews.\nkdb Insights Enterprise\nViews is an embedded version of\nKX Dashboards\nwhich enables you to create visualizations of your data. Using the drag and drop interface you can build\nViews\ncontaining feature rich components such as charts, maps, data grids and many other components. These visualization help you to analyze and gain insights from your data.\nkdb Insights Enterprise\nsupports over 40\ncomponents\n, which can be added to your\nViews\n.\n\n## Demo views\n\nkdb Insights Enterprise comes populated with a set of demonstration Views which provide a catalog of fully interactive examples designed to illustrate the end-to-end capabilities of\nkdb Insights Enterprise\nvisualization and analytic components.\nTo access these demo Views:\n- Use one of the following methods:ClickViewson the left-hand menu and then clickDemosClickView Demoson theViewsquick action onthe Overview page.\n- Click on a View name to open it. For example clickVisualizeto open the Visualize View.\nThis dashboard presents a rich set of interactive visualizations designed to support exploratory data analysis and insight generation across diverse datasets.\nThe first tab displayed shows key chart types including:\n- Bar & Line Charts: Ideal for trend and comparison analysis across categories or time.\n- Bubble & Scatter Plots: Useful for multivariate analysis and identifying correlations or clusters.\n- Area Charts: Highlight cumulative values and trends over time.\n- Boxplots & Violin Plots: Provide statistical summaries and distribution insights.\n- Candlestick Charts: Tailored for financial time series, showing OHLC (Open, High, Low, Close) data.\n- Click on other tabs to continue to explore this sample dashboard.\nExplore the complete set of demo Views to help you understand the full visualization capabilities of kdb Insights Enterprise.\nStream\nSensors\nData\nHealth\nMaps\nGauge\nChartGL\nThe following pages provide details on how to create and configure Views:\n- Quickstart guide\n- Guide to building visualizations\nOn this page you can see how to\naccess an index of views\n.\n\n## Index of views\n\nClick on\nViews\nin the\nleft-hand menu\nto view a list of all your views.\nThe following details are displayed:\n\n| Column | Description |\n| --- | --- |\n| Name | The name of each of the Views. Click on the name to go to theview screen. Views are listed alphabetically and can be reordered by clicking the column name. This listing includes the names of demo views.To filter the list of names click the filter icon and enter a filter value. You can refine your filter further using AND/OR options to add additional criteria. |\n| Tags | TheTagsassociated with this view. When the number of Tags exceeds the visible space, a+nindicator appears to show how many additional tags are not currently displayed. Click the number to display the extra tags.You can sort the contents of the column by clicking the column heading.You can alsofilter the listing by tag. |\n| Package | The name and version of thepackagethat contains this view. |\n| â® | The last column contains an action menu represented by a three-dot icon (â®). This menu provides additional options or actions specific to each row entry. SeeActionsfor details. |\n\nClick\nCreate View\nto\ncreate a new view\n.\n\n### Tags\n\nTags provide an effective way to organize Views, making them easier to manage and arrange. As the number of Views increases, using tags helps maintain a scalable structure, reducing clutter and confusion. Tags also streamline access and sharing, allowing users to quickly find and share relevant Views without having to sort through unrelated content.\nTags are applied when you\ncreate a View\nor by\nadding them to existing Views\n.\n\n#### Filter Tags\n\nIn the\nViews index\nyou can filter Views by tags, helping you quickly locate the relevant Views and reduce time spent searching.\n- ClickFilterto open the tag filter dropdown. This shows a list of tags applied to the Views you have access to.Each tag is accompanied by a checkbox and the number of Views that use it.TheAll Tagsoption lets you quickly select or deselect all tags at once.TheNo Tags Assignedoption lets you exclude untagged Views from the results.\n- Select one or more tags to filter the index.The list of Views updates to show only those that include all selected tags.For example, selecting theforexandequitiestags filters the index to show only Views containing both.\n\n## View Actions\n\nFrom the\nViews index\nor the\nViews\noption on the left-hand menu, click on the three dots beside the View name to access the following actions:\n- Share\n- Rename\n- Edit Tags\n- Delete\n\n## Next steps\n\n- Read thequickstart guideto setting up a view.\n- Read thedetailed guide to setting up views.\n\n# Further reading\n\nThe following pages provide guided walkthroughs on creating Views:\n- A guided walkthrough on setting up Views.\n- A guided walkthrough on adding a Map component to a View.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 815,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-f0f6b872aef0",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Get_Started/free-trial.htm",
    "title": "7 Day Free Trial",
    "text": "\n# kdb Insights EnterpriseFree Trial\n\nSign up for a\n7-day free trial\n.\nThis allows you to try\nkdb Insights Enterprise\nand experience how KX gives developers, data scientists, and business leaders the ability to deliver production-ready data-driven decisions from time series data.\nNote\nThe purpose of this free trial is to showcase the\nkdb Insights Enterprise\nuser interface. If you want to trial a full-featured version of\nkdb Insights Enterprise\n, sign up for a 30-day free trial of the Managed\nkdb Insights Enterprise\nDeployment on the Azure Marketplace.\nUse the\nproduct tour\nto experience the power and speed of the data analytics in\nkdb Insights Enterprise\nusing SQL, Python and q.\nUse the\nguided walkthrough\nto learn how to build databases and pipelines with\nkdb Insights Enterprise\n.\nUse the\nindustry tutorials\nto build a real-time trading application or learn how to apply predictive analytics for manufacturing.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 149,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-e55243b3bb05",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/get-data-object-storage.htm",
    "title": "Ingest Object Storage Data",
    "text": "\n# Ingest Object Storage Data\n\nThe page provides a walkthrough to demonstrate how to ingest data from object storage into a database.\nWe have provided a\nweather\ndataset, hosted on each of the major cloud providers, for use in this walkthrough.\nInformation\nNo prior experience with q/kdb+ is required to build this\npipeline\n.\nYou must create the\ninsights-demo\ndatabase, as described in the\ncreate a database walkthrough\n, before you begin using the Import Wizard to create the pipeline.\nThe following sections describes how to:\n- Create the pipelineand add it to theinsights-demopackage createdhere. This pipeline is comprised of the following nodes:Readers. To read data from its source. Either Google Cloud Storage, Amazon S3 or Microsoft Azure Storage.Decoders. To decode the ingested csv data.Schema. To convert the data to a type compatible with a kdb+ database.Writers. To write the data to akdb Insights Enterprisedatabase.\n- Deploy the pipeline. To run the pipeline you have just created to ingest data into the insights-demo database.\n- Teardown the pipeline. The pipeline can be torn down after data has been ingested. This frees up resources and is good practice.\n\n## Create the pipeline\n\nUse the Import Wizard to create the pipeline:\n- On theOverviewpage, in theQuick Actionspanel underDatabaseschooseImport Data.\n- In theImport your datascreen select a cloud provider;Google Cloud Storage,Microsoft Azure Storage,Amazon S3.\n- Complete the reader properties for the selected cloud provider.Google Cloud StorageMicrosoft Azure StorageAWS S3PropertiesSettingValueGS URI*gs://kxevg/weather/temp.csvProject IDkx-evangelismTenantNot applicableFile ModeBinaryOffset0ChunkingAutoChunk Size1MBUse WatchingNoUse AuthenticationNoPropertiesSettingValueMS URIms://kxevg/temp.csvAccountkxevgTenantNot applicableFile ModeBinaryOffset0ChunkingAutoChunk Size1MBUse WatchingUncheckedUse AuthenticationUncheckedPropertiesSettingValueS3 URIs3://kxs-prd-cxt-twg-roinsightsdemo/weather.csvRegioneu-west-1File ModeBinaryTenantkxinsightsOffset0ChunkingAutoChunk Size1MBUse WatchingNoUse AuthenticationNo\n- ClickNextto open theSelect a decoderscreen.\n- SelectCSV, as shown below, as the weather data is a csv file.\n- In theConfigure CSVscreen keep the default CSV decoder settings.\n- ClickNextto open theConfigure Schemascreen.\n- In theConfigure Schemascreen, leave the following unchanged:SettingValueApply a SchemaEnabledData FormatAnyClickLoad Schemaset the following values:SettingValueDatabaseinsights-demoTableweather\n- ClickLoad.\n- ClickNextto open theConfigure Writerscreen.\n- Configure the writer settings as follows:SettingValueDatabaseinsights-demoTableweatherLeave the remaining settings unchanged.\n- ClickOpen Pipelineto display theCreate Pipelinedialog and set the following values:SettingValuePipeline Nameweather-1Select a Packageinsights-demo\n- ClickCreate.Ifinsights-demodoes not appear on packages list create it, as describedhere.\n- You can review thePipelineas shown below. Note that the first node in the pipeline differs depending on the selected reader type.\n- ClickSave.\nAt this stage, you are ready to deploy the pipeline to ingest the data.\n\n## Deploy the pipeline\n\nDeploy the package containing the database and pipeline in order to ingest the data into the database.\n- Go to thePackage Indexpage and click on the three dots besideinsights-demopackage and clickDeploy.NoteIt may take Kubernetes several minutes to spin up the necessary resources to deploy the pipeline.If the package or its database are already deployed you must tear it down. Do this on thePackage Indexpage by clicking on the three dots besideinsights-demopackage and clickTeardown.\n- You can check the progress of the pipeline under theRunning Pipelinespanel of theOverviewtab. The data is ready to query whenStatusisFinished.\nWarning\nOnce the pipeline is running some warnings may be displayed in the\nRunning Pipelines\npanel of the\nOverview\ntab, these are expected and can be ignored.\n\n## Pipeline teardown\n\nOnce the CSV file has been ingested, the weather pipeline can be torn down. Ingesting this data is a batch ingest operation, rather than an ongoing stream, so it is ok to teardown the pipeline once the data is ingested. Tearing down a pipeline returns resources, so is a good practice when it is no longer needed.\n- ClickXinRunning Pipelinespanel on theOverviewtab to teardown a pipeline.\n- CheckClean up resources after teardownas these are no longer required now that the CSV file has been ingested.\n- ClickTeardown Pipeline.\n\n## Troubleshoot pipelines\n\nIf any errors are reported they can be checked against the logs of the deployment process.  Click\nView diagnostics\nin the\nRunning Pipelines\npanel of the\nOverview\ntab to review the status of a deployment.\n\n## Next steps\n\nNow that data has been ingested into the weather table you can:\n- Query the weather data\n- Visualize the weather data\n- Ingest other data\n\n### Further reading\n\nUse the following links to learn more about specific topics mentioned in this page:\n- Import wizard\n- Pipelines index\n- Building a pipeline\n- Pipeline Operators\n- Troubleshooting pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 700,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-429e21dc41df",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/KXI/previous-release-notes.htm",
    "title": "Previous",
    "text": "\n# kdb Insights Enterprise- Previous Release Notes\n\nThis page provides the link to release notes from past releases. For the most recent release, refer to the\nlatest release\npage.\n\n## 1.16.5\n\nRelease date: 2026-02-10\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\n- Resolved an issue where partitions migrated to object storage could be lost if a batch ingest was run before the next end-of-day (EOD) process.\nSecurity\n- Various CVEs were remediated as part of this release.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.5.tgz | insights-1.16.5.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.4-py3-none-any.whl | kxicli-1.16.4-py3-none-any.whl |\n| RT q interface | rt.1.16.4.qpk | rt.1.16.4.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.21.tgz | insights-on-k8s-1.1.21.tgz |\n\n\n## 1.16.4\n\nRelease date: 2025-12-17\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nStream Processor\nBLUE SERVICES\n- Resolved a datatype mismatch for the timeout parameter that occurred when the database writer passed parameters to a stream writer upon initialization. This issue prevented Reliable Transport from archiving RT log files.\nPackaging\n- Resolved the inability to run.kxi.packages.*commands in the scratchpad.\nSecurity\n- Various CVEs were remediated as part of this release.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.4.tgz | insights-1.16.4.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.4-py3-none-any.whl | kxicli-1.16.4-py3-none-any.whl |\n| RT q interface | rt.1.16.4.qpk | rt.1.16.4.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n## 1.16.3\n\nRelease date: 2025-11-28\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nPackaging\n- The version ofkdb Insights Enterpriseand the CLI are now checked against each other for compatibility during the installation/upgrade, even when using--force. This avoids  errors or deployment issues due to version misalignment.\n- Packages are downloaded into thetmpDirthat is specified on the package spec. A warning in the CLI has been added to flag when the temporary directory is less than 100 MB to avoid this operation failing.\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Resolved an issue where batch ingest operations would fail following an upgrade from older versions to 1.16.\n- Resolved an issue where, after redeploying a database, the teardown operation resulted in a redeploy instead of a teardown.\nViews\n- Performing a PDF export of a chart from a View should no longer result in the PDF being blank.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.3.tgz | insights-1.16.3.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.3-py3-none-any.whl | kxicli-1.16.2-py3-none-any.whl |\n| RT q interface | rt.1.16.0.qpk | rt.1.16.0.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.10.tgz | insights-on-k8s-1.1.10.tgz |\n\n\n## 1.16.2\n\nRelease date: 2025-11-14\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nPackaging\nBLUE SERVICES\n- Resolved issues that were causing CPU and memory spikes in the Package Manager.\n- Fixed an issue where thekxi pm deploywould pick up wrong version.\n- Resolved an issue where nested package loads would lose track ofcurrent_package.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.2.tgz | insights-1.16.2.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.2-py3-none-any.whl | kxicli-1.16.2-py3-none-any.whl |\n| RT q interface | rt.1.16.0.qpk | rt.1.16.0.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-0.4.36.tgz | insights-on-k8s-0.4.36.tgz |\n\n\n## 1.16.1\n\nRelease date: 2025-11-10\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nStream Processor\n- Upgraded thelibrdkafkalibrary to version 2.11.1.\nDatabase\n- Added a new endpoint,/ws/v1/subscriber/nodes, to the Service Gateway. This allows users to query the available websocket streaming endpoints.\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Resolved a timing issue with the inter-process signals that could cause processes that were behind in ingesting data to skip ahead in the data stream.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.1.tgz | insights-1.16.1.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.1-py3-none-any.whl | kxicli-1.16.1-py3-none-any.whl |\n| RT q interface | rt.1.16.0.qpk | rt.1.16.0.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-0.4.36.tgz | insights-on-k8s-0.4.36.tgz |\n\n\n## 1.16\n\nRelease Date 2025-10-29\nkdb Insights Enterprise\n1.16 delivers performance and security improvements, further extensibility and resiliency, enhancements to APIs, and user workflow improvements. Read on to find out more!\n\n### New Features\n\n- Entitlements no longer in Beta\n- Improved security for pipelines\n- Configurable timeout on the Query Window\n\n#### Entitlements is no longer in Beta\n\nBoth\ndata entitlements\nand\npackage entitlements\nare now out of beta. Package owners and Entitlements Administrators can configure:\n- Package entitlements: this limits the ability to view, edit, or deploy the package details. The package details include the database, pipeline and view configuration.\n- Database and Row Level entitlements:  this limits the ability to query the data a user or pipeline can read from a database.\nRefer to the\nEntitlements documentation\nfor more details.\n\n#### Improved security for pipelines\n\nThis release introduces enhanced security for managing sensitive information in pipeline configurations. You can now reference environment variables in node parameters and have them resolve at runtime. These environment variables can be mounted from Kubernetes secrets created outside the package, avoiding package or pipeline definitions from containing sensitive information.\nRefer to\nsecure pipelines with Kubernetes secrets\nfor more information and to\nenvironment variable and secrets\nfor details on how to set Kubernetes secrets in the Web Interface.\n\n#### Configurable timeout on the Query Window\n\nIn the Query Window you can now specify a custom timeout value ranging from a few seconds to several hours. This timeout value will be used in both Python and q scratchpad queries, as well as database queries. This is particularly useful for long running queries and installing python packages. Refer to\nexecution timeout\nfor more details.\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nPDF width improvements in Views\n- When saving a View as a PDF you were limited to the standard size of 1920x1080. You can now choose the current preview size, enabling you to save based on much larger screen sizes. Refer to theViews guidefor details.\nDatabase\n- Performance improvements for table counts: Improved the performance of loading table count caching, for data maintained by Data Access processes.\n- The Data Access processes now output a warning or error log each minute if they have not received end of replay message.\n- Introduced more robust validation of UDA parameters. A warning is issued if any parameter is missing from registration, and errors if a registered parameter does not exist.\nSQL2 Queries\n- SQL2 queries now support=when referencing a string column and searching for exact values.\n- You can now performINNER,OUTER,LEFT,RIGHT,CROSS,FULLjoins on an arbitrary number of tables, using the=operator.\n- There is now support for non-aggregate operations on group by columns.\nRefer to\nSQL2\nfor details\nQuery Window\n- In the Basic Query tab, under Optional Parameters, fields that list columns â such as Select Columns â are now searchable. Refer tobasic queriesfor details.\n- Placeholders have been added to the Query Window to show you the shortcut keys for executing the current line or selection.\n- The default log level for scratchpad deployments is nowINFO.\nWeb Interface\n- You can now import a.kxipackage into Insights Enterprise from the web interface. Seeimport a packagefor details.\n- TheImport Wizardnow allows you to change the Node version if multiple versions are available.\n- Whendeleting a package, the user can now choose whether to delete all data related to the package, which includes database contents, pipeline checkpoints, and user states.\nPackage Manager\n- Package roles now control access to the Insights Web Interface. Legacy Package API roles have been deprecated and replaced with updated roles. Users assigned composite roles (e.g., Maintainer) will not be affected by this change.\n- You can now use thekxi pm editcommandto edit the specifications of a currently running pipeline. This will restart the pipeline in order for the changes to take effect.\n- Thekxi pm bouncecommandallows you to target pipelines and databases separately for restarts as well as restart the whole package. You can also restart individual components in a running package using thekxi pm bouncecommand.\n- Thekxi pm field infocommand was added to get up-to-date server side Field information.\n- The following changes were made to thekxi package packitcommand:Target directory can now be specified by--targetBy default it will keep the current version of your package in the.kxiuse the checkpoint command if you want automatic version updatesThe--tagoption is now deprecated and is the default behaviorThe--override-deps/--keep-unlocked/--all-depsoptions are now deprecated\n- Thekxi pm journalcommand and corresponding endpoint shows successful events that have been executed in the Package Manager. This feature requires package admin role.\nRefer to the\nPackage Manger Reference\npage for more details.\nReliable Transport\n- RT server logs are now deleted by default.\n- We now provide environment variables which, if set by RT clients, will determine the replicator endpoint to connect to. Refer toUsing the C interfacefor more details.\nStream Processor\n- The new version of the Parquet reader, v2, now manages the file downloads more efficiently against available disk space across all file readers in the pipeline. The v1 version operates independently from any other readers in the pipeline and could exhaust available space if multiple files are downloaded at the same time. Refer toingesting data from object storagefor more details.\nTerraform Scripts\n- A new guide is available for helping you upgrade your Kubernetes version for a Client Terraform Script environment. Read more about how toupgrade a Kubernetes clusterandupgrade third-party dependencies.\n\n### Fixes\n\nThis release includes numerous bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Switched Service Gateway encoding to UTF-8 to support characters such as ä½ å¥½.\n- Fixed an issue whereselect *,* from tablewould fail with a type error if one of the columns was of symbol type.\n- Fixed an issue whereselect exchangeID,* from exchangewill now return theexchangeIDcolumn twice, matching the Postgres format. The previous behavior would return anexchangeIDcolumn and anexchangeID1column.\n- Fixed an issue where UDA parameters using large numbers were being rounded.\n- Cleaning up persistent storage now requiresinsights.package.cleanuprole.\n- Built-in APIs will now filter out partitioned table records with timestamps of-0Wpinstead of hitting a domain error.\nPipelines\n- Fetching details of non-existent pipelines wasn't returning a 404 error code, this has been resolved.\nPackaging\n- Extra fields are now ignored instead of generating errors on Package entities.\nAzure\n- Fixed issue with redeploy not deploying the database when using an azure cluster.\nSecurity\n- Security patches have been applied and CVEs have been remediated.\nWeb Interface\n- Fixed an issue where graphs werenât being plotted in the Scratchpad after running a database query.\n- When running SQL queries in the Query Window, the CTRL+D or âD key will now execute the current line or selection, rather than the entire editor.\n- Database configurations can now be exported for running databases.\nUpgrade\n- When upgrading from an environment where entitlements are disabled to one where entitlements are enabled, the Owner field for existing package entitlements is not automatically populated. After the upgrade a user with theinsights.role.administratorrole needs to use the entitlements update command to add an entitlements owner to any existing packages.\n\n### Important Upgrade and Deployment Considerations\n\n\n#### Upgrades\n\n- If upgrading from version 1.14.x or 1.15.x to 1.16, and you have Entitlements enabled, you must disable entitlements first. You can then perform the upgrade to 1.16, and then  re-enable entitlements as part of the upgrade.\n- To run an upgrade or rollback where entitlements is enabled in either version, when callingkxi install run, you must have theinsights.role.administratorrole.\n- When you upgrade and turn entitlements on, you need to ensure that the user that deploys any pipeline that reads data from a kdb Insights Database Reader has entitlements to read data from the database.\n\n#### Database\n\nIt is recommended to have an inventory file in place when one of your database tiers resides on object storage to help speed up reload times. Previously, the file information was configured by  setting the\nKX_OBJSTR_INVENTORY_FILE\nenvironment variable. This is no longer supported, as both SM and DAPs now handle this by themselves. Refer to\nObject storage inventory files\nfor more details.\nIf you have an inventory file in place, it will be necessary to change the configuration of its location reference. In your package file, you will now need  to specify the location of the inventory file using the\nlocation\nparameter of the\nsm.tiers[].inventory\nproperty.\nThis sample configuration illustrates  an S3 tier defined in a package file:\nBash\nCopy\n\n```\n- name: s3    mount: hb    store: s3://kxi-sm-example/db    inventory:     enabled: true     location: [file location here]\n```\n\nNote\nThe DAP will no longer start if the environment variable is set directly.\n\n#### Pipelines\n\nBreaking change:\nFor Python pipelines using Kafka writers with a schema registry, the\nauto_register\nparameter now defaults to false (previously was true.). This is to match the behavior for Q and UI pipelines where auto register default value is false.\n\n#### Query failures and Postgres Keycloak\n\nA resilient Postgresql and Keycloak implementation is planned for Insights Enterprise 1.17.0.\nFor Insights Enterprise 1.16.x, if the Postgresql pod is killed for whatever reason, it is strongly recommended to restart Keycloak as well.\nThis is to mitigate the possibility of Keycloak getting into a extended stalled state which could impact queries being serviced, and might result in errors in the logs. Typically, Keycloak will recover but this coordinated restart is the best way to protect against this potential failure.\nFurthermore, such extended outages can be mitigated by adding the following setting to the values file which reduces the\nkxi-ent-srv\nentitlement cache refresh rate from 10s to 60s, reducing the load on Keycloak:\nBash\nCopy\n\n```\nkxi-ent-srv:    cacheRefresh: 60\n```\n\n\n### Deprecations\n\nSome of the\nkxi package\ncommands have been deprecated and removed in favor of their\nkxi pm\nequivalents.\n- kxi package remote-confighas been deprecated; usekxi pm configinstead\n- kxi package remote-converthas been deprecated; usekxi pm convertinstead\n- kxi package remote-pushhas been deprecated; usekxi pm pushinstead\n- kxi package remote-pullhas been deprecated; usekxi pm pullinstead\n- kxi package remote-deployhas been deprecated; usekxi pm deployinstead\n- kxi package remote-teardownhas been deprecated; usekxi pm teardowninstead\n- kxi package installhas been removed\n- Changes tokxi package packit: KXI-64960: [pakx] Add --target for packit CLI command:--tag option is now deprecated and is the default behavior.--override-deps/--keep-unlocked/--all-deps options are now deprecated.\n\n### Third-party Dependencies\n\nkdb Insights Enterprise\n1.17\nstandalone install\nsupports the following versions of third-party dependencies:\n- cert-manager-1.18.2\n- nginx-ingress-4.11.5\n- rook-ceph-1.17.7\n- Kubernetes-1.33\nThese versions are used in the\n1.17\nrelease of the standalone infrastructure\ninstallation scripts\n.\nPlease consult these pages for important information on supported versions:\n- OpenShift\n- OpenSource K8s\n- EKS, AKS, and GCP\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.16.0.tgz | insights-1.16.0.tgz |\n| Operator | kxi-operator-1.16.0.tgz | kxi-operator-1.16.0.tgz |\n| CLI | kxicli-1.16.0-py3-none-any.whl | kxicli-1.16.0-py3-none-any.whl |\n| RT q interface | rt.1.16.0.qpk | rt.1.16.0.qpk |\n| RT C interface | kxi-c-sdk 1.16.0 | kxi-c-sdk 1.16.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.16.0 | kxi-csharp-sdk 1.16.0 |\n| Infrastructure | kxi-terraform-1.16.0.tgz | kxi-terraform-1.16.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-0.4.36.tgz | insights-on-k8s-0.4.36.tgz |\n\n\n### Summary\n\nWe hope you find some useful features that optimize your\nkdb Insights Enterprise\nexperience. Try them out and\nemail\nour Support Team if you need any help.\nWe look forward to bringing you even bigger features in\nkdb Insights Enterprise\n, coming soon!\n\n## 1.15.0\n\nRelease Date 2025-09-04\nkdb Insights Enterprise\n1.15 delivers greater interoperability, performance improvements, and enhancements to APIs and developer workflows. Read on to find out more!\n\n### New Features\n\n\n#### Natively work with Avro data feeds\n\nThe Stream Processor now supports the ingestion of data feeds in Avro message format using a new Avro encoder/decoder. This feature also includes support for Avro-encoded messages with the Kafka Schema Registry.\nThis functionality is available with:\n- The q and Python Stream Processor interfaces, seeEncodersandDecoders.\n- Pipelines built using the Web Interface, seeEncodersandDecoders.\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nDatabase\n- You can now load a UDA without restarting core components like Data Access Processes (DAPs) and Aggregators (AGGs). This feature provides users with a faster, improved workflow for testing and deploying changes. Refer toLoad a Package.\n- The performance of loading table counts have been improved through more efficient caching.\n- You can now configure the Service Gateway to connect to multiple Resource Coordinators, enhancing query resilience. To enable this, set theKXI_SG_RC_ADDRenvironment variable on the Service Gateway to a comma-separated list of Resource Coordinator addresses. For example,KXI_SG_RC_ADDR=sgrc:5050,sgrcalt:5050. SeeQuery Resilience.\n- Distinguished parameterscopenow takesdapas a key to allow a user to target a specificDAPin a query. Note this parameter is supported only ifscope.assemblyis specified, and it cannot be used withscope.tier. Refer toScopefor more details.\n- ThegetMetaAPI has been improved to give users better visibility of their system's processes. Thedapkey ofgetMetanow gives detail on individual DAPs when the new parameteradvancedis set totrue. Previously,getMetawould expose information only on tiers, but not individual processes. Refer toGet-meta.\nPackaging\nWhen deleting a package, the user can now choose whether to delete all data related to the package. This includes database contents, pipeline checkpoints, and user states.\n\n### Fixes\n\nThis release includes numerous bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Resolved an issue that prevented using peaching over arguments with .kxi.selectTable. For example:.kxi.selectTable peach 4#enlist`table`startTS`endTS`filter!(table;startTS;endTS;enlist(in;`sym;enl`nada))\n- .kxi.sqlqueries that useEXCEPT ALLhave been corrected to align with postgreSQL behavior, in which duplicates would be included in query results.\nPackaging\n- Resolved an issue where the Package Manager was unable to unpack a package that contained a user-defined function.\n- Packages now allow column names to end with_.\nWeb Interface\n- The Performance Tuning Wizard previously allowed users to attempt re-sorting data already on-disk - an unsupported operation that caused failures. To prevent this, the Performance Tuning Wizard will no longer be available once data has been ingested into the database.\nSecurity\n- Various CVEs were remediated as part of this release.\n\n### Third-party Dependencies\n\nkdb Insights Enterprise\n1.15.0\nstandalone install\nsupports the following versions of third-party dependencies:\n- cert-manager-1.17.2\n- nginx-ingress-4.11.5\n- rook-ceph-1.17.2\n- Kubernetes-1.32\nThese versions are used in the 1.15.0 release of the standalone infrastructure\ninstallation scripts\n.\nPlease consult these pages for important information on supported versions:\n- OpenShift\n- OpenSource K8s\n- EKS, AKS, and GCP\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.15.0.tgz | insights-1.15.0.tgz |\n| Operator | kxi-operator-1.15.0.tgz | kxi-operator-1.15.0.tgz |\n| CLI | kxicli-1.15.0-py3-none-any.whl | kxicli-1.15.0-py3-none-any.whl |\n| RT q interface | rt.1.14.0.qpk | rt.1.14.0.qpk |\n| RT C interface | kxi-c-sdk 1.14.0 | kxi-c-sdk 1.14.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.15.0 | kxi-java-sdk 1.15.0 |\n| RT C# interface | kxi-csharp-sdk 1.14.0 | kxi-csharp-sdk 1.14.0 |\n| Infrastructure | kxi-terraform-1.14.0.tgz | kxi-terraform-1.14.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-0.4.28.tgz | insights-on-k8s-0.4.28.tgz |\n\n\n## Releases Prior to 1.15\n\nFor release notes prior to Release 1.15, refer to the\nKX legacy documentation site.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3699,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-43afa31eddd5",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Help___Support/diagnostics.htm",
    "title": "Diagnostics",
    "text": "Declan Fallon START\n\n# Diagnostics\n\nThis page describes how to troubleshoot database, pipeline and system errors with Diagnostics, accessed through the\nkdb Insights Enterprise Web Interface\n.\nDiagnostics help you to monitor system health, diagnose issues, and maintaining optimal performance of kdb Insights Enterprise deployments.\n\n## Access Diagnostics\n\nThere are a few ways to access diagnostics information.\n- ClickDiagnostics, underMANAGE, in the left-hand-side bar to open theDiagnosticsscreen.ImportantPermissionsEnsure you have the appropriateKeycloak role permissions, otherwise the Diagnostics option is not displayed.This screen has a search box and 2 dropdowns. Use these toselect a database or pipelinethat you want to troubleshoot.Expand either theDatabasesorPipelinesdropdowns and select a database or pipeline. Alternatively, type the name in the search box. This opens the diagnostics for that entity.\n- You can access Database diagnostics from theOverviewtab of the Database screen. You can do one of the following:Click the button corresponding to the reported event (Fatal,Error,Warning)ClickView all Logs\n- You can access Pipeline diagnostics by clicking theView Diagnosticsicon for a selected Pipeline in theRunning Pipelinesof theOverview page.\nFrom the Diagnostics screen, you can now view\nApplication Logs\nand/or\nDiagnostic Events\nfor the selected Database or Pipeline.\n\n## Application Logs\n\nThe\nApplication Logs\ntab displays logs generated by a database or pipeline deployment. Results are paged and can be exported, refreshed, searched, and sorted.\n- The logs can be filtered by ticking one of the event options:FatalErrorWarnings\n- When you access these logs from the Database screen, by clicking the event icon, the logs are filtered based on the event selected.\n- Reset the filters by clicking theReset Column Filtersicon.\n- The time since last update is displayed next to the refresh icon for both events and logs.\n- Download a '.json' report of events or logs by clicking theDownload Tableicon.\n- Refine your results based on a value entered in theQuick Searchbox.\n\n## Diagnostic Events\n\nThe\nDiagnostic Events\ntab displays reported events generated by a database or pipeline deployment.\n- Results are paged.\n- Successful events are identified with a green tick.\n- Warnings are identified with a yellow tick.\n- The time since last update is displayed next to the refresh icon for both events and logs.\n- Click on a header column to sort and filter.\n- Download a '.json' report of events or logs by clicking theDownload Tableicon.\n- Refine your results based on a value entered in theQuick Searchbox.\nTip\nDiagnostic data is cached; click the refresh icon to update. Alternatively, close and re-open the current tab from the ribbon menu.\n\n## Batch Ingest\n\nThe\nBatch Ingest\ntab, is displayed when you select a database from the list under\nDiagnostics\n. This tab is updated when data has been directly written to the HDB. This is done when a pipeline contains a node that writes directly to the database, see\nkdb Insights Database writer node\nfor details.\nThe information displayed is described in the following table.\n\n| Column | Description |\n| --- | --- |\n| Pipeline | The name of the pipeline that is ingesting data into the database. |\n| Status | The current status of the ingest. The values are:QueuedPendingProcessedCompletedError |\n| Progress | This column indicates the progress of the data ingest. The options are:Preparing for HDBAwaiting for HDB% written to HDBIngest completeIngest error |\n| Destination | The name of the table in this database that the data has been ingested into. |\n\nInformation about data ingest is retained, in this tab, for 30 days. If the table has no data then a message is displayed saying there has been no batch ingest to the last 30 days.\n\n## System Notifications\n\nTo view system notifications:\n- Click theSystem Notificationsicon in theribbon menufor real-time notifications of events in the current session as shown below.\n- A slider panel of system notifications is displayed. You can filter results with a text search.\n- Click the new tab link for an expanded view of notifications.\n\n## Next steps\n\n- For further assistance contactProduct Support.\nDeclan Fallon END. Natalie Tanner edits May 2024",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 672,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-c4fad08744dc",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/get-data-kafka.htm",
    "title": "Stream Data from Kafka",
    "text": "\n# Stream Data from Kafka\n\nThis page provides a walkthrough on how to use\nkdb Insights Enterprise\nto stream data from Kafka to monitor NYC subway train punctuality for travel planning.\nApache Kafka\nis an event streaming platform that seamlessly integrates with\nkdb Insights Enterprise\n, enabling real-time data processing through pipelines connected to Kafka data sources.\nWe have provided a Kafka subway feed for use in this walkthrough, which generates live alerts for NYC Subway trains tracking arrival time, station location co-ordinates, direction, and route details.\nInformation\nBefore you build your pipeline, you must ensure the\ninsights-demo\ndatabase is created, as described in the\nCreate a Database\nwalkthrough.\nBefore you build your pipeline, you must ensure the\ninsights-demo\ndatabase is created, as described\nhere\n.\nThe following sections describes how to:\n- Create a pipeline. Create thesubwaypipeline and add it to theinsights-demopackage. This pipeline contains the following nodes:Reader: The reader stores details of data to import, including any required authentication.DecoderThis decodes Kafka event data, which is in JSON, to a kdb+ friendly format (a kdb+ dictionary).Transform: This applies a schema which converts data to a type compatible with a kdb Insights Database. Every imported data table requires a schema and every data table must have a timestamp key to be compatible with kdb's time series columnar database. Theinsights-demodatabase has a predefined schema for subway data.Writer: This writes transformed data to thekdb Insights Enterprisedatabase.Map: This node usesenlistto convert the decoded data to a kdb+ table prior to deployment.\n- Ingest the data. To run the pipeline you have just created to ingest data into the insights-demo database.\n\n## Create a pipeline\n\nTo setup the pipeline, containing the nodes described in the previous section:\n- On theOverviewpage, chooseImport DataunderDatabases:\n- In theImport your datascreen select theKafkareader.\n- In theConfigure Kafkascreen:Enter values for:SettingValueBrokerkafka.trykdb.kx.com:443TopicsubwayThe default values can be accepted for the following:SettingValueOffsetEndUse TLSUncheckedUse Schema RegistryUncheckedOpen theAdvanceddrop-down and checkAdvanced Broker Options.Click+underAdd an Advanced Configurationand enter the following key value-pairs:KeyValuesasl.usernamedemosasl.passworddemosasl.mechanismSCRAM-SHA-512security.protocolSASL_SSLClickNext.\n- In theSelect a decoderscreen clickJSON.\n- In theConfigure JSONscreen clickNext, leavingDecode eachunchecked.\n- In theConfigure Schemascreen:SettingValueData FormatAnyClick theLoad Schemaicon next toParse Strings.SettingValueDatabaseinsights-demoTablesubwayClickLoad.ClickNext.\n- In theConfigure Writerscreen:Selectinsights-demoas the database. This is the database you createdhere.Selectsubwayas the table.Keep the default values for the remaining fieldsSettingValueWrite Direct to HDBUncheckedDeduplicate StreamCheckedSet Timeout ValueUnchecked\n- ClickOpen Pipelineto open a view of the pipeline.SettingValuePipeline Namesubway-1Select a Packageinsights-demo\n- ClickCreate.\n- In the pipeline template:Click-and-drag aMapnode, from the list ofFunctions, into the workspace.Remove the connection betweenDecoderandTransformnodes by right-clicking the link and selectingDelete Edge.Connect theMapnode to theDecoderandTransformnodes.Click on theMapnode to edit its properties and set theenlist dataas shown below.qCopy{[data]enlist data}ClickApplyto apply these changes to the node.\n- ClickSave.\n\n## Deploy the pipeline\n\nDeploy the package containing the database and pipeline in order to ingest the data into the database.\n- Go to thePackagesindex and click on the three dots besideinsights-demopackage and clickDeploy.NoteIt may take Kubernetes several minutes to release the necessary resources to deploy the pipeline.If the package or its database are already deployed you must tear it down. Do this on thePackageindex by clicking on the three dots besideinsights-demopackage and clickTeardown.\n- Check the progress of the pipeline under theRunning Pipelinespanel on theOverviewtab. The data is ready to query whenStatus=Running.\nWarning\nPipeline warnings\nOnce the pipeline is running some warnings may be displayed in the\nRunning Pipelines\npanel of the\nOverview\ntab, these are expected and can be ignored.\n\n## Next steps\n\nNow that data has been ingested into the subway table you can:\n- Query the subway data\n- Visualize the subway datafrom the data\n- Ingest other data\n\n### Further reading\n\nUse the following links to learn more about specific topics mentioned in this page:\n- Import wizard\n- Pipelines Index\n- Building a pipeline\n- Pipeline Operators\n- Troubleshooting pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 622,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-13d55e45a7c5",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/Azure_Marketplace/previous-release-notes.htm",
    "title": "Previous",
    "text": "\n# kdb Insights Enterpriseon Azure Marketplace - Release Notes\n\nThis page outlines release notes from releases that were previously available on the Azure Marketplace. For the most recent release of kdb Insights Enterprise, refer to the\nlatest release\npage.\nThe deployment of kdb Insights Enterprise is no longer available on the Azure Marketplace. Refer to the\nDeployment options\nguide for further details.\n\n## 1.16.3\n\n\n### Release Date\n\n2024-07-31\nRefer to the main release notes of\nkdb Insights Enterprise\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.18.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.7\n- Kubernetes - 1.33\n- Istio 1.27.3\n\n## 1.14.2\n\n\n### Release Date\n\n2024-07-31\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.14.0\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.33\n- Istio 1.26.1\n\n## 1.14.1\n\n\n### Release Date\n\n2024-17-11\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.14.0\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.31\n- Istio 1.26.1\n\n## 1.14.0\n\n\n### Release Date\n\n2025-07-04\nRefer to the main release notes of\nversion 1.14.0\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.31\n- Istio 1.26.1\n\n## 1.13.5\n\n\n### Release Date\n\n2025-06-11\nRefer to the main release notes of\nversion 1.13.5\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.31\n- Istio 1.26.1\n\n## 1.13.2\n\n\n### Release Date\n\n2025-05-15\nRefer to the main release notes of\nversion 1.13.2\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.1\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.31\n- Istio 1.25.0\n\n## 1.12.6\n\n\n### Release Date\n\n2025-04-16\nRefer to the main release notes of\nversion 1.12.6\n.\n\n### New Features\n\n- The Azure Marketplace deployment now usesAzure Managed Disksfor Rook-Ceph.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.16.3\n- nginx-ingress - 4.11.3\n- rook-ceph - 1.16.5\n- Kubernetes - 1.31\n- Istio 1.24.1\n\n## 1.11.1\n\n\n### Release Date\n\n2024-11-21\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.11.1\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.14.5\n- nginx-ingress - 4.10.1\n- rook-ceph - 1.14.4\n- Kubernetes - 1.29\n- Istio 1.20.2\n\n## 1.11.0\n\n\n### Release Date\n\n2024-09-26\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.11.0\n.\n\n### New Features\n\n- The Azure Marketplace deployment now supports the selection of apre-existing Virtual Networklocated in another resource group.\n- You can configure the AKS cluster to use aproxyfor outbound internet access.\n- You can choose the password used to access the encryption key for theData at Rest Encryption (DARE)feature now available inkdb Insights Enterprise.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.14.5\n- nginx-ingress - 4.10.1\n- rook-ceph - 1.14.4\n- Kubernetes - 1.29\n- Istio 1.20.2\n\n## 1.10.1\n\n\n### Release Date\n\n2024-07-29\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.10.1\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.14.5\n- nginx-ingress - 4.10.1\n- rook-ceph - 1.14.4\n- Kubernetes - 1.29\n- Istio 1.20.2\n\n## 1.10.0\n\n\n### Release Date\n\n2024-06-28\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.10.0\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.14.5\n- nginx-ingress - 4.10.1\n- rook-ceph - 1.14.4\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.9.4\n\n\n### Release Date\n\n2024-06-20\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.9.4\n.\n\n### Known Issues\n\nExpand to see the list of known issues here\nDeployment is not possible in the following regions:\n- Australia Central 2\n- Germany West Central\n- Southeast Asia (Stage)\n- Australia South East\n- North Central US (Stage)\n- South Central US (Stage)\nAs a workaround, try to deploy into a different Azure Region.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.3\n- nginx-ingress - 4.9.0\n- rook-ceph - 1.13.1\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.9.3\n\n\n### Release Date\n\n2024-05-30\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.9.3\n.\n\n### Known Issues\n\nExpand to see the list of known issues here\nDeployment is not possible in the following regions:\n- Australia Central 2\n- Germany West Central\n- Southeast Asia (Stage)\n- Australia South East\n- North Central US (Stage)\n- South Central US (Stage)\nAs a workaround, try to deploy into a different Azure Region.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.3\n- nginx-ingress - 4.9.0\n- rook-ceph - 1.13.1\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.9.2\n\n\n### Release Date\n\n2024-05-15\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.9.2\n.\n\n### Known Issues\n\nExpand to see the list of known issues here\nDeployment is not possible in the following regions:\n- Australia Central 2\n- Germany West Central\n- Southeast Asia (Stage)\n- Australia South East\n- North Central US (Stage)\n- South Central US (Stage)\nAs a workaround, try to deploy into a different Azure Region.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.3\n- nginx-ingress - 4.9.0\n- rook-ceph - 1.13.1\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.9.1\n\n\n### Release Date\n\n2024-05-03\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.9.1\n.\n\n### New Features\n\n- Deployment allows theNetworkingModel to be Traditional or Overlay.\n\n### Known Issues\n\nExpand to see the list of known issues here\nThe deployment may fail at the final step due to a certificate related error message.\nThis issue is known to impact the following Azure Regions:\n- East US\n- UK South\nOther regions may experience similar issues.\nAs a workaround, try to deploy into a different Azure Region.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.3\n- nginx-ingress - 4.9.0\n- rook-ceph - 1.13.1\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.9.0\n\n\n### Release Date\n\n2024-04-03\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.9.0\n.\n\n### New Features\n\n- Improved deployment experienceAdditional tabs have been added to the Marketplace deployment and the relevant settings are now grouped together to improve the user experience.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.3\n- nginx-ingress - 4.9.0\n- rook-ceph - 1.13.1\n- Kubernetes - 1.27\n- Istio 1.20.2\n\n## 1.8.3\n\n\n### Release Date\n\n2024-02-12\nRefer to the main release notes of\nversion 1.8.2\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.2\n- nginx-ingress - 4.8.3\n- rook-ceph - 1.11.2\n- Kubernetes - 1.27.3\n- Istio 1.19.1\n\n## 1.8.2\n\n\n### Release Date\n\n2024-01-29\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.8.2\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.2\n- nginx-ingress - 4.8.3\n- rook-ceph - 1.11.2\n- Kubernetes - 1.27.3\n- Istio 1.19.1\n\n## 1.8.0\n\n\n### Release Date\n\n2023-12-19\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.8.0\n.\n\n### New Features\n\n- Data EncryptionData in transit withinkdb Insights Enterprisecan now be encrypted, providing enhanced security. This feature meets compliance and regulatory requirements and protects against cyber-security attacks. You can enable this as part of your deployment. For more details, refer toEncryption of data in transit.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.2\n- nginx-ingress - 4.8.3\n- rook-ceph - 1.11.2\n- Kubernetes - 1.27.3\n- Istio 1.19.1\n\n## 1.7.3\n\n\n### Release Date\n\n2023-11-13\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.7.3\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.13.2\n- nginx-ingress - 4.7.3\n- rook-ceph - 1.11.2\n- Kubernetes - 1.27.3\n\n## 1.7.2\n\n\n### Release Date\n\n2023-10-27\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.7.2\n.\n\n### New Features\n\n- kdb Insights Enterprisecan now be deployed with theData Entitlementsbeta feature turned on.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.12.2\n- nginx-ingress - 4.7.1\n- rook-ceph - 1.11.2\n- Kubernetes - 1.26.3\n\n## 1.7.1\n\n\n### Release Date\n\n2023-09-26\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.7.1\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.12.2\n- nginx-ingress - 4.7.1\n- rook-ceph - 1.11.2\n- Kubernetes - 1.26.3\n\n### Fixes\n\nRook-Ceph version\n1.11.7\nis replaced with version\n1.11.2\ndue to an issue collecting Ceph metrics. This caused the\nMonitoring Workbook\nto contain some empty charts on the\nCluster overview\nand\nRook-Ceph\ntabs.\n\n## 1.7.0\n\n\n### Release Date\n\n2023-09-04\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.7.0\n.\n\n### Known issues\n\nThere is an issue with\nrook-ceph - 1.11.7\ncollecting ceph metrics, causing the\nkdb Insights EnterpriseMonitoring Workbook\nto contain some empty charts on the\nCluster overview\nand\nRook-Ceph\ntabs. This has been\nresolved\nin version 1.7.1, please contact\nKX Support\nto upgrade.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\ntakes care of the third party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.12.2\n- nginx-ingress - 4.7.1\n- rook-ceph - 1.11.7\n- Kubernetes - 1.26.3\n\n## 1.6.2\n\n\n### Release Date\n\n2023-07-13\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.6.2\n.\n\n### New Features\n\n- By default,kdb Insights Enterpriseis now deployed with Private IP address access to the UI and API endpoints. Refer to :General Configurationfor details on the deployment optionsIP address configurationfor details on the network connection optionsWarningUsing the default ofPrivate IP addressrequires additional actions to allow users to connect to the UI or the REST endpoints.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\ntakes care of the third party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.12.1\n- nginx-ingress - 4.7.0\n- rook-ceph - 1.11.2\n- Kubernetes - 1.25.6\n\n## 1.5.3\n\n\n### Release Date\n\n2023-06-19\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.5.3\n.\n\n## 1.5.2\n\n\n### Release Date\n\n2023-06-08\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.5.2\n.\n\n## 1.5.0\n\n\n### Release Date\n\n2023-05-19\nPlease see the main release notes of\nkdb Insights Enterpriseversion 1.5.0\n.\n\n### New Features\n\n[NEW] Improved stability\n- In this new marketplace version, thekdb Insights Enterprisecore system components are hosted in the System Node Pool along with the Azure services. This means the User Node Pool can be sized specifically for your database and data workflow needs.\n- Your System Node Pool can remain stable while your User Node Pool can be dynamic.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\ntakes care of the third party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.11.1\n- nginx-ingress - 4.6.0\n- rook-ceph - 1.11.2\n- Kubernetes - 1.26.3\n\n## 1.4.0\n\n\n### Release Date\n\n2023-03-01\nkdb Insights Enterpriseversion 1.4.0\n\n### New Features\n\nkdb Insights Enterprise\nis generally available on the Azure Marketplace as a KX Managed App plan.\n[NEW] Easy deployment\n- You can now use the Azure Marketplace configuration wizard to get up and running quickly and easily withkdb Insights Enterprise.\n[NEW] Try before you buy\n- You can have your first month free of any KX license fees. Simply click through the marketplace to deploy the Product and enjoy!\n[NEW] v1.4.0 Features\n- Check out thenew featuresin the 1.4.0 release.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\ntakes care of the third party prerequisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment;\n- cert-manager - 1.11.0\n- nginx-ingress - 4.5.2\n- rook-ceph - 1.10.11\n- Kubernetes - 1.25.5",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2571,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-b52a9a14734a",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Deployment/deployment-options.htm",
    "title": "Deployment Options",
    "text": "\n# Deploykdb Insights Enterprise\n\nThis page outlines the available deployment options for\nkdb Insights Enterprise\n, including cloud-based, on-premise, and fully managed solutions.\nYou can  deploy\nkdb Insights Enterprise\nusing several options â whether in the cloud or on-premise. Choose the deployment path that best fits your infrastructure and operational model.\n\n## Deployment Options Overview\n\nThere are two primary deployment models:\n- Cloud Kubernetes clusterrunning on ACS (Azure Cloud Services), AWS (Amazon Web Services) or GCP (Google Cloud Platform): Self-managed infrastructure and installation using Terraform and Helm\n- On-Premise: For environments hosted in your own data center or local servers\nEach model supports flexible scaling and administration.\n\n## Free trial\n\nGet up and running in minutes with a 7-day free trial of\nkdb Insights Enterprise\n.\nMore about the free trial\n\n## Deploy to ACS, AWS or GCP\n\nUse  Terraform scripts to\ndeploykdb Insights Enterprise\nto your own Kubernetes cluster hosted on ACS (Azure Cloud Services), AWS (Amazon Web Services), or GCP (Google Cloud Platform). You can manage the system yourself, or opt for our\nmanaged service\n.\nThis deployment model involves two separate stages:\n- Infrastructure Deployment (Terraform)Use Terraform scripts to provision a secure and production-ready Kubernetes cluster. This includes setting up:Node poolsNetworking and VPCsSecurity rules and namespacesThese scripts do not installkdb Insights Enterpriseâthey simply create the infrastructure it will run on.\n- Application Installation (kdb Insights Enterprise)Once the infrastructure is in place, follow thekdb Insights Enterprise installation procedureto deploy the application on top of your newly created cluster.\nYou can self-manage the infrastructure and application, or engage KX for assistance with setup, validation, or ongoing support.\n\n## Deploy On-premise\n\nkdb Insights Enterprise\ncan also be deployed within your private infrastructure, such as a data center, private cloud, or secure internal network.\nThis deployment model involves two separate stages:\n- Infrastructure Deployment (Kubernetes Cluster)You must first provision and configure a Kubernetes cluster within your on-premise environment. This can be done using one of the supported container platforms::OpenShift Container Platform (OCP)â refer toOpenShift infrastructure prerequisitesKubernetes Container Platformâ refer toKubernetes Infrastructure Prerequisites\n- Application Installation (Insights)Once your Kubernetes infrastructure is in place, follow theinstallation procedureto deploykdb Insights Enterpriseon top of your newly created cluster.\nYou are responsible for provisioning and managing both the infrastructure and application layers in this model.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 379,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-f1f3d6068588",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/KXI/release-notes.htm",
    "title": "Latest",
    "text": "\n# kdb Insights Enterprise- Release Notes\n\nThis page details the release notes for the latest\nkdb Insights Enterprise\nrelease. To find the release notes for an older release, refer to the\nprevious releases\npage.\n\n## 1.17.5\n\nRelease date: 2026-02-11\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Resolved an issue where partitions migrated to object storage could be lost if a batch ingest was run before the next end-of-day (EOD) process.\n- Storage Manager now performs overwrite cleanup more robustly on NFS-style filesystems. Previously, there were instances where batch ingestion on NFS-style storage could fail when overwriting an existing HDB partition with a \"Directory not empty\" error. This was due to hidden .nfs* files present.\nSecurity\n- Various CVEs were remediated as part of this release.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.5.tgz | insights-1.17.5.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.3-py3-none-any.whl | kxicli-1.17.3-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.1.tgz | kxi-terraform-1.17.1.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.21.tgz | insights-on-k8s-1.1.21.tgz |\n\n\n## 1.17.4\n\nRelease date: 2026-02-02\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nReliable Transport\nBLUE SERVICES\n- Resolved an issue where RT did not truncate its logs even after the conditions to trigger the operation were met\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.4.tgz | insights-1.17.4.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.3-py3-none-any.whl | kxicli-1.17.3-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.1.tgz | kxi-terraform-1.17.1.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n## 1.17.3\n\nRelease date: 2026-01-20\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nScratchpad\n- The scratchpad now has a variable.com_kx_edi.dbQueryTimeout, which caches the value of the UI timeout widget in milliseconds for the current scratchpad request. This can be used when doing HTTP or IPC calls from the scratchpad, such as database queries, to keep the requestâs timeout in sync with the scratchpadâs timeout.Copyresult: gateway (`.myUDAs.getData; args; `; enlist[`timeout]!enlist .com_kx_edi.dbQueryTimeout)\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nQueries\nBLUE SERVICES\n- Fixed an issue where the timestamp filter of.kxi.selectTablewould ignore the filter. This occurred when specifying timestaps that were out-of-purview for the target DB. Now an out-of-purview filter will just return in-purview DAP data that matches filter constraint.\n- Resolved an issue introduced in 1.17 where first line comments of.kxi.qsqlwould cause a query to error.\nSecurity\n- Various CVEs have been remediated as part of this releas.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.3.tgz | insights-1.17.3.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.2-py3-none-any.whl | kxicli-1.17.2-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.1.tgz | kxi-terraform-1.17.1.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n## 1.17.2\n\nRelease date: 2026-01-12\n\n### Fixes\n\nThis release includes bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nReliable Transport\nBLUE SERVICES\n- Fixed an issue where an RT log file was mistakenly archived prematurely.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.2.tgz | insights-1.17.2.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.1-py3-none-any.whl | kxicli-1.17.1-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.0.tgz | kxi-terraform-1.17.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n## 1.17.1\n\nRelease date: 2026-01-06\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nTerraform Scripts\n- An optional toggleTF_VAR_enable_pre_bootstrap_user_datahas been added to enable or disable custom pre-bootstrap user data for EKS-managed node groups. The default isdisabledto avoid impacting existing clusters.\n\n### Fixes\n\nThis release includes numerous bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nPackaging\nBLUE SERVICES\n- Resolved the inability to run.kxi.packages.*commands in the scratchpad.\nTerraform Scripts\n- Fixed an issue with openVPN that caused connections to a cluster to fail.\nStream Processor\n- Resolved a datatype mismatch for the timeout parameter that occurred when the database writer passed parameters to a stream writer upon initialization. This issue prevented Reliable Transport from archiving RT log files.\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.1.tgz | insights-1.17.1.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.1-py3-none-any.whl | kxicli-1.17.1-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.0.tgz | kxi-terraform-1.17.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n## 1.17\n\nRelease Date 2025-12-12\nkdb Insights Enterprise\n1.17 delivers greater resiliency for critical services, more storage class options, and a wide variety of improvements across the system. Read on to find out more!\n\n### New Features\n\n- High Availability of Authentication Management\n- Preview Option for the Query Window\n\n#### High Availability of Authentication Management\n\nThe underlying systems  that manage authentication in kdb Insights Enterprise (Keycloak and its PostgreSQL database) have been upgraded to a version that supports high-availability configuration. This delivers greater resiliency for this critical service, and is now the default configuration. This upgrade also makes upgrade and rollback of these components much more seamless.\nSee\nKeycloak and PostgreSQL configuration\nfor more details.\nNote\n- Access to the Keycloak administration console has been updated tohttps://{INSIGHTS_HOSTNAME}/admin.\n- The path used to get tokens for REST queries has changed from/auth/realms/insights/protocol/openid-connect/tokento/realms/insights/protocol/openid-connect/token.\nImportant\nWhen rolling back to a version prior to 1.17, the rollback process reuses the existing PersistentVolumeClaim (PVC) from the previous PostgreSQL installation to restore the original database state.\nChanges made to the database with the upgraded system will be lost.\n\n#### Preview Option for the Query Window\n\nThe database preview API is now exposed within the Query window, giving users the ability to view a configurable subset of records based on a date range. This offers a fast and simple method to evaluate a table, view/check data, and identify the columns available.\nRefer to\nquery preview\nfor details of the API and to\nquery window\nfor an overview of the Query Window.\n\n### Improvements\n\nThis release includes a variety of improvements across the entirety of\nkdb Insights Enterprise\n.\nExpand to see the details of the full set of improvements.\nTerraform\n- Client terraform scripts now support using Lustre-based storage on Azure, AWS and GCP. If selected at install time, the Lustre csi driver will be deployed and a Lustre storage class created. See the Configuration section forGCP,AWS, orAzure.\nStream Processor\n- The Web Interface now supports setting the Kafka Consumer Group ID for Kafka Reader nodes. See thegroupparameter for theKafka readerfor more details.\n- autoRegistersupport has been added for Avro schemas using the Kafka writer. For details, refer to theautoRegisterparameter of theKafka writer.\nDatabase\n- When using multiplelabelsin a query, you can now set the environment variableKXI_ALLOW_PARTIAL_LABELSto allow partial results to be returned in the event that the relevant data for each label does not fully intersect. This will return the data for the label combinations that do exist, and will also provide information on which combinations could not return data. See theKXI_ALLOW_PARTIAL_LABELSparameter for details.\n- Added the ability to use date as thegroupBycolumn ingetDataqueries. See thegroupByparameter for getDatafor more details.\n- A subset of a schema's tables can now be imported viaInitial Import. Any tables that did not yet have data will be created empty at startup, following the database validation operation, allowing for future data ingestion.\n- When defining a User-Defined Analytic, thescopefield can now be set as mandatory.\n- Added a new helper function,kxi.appendLabels, for User-Defined Analytics creation, which takes table data and adds the current DAPs assembly labels to the table.\n- The database now supports changing a column from scalar to compound.\n- The Storage Manager (SM) now checks the types of the nested columns before ingesting the data, which prevents database (DB) corruption if, for example, some incoming data has symbols in a column configured as strings.\nPackage Manager\n- ThePackage Manager referencecan now be configured to be highly available and support multiple replicas, ensuring that the Web Interface and the CLI are always able to administer packages and provide package details.\n- Anykxi pmcommands that accept a VERSION filter now also accept a requirement specifier on top of an exact version string. See the following examples:bashCopykxi pm rm my-pkg <2.0.0kxi pm rm my-pkg !=1.0.0kxi pm pull my-pkg ~=1.0.0kxi pm pull my-pkg >0.0.1\nScratchpad\n- The default for pre-provisioned scratchpad instances has been changed from 3 to 1 to preserve resources.\nRT Java SDK\n- The ability to pass arguments to a DataAccessClient's ping method has been added, allowing you to ping a particular assembly. Refer to thepingdocumentation for details.\n\n### Fixes\n\nThis release includes numerous bug, stability and security fixes.\nExpand to see the details of the full set of improvements.\nDatabase\nBLUE SERVICES\n- Fixed an issue in the Service Gateway where subscription requests sent to the client might not receive a SUBNAP response.\n- There is now protection against accidentally assigning globals in.kxi.qsql. A side-effect of this is that queries such that make use oftableName.columntype notation will no longer work. These can be fixed by changing totableName`column. For example,min trade.priceis fixed by switching tomin trade`price.\n- Fixed an issue where streaming data sources in Views could fail with a \"Stream closed\" error on redeploy of a pipeline.\n- Resolved an issue with theKXI_SM_EOI_REQUESTS_PENDINGmetric, which was incorrectly displaying negative values after emergency EOIs, even though there was no actual processing problem.\n- It is no longer possible to batch ingest data where the date ofprtnColdiffered from the defined partition date. If this is attempted, the batch ingest will be rejected entirely.\n- When a schema is changed, the old schema will be honoured for any data added to RT logs before the database restart and schema conversion. Publishers must not begin publishing in the new schema before the database is restarted to perform the schema change; otherwise, a schema mismatch error will occur.\n- Improved the error handling of database migration to object storage. Previously, in some cases, if the upload failed, data loss was possible.\n- Batch ingest now explicitly rejects incoming data for the time range overlapping with object storage tier. Previously, the Storage Manager discarded such data.\n- Added a validation rejecting non-partitioned data provided to batch ingest in a date partition. Previously, such data was erroneously written to HDB partition.\n- When EOD writedown discards data older than the configured retention period, SM now prints a warning message.\n- Fixed an issue where, withKXI_LATE_DATAdisabled, DAP purviews could be incorrect after SM restart, causing query timeouts.\nStream Processor\n- Upgraded librdkafka version from 1.9.2 to 2.11.1.\nSecurity\n- Security patches have been applied and CVEs have been remediated.\nWeb Interface\n- Fixed an issue in Views where the value for thescopeparameter for a UDA does not always get saved and reused.\n\n### Important Upgrade and Deployment Considerations\n\n\n#### Upgrades\n\n- Following a Keycloak upgrade, usernames must now be at least 3 characters long. Existing usernames with fewer than 3 characters will remain unchanged.\n- When interacting with the following endpoints using curl:/clientcontroller/leave/clientcontroller/enrol/informationservice/details/{id}The request must include the header--header \"Content-Type: application/json\". Requests that do not include this header will fail or be rejected by the service.\n- If your upgrade includes a change to enable or disable encryption, Keycloak may experience issues that manifest as problems in Insights CLI's operation. The potential issues and their suggested resolution are as follows:At Insights CLI Browser Auth Challenge, the challenge URI takes a prolonged time to load in your browser. To resolve, wait 3 minutes then try loading again.At Insights CLI Browser Auth Challenge, the CLI fails with400 Bad Request. To resolve, run the CLI upgrade operation again.While performing the upgrade, Insights CLI timeouts waiting for Keycloak replicas to becomeReadyand aborts. To resolve, kill the 3 active Keycloak pods. Kubernetes will start new pods which will resolve the problem.\n- The KXI CLI depends on Helm to install/upgrade and reconfigure kdb Insights Enterprise on your Kubernetes cluster. Please make sure that your Helm version is the latest 3.x version, but below version 4.x.\n- If you perform a rollback from version 1.17 to version 1.16.x, thePreview query optionwill still appear in the UI. However, this feature is not supported in version 1.16.x, and attempting to use it will result in the following error:Error: path not found. You can safely ignore the option, or perform aShift‑Reloadto clear the cached UI, which will remove the feature from view.\n\n#### Default Configurations\n\nThe default number of Resource Coordinators (RC) deployed has been changed to 1. This is to align to the default setting of 1 for Data Access Processes (DAPs). In general, there needs to be as many (or more) DAPs as there are RCs. See\nOrdinal Connection\nfor details.\n\n#### Scratchpads\n\nIf either the\nkxi install delete\ncommand or running an upgrade hangs, this may be caused by a hanging scratchpad job which will require manual intervention. This is due to the scratchpad managerâs cleanup job trying to download a bitnami kubectl image that no longer exists. This can be resolved via a manual deletion of the job:\nbash\nCopy\n\n```\n$ kubectl delete job insights-kxi-scratchpad-manager-cleanup-hook -n $MY_NAMESPACE\n```\n\nFollowing this, clean up\nboth\nthe scratchpads and services manually, which can be done from k9s, or the terminal as follows:\nbash\nCopy\n\n```\n$ kubectl get pods -n $MY_NAMESPACENAME                                                              READY   STATUS    RESTARTS   AGEinsights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd-544r59xc   2/2     Running   0          6m38s$ kc delete pod insights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd-544r59xc -n $MY_NAMESPACEpod \"insights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd-544r59xc\" deleted$ kc get svc -n $MY_NAMESPACENAME                                                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGEinsights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd      ClusterIP   10.218.30.120   <none>        80/TCP,8181/TCP   27m/mnt/c/Users/bjeffery [aws-cyan:amber] $ kc delete svc insights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd -n $MY_NAMESPACEservice \"insights-scratch-benj-2d7ebc17a998f9deb79ead9511edbbdd\" deleted\n```\n\nIf you're installing an unpatched version, you'll need to update your values file to correct the image names and disable scratchpad quick startup:\nbash\nCopy\n\n```\nkxi-scratchpad-manager:  scratchpadQuickStartup:    enabled: false  cleanup:    enabled: true    image:      repository: \"bitnamilegacy\"      component: \"kubectl\"      tag: \"1.33.3\"  initDBJob:    image:      repository: \"docker.io/bitnamilegacy\"      component: \"postgresql\"      tag: \"15.0.0-debian-11-r1\"      useLocal: true\n```\n\n\n#### Database\n\nIt is recommended to have an inventory file in place when one of your database tiers resides on object storage to help speed up reload times. Previously, the file information was configured by setting the\nKX_OBJSTR_INVENTORY_FILE\nenvironment variable. This is no longer supported, as both SM and DAPs now handle this by themselves. Refer to\nObject storage inventory files\nfor more details.\nIf you have an inventory file in place, it will be necessary to change the configuration of its location reference. In your package file, you will now need to specify the location of the inventory file using the\nlocation\nparameter of the\nsm.tiers[].inventory\nproperty.\nThis sample configuration illustrates an S3 tier defined in a package file:\nbash\nCopy\n\n```\n- name: s3    mount: hb    store: s3://kxi-sm-example/db    inventory:     enabled: true     location: [file location here]\n```\n\nNote\nThe DAP will no longer start if the environment variable is set directly.\n\n### Third-party Dependencies\n\nkdb Insights Enterprise\n1.17\nstandalone install\nsupports the following versions of third-party dependencies:\n- cert-manager-1.19.1\n- nginx-ingress-4.14.0\n- rook-ceph-1.18.7\n- Kubernetes-1.33\nThese versions are used in the  1.17 release of the standalone infrastructure\ninstallation scripts\n.\nPlease consult these pages for important information on supported versions:\n- OpenShift\n- OpenSource K8s\n- EKS, AKS, and GCP\n\n### Artifacts\n\n\n| Type | Nexus location | Downloads Portal location |\n| --- | --- | --- |\n| Enterprise | insights-1.17.0.tgz | insights-1.17.0.tgz |\n| Operator | kxi-operator-1.17.0.tgz | kxi-operator-1.17.0.tgz |\n| CLI | kxicli-1.17.0-py3-none-any.whl | kxicli-1.17.0-py3-none-any.whl |\n| RT q interface | rt.1.17.0.qpk | rt.1.17.0.qpk |\n| RT C interface | kxi-c-sdk 1.17.0 | kxi-c-sdk 1.17.0 |\n| RT Python interface | kxi-rtpy-1.14.0 | kxi-rtpy-1.14.0 |\n| RT Java interface | kxi-java-sdk 1.16.0 | kxi-java-sdk 1.16.0 |\n| RT C# interface | kxi-csharp-sdk 1.17.0 | kxi-csharp-sdk 1.17.0 |\n| Infrastructure | kxi-terraform-1.17.0.tgz | kxi-terraform-1.17.0.tgz |\n| kxi-management-service | kxi-management-service-1.0.0.tgz | kxi-management-service-1.0.0.tgz |\n| insights-on-k8s | insights-on-k8s-1.1.19.tgz | insights-on-k8s-1.1.19.tgz |\n\n\n### Summary\n\nWe hope you find some useful features that optimize your\nkdb Insights Enterprise\nexperience. Try them out and\nemail\nour Support Team if you need any help.\nWe look forward to bringing you even bigger features in\nkdb Insights Enterprise\n, coming soon!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3061,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-6c0077987083",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Get_Started/about-insights-enterprise.htm",
    "title": "About kdb Insights Enterprise",
    "text": "\n# Aboutkdb Insights Enterprise\n\nThis page briefly describes\nkdb Insights Enterprise\n, its key features and available deployment options.\nkdb Insights Enterprise\ncaptures and analyzes your high-volume, high-velocity, machine-generated, time-series data. Built on top of\nkdb+\n, our column-based, relational, time-series database,\nkdb Insights Enterprise\nprovides a full range of pre-configured services from the\nInsights SDK\nproduct. Refer to the\nreference architecture\nto get you started, or customize the system to meet your organizationâs needs.\n\n## Key features\n\n\n### Data discovery and exploration\n\n- Ingest data from multiple sources, including historical and real-time, process it, and use the generated insights to inform decision-making processes in your business.\n- Interact with the product through theweb interface,command line interface, orVScode plugin.\n- Import, transform, andquery datausingq, Python or SQL.\n- Creategraphs and chartsto identify patterns and trends, and to present results.\n\n### Scalability\n\nInsights Enterprise is built on top of Kubernetes, allowing the underlying cloud platform to handle scaling and orchestration. The system scales to meet your requirements.\n\n### Security and access\n\nRestrict access to Insights Enterprise with robust authentication, using user identity\nauthentication and authorization\n, where permission to use system features is defined in user accounts.\n\n### Available and fault tolerant data\n\n- Designed with process level redundancy and node assignment in mind.\n- Retain multiple copies of data, ensuring that no data point is duplicated or lost.\n- Apply multiple process redundancy for both streaming analytics and query processing, enabling individual process, node, and availability zone failures without impacting primary data functionality.\n- Deploy across multiple availability zones to handle single AZ failures.\n\n### Diagnostics\n\nProvides a self-help\ndiagnostics\nfacility in the\nWeb Interface\nto view logs of historical and recent events.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 283,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-c5b0b9405fbd",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/build-a-database.htm",
    "title": "Create a Database",
    "text": "\n# Create a Database\n\nThis page guides you through the steps to create a database and add it to a package, using the\nkdb Insights Enterprise\nWeb Interface.\nThe\nCreate a database\nsection guides you through building the\ninsights-demo\ndatabase and adding it to the\ninsights-demo\npackage.\nDetails on how to set up pipelines and ingest data into this database are provided in the other\nwalkthroughs\n.\n\n## Create a database\n\nThis section explains how to create a new database, add it to a package, and set up the schema for the new database in the Insights Enterprise Web Interface.\n- On theOverviewpage, chooseCreate newunderDatabasesin theQuick Actionssection.\n- In theCreate Databasedialog set the following values:SettingValueDatabase Nameinsights-demoSelect a PackageCreate new packagePackage Nameinsights-demo\n- ClickCreate.\n- On theSchema Settingstab clickCode Viewto open theSchema Code View. You can use this to add large schema tables, in JSON format.\n- Replace the existing code with the following JSON.insights-demo schemaThis JSON code is used in all the walkthrough examples that use the insights-demo database.JSONCopy[{\"name\":\"weather\",\"type\":\"partitioned\",\"primaryKeys\": [],\"prtnCol\":\"timestamp\",\"sortColsDisk\": [\"airtemp\"],\"sortColsMem\": [],\"sortColsOrd\": [\"sensor\"],\"columns\": [{\"type\":\"timestamp\",\"name\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"sensor\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"sorted\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"airtemp\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"sorted\",\"compound\":false},{\"name\":\"name\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"borough\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"longitude\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"latitude\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"color\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false}]},{\"columns\": [{\"type\":\"symbol\",\"name\":\"trip_id\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"arrival_time\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"sorted\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"stop_id\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"sorted\",\"compound\":false},{\"name\":\"stop_sequence\",\"type\":\"long\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"stop_name\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"stop_lat\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"stop_lon\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_id\",\"type\":\"long\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"trip_headsign\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"direction_id\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_short_name\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_long_name\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_desc\",\"type\":\"string\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_type\",\"type\":\"long\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_url\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"route_color\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false}],\"primaryKeys\": [],\"type\":\"partitioned\",\"prtnCol\":\"arrival_time\",\"name\":\"subway\",\"sortColsDisk\": [\"stop_id\"],\"sortColsMem\": [],\"sortColsOrd\": [\"arrival_time\"]},{\"columns\": [{\"type\":\"string\",\"name\":\"name\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"lat\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"sorted\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"long\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"sorted\",\"compound\":false},{\"name\":\"neighborhood\",\"type\":\"string\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"airquality\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"trafficcongestion\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"bikeacccnt\",\"type\":\"int\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"caracccnt\",\"type\":\"int\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false}],\"primaryKeys\": [],\"type\":\"partitioned\",\"prtnCol\":\"timestamp\",\"name\":\"health\",\"sortColsDisk\": [\"long\"],\"sortColsMem\": [],\"sortColsOrd\": [\"lat\"]},{\"columns\": [{\"type\":\"symbol\",\"name\":\"event_id\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"incident_time\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"sorted\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"nypd_precinct\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"sorted\",\"compound\":false},{\"name\":\"borough\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"patrol_borough\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"call_x_geo\",\"type\":\"long\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"call_y_geo\",\"type\":\"long\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"radio_code\",\"type\":\"symbol\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"description\",\"type\":\"string\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"crime_in_progress\",\"type\":\"string\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"call_timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"dispatch_timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"arrival_timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"closing_timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"latitude\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false},{\"name\":\"longitude\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false}],\"primaryKeys\": [],\"type\":\"partitioned\",\"prtnCol\":\"arrival_timestamp\",\"name\":\"crime\",\"sortColsDisk\": [\"nypd_precinct\"],\"sortColsMem\": [],\"sortColsOrd\": [\"incident_time\"]}]\n- ClickApplyto setup the database schema.\n- ClickSave.\n- Navigate to thePackageindex to see the package containing the database.\n- Next, create pipelines to ingest data into this database:Ingest object storage data from one of the major cloud providersIngest from Apache KafkaIngest Protocol Buffers data\n\n### Further reading\n\nUse the following links to learn more about specific topics mentioned in this page:\n- Creating and configuring databases in the web interface\n- Schema configuration in the web interface\n- Deploying a database in the web interface\n- Working with packages",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 279,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-19869d5b4683",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Deployment/Standalone/kxi-install.htm",
    "title": "Installation",
    "text": "\n# kdb Insights EnterpriseInstallation\n\nThis page details the steps to deploy\nkdb Insights Enterprise\nin a pre-configured Kubernetes cluster. Once installed, you can build and deploy your workloads.\nIf you want to upgrade your\nkdb Insights Enterprise\ndeployment, refer to the\nupgrade guide\n.\n\n## Prerequisites\n\n- Cluster and shared servicesBefore installingkdb Insights Enterprise, you must have an appropriate Kubernetes cluster in place with the necessary shared services available.kdb Insights Enterpriseprovides a number ofTerraform scriptsthat enable the quick deployment of the necessary cluster services on all three main cloud providers.AWSGCPAzureOpenShiftOSS K8sTo install into an existing cluster, ensure that all necessary infrastructureprerequisiteshave been met.\n- Kubernetes accessYou must have access to the Kubernetes cluster along with a number of third-party tools to interact with the cluster, including anactive kubeconfig with a namespace.\n- HelmHelmis used to deploy and managekdb Insights Enterprise.\n- DNSIn order for external clients to interact withkdb Insights Enterprise, aDNS record must be setup\nwith your DNS provider.\n- kdb Insights EnterpriselicenseThe install process requires akdb Insights Enterpriselicense. To obtain a valid license, speak to your KX sales representative.NoteThe self-service licensing model previously in use for KX products has been deprecated. Contact your sales representative if you have questions about this, and wish to discuss moving to the new model.\n- Default registriesYou must have credentials to the appropriate Helm and Docker registries for charts and images. The install prompts you for repository locations and access details. Default registries:Helm:https://portal.dl.kx.com/assets/helm/Docker:portal.dl.kx.com\n\n## Validation\n\nTo validate that the cluster is able to run\nkdb Insights Enterprise\n, use the helm chart provided, that can be deployed to your cluster. Read the\nvalidation documentation\n.\n\n## Installation overview\n\nWhen you deploy\nkdb Insights Enterprise\n, the install command does the following:\n- Generates avalues.yamlconfig file to use for repeatable future installs\n- Installs the kdb Insights Operator to thekxi-operatornamespace\n- Deploys the application to the cluster using the generatedvalues.yaml\n\n## Configure the CLI\n\nThe\nKXI Command Line Interface (CLI)\ntool provides an interactive installer that deploys\nkdb Insights Enterprise\non a Kubernetes cluster.\nThe following steps configure the CLI ready to deploy the application to the cluster:\n- Install the CLIon the machine you will use to access the cluster and run the install from.\n- Ensure you have the following variables availablevariableexamplefurther detailsINSIGHTS_HOSTNAMEkxi-insights.domain.comDNS Hostname setupKX_LIC/home/app/q/kx.licPath to KX LicenseINSIGHTS_VERSION1.17.0\n- Configure the CLI by calling the following:bashCopykxi configureRunning this command sets theCLI supported settingslike the hostname and the namespace, and  creates a~/.insights/cli-configconfiguration file which stores default values for frequently used options.\n- Before running the install, run thekxi install setupcommand, as follows:bashCopykxi install setup --version $INSIGHTS_VERSION --hostname $INSIGHTS_HOSTNAME --license-filepath $KX_LICThis command guides you through the configuration options including the following:For the Helm and Docker credentials.To createKeycloakadmin passwords, which you should save for future use. These passwords enable you to log into the Keycloak administration console.Encryption of data in transitis enabled by default for new installations. When you upgrade kdb Insights Enterprise, the CLI provides the option to enable or disable encryption in transit.To setkdb Insights Enterprise-specific configuration options:EntitlementsQuery Environment (QE)Thekxi install setupcommand generates avalues.yamlconfig file to use for repeatable future installs.\nFor information on further configuration and customizations available refer to the\nConfiguration guide\n.\nInformation\nTo change any of these settings after installation, perform an\nupgrade\n.\nYou can, alternatively, supply all setup and installation options using\ncommand line arguments\nor\nCLI configuration\n. Refer to the\noptions precedence\nfor a description of the order in which these settings are read.\n\n### Deploy on Kubernetes\n\nYou can deploy\nkdb Insights Enterprise\nto the default namespace or a uniquely created namespace.\nHowever, two deployments cannot share the same namespace.\n- Before you deploy, check the active namespace to ensure there are no current deployments. The following command returns any instances of the application currently deployed to the active namespace:bashCopyhelm ls\n- Deploykdb Insights Enterpriseby running the following command:bashCopykxi install run --version $INSIGHTS_VERSION --hostname $INSIGHTS_HOSTNAME --license-filepath $KX_LIC\nThe CLI attempts to install the following additional components along with the application, if they are not present:\nThe kdb Insights Operator is deployed within the\nkxi-operator\nnamespace as a single deployment shared across all\nkdb Insights Enterprise\ndeployments on the cluster.\nAn Operator version is only compatible with corresponding minor versions of\nkdb Insights Enterprise\n. For example, version 1.17.X of the Operator supports 1.17.X of the application.\nThe CLI attempts to lookup the latest compatible Operator version for the target application version.\n\n#### Deploy behind a proxy server\n\nkdb Insights Enterprise\nsupports deployment behind a proxy server if you use Kubernetes on the Azure platform. Refer to Microsoft's\nHTTP proxy support in Azure Kubernetes Service (AKS)\nfor important details about running a proxy server through the Azure platform.\nAfter\nkdb Insights Enterprise\ncreates a\nvalues.yaml\nconfig file during installation, specify the following settings in the file:\nbash\nCopy\n\n```\nglobal:  ## @section proxy Configure Proxy Env details  ## @param httpProxy Set the proxy details for 'http_proxy' and 'HTTP_PROXY'  ## @param httpsProxy Set the proxy details for 'https_proxy' and 'HTTPS_PROXY'  ## @param noProxy Set the proxy details for 'no_proxy' and 'NO_PROXY'  # proxy: {}\n```\n\nAppend the following example, of a valid proxy configuration entry, to your existing\nvalues.yaml\nfile and enter the appropriate settings.\nbash\nCopy\n\n```\nglobal:  proxy:    httpProxy: http://1.1.1.1:3128/    httpsProxy: http://2.2.2.2:3128/    noProxy: *.insights.svc\n```\n\nImportant\nImportant considerations for using a proxy with AKS\nAs a result of how AKS handle proxy servers, you must be aware of the following considerations:\n- When your AKS cluster is already setup with proxy settings, yourkdb Insights Enterprisevalues.yamlfile must contain both the proxy settings from AKS and the application-specific values, for example, the ingress host ofkdb Insights Enterprise.\n- If any changes occur to the AKS cluster proxy values, you must update thevalues.yamlfile for yourkdb Insights Enterprisedeployment to match. Then, after the AKS cluster settings have been applied, run an upgrade for yourkdb Insights Enterprisedeployment to use the new proxy settings.\n- If you upgrade yourkdb Insights Enterprisedeployment to a newer version, or if the ingress host name is changed, then you must ensure your proxy settings align with the above requirements.\n\n### Deploy on OpenShift\n\nApply OpenShift\nSCC\nto kdb Insights namespaces first.\nbash\nCopy\n\n```\noc adm policy add-scc-to-group nonroot-v2 system:serviceaccounts:kxioc adm policy add-scc-to-group privileged system:serviceaccounts:kxi-operatoroc adm policy add-scc-to-group nonroot-v2 system:serviceaccounts:kxi-managementoc adm policy add-scc-to-group privileged system:serviceaccounts:kxi-management\n```\n\nFor OpenShift deployments, do the following:\n- Generate thevalues.yamlfile by populating the required parameters:bashCopykxi install setup --namespace kxi\n- Open thevalues.yamlwith your preferred editor and append openshift required values:bashCopyglobal:clusterProvider: openshiftencryption:enabled:truewithinMesh:nginx:falseoperator:falseprometheus:falsekxi-operator:config:mount:storageClass:\"ocs-storagecluster-cephfs\"packages:useLocalValues:truestorageClass:\"ocs-storagecluster-cephfs\"storageSize: 10Gi\n- Deploykdb Insights Enterpriseby running the following command:bashCopykxi install run --filepath=values.yaml --version=$INSIGHTS_VERSION --namespace kxi\n\n#### OpenShift specific config\n\nTo define storage class values for use in kdb Insights Enterprise and ensure they appear in the Web Interface, include the following configuration:\nbash\nCopy\n\n```\nglobal:  insightsConfig:    storage:      database-single:        provisioners:          # Priority list of default provisioners to select for each component.          default: [\"rook-ceph.cephfs.csi.ceph.com\"]          # Indicates the set of supported provisioners within different components          # in the system.          supported: [\"rook-ceph.cephfs.csi.ceph.com\"]      database-cluster:        provisioners:          # Boolean to indicate all are supported          supported: true          default: [\"rook-ceph.cephfs.csi.ceph.com\"]\n```\n\n\n## Post install\n\nOnce the\nkdb Insights Enterprise\nhas been installed you should check to see that it is running.\nAll pods should enter a\nRunning\nstate after a short period of time.\nshell\nCopy\n\n```\nkubectl get pods -l app.kubernetes.io/instance=insights\n```\n\n\n## Next steps\n\nAfter installing the base system, deploy a package to ingest and analyze data.\n- Deploying a package in the UI\n- Deploying a package in the CLI",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1248,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-43db5d2a8fcd",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/packages-ui.htm",
    "title": "Packages",
    "text": "\n# Packages in the Web Interface\n\nThis page describes how to manage packages using the\nkdb Insights Enterprise\nWeb Interface.\nkdb Insights Enterprise\npackages are groups of components that can be managed as one entity.\nFor information on how to manage packages from outside\nkdb Insights Enterprise\nWeb Interface, see\nkxi package creation in the CLI\n.\n\n## View index of packages\n\nClick on\nPackages\n, under\nMANAGE\n, in the\nleft-hand menu\nto view a list of all your packages. This listing includes details of all packages in your deployment; both those\ncreated in the web interface\nand using the\nCLI\n.\nThe following information is displayed:\n\n| Heading | Description |\n| --- | --- |\n| Name | The name of the package. Click on the name to open thepackage entitiesdialog. |\n| Version | The version of the package. When youcreate a packagein the web interface it is assigned a version of1.0.0. To upgrade package versions use the CLI.If your deployment includes multiple versions of a package, and none of them are currently deployed, the latest version is shown here. If your deployment includes multiple versions of a package, and one of them is currently deployed, the deployed version is shown here. |\n| Status | The status of the package. The options are:InactiveRunningErrorInitializing |\n| Author(s) | The name of the user who created the package. |\n| Last Pushed by | The name of the user that last pushed the package from the CLI. Seeuploadingfor details. |\n| Last Deployed by | The name of the user that last deployed the package. Seedeploying a packagefor details. |\n| Entitlements | The entitlements you have for the package. These can be:Admin,Read,Write, EXecute. Seepackage entitlementsfor details. |\n\nClick on any of the column headers to change the sort order of the data.\nTo the right of the package details are three dots which open a menu of actions.\nThe options displayed depend on whether the package is active or inactive.\n\n| Action | Inactive | Active |\n| --- | --- | --- |\n| Deploy | Y | N |\n| Rename | Y | N |\n| Export | Y | Y |\n| Delete | Y | N |\n| Teardown | N | Y |\n\n\n### Filter Packages\n\nYou can refine the data displayed on the Packages Index based on the status of the package.\n- ClickFilterto display the available statuses.\n- Select the status(es) you want to filter by.\n    The listing updates to show packages with the selected status(es). The following example shows the index filtered by a status ofRunning.\n\n## Package Entities\n\nTo view the entities contained within a package; including database(s), pipeline(s), and view(s):\n- Click on the package link in theNamecolumn on thepackages indexor\n- Click on the package name in the breadcrumb above theNamein database, pipeline, or view screens. In the following example, the package nameinsights-demo v2.0.5appears above the databaseNamefield in a database screen.The following shows an example of thePackage Entitiesdialog containing multiple entities.The type of entity is identifiable by the following icons, with the name of the entity displayed beside the icon:IconEntity typeDatabase(s)Pipeline(s)View(s)Click the entityNameto open the entities details in the main window.ClickCloseto close the Package Entities dialog and review the entity details.\n\n## Search\n\nUse the search box, in the top right-hand corner of the index page, to refine the list of packages by searching for a specific package or packages that match your search criteria.\n\n## Create a package\n\nThe\nCreate Package\nbutton is visible at the top right-hand side of the packages index page.\nTo create a package:\n- ClickCreate Packageto open the dialog shown below.\n- Enter a name for the package. The name must be unique and between 2-32 characters long. It can include only lowercase and uppercase characters, as well as hyphens. The name must start and end with an alphanumeric character. If any of these rules are not met, the breached rule is highlighted as you type, andSaveis disabled.\n- ClickSave. A notification is displayed saying that the package was created successfully. An audit log message is written and audit logging is turned on. The new package appears on thepackages indexwith a version of 1.0.0 and a status of Inactive.\n\n## Deploy a package\n\nDeploying a package means that all entities within the package are deployed. Any entities that are in an unsaved state, are saved prior to deployment.\nWarning\nA package cannot be deployed if one or more of its pipelines are deployed separately.\nNote\nThe Deploy option is unavailable for packages that contain only 1 or more Views, or no entities at all.\nTo deploy an inactive package:\n- Click on the three dots, to the right of the packages entitlements, to open the actions menu, and clickDeploy.TheDeploy Entitiesdialog shows the entities contained in the package.\n- ClickDeploy. A message is displayed indicating that the entities have been deployed.\n\n## Rename a package\n\nTo rename an inactive package:\n- Click on the three dots, to the right of the packages entitlements, to open the actions menu, and clickRename.\n- Enter the new name, ensuring it meets the package name criteria, as describedhere, and clickSave.\nWarning\nRenaming requires updating any Pipelines that use Streams in this Package, as well as the deployment configuration.\nWhen you rename a package containing a pipeline that is using\nkdb Insights Stream Reader\nor\nkdb Insights Stream Writer\nyou must update the\nAssembly Integration\nfield(s), in the pipeline\nSettings\ntab, replacing the old package name with the new package name. Where the documentation states\nassembly\nreplace with the word\npackage\n.\n\n## Import a package\n\nTo import a\n.kxi\npackage in the Web Interface:\n- ClickImport Packageat the top right-hand side of the Packages page.\n- In the newImport Packagedialog, clickSelect Fileor drag and drop your file.\n- If you are importing a package that already exists, check theOverwrite existing packagecheck box to replace it with the imported package. Otherwise, rename the file to import.\n- ClickClose.\n\n## Export a package\n\nTo export an active or inactive package:\n- Click on the three dots, to the right of the packages entitlements, to open the actions menu, and clickExport.\n- A[package-name-version].kxifile is downloaded to your local machine.\n- You can then unpack this package on your local machine and edit it in the CLI. Seeunpackfor details.\nTo upload a\n.kxi\npackage file, to your\nkdb Insights Enterprise\n, use the push command in the CLI, see\nupload\nfor details.\n\n## Delete a package\n\nWarning\nDeleting a package deletes all the entities; (database(s), pipeline(s) and view(s)), it contains and cannot be undone. The delete option is only available when a package is not running.\nTo delete an inactive package:\n- Click on the three dots, to the right of the packages entitlements, to open the actions menu, and clickDelete.TheDelete Packagedialog opens, showing details of the selected package and the entities it contains.\n- TypeDELETEto confirm and clickDelete Package.A notification is displayed to say the package has been successfully deleted.\n\n## Teardown a package\n\nTo tear down an active package:\n- Click on the three dots, to the right of the packages entitlements, to open the actions menu, and clickTeardown.TheTeardown Packagedialog opens, showing details of the selected package and the entities it contains.\n- CheckClean up entitiesafter teardownif you want to delete all data associated with a database and all pipelines, checkpoints, and user states. Otherwise, leave it unchecked.\n- ClickTeardown Package. A message is displayed, indicating that the entities have been successfully torn down.\nIf any problems arise, an error message is displayed, go to thelogsfor further details.\n\n## Further reading\n\nPackages Overview",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1278,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-6a03112d623d",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Get_Started/cli-installation.htm",
    "title": "CLI Installation",
    "text": "\n# CLI Installation\n\nThis page explains how to install, upgrade, and uninstall the KXI Command Line Interface (CLI).\nThe supported Python versions are 3.10, 3.11, 3.12 and 3.13.\nNote\nIf you are using Windows Subsystem for Linux (WSL), we recommend installing\nwslu\nto simplify authentication. This utility allows you to open a web browser directly from within WSL. To install\nwslu\n, run the following command:\nbash\nCopy\n\n```\nsudo apt install wslu\n```\n\nRefer to the\nKX Downloads Portal\nfor the available versions.\n\n## Install\n\nNote\nSupported platforms\n: You can set up the CLI on both Windows and Linux.\nTo install the CLI from the KX Downloads Portal, use one of the following commands with the appropriate credentials.\nuv\npipx\npip\nUse the\nUV Package manager\nfor installation. This tool doesn't have any prerequisites. You can install the CLI without Python on your system.\n- Install UVif you don't already have it. We suggest usingwingetfor windows andbrewfor macOS/Linux as they automatically install the tool on the Path.For Windows:winget install uvFor macOS/Linux:brew install uv\n- Install the CLI with UV into a separate 3.11 Python environment:NoteYou can configure the CLI to use a different Python version by changing-p 3.11.shellCopyuv tool install -p 3.11 --extra-index-url https://portal.dl.kx.com/assets/pypi kxicli --forceTo install a specific version, run:bashCopyKXI_CLI_VERSION=x.y.z# replace with the version you want to installuv tool install -p 3.11 --extra-index-url https://portal.dl.kx.com/assets/pypi kxicli==\"$KXI_CLI_VERSION\"--force\n- Ensure you have one of the supported Python versions above on your system.\n- Installpipxon your system.\n- Make sure you have the latest version ofpipx.\n- Ensurepipxis running with the expected Python version.NoteWhich Python version ispipxusing ?Check the output ofpipx environmentlook at the value ofPIPX_DEFAULT_PYTHON. If this is wrong, download the correct Python version and set the environment variablePIPX_DEFAULT_PYTHONto the location of that Python executable.\"Importantpipxmust be available on your PATH.WarningIf you have previously usedpipto install the CLI you must runpip uninstall kxiclibefore installing withpipxto remove the old version from the PATH\n- To install the CLI from the KX Downloads Portal, runpipx installas follows:bashCopypipx install kxicli --pip-args=\"--no-input --extra-index-url https://portal.dl.kx.com/assets/pypi/\"--forceTo install a specific version, run:bashCopyKXI_CLI_VERSION=x.y.z# replace with the version you want to installpipx install kxicli --pip-args=\"--no-input --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicli==$KXI_CLI_VERSION\"--force\nIf the installation fails, you can run\npipx\nwith the\n--verbose\nmodifier to find more information.\nshell\nCopy\n\n```\npipx install kxicli  --verbose --pip-args=\"--no-input --extra-index-url https://portal.dl.kx.com/assets/pypi/\" --force\n```\n\nWarning\nThis method should only be used by developers familiar with the Python ecosystem.\n- You need to install Python and the Python package managerpipon your system.NotePython and pip command namespythonandpipmust be available on your PATH. If alternative commands such aspython3are required, update the commands appropriately.\n- To install the CLI from the KX Downloads Portal, runpip installas follows:bashCopypip --no-input install --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicliTo install a specific version, run:bashCopyKXI_CLI_VERSION=x.y.z# replace with the version you want to installpip --no-input install --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicli==$KXI_CLI_VERSION\nIf the installation fails, you can run\npip\nwith the\n--verbose\nmodifier to find more information.\nshell\nCopy\n\n```\npip --no-input install --verbose --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicli==$KXI_CLI_VERSION\n```\n\nOnce installed,\nkxi\nis available for you to execute.\n\n## Upgrade\n\nTo upgrade the CLI, run one of the following commands as shown in the examples below:\nuv\npipx\npip\nbash\nCopy\n\n```\nuv tool install -p 3.11 --extra-index-url https://portal.dl.kx.com/assets/pypi kxicli --upgrade --force\n```\n\nbash\nCopy\n\n```\npipx upgrade kxicli --pip-args=\"--no-input --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicli\" --force\n```\n\nbash\nCopy\n\n```\n# pass the '--upgrade' flag to pippip --no-input install --upgrade --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxicli\n```\n\n\n## Uninstall\n\nTo uninstall the CLI, run one of the following commands as in the below examples:\nuv\npipx\npip\nbash\nCopy\n\n```\nuv tool uninstall kxicli\n```\n\nbash\nCopy\n\n```\npipx uninstall kxicli\n```\n\nbash\nCopy\n\n```\npip uninstall kxicli\n```\n\n\n## Air-gapped environments\n\nBy default,\npip\ninstalls dependencies from the internet. To install the CLI in an air-gapped environment, where internet access is restricted, you must download the dependencies on a machine connected to the internet and transfer the dependencies to the air-gapped environment.\n- Download the CLI and dependencies on an internet-enabled machineshellCopypip --no-input download -d bundle setuptools wheel --extra-index-url https://portal.dl.kx.com/assets/pypi/ kxiclitar -zcf bundle.tar.gz bundle\n- Copybundle.tar.gzto your air-gapped environment and run these commands to install itshellCopytar -zxf bundle.tar.gzpip install --no-index --find-links bundle kxicli\nNote\nThe above commands assume the interpreter and system that the dependencies are downloaded on match those of the target environment. If this is not the case, use the\n--platform,--python-version,--implementation, and--abi\noptions to fetch the dependencies matching the target environment.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 735,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-7eafc38609c4",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/walkthroughs-index.htm",
    "title": "Index of Walkthroughs",
    "text": "\n# Walkthroughs & Examples Index\n\nThis page provides links to a number of guided walkthroughs/tutorials and examples designed to help you develop skills to exploit the power of\nkdb Insights Enterprise\n.\n\n## Creating a database\n\n- Create a database and add it to a package\n\n## Ingesting data\n\n- Ingest object storage data from one of the major cloud providers\n- Ingest streaming data from Apache Kafka (Web Interface)\n- Ingest SQL data\n- Ingest Protocol Buffers data\n- Ingest from Parquet files\n- Postgre SQL Ingest\n- Ingest streaming data from Apache Kafka (CLI)\n- More examples ingesting using the CLI can be found here\n\n## Querying and visualizing data\n\n- Query ingested data\n- Visualize ingested data on data grids\n- Visualize Kafka streaming data ingested\n- Visualize ingested data on a map\n\n## Industry examples\n\nFinance\n- Develop trading strategies using real time and historical tick data\n- Machine learning model to create stock predictions in real-time\nManufacturing\n- Apply deep neural networks to streaming IoT data using MQTT and Tensorflow",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 175,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-4789b25808dd",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Interfaces/lang-interfaces.htm",
    "title": "Language Interfaces",
    "text": "\n# Language Interfaces\n\nThe kdb Insights interfaces are libraries created in different languages for developers to integrate with:\n- kdb Insights Enterprisefrom outside of you cluster via load balancers or from inside your cluster without the need for load balancers.\n- kdb Insights Reliable Transport\nUse the\ngetting started\nguide to learn more about\npublishing\n,\nsubscribing\nand\nquerying\ndata.\n\n## Interfaces available\n\nThe following language interfaces are available:\n- C\n- C#\n- Java\n- Python\n- q (RT.qpk)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 80,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-c88ded4cb4ea",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/data-query-overview.htm",
    "title": "Data Query Overview",
    "text": "\n# Data Query Overview\n\nThis page describes how to query data in kdb Insights Enterprise.\nThe kdb Insights data access services provide efficient data access through routing and APIs. This is done through two microservices which can be used together or independently, the\nService Gateway (SG)\nand the\nData Access Processes (DAPs)\n.\nThe\nData Access Processes (DAPs)\nprovide read-only access to all data stored in a kdb+ database. DAPs support access through several APIs:\n- getData- Retrieve data from a table in kdb Insights.\n- qsql- Execute a qSQL query on a specific tier of a database.\n- sql- Execute a SQL query across a database.\n- sql2- Execute a SQL2 query across a database.\n- preview- Efficiently retrieve a small sample of a table.\nDAPs react to data control messages, typically sent by the kdb Insights\nStorage Manager\n. These messages help synchronize\ntemporal purviews\n, which are semantic labels and the range of timeseries data available in a given DAP. This synchronization distributes timeseries queries across multiple DAPs.\nTODO -> picture of SG -> DAP <- SM.\nTODO do we need an intro for DAPs, RC and SG?\nThe\nService Gateway (SG)\nprovides a unified access point for data distributed across multiple DAPs. It provides request queueing, query routing, and response aggregation capabilities.\nAt a high level, the Service Gateway component is responsible for:\n- Accepting and validating service requests\n- Identifying the locale for service execution based on metadata. This metadata includes a combination of previously known configuration information, dynamic system topology information, and data contained in the request instance (such as its arguments and potentially other aspects of accompanyingout-of-bound information).\n- Selecting one or more DAPs to execute the request\n- Shepherding the request to and triggering execution on the selected DAPs\n- Triggering any aggregation of partial results from multiple DAPs\n- Shepherding results back to original caller\n\n## Architecture\n\nA high-level architecture of the relation between the Service Gateway and the Data Access Processes is depicted below, which also introduces the three internal services that make up the Service Gateway: the Gateway (GW), Resource Coordinator (RC), and Aggregator (Agg).\n\n| process | description |\n| --- | --- |\n| Client | A client process that connects to Gateway (GW) process to issue API calls. |\n| Service Gateway | Receives API requests from clients, forwards to appropriate DAPs, returns results. |\n| Resource Coordinator | Makes routing decisions based on DAP data purviews and availability. |\n| Aggregator | Aggregates responses from DAPs. |\n| RDB | A DAP that has access to the most recent data. |\n| IDB | A DAP that has access to today's data excluding the most recent data that is in the RDB. |\n| HDB | A DAP that holds all historical data dating. |\n\nNote: If this diagram or table is updated, update ../configuration/routing.md as well\nUnless otherwise specified, the arrows in the diagram represent asynchronous q IPC communication between processes. Query flow is as follows.\n\n| arrow | description |\n| --- | --- |\n| 1 | Client makes API request to a Service Gateway replica. This can be synchronous or asynchronous. |\n| 2 | The Service Gateway forwards the request to the Resource Coordinator. |\n| 3 | The Resource Coordinator sends partial requests to each DAP relevant to the query based on purview. |\n| 4 | DAPs forward their responses to a single Aggregator for aggregation. |\n| 5 | The Aggregator sends the response to the same Service Gateway the client connected to. |\n| 6 | The Service Gateway sends the response back to client. |\n\n\n## Query routing\n\nOne of the key features of the Service Gateway is its routing capabilities. It uses its\nconfiguration\nand the DAPs\ndata purviews\nto route requests exactly where it needs to. Every request specifies a temporal range and the labels of interest. The SG partitions the request across DAPs that can satisfy it, aggregates the results from each, and returns the consolidated response to the client that made the call.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 682,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-6f675d45c8ed",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Reference/rest-api-reference.htm",
    "title": "REST API",
    "text": "\n# REST API reference\n\nThis page introduces how\nkdb Insights Enterprise\nuses OpenAPI to document REST APIs and support client code generation.\nkdb Insights Enterprise\nprovides a REST API that you can use to interact with the platform.\n\n## Authentication\n\nRequests to the REST API are authenticated using Bearer tokens.\nFor more information about authentication and obtaining a token, see\nAuthentication\n\n## View the API reference\n\nYou can view the API reference by browsing to\ntext\nCopy\n\n```\nhttps://${INSIGHTS_HOSTNAME}/openapi\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 80,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-4c6076803dad",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/get-data-sql-database.htm",
    "title": "Ingest SQL Data",
    "text": "\n# Ingest SQL Data\n\nThis walkthrough demonstrates how to ingest SQL data into\nkdb Insights Enterprise\n.\nA health dataset, hosted on a PostresSQL database, has been provided for use in this walkthrough, which has records of air quality, traffic congestion, car and bike accidents for a day in New York. Once ingested, it is ready to be queried and visualized.\nInformation\nNo prior experience with q/kdb+ is required to build this\npipeline\n.\nThe following sections describes how to:\n- Create a database: Create a new database, add it to a package, and set up the schema for the new database.\n- Create a pipeline: The pipeline comprises the following nodes:Readers: Reads data from its sourceDecoders: Defines the type of data importedSchema: Converts the data to a type compatible with a kdb+ databaseWriters: Writes the data to akdb Insights Enterprisedatabase\n- Ingest the data: Deploy the pipeline you have just created to ingest data into the database.\n\n## Create a database\n\n- From theOverviewpage, chooseCreate newunderDatabases:\n- In theCreate Databasedialog set the following valuesSettingValueDatabase Namehealth-demo-dbSelect a PackageCreate new packagePackage Namehealth-demo\n- ClickCreate.\n- On theSchema Settingstab, clickCode Viewto open theSchema Code View. Here you can add large schema tables in JSON format.\n- Replace the existing code with the following JSON.health-demo-dbThis JSON code is used in all the walkthrough examples that use thehealth-demo-dbdatabase.JSONCopy[{\"columns\": [{\"type\":\"string\",\"name\":\"name\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"lat\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"sorted\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"long\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"sorted\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"neighborhood\",\"type\":\"string\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"airquality\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"trafficcongestion\",\"type\":\"float\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"bikeacccnt\",\"type\":\"int\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"caracccnt\",\"type\":\"int\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"},{\"name\":\"timestamp\",\"type\":\"timestamp\",\"attrMem\":\"\",\"attrOrd\":\"\",\"attrDisk\":\"\",\"compound\":false,\"foreign\":\"\"}],\"primaryKeys\": [],\"type\":\"partitioned\",\"prtnCol\":\"timestamp\",\"name\":\"health\",\"sortColsDisk\": [\"long\"],\"sortColsMem\": [],\"sortColsOrd\": [\"lat\"]}]\n- ClickApply.\n- ClickSave.\n- ClickDeployand in the resources summary screen clickDeployagain.\n- Next, use the Import Wizard to ingest data into this database.\n\n## Import data\n\n- On theOverviewpage, chooseImport DataunderDatabases:\n- In theImport your datascreen, select PostgreSQL from theRelationalreaders, shown below.\n- Enter the following values for the PostgresSQL database in theConfigure PostgreSQLscreen.SettingValueServerpostgres.trykdb.kx.comPort5432DatabasedemoUsernamedemoQueryselect * from health\n- ClickNext.\n- In theConfigure Schemascreen:Leave the following settings unchanged:SettingValueApply a SchemaCheckedData FormatAnyClick theLoad Schemaiconand set the following values:SettingValueDatabasehealth-demo-dbTablehealthClickLoad, and thenNextto open theConfigure Writerscreen.\n- In theConfigure Writerscreen, specify the following settings:SettingValueDatabasehealth-demo-dbTablehealthWrite Direct to HDBNoDeduplicate StreamYesSet Timeout ValueNo\n- ClickCreate Pipelineto display theCreate Pipelinedialog.SettingValuePipeline Namehealth-demo-1Select a PackageCreate new packagePackage Namehealth-demo-1. For this example, the database and pipeline must be in different packages.ClickCreate.\n- ClickSave.Thehealth-demo-1pipeline is now available underPipelinesin the left-hand menu.\n\n## Deploy the pipeline\n\nDeploy the pipeline to read the data from its source, transform it to a kdb+ compatible format, and write it to the\nhealth-demo-db\ndatabase.\n- Wait for the database to deploy. When a green tick appears besidehealth-demo-dbunderDatabasesin the left-hand menu it is deployed successfully.\n- Click on thehealth-demo-1pipeline tab and clickSave & Deployin the top panel, as shown below.\n- In theEnter password for PostgreSQLscreen, enterdemoand clickOK.\n- Check the progress of the pipeline under theRunning Pipelinespanel on theOverviewtab.\nThe data is ready to query when\nStatus\nis set to\nFinished\n. Note that it may take several minutes for the pipeline to start running.\nWarning\nOnce the pipeline is running, some warnings may be displayed in the\nRunning Pipelines\npanel of the\nOverview\ntab. These are expected and can be ignored.\n\n## Query the data\n\nTo verify that the data has been ingested you can query it as follows.\n- ClickCreate newunderQuerieson theOverview page.\n- In theBasictab of the Query Builder section of the screen, selecthealthas theTable Name.\n- ClickRun Queryto return data. The results are displayed in the Output section at the bottom of the screen.\n\n## Next steps\n\nNow that your\nPipeline\nis up and running you can:\n- Build a visualizationfrom the data.\n\n### Further reading\n\nUse the following links to learn more about specific topics mentioned in this page:\n- The import wizard\n- Reader nodes\n- Transform nodes\n- Writer nodes",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 599,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-2e70f34b6f49",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/Azure_Marketplace/release-notes.htm",
    "title": "Latest",
    "text": "\n# kdb Insights Enterpriseon Azure Marketplace - Release Notes\n\nThis page outlines release notes from releases that were previously available on the Azure Marketplace. For the most recent release of kdb Insights Enterprise, refer to the\nlatest release\npage.\n\n## 1.16.3\n\n\n### Release Date\n\n2024-07-31\nRefer to the main release notes of\nkdb Insights Enterprise\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.18.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.7\n- Kubernetes - 1.33\n- Istio 1.27.3\n\n## 1.14.2\n\n\n### Release Date\n\n2024-07-31\nRefer to the main release notes of\nkdb Insights Enterpriseversion 1.14.0\n.\n\n### Third party dependencies\n\nThe Azure Marketplace deployment of\nkdb Insights Enterprise\nmanages the third-party pre-requisites for you. The following versions are installed to your selected subscription as part of the marketplace deployment:\n- cert-manager - 1.17.2\n- nginx-ingress - 4.11.5\n- rook-ceph - 1.17.2\n- Kubernetes - 1.33\n- Istio 1.26.1",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 178,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-bed402b0e9f5",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/kdb-insights-database.htm",
    "title": "kdb Insights Database",
    "text": "\n# kdb Insights Database\n\nThis page provides an overview of the kdb Insights Database, a high-performance, scalable, cloud-first time-series database.\nThe kdb Insights Database is a distributed time-series database built upon kdb+, providing real-time data ingestion and query, temporal storage tiering, and scalable query routing.\nThe\nkdb Insights Database\nis made up of the following components:\n- Query- providing scalable query routing through labels and temporal purview for real-time or historical data\n- Storage- providing fast and resilient data write-down\n\n## Use cases\n\nThe kdb Insights Database provides:\n- SQL, qSQL, and structured APIs for querying via q IPC or REST.\n- a parallelized write-down process resilient against failures.\n- separation of write-down operations from other system processes and their functions, e.g. serving queries.\n- a tiered database that seamlessly moves data through the tiers, including object storage.\n- configurable compression for tiers of the database.\n- a sharded data model facilitating query routing via metadata and temporal range.\n- capability for single-shard and multi-shard queries.\n- a uniform view of data across all tiers within all shards.\n- callback registration for custom q APIs and lifecycle hooks.\n\n## Components\n\nThe kdb Insights database is composed of a number of core components that distribute the workload of storing and querying data.\n- Service Gateway(Gateway, Route, Aggregate) - providing scalable query routing through labels and temporal purview\n- Data Access Process(Read) - providing read access to data, whether real-time or historical\n- Storage Manager(Write) - providing fast and resilient data write-down\nWhen assembled together, these components provide the data resilience, data read/write, and query capabilities for high-performance real-time and historical data analytics.\n\n## Core concepts\n\n\n### Package (shard)\n\nEach kdb Insights Database is configured via a combination of environment variables and a\npackage\n. The package configuration outlines the database schema, storage volumes and tiering, and read/write processes.\nEach package is a unit that defines a single semantic data shard. Separate packages could be created that are semantically similar (share a schema, but differ in\nlabel metadata\n) for volume scaling, or could instead be semantically different, containing an entirely different database schema. The query routing offered by the\nService Gateway\nis capable of connecting to, and serving queries for, any number of semantically similar or distinct data shards at once, providing the capability for cross-shard query and join.\nThus, the\nService Gateway\nis the only component of the kdb Insights Database that does not exist within a package, since it provides uniform access to many at once.\n\n### Data tiers\n\nAs real-time data ages, it is written down by the\nStorage Manager\nfrom fast storage volumes to more cost-effective, though slower, volumes, including object storage as a final volume.\n- RDB - real-time in-memory database - (most recent) data, this is the fastest tier.\n- IDB - intra-day disk database - data older than the RDB holds, but not yet written to the HDB.\n- HDB - historical disk database - data older than the current day, written into daily partitions, and segmented across storage media.\nEach of these tiers are independent referred to as\nData Access Processes\nwith a unique\ntemporal purview\n.\n\n### Temporal purview\n\nThe\ntemporal purview of a Data Access Process\nrefers to the timespan of data that it provides access to. The DAPs share their purview as metadata to the\nService Gateway\n, along with their\nlabel metadata\n, to aid in query routing. Unlike labels, a DAP's temporal purview changes throughout the life of the process as data ages and is moved through storage media.\nFor example, an RDB will have a data purview for the last 10 minutes of data, an IDB will have a 10 minute lag of the RDB up to the last day and the HDB will have anything older than the last day.\n\n### Labels\n\nEach package is annotated with key-value label metadata, such as:\nYAML\nCopy\n\n```\n# us-depa-db-shard.yamllabels: region: US department: DepA# can-depa-db-shard.yamllabels: region: CAN department: DepA# us-depb-db-shard.yamllabels: region: US department: DepB# can-depb-db-shard.yamllabels: region: CAN department: DepB\n```\n\nIf these four packages each contain an RDB, IDB, and HDB, then the data is laid out across Data Access Processes as:\nThe Service Gateway can then route queries to the appropriate Data Access Processes, combining the results as necessary.\nFor example, the below API query to\ngetData\nrequests data for all packages with\nregion=CAN\n, and requests only data earlier than the current day, so all\nregion=CAN\nwill contribute one HDB to satisfy the query.\nThe partial results from each HDB will then be combined by the\nAggregator\ncomponent of the\nService Gateway\n.\nWarning\nQueries without labels are global and will aggregate data from all packages that have the the target table. This can lead to a mismatch error if there are multiple versions of the same table and no labels are provided in the query. Use labels in your query whenever you want to select a specific table from a single package.\n\n### Schema and attributes\n\nWhile the table\nschema\nwithin each DAP type (RDB, IDB and HDB) is the same (each contains the same set of tables, columns, and their types), each can have different column attributes set to facilitate different query patterns.\nThe use of attributes has a significant impact on query performance and write-down characteristics, so query patterns should be considered when designing a schema. The following column attributes are available:\nThe use of attributes has a significant impact on query performance and write-down characteristics, so consider query patterns when you design a schema. For details on how to set attributes on the schema, refer to\nAttributes\n.\nMore information about attributes, their use, and tradeoffs is available in the\nkdb+ documentation\n.\n\n## Next Steps\n\n- To learn more about configuring a database, seedatabase configuration.\n- To learn more about querying a database, seedatabase queries.\n- To learn more about database storage, seedatabase storage options.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 983,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-14c7c1afe312",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/query-window.htm",
    "title": "Query Window",
    "text": "\n# Query Window\n\nThis page introduces you to the Query window which is used to query and analyze data in your\nkdb Insights Enterprise\ndeployment.\n\n## Open the Query window\n\nThe\nQuery\nwindow is used for data query and analysis, providing an isolated location where you can explore available data, write/debug pipelines, and prototype analytics without impacting production workloads.\nUse one of the following methods to open the\nQuery window\n:\n- Click+on the ribbon menu and selectQuery\n- Click+besideQuerieson theleft-hand menu.\n- ClickCreate New, on theQueries tab, of theQuick Actionspanel, on theOverviewpage.\n- ClickCreate Queryon theQueries Index.\n\n## Query window layout\n\nThe\nQuery\nwindow, introduced above, is divided into three panels, which are illustrated here and described below:\n- Use theQuery & Load Datapanel to query the database, using eithera preview query,a basic query,SQL,q, orUDAallowing you flexibility in choosing how you access your data.\n- TheScratchpadpanel is where you can performad hoc querieson data you have queried from the database. You can complete further analysis and development inPythonorq. This allows you to create functions for use throughoutkdb Insights Enterprise.\n- TheOutputtabs provide areas where you can view the output from your queries, as either raw data in theConsole, a formattedTable, or a chart in theVisualtab.\nThe top of the window has three buttons:\n\n| Delete | Click this to delete a query.Click Delete, in the dialog displayed, to confirm. |  |\n| --- | --- | --- |\n| View | Use the toggles to control the display of the Query, Scratchpad and Output panels.This lets you free up space on the screen if you are not using a panel. |\n| Save | Click to save changes to the query.If there are unsaved changes a star is displayed after the query name in the ribbon at the top of the screen. |\n\nInformation\nSaving a query retains the layout of the query tab, including the selected tabs and panel sizes.\n\n### Memory usage\n\nThe scratchpad memory usage and limit are shown at the bottom of the Query window.\n\n| Used | The size of all values stored in memory. |  |\n| --- | --- | --- |\n| Heap | The amount of memory allocated to the scratchpad process.This can grow unpredictably when working with large tables.This lets you free up space on the screen if you are not using a panel. |\n| Total | The limit on how much memory the scratchpad process can allocate before triggering an out-of-memory restart. |\n\n\n## Next steps\n\n- Use the Query panel to query the database\n- Use the Scratchpad to perform ad-hoc queries\n- Develop Python code within the scratchpad\n- Develop q code within the scratchpad\n- Visualize results in the console and graphically\n\n## Further reading\n\n- Query with API\n- SQL query\n- Build enhanced Views",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 472,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-4730b4512eff",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/database-storage.htm",
    "title": "Database Storage",
    "text": "\n# Database Storage\n\nThis page explains how to set up storage in\nkdb Insights Enterprise\n\n## Overview\n\nData storage within the kdb Insights Database is handled by the Storage Manager (SM) for on-disk data persistence in conjunction with the Data Access Processes which provide data presentation of on-disk data as well as in-memory presentation of data not yet persisted to disk. The Storage Manager is responsible for writing data down in a resilient way maintaining database consistency while moving data from fast to slower and cheaper storage media, such as from SSDs to spinning disks to object storage.\nNote\nIn-memory data is persisted in a write-ahead log for recovery\n\n## Current features\n\n- Data-agnostic in design and implementation\n- Commits ingested data to disk in an organized, fault-tolerant manner that is resilient against failure at any point during write-down operations and efficient for querying\n- Supports various types of storage (memory, NVMe, SSD, etc.)\n- Provides parallelism of maintenance operations where possible, to reduce elapsed time\n- Supports late data (data arriving on a day different from that with which it is associated)\n- Supports standard kdb+ table types (basic, splayed, partitioned)\n- Supports tiered storage with configurable migration and compression policies\n- Supports starting off from a pre-existing standard kdb+ partitioned database (which gets converted into SM format)\n- Supports migrating historical data to object storage\n- Supports offline schema changes by reacting to configuration changes at initialization\n- Supports large batch data ingest by merging an external kdb+ HDB on-demand\n- Supports point-in-time snapshot of on-disk databases\n\n## Components\n\nThe Storage Manager is comprised of four processes:\n- SMis responsible for the coordination of the write-down operations, as well as exposing the front end interface for other microservices to communicate with SM.\n- EOIis responsible for performing the end-of-interval operation that persists a portion of an in-memory data store to disk, stored in IDB partitions.\n- EODis responsible for performing the end-of-day operation that persists the entirety of the on-disk IDB data to disk, stored in HDB partitions.\n- DBMis responsible for performing the migration of data between HDB storage tiers, which are unique portions of the on-disk database spread across various storage volumes. Such volumes are commonly of various storage types, ranging from high-performance storage (for most-recent, business-critical data) to slower, cheaper storage for data less frequently accessed, possibly in a compressed format.\n\n## Distributed storage\n\nTime-series data is stored horizontally by time across storage media, and vertically through labeled package shards.\nEach horizontal tier of the data allows separate attributes to be applied for accelerating queries against that tier.\n\n## Storage lifecycle\n\n\n### Stream partitioning (IDB, EOI)\n\nThe Storage Manager splits the incoming data stream into stream partitions by injecting signals directly into the data stream. These signals demarcate the start and end of intra-day partitions to be written to the IDB (known as intervals).\nAll data received within an interval is buffered in-memory, spilling to disk if necessary to keep RAM limits under control (controlled by the\nblockSize\ntable configuration) and is written down to a new interval partition within the IDB upon receipt of the end-of-interval signal. These frequent write-downs to the IDB relieve memory pressure from SM (and Data Access Process) by quickly getting data to disk.\nInformation\nQuery performance impact\nCurrently, the in-memory Data Access Process (RDB) only holds data that is not written to disk. Thus, a longer interval write-down frequency allows more data in-memory for queries, while a shorter writedown frequency allows less data in-memory causing more queries to need to read from disk.\n\n### Date partitioning (HDB, EOD)\n\nWhen the day rolls over to a new UTC date (as determined by SM when generating the EOX signal), a configurable amount of time is spent waiting to allow pending publishes from the last day to arrive, after which point the stream signal produced by SM is an end-of-day signal. Upon receipt of this signal from the stream, SM begins collecting all data from the IDB and populating a new HDB partition for the last days data. At this point, the IDB is cleared for the next days data.\nWhile the HDB is being populated with the previous day's IDB data, the IDB is concurrently being updated with new int partitions for the current day so that the IDB does not fall behind the ingestion stream.\n\n### Storage tiering\n\nAfter each EOD completes populating a new partition in the HDB for the previous day's data, checks are performed on the HDB for partitions that should be migrated to slower storage or object storage based on the age of the partition and the tiering configuration.\nIf used, an object storage must be used as the last HDB storage tier.\nNote\nObject storage tier is immutable\nNote that all date partitions written to object storage are treated as immutable, and the late table data targeting any partition already migrated to object storage are discarded (with a warning message in DBM logs).\n\n## Late data\n\n\n## Resilience and self-healing\n\n\n### Consistent database view\n\nTo achieve the instant reloading of HDB and IDB and full recoverability from any write-down failure, SM creates a loadable kdb+ database where the table directories are symbolic links to the versioned physical table data. If an existing kdb+ database is detected in SMâs configured first HDB-tier directory on the first run, it is enhanced with all the symbolic links SM needs for managing writedown.\n\n### Data consistency during writedown\n\n\n## Logging\n\n\n### Signal generation\n\nThe\nSM\nprocess emits EOX signals into the message bus and tracks its sequence ID:\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.208z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Signalling EOIa, seqid=2607, ts=2022.12.06 04:13:45\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:45.237z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Pushing eoi signal with payload=(`_prtnEnd;...)\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:45.255z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Incremented EOI seq id to 2608 from 2607 (after signal)\",\"service\":\"smc\"}\n```\n\nIf the message bus is operational and ingestion is keeping up, the\nEOI\nprocess immediately gets the EOX signal from the stream and begin a writedown.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.270z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Received EOIa trigger event with payload=[...]\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.270z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Set tracked seq id to 2607\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.270z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] EOI initiated for 2022.12.06 2022.12.06D04:03:45.000000000 600 2607 [...]\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.270z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Proceeding with EOI writedown\",\"service\":\"eoi\"}\n```\n\nEach writedown processes a set of partitioned tables, with each table indicating when it is processed:\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.356z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Starting EOI writedown\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.405z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Processing 1 partitioned table\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.405z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Starting write of partitioned table (trace)\",\"service\":\"eoi\"}\n```\n\nThe initial locations for the data are primed by writing a 0-row table. These 0-row writes are expected even if data has been received.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.407z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Writing 0 rows to: (`:/data/db/idb/data/2022.12.06/15/trace.ss/;12;0;0)\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.427z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Writing 0 rows to: (`:/data/db/idb/data/2022.12.06/15/trace/;12;0;0)\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.443z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Writing 0 rows to: (`:/data/db/idb/data/2022.12.06/15/trace.sl/;12;0;0)\",\"service\":\"eoi\"}\n```\n\nFollowing the 0-row writes, a chunked append of the ingested data to the target partitions is performed.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.461z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Appending :/data/db/idb/data/2022.12.06/14/trace.ss/ to :/data/db/idb/data/2022.12.06/15/trace/, ind=0, len=0, chunks=0\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.461z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Appending :/data/db/idb/data/2022.12.06/14/trace.ss/ to :/data/db/idb/data/2022.12.06/15/trace.ss/, ind=0, len=0, chunks=0\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.462z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] Appending :/data/db/idb/data/2022.12.06/14/trace.sl/ to :/data/db/idb/data/2022.12.06/15/trace/, ind=0, len=30000, chunks=1\",\"service\":\"eoi\"}\n```\n\nWhen all chunks are appended, the data is sorted (if applicable) and attributes are applied. Note, these operations can be RAM intensive.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.477z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Sorting :/data/db/idb/data/2022.12.06/15/trace, cols=,`sensorID rows=30000, size=1140175 chunks=1 crows=30000\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.517z\",\"component\":\"SM\",\"level\":\"DEBUG\",\"message\":\"[eoi] setAttrs      : applying p# to sensorID in :/data/db/idb/data/2022.12.06/15/trace/\",\"service\":\"eoi\"}\n```\n\nFinally, each table records the ingested size of the table within that interval.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.522z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Finished write of partitioned table (trace)\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.546z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Table size of trace: 1143583\",\"service\":\"eoi\"}\n```\n\nWhen all tables have been written down, the writedown is complete and data is flushed to storage.\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:45.909z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Finished EOI writedown, duration=0D00:00:00.553250257\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:45.909z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Flushing filesystem with :/data/db/idb/eoxaStatus\",\"service\":\"eoi\"}\n```\n\nFinally, the\nEOI\nprocess dispatches to the\nSM\nprocess to commit the new database view, notify client query processes, and cleanup any data no longer required (old views or temporary files used during the write-down).\ntext\nCopy\n\n```\n{\"time\":\"2022-12-06T04:13:48.913z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] EOI metadata: [kxi_sm_eoi_duration_seconds=3.64;kxi_sm_eoi_records=30000]\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:48.913z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Calling to SM to complete EOI...\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:48.913z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Requesting completion of EOI 2607 from main SM process\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:48.914z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] EOI 2607 complete in EOI process. Finalizing EOI.\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:48.914z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Committing EOI...\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:48.914z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Flushing filesystem with :/data/db/idb/eoxaStatus\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:48.914z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Flushing filesystem with :/data/db/hdb/current/hdbStatus\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:48.915z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] EOI commit\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:53.904z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] EOI committed\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:53.909z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Notifying client processes of EOI completion for soiTS=2022.12.06D04:03:45.000000000, intv=600, threshold=2022.12.06 04:13:45\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:53.915z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Finished sending reload signals, expecting acknowledgment from 1 client\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:54.237z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] Starting post-reload-notification processing\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:54.237z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] EOD dispatch requested, but nothing in EOD queue. Ignoring.\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:54.237z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[sm] EOI 2607 finalized. Next EOI scheduled in 0D00:09:50.970402987 at 2022.12.06D04:23:45.208327841.\",\"service\":\"smc\"}{\"time\":\"2022-12-06T04:13:54.238z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] SM EOI completion successful. Setting new partition and cleaning up.\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:54.490z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Removing old IDB link directory :/data/db/idb/idb.1\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:54.638z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Reporting metric to licensing server. Key: insights.ingest.bytes bytes: 1143583.\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:54.639z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] Reporting metric to licensing server. Key: insights.md.bytes bytes: 103501.\",\"service\":\"eoi\"}{\"time\":\"2022-12-06T04:13:54.639z\",\"component\":\"SM\",\"level\":\"INFO\",\"message\":\"[eoi] EOI 2607 complete. started: 2022.12.06D04:13:45.272947242 finished: 2022.12.06D04:13:54.639189234 elapsed: 0D00:00:09.366241992 latency: 0D00:10:09.639189234\",\"service\":\"eoi\"}\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1398,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-de7eea299647",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/system-info.htm",
    "title": "System Information",
    "text": "\n# System Information\n\nThis page describes the options under System Information in the Web Interface.\nThe System Information\nribbon menu\nicon contains information on the following:\n- Product Versions:Insights: The version ofkdb Insights Enterpriseyou are running.Views: The version ofKX Dashboardsyou are running.\n- Beta Features: This indicates whetherBeta Featuresare enabled or disabled on your environment.\n- License Number: This displays yourkdb Insights Enterpriselicense number. Refer to Licensing for further details about KX Licensing.\n- Expiry date: This is the expiry date for yourkdb Insights Enterpriselicense.\n- Encryption in Transit: This displays the status ofEncryption in Transit.\n- Entitlements: This displays whether or notentitlementsare enforced. The status is either:Enforced: The packages you can view and interact with, and data you can query is determined by the groups you belong to.Not EnforcedYou can view and interact with all packages, and query all data from deployed databases.\n- Query Environment(s): This indicates whetherQuery Environmentsare enabled or disabled on your environment.\n\n## Encryption in Transit\n\nThe System Information tab displays the current status of\nEncryption of data in transit\n. This is either Enabled or Disabled, and is enabled by default.\nAn\nupgrade\nis required to change the status.\n\n## Further reading\n\n- Navigate the web interface",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 203,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-837521bfed28",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/query-panel.htm",
    "title": "Query Panel",
    "text": "\n# Query & Load Data Panel\n\nThis page describes how to retrieve data from your\nkdb Insights Enterprise\ndeployment for analysis using the Query & Load Data panel.\nThe\nQuery & Load Data\npanel is the top left-hand section of the\nQuery window\nand is used to query your database.\nIf this panel is not visible, click\nView\n, in the top left-hand corner, and toggle\nQuery Panel\nto display it.\n\n## Create Query\n\nTo create a query:\n- Open theQuery & Load datawindow by clicking+besideQueriesin the left-hand menu, or using one of the other methodsdescribed here.\n- Create a query using one of the following methods:Preview queryBasic querySQL based queryq based queryQuery using User Defined Analytics (UDA)\n- You have the option to set anOutput Variable.This is not required but is useful for further inspection of the data inPythonandq.Choose the language for your output variable by clicking on eitherqor python on theScratchpad.The value entered is checked to ensure it is a valid q or python variable, based on the language selected.\n- Once your query is created, clickRun Query.\n- The result of your query is assigned to a variable inPythonorqbased on the language currently selected in thelanguage tabof the scratchpad.\n- View and analyze your query output in theoutput panel, at the bottom of the screen.\n- Each query is assigned a system defined name, displayed in the title bar of the query window.ClickSaveto save the query with the system defined name. A notification is displayed indicating that the query has been saved.NoteSaved queriesThe saved query is tied to your current browser session and device. Other users cannot access your saved query.To rename and save the query:Click the pencil icon beside the query name in the title bar and enter a new name.ClickOK.Saved queries are listed on theleft-hand menuunderQueries.\n- ClickClearto clear your query and start a new one.\n\n## Preview query\n\nThe\nPreview\ntab is a lightweight query option to fetch small samples of a table using minimal time and resources. It returns up to\nlimit\nrows from the specified table and time range. It is guaranteed to search all available data for results. If fewer than limit rows are returned, they contain all the available data. Beyond this, there are no guarantees as to which particular rows are returned, and the choice is subject to arbitrary implementation details.\n\n| Parameter | Required | Description |\n| --- | --- | --- |\n| Table Name | yes | The name of the table to retrieve data from. |\n| Start Date | no | Inclusive start time of the period of interest. This field is optional. By default, the preview is from the full temporal range of the database. |\n| End Date | no | Exclusive end time of period of interest. This field is optional. By default, the preview is from the full temporal range of the database. As the end time is exclusive, to include a full day, such as 2025.01.01, the end time should be midnight of the next day (for example, 2025-01-02T00:00:00) |\n| Limit | no | Maximum number of rows to return. The default limit is 100. |\n\nYou can set an\nOutput Variable\nto which this data is assigned. Refer to\nCreate Query\nfor details about output variables.\n\n## Basic query\n\nThe\nBasic\ntab of the\nQuery & Load Data\npanel allows you to retrieve data from the database without having to write code. You can use it to return data within specific ranges and apply\nfilters\n, to that data. The Basic tab is the first tab displayed when you open the\nQuery window\n.\nThis screen contains:\nThe following table describes the parameters of the\nBasic\ntab.\n\n| Parameter | Description |\n| --- | --- |\n| Table Name | The name of the table that you're querying. Click on this field to display a list of tables that are available in deployed databases. Click on a table name or use the search box to refine the list.The messageNo deployed tables availableis displayed when there are no tables because no databases have been deployed. |\n| Range | This enables you to specify an initial query condition based on either:ROWS,TIME, orDATE.Click on the drop-down list to open theSelect Rangedialog. Choose from the predefined options, or select the custom option at the end of the list.If the selected table is a splayed table, the time and date range options are not displayed.ROWSTheLast 100 rowsare displayed by default.Custom (Rows)Select this to specify a customrow limit.TIMEIn theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed.Custom (Time)Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried.DATEIn theDATEcolumn, select one of the pre-defined date ranges or set a custom date. This is not shown if the table is splayed.Custom (Dates)Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. | ROWS | TheLast 100 rowsare displayed by default.Custom (Rows)Select this to specify a customrow limit. | TheLast 100 rowsare displayed by default. | Custom (Rows) | Select this to specify a customrow limit. | TIME | In theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed.Custom (Time)Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried. | In theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed. | Custom (Time) | Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried. | DATE | In theDATEcolumn, select one of the pre-defined date ranges or set a custom date. This is not shown if the table is splayed.Custom (Dates)Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. | In theDATEcolumn, select one of the pre-defined date ranges or set a custom date. This is not shown if the table is splayed. | Custom (Dates) | Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. |\n| ROWS | TheLast 100 rowsare displayed by default.Custom (Rows)Select this to specify a customrow limit. | TheLast 100 rowsare displayed by default. | Custom (Rows) | Select this to specify a customrow limit. |\n| TheLast 100 rowsare displayed by default. |\n| Custom (Rows) | Select this to specify a customrow limit. |\n| TIME | In theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed.Custom (Time)Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried. | In theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed. | Custom (Time) | Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried. |\n| In theTIMEcolumn, select one of the pre-defined time ranges or set a custom time scale. This is not shown if the table is splayed. |\n| Custom (Time) | Select this to specify a custom time. When this is selected theLast Minutesparameter is displayed. Specify the number of minutes of data you want queried. |\n| DATE | In theDATEcolumn, select one of the pre-defined date ranges or set a custom date. This is not shown if the table is splayed.Custom (Dates)Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. | In theDATEcolumn, select one of the pre-defined date ranges or set a custom date. This is not shown if the table is splayed. | Custom (Dates) | Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. |\n| Custom (Dates) | Select this to specify a custom date range. When this is selected, the two additional parameters are displayed.Start DateThe start date which defines the lower bound of your returned data. Use the date picker to define the date and time at which your query begins.End DateThe end date which defines the upper bound of your returned data. Use the date picker to define the date and time at which your query ends. |\n| Output Variable | You can set a name for the variable to which this data is assigned. Refer toCreate Queryfor details about Output Variables. |\n\nClick\nClear\nat any stage to reset the value in this tab.\n\n### Query options\n\nYou can select from the following query options by clicking\nAdd Query Option\nfrom under\nOPTIONAL PARAMETERS\n.\n\n|  | Row Limit |\n| --- | --- |\n| Filter |\n| Filter by label |\n| Select Columns |\n| Sort |\n| Define Aggregation |\n| Group Aggregation By |\n\nRow Limit\nis only available when a time or date value is specified in\nRange\n.\nNote\nCombining Filters\nYou can apply multiple filters, as well as combine filters of different types except for\nselect columns\nand\ndefine aggregations\n, which cannot be combined.\n\n#### Row Limit\n\nTo conserve memory usage, for very large datasets, you can set limits on the number of rows returned by your query. By default this is set to the last 100,000 rows.\nSelect\nCustom (Rows)\nfrom the\nRange\ndrop-down. From here you can:\n- Enter a new value inLimit (Rows), underOptional Parameters, to change the number of rows returned.\n- ClickFirstto have the first rows in your database returned.\n- ClickLastto have the last rows in your database returned.\n- Remove the row limit filter by clicking the icon beside the filter.\nAlternatively, if a time or date range has been specified and you want to change it to a row limit, click\nAdd Query Option\nunder optional parameters and click\nRow Limit\n.\n\n#### Filter\n\nThe\nFilter\noption is used for applying custom filtering to the query.\nTo apply a filter you must specify the following:\n\n| Field name | Description |\n| --- | --- |\n| Select Column | The column to be selected. |\n| Select Function | The function to be used to apply the filtering logic which is one of:inlikenot equalwithin<<==>=> |\n| FX Parameter | An input provided by you which defines the item against which the column is compared. |\n\n\n#### Filter By Label\n\nThe\nFilter By label\noption allows you to query specific tables.\nTo apply a\nFilter By label\nyou must specify the following:\n\n| Field name | Description |\n| --- | --- |\n| Key | Select the key value of the label that was assigned to the database. |\n| Value | This dropdown shows the database associated with the table selected in theTable Namefield. Choose one or more label values to query the corresponding tables. Using labels enables you to query various combinations of databases, refer toDatabase Labelsfor details. |\n\nIf no label is selected then all corresponding tables are queried.\n\n#### Select Columns\n\nThe\nSelect Columns\noption provides the ability to filter the query results to display selected columns only.\nClick on the\nSelect Columns\nfield, and choose the column(s) from the list displayed.\n- This list represents the columns in the table selected underTable Name.\n- Only oneSelect Columnsfilter is allowed.\n- TheDefine Aggregationsfilter cannot be combined with aSelect Columnsfilter.\n\n#### Sort\n\nThe\nSort\noption allows you to sort the results of a returned query based on the column chosen in the\nSelect Column\nfield.\n\n#### Define Aggregation\n\nThe\nDefine Aggregation\noption allows you to apply aggregations to specified columns of the table returning the result as an additional column.\nTo apply an aggregation you must specify the following:\n\n| Field name | Description |\n| --- | --- |\n| Set Return Column | The column the results of the aggregation are returned to. |\n| Select Aggregation | The aggregations available to you are as follows:countlastsumprd (product)minmaxallanyvar (variance)avg (average)dev (deviationsvar (sample variance)sdev (standard deviation) |\n| Select Column | The column the aggregation is applied to. |\n\nThe\nSelect Columns\nfilter cannot be combined with a\nDefine Aggregation\nfilter.\n\n#### Group Aggregation By\n\nThe\nGroup Aggregation By\noption allows you to group the results of aggregations based on the\nSelected Column\n.\n\n## Query using SQL\n\nUse the\nSQL\nquery option, in the Query panel, to query the tables available within your deployment using SQL syntax. These queries can run in a distributed manner across all available data associated with a table.\n- Enter your query in theSQLtab as shown below:\n- You can set anOutput Variableto which this data is assigned. Refer toCreate Queryfor details about Output Variables.\n- ClickRun Queryto run the query.\n- Review theresults.\nNote\nSQL query API\nThe ANSI SQL implementation used here is based on the kdb Insights\nSQL API\n.\nThis provides a subset of an ANSI SQL compliant API. The limitations of this are outlined\nhere\n.\n\n## Query using q\n\nTo query your data using qSQL, click the\nq\ntab in the\nQuery panel\n.\nNote\nThis option only works when Query Environment(s) are enabled.\nRefer to\nSystem Information\nfor details on how to check the status.\n- Select aDatabase. The messageNo deployed database availableis displayed when there are no databases deployed.\n- Select an option from theinstancesdrop-down.You can direct your query to a specific instance of your database(HDB, IDB, or RDB)to query data within the corresponding time window covered by that instance.Or set it todistributed, to distribute the query across all the available instances.\n- Enter your query in theqtab:\n- You can set anOutput Variableto which this data is assigned. Refer toCreate Queryfor details about Output Variables.\n- ClickRun Queryto run the query.\n- ClickRun Queryto run the query.\n- Review theresults.\n\n## Query using UDA\n\nUser Defined Analytics (UDAs) create custom analytics that are tailored to your specific needs, enabling you to extend the capabilities of kdb Insights beyond its standard functionality. See\nUDA Overview\nto learn more about UDAs.\nYou can run a UDA in the Query panel, and specify values for its required and optional parameters.  Refer to\nUDA parameters\nfor details about supported, unsupported and distinguished parameter types.\nTo run UDAs in the Query panel.\n- Click on theUDAtab, as shown below:TheUser Defined Analytic (UDA)drop-down contains a list of UDAs that have already beencreatedanddeployedto Insights Enterprise via a package.\n- Select a UDA from the list.The messageThere is no available UDAsis displayed when there are no UDAs available for query.A warning is displayed if the selected UDA cannot be queried. This occurs if there is no metadata associated with it or it contains required fields with unsupported types, preventing it from being queried.\n- Next, specify values for all required parameters.The required parameters of the UDA are displayed, as shown in the example below, with variations depending on the UDA definitions. Date fields feature date/time selectors for easy data entry. The following example UDA has two required parameters: column and multiplier.A notification is displayed if the selected UDA does not contain any required parameters.The system validates the entered parameter values to ensure they meet the required format and criteria, displaying an error message if the data is incorrect. The following shows the error displayed when a GUID is expected but not entered.\n- Next, click+ Add Parameterto configure any optional ordistinguishedparameters.You may need to use the right-hand scroll bar to locate theAdd Parameterbutton.You can remove any optional parameter by clicking the trashcan icon to the right of the parameter. In the following example thelabelsparameter is optional and can be removed.When a parameter references a database table, ensure that the database containing the table is deployed before running the query. Refer toDeploying a databasefor details.\n- You can set anOutput Variableto which this data is assigned. Refer toCreate Queryfor details about Output Variables.\n- ClickRun Queryto run the query.\n- The UDA is processed and returns the analytics results in thequery outputtabs.\nNote\nTo successfully call the UDA on tables outside of the package that it is defined, you need to define a\nscope parameter\nin the UDA metadata so that the\nservice gateway\nknows which aggregator to route the query through. However, the scope parameter is a dictionary argument and therefore is not supported by the UDA Query tab. Refer to\ndeploying UDAs from the CLI\nfor details on setting the scope parameter when deploying UDAs.\nCHECKING WITH BEN and Joel if this needs to be updated\n\n### UDA Parameters\n\nThis section provides information on the following:\n- Supported q types for UDA parameters\n- Distinguished parameters\n- Unsupported parameter types for UDA parameters\n\n#### Supported q types for UDA parameters\n\nThe supported q types for UDA parameters within the web interface are: AnyMap, Boolean, GUID, Byte, Short, Int, Long, Real, Float, Symbol, Timestamp, Month, Date, DateTime, Timespan, Minute, Second, Time, Char, String, Table, Dictionary, and Lists.\nRefer to\nDatatypes in kdb+\nfor details about these q types.\nA list of symbols is also supported when defined as follows:\nq\nCopy\n\n```\n.kxi.metaParam[`name`type`isReq`description!(`byCols;11 -11h;1b;\"Column(s) to count by.\")].\n```\n\nThe expected type of parameter is displayed under the data entry field.\nInformation\nMulti-type parameters\nThe web interface supports UDA parameters that can take on multiple types.  The parameter defaults to the first type listed when registering the UDAs metadata. An additional parameter must be used to cast multi-type parameters to their intended type within the logic of that UDA itself.\n\n#### Null and Infinity Values\n\nUser Defined Analytics (UDA) parameters support the inclusion of [nulls](#nulls) and [infinities](#infinities) for specific data types.\n\n##### Nulls\n\nIn q/kdb+, a null represents a missing or unknown value. Nulls can be entered as\n0N\nfor the following types:\nGUID\n,\nShort\n,\nInt\n,\nLong\n,\nReal\n,\nFloat\n,\nTimestamp\n,\nMonth\n,\nDate\n,\nDatetime\n,\nTimespan\n,\nMinute\n,\nSecond\n,\nTime.\n\n##### Infinities\n\nIn q/kdb+, infinity represents an unbounded or extreme value, useful for indicating values beyond any measurable range.\n- Enter positive infinity as0wand negative infinity as-0w.\n- Supported Types:Short,Int,Long,Real,Float,Timestamp,Date,Datetime,Timespan,Minute,Second,Time\nNote\nFormats for nulls and infinities\nWhile q expressions may support variants like 0Nh, 0W and 0Wh for nulls and infinities, these are not recognized when parsing from strings in the web interface. Therefore, use only 0N, 0w, and -0w for representing nulls and infinities in UDA parameters.\n\n##### Empty lists\n\nAll Compound types are data structures made up of other types, such as lists (1 2 3), dictionaries ((ab)! (1 2)), and nested lists ((1 2; 3 4)`), and general lists, support empty lists. These can be entered in the web interface as\n[]\n.\n\n#### Distinguished parameters\n\nWhen there are multiple databases running, distinguished parameters must be used to route the request to the DAPs where the UDA is defined. UDAs can accept distinguished parameters for routing, even if they are not included in the UDAâs metadata.\nThe distinguished parameters are labels, scope, startTS, endTS, inputTZ, outputTZ, table.\n\n##### Scope\n\nThe\nscope\nparameter is a JSON object containing a database name, and optionally either a tier name or as an advanced option, the name of a specific DAP instance. If you are querying from a query tab, and query environments are enabled, you need to append â-qeâ to your database name. For example:\nbash\nCopy\n\n```\n{\"assembly\":\"my-database-qe\"}{\"assembly\":\"trades\", \"tier\": \"hdb\"}{\"assembly\":\"trades\", \"dap\": \"hdb0\"}\n```\n\nRefer to\nScope\nfor further details.\n\n##### Labels\n\nThe\nlabels\nparameter is a JSON object with label names as keys and label values as values. DAPs are queried if they match at least one specified value for each label.\nbash\nCopy\n\n```\n{    \"region\": \"EU\",    \"exchange\": [\"automotive\", \"finance\"]}\n```\n\nRefer to\nDistinguished parameters\nfor further details on the parameters.\nInformation\nUnknown API error\nWhen distinguished parameters are not set, the error \"Querying database using (UDA) raised - Unknown API: Please try adding a distinguished parameter before running again\" appears because Insights cannot route the query. Set the parameters and try again.\n\n#### Unsupported parameter types\n\nParameters that can't be represented in the web interface are general lists (lists containing multiple types), functions, and nested types (that is, 77+t â memory-mapped list of lists of type t).\nThe supported temporal type is an atomic timestamp (-12h), which is of the form\nyyyy.mm.ddDhh:mm:ss.xxxxxxxxx\n.\nFor example, 2024.11.06D14:48:36.000000000. This is formatted in q as dateDtimespan which maps to 2024-11-06T14:48:36.000000000 when selecting a specific timestamp on the Query tab date picker.\nIf invalid parameter types are defined for a UDA, the following error is displayed:\n\n## Query output\n\nThe results of your query are displayed in the tabs of the output panel at the bottom of the Query window.\nSee here\nfor full details on the results and how they can be displayed.\n\n## Next Steps\n\n- Create ad hoc queries using the scratchpad\n- Perform further analysis and development in the scratchpad using q\n- Perform further analysis and development in the scratchpad using python\n\n## Further Reading\n\n- Query window introduction\n- Query APIs\n- Data query overview\n- Guided walkthrough on creating queries",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3860,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-909b4dff75ec",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/navigate-web-interface.htm",
    "title": "Navigate the Web Interface",
    "text": "\n# Navigate the  Web Interface\n\nkdb Insights Enterprise\nhas both a web interface and a\ncommand line interface\n. This page introduces you to the web interface, its features and functionality.\nThe web interface, described in this section, enables you to  manage databases and pipelines to ingest and store data, and to query and visualize that data.\n\n## Web Interface introduction\n\nAfter you\nlogin\nto the\nkdb Insights Enterprise\nweb interface, you are presented with the\nOverview\npage.\nThe web interface includes the following elements:\n- Left-hand menu\n- Ribbon menu\n- Quick ActionsandRecently Deployedpanels\nNote\nKeyboard navigation\n- Tab key to navigate to interactive elements\n- Enter to select\n- Esc to close menus.\n- Tab moves focus between interactive items, the arrow keys let you cycle through elements that the screen reader can read aloud.\n\n### Roles\n\nAccess to features in the kdb Insights Enterprise web interface is controlled by\nuser roles\n. You can only use the features assigned to your specific role. If you can't see certain functionalities mentioned in this documentation, please contact your system administrator.\nFor example, a user with the\ninsights.role.viewer.dashboard\nglobal role can only see views they're entitled to and cannot see/access any other artifacts (databases, tables, pipelines, packages).\nFor more information, refer to Web interface overview for\nViews-only users\n.\n\n## Left-hand menu\n\nThe following table describes the left-hand menu options and provides links to the functionality they support.\n\n| Icon | Description |\n| --- | --- |\n| Search | Perform a search to quickly locate entities in the left-hand panel. |\n| Overview | Return to theOverviewpage. |\n| Databases | Displays a list of available databases. From here you cancreate,deploy,exportordeletea database. |\n| Pipelines | Build,export,rename or deletea pipeline. |\n| Queries | Create,rename or deletea query. |\n| Views | Create,share,rename, ordeletea view. |\n| MANAGE |  |\n| Packages | Managepackages. |\n| Diagnostics | Troubleshoot pipeline and database errors withdiagnostics. |\n| RESOURCES |  |\n| Learning Hub | Access theKX learning hub. |\n| Documentation | Read theKX product documentation. |\n| Submit a Feature Request | Access theKX Feature Request Portal. |\n| Provide Feedback | Submit yourfeedbackon your experience with this product. |\n\n\n### Additional actions\n\nWhen you expand the menu on any of the left-hand menu items and hover over the entities listed you can see three dots to the right of the entity name indicating there are more actions available.\nClick on the three dots to see the following:\n- Databases actions\n- Pipelines actions\n- Queries actions\n- Views actions\n\n## Ribbon menu\n\nThe ribbon menu runs across the top of all web interface screens.\nClick\n+\nto open a menu of items for databases, pipelines, queries, and views.\nThe right-hand side of the ribbon menu contains the\nicons\ndescribed in the next section.\n\n### Ribbon menu icons\n\nThe\nribbon menu\ncontains the following icons:\n\n| Item | Description |\n| --- | --- |\n| System Notifications | Click the System Notifications icon to display alog of system errors and information messages. Clickfor a full-page view. |\n| System Information | Click theSystem Informationicon to display: thekdb Insights Enterpriseversion and license information, and the status ofencryption of data in transit. |\n| User Profile | Click the user profile button to display the name of the currently logged in user and access the option tologout. |\n\n\n## Overview\n\nThe Overview page is the home page for the\nkdb Insights Enterprise\nweb interface.\nOn the Overview page you see two main sections:\n- Quick Actions\n- Recently Deployed\n\n## Quick actions\n\nThe\nOverview\npage contains the following actions that allow you to build, import, query and visualize data.\n\n| Entity | Actions |\n| --- | --- |\n| Databases | Builddatabases and schemas.Import Datausing the import wizard to bring data into the system. |\n| Pipelines | Buildpipelines to ingest data into your databases. |\n| Queries | Buildqueries to explore your data using q, SQL, or python. |\n| Views | Buildviews to present and share your insights using views.View Demoswhich illustrate the power and versatility of the Views inkdb Insights Enterprise. |\n\n\n## Recently deployed\n\nThe lower section of the\nOverview\npage displays a table the provides detailed information about the most recently deployed databases and pipelines. A maximum of 15 rows is displayed.\n\n| Column Name | Description |\n| --- | --- |\n| Entity | The name of the deployed database or pipeline. For a complete list of all databases and pipelines on your environment, in addition to those that are deployed, go to thedatabase indexorpipeline index. |\n| Status | The status of the database or pipeline. For details see: -Database status values-Pipeline status valuesHover over the status value to display additional information. For example if anERROREDstatus is displayed, hovering over the status displays details of the error. |\n| Health | This column displays the health status of the database or pipeline. A value ofHealthyindicates there are no reported issues. A warning message is displayed when attention is required. Refer towarnings and remediationactions for details about these errors and how they can be resolved. |\n| Package | This is the name of the package that includes this database or pipeline. Click on the name to open thePackage Entitiesdialog. SeePackagesfor further details about packages. |\n| Uptime | The time since the database or pipeline was last deployed. |\n| Actions | Click the three dots in the last column to view a list of database or pipeline actions. See:Database actionsPipeline actions |\n\n\n## Web interface features\n\n\n### Supported browsers\n\nThe kdb Insights Enterprise web interface runs on the latest version of Chrome, Safari, Edge, and Firefox. We recommend that you use Chrome.\n\n### Search dropdown selection\n\nDropdown selectors, such as those used when creating an entity, feature a search option that streamlines selection and improves usability.\nIn searchable dropdowns like\nSelect a Package\n, shown below, begin typing in the input field to filter the list of available options. You can:\n- Use arrow keys or scroll bar to navigate the filtered list.\n- Press Enter or click to select an item.\n- Continue typing to refine the search or enter the full name directly.\n\n## Further reading\n\n- Create & manage databasesto store data\n- Create & manage pipelinesto ingest data\n- Create & manage queriesto interrogate data\n- Create & manage viewsto visualize data",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1077,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-add004ab00ee",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/query-scratchpad-python.htm",
    "title": "Scratchpad using Python",
    "text": "\n# Developing in the Scratchpad using Python\n\nThis page guides you through the execution of Python code and APIs using the\nScratchpad\n.\nThe Python environment in the Scratchpad offers a unique, self-contained location  where you can assign variables and create analyses that are visible only to you.\nThe following sections describe how to:\n- Execute code using Pythonin theScratchpad\n- Use KX defined Python libraries\n- Interact with custom codeusingPackages\n- Develop machine learning workflowsusing theMachine Learning Core APIs\n- Include q in your Python codeusingPyKX\n- Develop Stream Processor pipelinesusing theStream Processor API.\n- Debug Pipelines\nFor information on data visualization and console output in the Scratchpad see\nhere\n.\nNote\nIf you primarily develop in q seehere.\n\n## Execute Python code\n\nWhen executing code in the Scratchpad, keep these key points in mind:\n- The Python language is highlighted within the Scratchpad to indicate the use of Python code.\n- UseCtrl + EnterorCmd + Enter, depending on OS, to execute the current line or selection. You can execute the current line without selecting it.\n- ClickRun Scratchpadto execute everything in the editor.\n\n### Available Python Libraries\n\nThe Scratchpad supports the libraries currently installed within the Scratchpad image. The following is a selection of the most important data-science libraries included:\n- Machine learning libraries: keras, scikit-learn, tensorflow-cpu, xgboost\n- Data science libraries: numpy, pandas, scipy, statsmodels, h5py (numerical data storage), pytz (for working with timezones)\n- Natural language processing: spacy (parsing, tokenizing, named entity recognition), beautifulsoup4 (HTML and XML parsing)\n- Insights and kdb+ integration: kxi (the stream processor and assorted Insights-specific functionality), pykx (allows running q and Python code in the same process)\nFor the complete list, with versions, run the following in a scratchpad\nPython\nCopy\n\n```\nimport pkg_resources\"\\n\".join(sorted([package.key + \"==\" + package.version for package in pkg_resources.working_set]))\n```\n\nNote\nThe Scratchpad does not currently support the integration of your own Python library.\n\n## Interact with custom code\n\nBy using\nPackages\nyou can add custom code to\nkdb Insights Enterprise\nfor use in:\n- The Stream Processor when adding custom streaming analytics\n- The Database for adding custom queries.\nThe Scratchpad also has access to these APIs allowing you to\nload custom code\nand access\nuser-defined functions\nwhen prototyping workflows for the Stream Processor or developing analytics for custom query APIs.\nThe example below demonstrates a Scratchpad workflow that utilizes both the packages and\nUDF APIs\navailable within the Scratchpad.\nThese examples use UDFs named py_udf and map udf from a hypothetical ml package. These need to be updated to point to existing UDFs before they could actually be called.\nInformation\nCode snippet for ML Scratchpad example.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nPython\nCopy\n\n```\n    import kxi.packages as pakx    import pykx as kx    pakx.packages.list()    pakx.udfs.list()[['name', 'function', 'description']]    # Note: These examples use UDFs named py_udf and map udf from a hypothetical ml package.    # These would need to be updated to point to existing UDFs before they could be called.    # Load and use a UDF defined in Python    py_udf = pakx.udfs.load('py_udf', 'ml')    py_udf(kx.q('([]5?1f;5?1f)'), {'my_custom_param':1})    # Load and use a UDF defined in q    q_udf = pakx.udfs.load('map_udf', 'ml', '1.0.0')    q_udf(kx.q('([]5?1f;5?1f)'), {'my_custom_param':1})\n```\n\n\n## Develop machine learning workflows\n\nThe Scratchpad has access to a variety of machine learning libraries developed by the Python community, and by KX. Specifically, you can use the\nKX ML Python library\nto access its Model Registry functionality as well as some of the most used machine learning and NLP Python libraries:\n\n| library | version |\n| --- | --- |\n| beautifulsoup4 | 4.11.2 |\n| keras | 2.11.0 |\n| numpy | 1.22.4 |\n| pandas | 1.4.4 |\n| scikit-learn | 1.5.1 |\n| scipy | 1.7.3 |\n| spacy | 3.6.1 |\n| statsmodels | 0.13.1 |\n| tensorflow-cpu | 2.11.1 |\n| xgboost | 1.6.2 |\n\nThe example below shows how you can use this functionality to preprocess data, fit a machine learning model, and store this ephemerally within your Scratchpad session. (Note that storing models in this manner results in them being lost upon restarting the Scratchpad pod.)\nInformation\nCode snippet for ML Scratchpad example.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nPython\nCopy\n\n```\nimport pandas as pdfrom sklearn.linear_model import LinearRegressionimport pykx as kximport kxi.ml as mlml.init()# Generate random q data converting to Pandasraw_q_data = kx.q('([]asc 100?1f;100#50f;100?1f;y:desc 100?1f)')raw_pd_data = raw_q_data.pd()features = raw_pd_data.get(['x', 'x1', 'x2'])target = raw_pd_data['y']# Remove columns of zero variancedata = features.loc[:, features.var() != 0.0]data# Fit a model and produce predictions against original datamodel = LinearRegression(fit_intercept=True).fit(data, target)predictions = model.predict(data)# Create a new ML Registry and add model to the temporary registryml.registry.new.registry('/tmp')ml.registry.set.model(model, 'skmodel', 'sklearn', '/tmp')# Retrieve and use the model validating it is equivalent to persisted modelsaved_model = ml.registry.get.predict('/tmp', model_name = 'skmodel')all(saved_model(data) == model.predict(data))\n```\n\n\n## Include q code in Python development\n\nWhen your workflow requires it, you can tightly integrate q code and analytics within your Python code using\nPyKX\n. This library, included in the Scratchpad, allows you to develop analytics in q to operate on your Python or q data.\nBy default, database queries following the\nquerying databases guide\nreturn data as a\nPyKX\nobject rather than a Pandas DataFrame. Therefore, it may be more efficient to perform data transformations and analysis using q in PyKX before converting to Pandas/Numpy for further development.\nThe following basic example shows usage of the PyKX interface to interrogate data prior to conversion to Pandas:\nInformation\nCode snippet for PyKX usage within Python Scratchpad.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nPython\nCopy\n\n```\nimport pandas as pdimport pykx as kx# Generate random q dataqtab = kx.q('([]sym:100?`AAPL`MSFT`GOOG;prx:10+100?1f;vol:100+100?10000)')# Query data using sql statement to retrieve AAPL data onlyaapl = kx.q.sql(\"SELECT * from $1 where sym='AAPL'\", qtab)# Calculate vwap for stocks based on symbolvwap = kx.q.qsql.select(qtab, {'price' : 'vol wavg prx'}, by='sym')# Convert vwap data to Pandasvwap.pd()\n```\n\n\n## Develop Stream Processor pipelines\n\nThe Scratchpad can be used as a prototyping environment for Stream Processor pipelines, allowing you to easily publish batches to a pipeline, capture intermediate result, and step through your functions. Access to the pipeline API gives you the ability to simulate production workflows and test code logic prior to moving development work to production environments. This is facilitated through use of the\nStream Processor API\n.\nPipeline(s) run in the scratchpad are not listed under\nPipelines\non the\nOverview\npage, and must be managed from within the scratchpad. They are run in the scratchpad process, the same as when deployed using Quick Test.\nThe following example creates a pipeline for enriching weather data. It contains an error that can easily be debugged in the scratchpad.\n- Copy the following code to your Scratchpad to create the pipeline.PythonCopy# PyKX and sp are required for all pipelinesimportpykxaskxfromkxiimportsp# This pipeline will be enriching a stream of temperature and humidity records# with the dew point and apparent temperature.# The input to this function is a q table, which can be read and modified from Python.defenrich_weather(data):data['dewpoint'] = data['temp'] -.2* (100- data['humidity'])data['heatIndex'] =.5* (data('temp') +61+ (1.2* (data['temp'] -68)) + data['humidity'] *.094)returndata# Only one pipeline can be run at once, so the `teardown` is called before running a new pipeline# Because the scratchpad process hosting the pipeline is already running, there is no deployment step needed, just a call to sp.runsp.teardown(); sp.run(# While developing a pipeline, the fromCallback reader lets you send batches one at a time# This creates a q function `publish` to accept incoming batches.sp.read.from_callback(\"publish\")# Pipeline nodes are strung together using the | operator# To decode a CSV, strings coming from Python must be cast to q strings| sp.map(lambdax: kx.CharVector(x))# This parses a CSV file from a string to a q table.# The argument maps each column to a q data type| sp.decode.csv({'time':'timestamp','temp':'float','humidity':'float'})| sp.map(enrich_weather)# The result is written to a variable called `out` in the q namespace| sp.write.to_variable('out'))# Send a batch of data to the pipeline.# As this is an example of how to debug a pipeline, running this will throw an error.# Note: kx.q is used to evaluate q from Python. In this case, getting a reference to the function `publish`, and passing it an argument.kx.q('publish', (\"time,temp,humidity\\n\"\"2024.07.01T12:00,81,74\\n\"\"2024.07.01T13:00,\\\"82\\\",70\\n\"\"2024.07.01T14:00,83,70\"))\n- Publishing to the pipeline throws the error shown below.textCopyError: Executing code using (Python) raised - QError('TypeError(\"\\'Table\\' object is not callable\") - error in operator: map_1')The last part,error in operator: map_1, indicates the error is in the map node. To debug the function, cache the incoming batch to a global,\nthen redefine the function, rerun the pipeline, and resend the batch.PythonCopydefenrich_weather(data):globalcachecache = datadata['dewpoint'] = data['temp'] -.2* (100- data['humidity'])data['heatIndex'] =.5* (data('temp') +61+ (1.2* (data['temp'] -68)) + data['humidity'] *.094)returndata\n- After resending the batch, evaluatingcachedisplays the batch to the console. It looks correct so far.textCopytime                          temp humidity dewpoint----------------------------------------------------2024.07.01D12:00:00.000000000 81   74       75.82024.07.01D13:00:00.000000000 82   70       762024.07.01D14:00:00.000000000 83   70       77\n- After assigningdata = cache, it's possible to step through the code. The error occurs when evaluating the last line, and\nselecting and evaluating sections of that line. The error comes fromdata('temp'). Updating this todata['temp']resolves the error.PythonCopydata = cachedata['dewpoint'] = data['temp'] -.2* (100- data['humidity'])data['heatIndex'] =.5* (data('temp') +61+ (1.2* (data['temp'] -68)) + data['humidity'] *.094)\n- You can now redefine the corrected function, rerun the pipeline, pass it multiple batches, and inspect the output.PythonCopykx.q('publish', (\"time,temp,humidity\\n\"\"2024.07.01T12:00,81,74\\n\"\"2024.07.01T13:00,\\\"82\\\",70\\n\"\"2024.07.01T14:00,83,70\"))kx.q('publish', (\"time,temp,humidity\\n\"\"2024.07.02T12:00,87,73\\n\"\"2024.07.02T13:00,90,74\\n\"))# The output is a q variable in the global namespace,# so it must be referenced via kx.qkx.q('out')textCopytime                          temp humidity dewpoint heatIndex--------------------------------------------------------------2024.07.01D12:00:00.000000000 81   74       75.8     82.2782024.07.01D13:00:00.000000000 82   70       76       83.192024.07.01D14:00:00.000000000 83   70       77       84.292024.07.02D12:00:00.000000000 87   73       81.6     88.8312024.07.02D13:00:00.000000000 90   74       84.8     92.178\n\n## Debugging pipelines\n\nPipelines written in the web interface and run via Quick Test are evaluated in the Scratchpad process. Therefore, any global variables cached in a web interface pipeline are available in the Scratchpad, and any global variables defined in a Scratchpad are available in a pipeline process.\nYou can step through web interface pipeline functions in-place, as the hotkeys for evaluating code in the pipeline editors.\n\n## Known issues\n\n- STDOUT and STDERR (including the effects ofprintstatements) are not shown in the console.\n- Statements that open a prompt, likehelp()orimport pdb; pdb.set_trace(), make the Scratchpad unresponsive.\n\n## Further reading\n\n- Data query overview\n- Create queries using the query builder\n- Perform further analysis in the Scratchpad using q\n- Create advanced Visualizations",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1712,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-d3e8ab3791eb",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/get-data-protocol-buffers.htm",
    "title": "Ingest Protocol Buffers Data",
    "text": "\n# Ingest Protocol Buffers Data\n\nThis page provides a walkthrough to guide you through the steps to create a pipeline to ingest\nProtocol buffers\ndata into\nkdb Insights Enterprise\n.\nProtocol buffers\nare Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. We have provided a\ncrime\ndata set for use in this walkthrough, which is a record of events from March 31\nst\n2022 in NYC, organized by precinct, including location coordinates of the call, dispatch times, and a description of the crime.\nInformation\nNo prior experience with q/kdb+ is required to build this\npipeline\n.\nBefore you build your pipeline you must ensure the\ninsights-demo\ndatabase is created, as described in\nthis walkthrough\n.\n\n## Create a pipeline\n\n- On theOverviewpage, chooseCreate newunderPipelines:\n- In theCreate Pipelinedialog set the following values:SettingValuePipeline NamecrimeSelect a Packageinsights-demoIfinsights-demois not available for selection (if it has already been deployed), open the packages index and selectTeardownfrom the actions menu besideinsights-demo.Ifinsight-demodoes not appear on packages list, create it as described inthis walkthrough.\n- Next, add the nodes to the pipeline, as described in the following sections;Expression node,Decoder node,Transform node,andWriter node\n\n### Add an Expression node\n\nThis node pulls in the crime data.\n- Click-and-drag anExpressionnode from theReaders, into the workspace.\n- Click on theExpressionand add the followingqcode to theConfigure Expression Nodepanel. This code pulls in the crime data set.qCopyURL:\"https://code.kx.com/kxiwalkthrough/data/crime.msg\";resp:.kurl.sync[(URL;`GET;(::))];if[200<> first resp;' last resp];\"\\n\"vs last resp\n- ClickApplyto apply these changes to the node.\n\n### Add a Decoder node\n\nThis node reads in this data in a format compatible with kdb+.\n- Click-and-drag theProtocol Buffersdecoder node from the list ofDecodersinto the central workspace, and connect it to theExpressionnode.\n- Select theDecodernode.\n- Add the following settings to the right-hand property panel:SettingValueMessage NamecrimeMessage Definitioncopy the JSON code block belowAs ListdisabledJSONCopysyntax =\"proto3\";message crime {uint64 event_id =1;string incident_time =2;uint64 nypd_precinct =3;string borough =4;string patrol_borough =5;uint64 call_x_geo =6;uint64 call_y_geo =7;string radio_code =8;string description =9;string crime_in_progress =10;string call_timestamp =11;string dispatch_timestamp =12;string arrival_timestamp =13;string closing_timestamp =14;double latitude =15;double longitude =16;}\n- ClickApplyto apply these values to the node.\n\n### Add a Transform node\n\nThis node transforms the crime data fields to kdb+ types compatible with the\ninsights-demo\nkdb+ database.\n- Click-and-drag theApply Schemanode from the list ofTransformnodes and connect it to theDecodernode.insights-demohas a predefined schema forcrimedata which transforms the data to a kdb+/q format.\n- Select theApply Schemanode.\n- In theConfigure Apply Schema Nodesection leave theData Formatsetting set toAny.\n- Click theLoad Schemaicon, select theinsights-demodatabase and thecrimetable from the dropdowns as shown below.\n- ClickLoad.\n- ClickApplyto apply changes to the node.\n\n### Add a Writer node\n\nThis node writes the transformed crime data to the\nkdb Insights Enterprise\ndatabase.\n- Click-and-drag thekdb Insights Databasenode from the list ofWriternodes into the workspace and connect it to theTransformnode.\n- Select theWriternode and add the following settings to the right-handConfigure kdb Insights Databaseproperty panel:SettingValueDatabaseinsights-demoTablecrimeWrite Direct to HDBNoDeduplicate StreamYesSet Timeout ValueNo\n- ClickApplyto apply these settings to the node.\n\n## Review the pipeline\n\nThe final pipeline looks like this:\n\n## Save the pipeline\n\nYou now need to save the\nPipeline\n.\n- ClickSave.\n\n## Deploy the pipeline\n\nDeploy the package containing the database and pipeline in order to ingest the data into the database.\n- Go to thePackage Indexpage and click on the three dots besideinsights-demopackage and clickDeploy.NoteIt may take Kubernetes several minutes to release the necessary resources to deploy the pipeline.If the pipeline or database in the package are already deployed you must tear it down. Do this on thePackage Indexpage by clicking on the three dots besideinsights-demopackage and clickTeardown.\n- Check the progress of the pipeline under theRunning Pipelinespanel of theOverviewtab. The data is ready to query whenStatus=Running.\nWarning\nOnce the pipeline is running some warnings may be displayed in the\nRunning Pipelines\npanel of the\nOverview\ntab. These are expected and can be ignored.\n\n## Next steps\n\nNow that your pipeline is up and running you can:\n- Query the crime data\n- Visualize the crime data\n- Ingest other data\n\n## Further reading\n\nUse the following links to learn more about:\n- Import wizard\n- Pipelines index\n- Building a pipeline\n- Pipeline operators\n- Troubleshooting pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 686,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-75cdb569e636",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/query-scratchpad.htm",
    "title": "Scratchpad",
    "text": "\n# Scratchpad\n\nThis page describes how to use the Scratchpad panel, in the Query window to query the database, develop and debug pipelines, or run arbitrary q and Python code.\nOnce you are familiar with the\nfeatures\nand\nlayout\nof the Scratchpad you can proceed with using it to execute the\nfunctionality\n.\nResults of your queries can be viewed in the\nOutput Panel\nat the bottom of the screen.\nIf these panels are not visible, click\nView\n, in the top left-hand corner, and toggle\nScratchpad Panel\nand/or\nOutput Panel\nto display them.\n\n## Scratchpad features\n\nA scratchpad is a temporary q process that runs in a Rocky Linux container. Each user has their own scratchpad. While you can save the contents of the editor, any in-memory or on-disk state is only retained until the scratchpad session ends.\nThis section describes:\n- Scratchpad resources\n- Disk operations\n- Scratchpad restart\n- Logs\n- Keycloak permissions\n\n### Scratchpad resources\n\nThe CPU and memory resources for the Scratchpad can be configured within your\nvalues.yml\n. See\nScratchpad resources\nfor details. Any changes made to these resources apply to all users.\nA Scratchpad is limited (by default) to 500 MiB of RAM and 50 MiB of disk. When exceeded, the Scratchpad process shuts down and restarts. To optimize performance, limit the amount of data returned in your query; for example:\nselect[10000] from myTable\nto return 10,000 rows of data.\n\n### Disk operations\n\nThe scratchpad environment includes a 250 MB disk mounted to\n/tmp\nthat can be used for temporary file storage. This folder is ephemeral, and is cleared when the Scratchpad process terminates due to logout, reset or crash. The rest of the file system is read-only.\nFor example, if you install Python packages in the Scratchpad; these must be installed to\n/tmp\n, which is the only writable location. You must be aware of the disk space limit, as installing packages or writing too much data may exhaust the available space.\nPipelines also use on-disk storage. This means both pipeline quick tests and pipelines written in q or Python can write to /tmp, and excessive usage can trigger a Scratchpad restart.\n\n### Scratchpad restart\n\nScratchpads terminate after one hour of inactivity, or if a resource limit is exceeded. After a Scratchpad restart, any in-memory or on-disk state is lost.\n\n### Logs\n\nEach user gets their own Scratchpad. Each Scratchpad is a q process running in a pod. Use\nkubectl get pod <pod-name>\nto get information about your Scratchpad pod.\nScratchpad pods are named\n<release-name>-scratch-<first 7 characters of user name>-<hash>\n(e.g.\ninsights-scratch-jsmith-c5415851ba45a47c4f7c50bc418dae46-vdhcc\n).\nAn example of the output returned when you run\nkubectl get pod insights-scratch-jsmith-c5415851ba45a47c4f7c50bc418dae46-vdhcc\nis shown below.\ntext\nCopy\n\n```\nNAME                                                              READY   STATUS    RESTARTS       AGEinsights-scratch-jsmith-c5415851ba45a47c4f7c50bc418dae46-vdhcc    1/1     Running   0              15m\n```\n\nThe following information is displayed:\n- NAME- The name of the pod (example-pod).\n- READY- The number of containers in the pod that are ready (1/1).\n- STATUS- The status of the pod (Running).\n- RESTARTS- The number of times the pod has been restarted (0).\n- AGE- How long the pod has been running (5m).\n\n### Scratchpad log capture and retrieval (BETA)\n\nNote\n[CDATA[\n\n\t\t]]\nBeta - For evaluation and trial use only\nScratchpad log capture and retrieval currently in beta.\n- Referhere to the standard termsrelated to beta features.\n- We invite you to use this beta feature and to provide feedback using theIdeas portal.\n- During deployment, this feature is disabled by default.\nThe logs written by\nshow\nstatements in q and\nprint\nstatements in Python, as well as messages written to STDERR, are written to a buffer that can be queried. This captures log messages for code run in the scratchpad editor, pipeline editors, and pipelines deployed using quick test. It does not include show statements in qsql database queries or UDAs.\nWarning\n[CDATA[\n\t\t]]\nKnown issue with large log volumes\nLarge log volumes, such as writing over 100,000 bytes at a time or continuous logging at over 5,000 bytes a second, can cause the scratchpad to hang. As such this is still a beta feature, and must be explicitly enabled by running the q command\n.com_kx_edi.cacheLogs[]\n.\n\n\t\tUsing\nshow\nor\n0N!\nrather than the file handles 1 and 2 when logging in q can help avoid this bug, as they limit the length of each line, and the number of rows.\n- Logged messages are not available in the query that generates them, but are available in subsequent queries, with a slight delay for large messages.Copyq)show `Hello`; .com_kx_edi.getLogs[]()q).com_kx_edi.getLogs[]\"`Hello\"\n- When the log buffer reaches one million characters, the oldest half is discarded.\n\n#### Enable log capture and retrieval\n\nThis is a beta feature and is disabled by default. If you attempt to capture or retrieve logs while it is disabled an error is displayed.\nTo enable this feature, enter\n.com_kx_edi.cacheLogs[]\nin the Scratchpad, as shown below.\nInformation\nIf the Scratchpad resets the logging feature has to be re-enabled.\n\n#### Capture output written to STDOUT and STDERR\n\nTo use this feature, enter the appropriate command in the Scratchpad panel. Commands can be written in either q or Python.\nq\npython\n\n| command | Description |\n| --- | --- |\n| .com_kx_edi.getLogs[] | Get all STDOUT and STDERR logs for the scratchpad process from the time that logging is enabled |\n| .com_kx_edi.clearLogs[] | Clear all STDOUT and STDERR logs.. |\n\n\n| command | Description |\n| --- | --- |\n| scratchpad_logs.get_logs() | Get all STDOUT and STDERR logs for the scratchpad process from the time that logging is enabled. |\n| scratchpad_logs.clear_logs() | Clear all STDOUT and STDERR logs. |\n\nThe following example demonstrates how to run\ngetlogs\nusing the q syntax. Enter the command in the Scratchpad, then click\nCTRL+D\nto display the results in the Console.\n[CDATA[\n        ]]\n\n### Keycloak Permissions\n\nThe following Scratchpad roles are available for fine-grained permissions.\n\n| Role | Description |\n| --- | --- |\n| insights.scratch.display | Evaluating q and Python expressions, and displaying or visualizing the results |\n| insights.scratch.data | Querying a database using the query builder. |\n| insights.scratch.sql | Querying a database SQL. |\n| insights.scratch.qsql | Querying a database using the qsql. |\n| insights.scratch.pipeline.test | Running \"Quick Test\" pipelines in the Scratchpad |\n\nSee\nkeycloak permissions\nfor further details on roles.\n\n## Scratchpad Layout\n\nThe Scratchpad is the top right-hand section of the\nQuery window\n.\nIf this panel is not visible, click\nView\n, in the top of the window, and toggle\nScratchpad Panel\nto display it.\nThis panel is comprised of the following elements:\n- Editor- The Scratchpad is a scrollable editor window used to enter your code. The 2 tabs at the top right-hand corner of the window allow you to toggle betweenqandPythoneditors.\n- Run All- This button is used to run all the code entered in the editor. Theoutputis displayed in the tabs below the editor. To run only the current line or highlighted selection, useCtrl + DorCtrl + Enteron Windows, orâEnterorâEon mac.\nHotkey for quick access to documentation\nThe scratchpad supports using the\nCtrl+/\nhotkey to open documentation related to the selected operators, keywords, and values within single-letter namespaces.\nNote\nIf youâre having trouble scrolling in the Scratchpad while using Firefox, make sure that scrollbars are set to always be visible.\n\n## Scratchpad functionality\n\nThe\nScratchpad\nsupports the following:\n- Creating ad hoc queries\n- Viewing query results\n- Developing and running q code in a Scratchpad\n- Developing and running python code in a Scratchpad\n- Use query APIs to interact with the database\n- VSCode Integration\n\n## Create ad hoc queries in Scratchpad\n\nTo perform an ad hoc query using the Scratchpad:\n- Create a query using theQuery panel.\n- When analyzing the data retrieved from yourkdb Insights Enterprisedatabase, you can use either q or Python.Click on either theqorPythontab on the Scratchpad, depending on your language preference and enter your query. Note that when you usePython, only the first line of code is processed.The following example shows a query which returns the results to an output variable called weather. This variable is then used in theqquery in the Scratchpad to perform a further ad hoc query to refine the results.\n- ClickRun All.\n- View and analyze the results in theoutput panel.\nNote\nRunning a selected line of code context is set by preceding lines, similar to\n\\d .myContext\nor\nsystem \"d .myContext\"\n. The global context is used in the absence of a preceding line.\nRead more about using\nq\nand\nPython\nin your queries.\n\n## Query output\n\nClicking\nRun Query\nin the\nQuery Panel\n, or\nRun All\nin the\nScratchpad\n, returns the results of data queried using the\nQuery panel\nor by\nad hoc queries\nin the Scratchpad. You can view these results in one of the tabs at the bottom of the\nQuery Window\n:\n- Console\n- Table\n- Visual\nIf this panel is not visible, click\nView\n, in the top left-hand corner, and toggle\nOutput Panel\nto display it.\nBy default, data is displayed in the\nConsole\ntab. To display the results in another tab, switch the tab you want and click\nRun Query\nagain and\nRun All\nif you have an ad-hoc query.\nYou can specify a custom timeout for queries executed in both the Query Panel and the Scratchpad, see\nExecution Timeout\nfor details.\n\n### Expand Output display\n\nTo expand the output panels to full screen:\n- Click the icon in the top right-hand corner of the output panels.When the panel is expanded, all other panels on the Query window are hidden.\nTo restore the output panel to normal size:\n- Click the icon in the top right-hand corner of the expanded window.This restores the view of the other panels.\nThe following example shows how to use the expand icon on the Table panel.\n\n### \n\nReset or cancel a query\nTo reset or cancel a query in the Query Panel or Scratchpad, click\nReset\nor\nCancel\nin the bottom-left corner of the window.\n- Resetrestarts the scratchpad process, clearing the entire scratchpad state including any variables you have defined in q or Python. Use this to reset an unresponsive scratchpad, free up memory, recover from accidental overwrites, or ensure your code runs without relying on previously defined global variables. Note that it take ups to 30 seconds for the scratchpad ton become responsive after restarting.\n- Cancelapplies to the operation the scratchpad is currently processing, whether that is a DB query, scratchpad query, test deploy, or even a query from VS Code or a different browser. Canceling long-running queries may not free up resources, seeCancel button limitationfor details.\nReset and cancel both clear all pending scratchpad operations. This ensures that if a problematic request was sent repeatedly, restarting the scratchpad wonât immediately re-enter the same faulty state.\nNote\nCancelled database queries may continue running on the database for up to thirty seconds.\n\n### Execution Timeout\n\nUse the\nExecution Timeout\nfield, in the\nOutput Panel\n, to set a custom timeout for queries executed in both the\nQuery Panel\nand\nScratchpad\n.\nYou can specify a duration in\nSeconds\n,\nMinutes\nor\nHours\n.\n- If the timeout is insufficient, an error is displayed indicating that an error has occurred.\n- When a custom timeout value is set, that value is used for subsequent queries from that scratchpad.\n- Each new scratchpad you open defaults to 30 seconds execution timeout.\nImportant\nManaging Long-Running Queries and Session Timeouts\n- Cancel Button LimitationThe Scratchpadcancel buttoncannot cancel a query that has been sent to the database. The scratchpad becomes responsive again, but the query continues running on the DAP (Data Access Process) until it completes or times out. The scratchpad does not receive the result of cancelled DB queries..\n- SSO Session Timeout RiskFor long-running queries, ensure your Keycloak SSO session settings are configured to exceed the expected query duration. Otherwise, you may be signed out ofkdb Insights Enterprisebefore the query completes, even if the timeout value is sufficient.The following Keycloak session properties must be set to values longer than the expected query time (setting these to zero may not be sufficient):SSO Session IdleSSO Session MaximumClient Session Idle\n- Scratchpad Inactivity TimeoutYou must also ensure the Scratchpad Managerâs inactivity timeout is longer than the expected query duration. Any time spent waiting for a query response is counted as inactivity, which could cause the session to be terminated prematurely.To configure this:At install time, set the following in thevalues.yamlfile:values.yamlCopykxi-scratchpad-manager:scratchpadMaxInactivityMins: 120To update a running instance, edit the equivalentSCRATCHPAD_MAX_INACTIVITY_MINSenvironment variable in the Scratchpad deployment.\n\n### Console\n\nThe console is the default display for\nquery output\n. Right-click in the console for additional options.\nThese options are described in the following table.\n\n| Option | Description |\n| --- | --- |\n| Clear | Clears the console. |\n| Toggle Source Expressions | Toggles the display of the code expression in the console. |\n\n\n### Table\n\nThe\nTable\ntab is a more structured display for\nquery output\nwith additional filter options. Results are paged and you can\nfilter\nthem.\n\n| Setting | Description |\n| --- | --- |\n| First | Display the first n items. n depends on the number selected from the # of results drop-down. |\n| Last | Display the last n items. n depends on the number selected from the # of results drop-down. |\n| Random | Display a random selection of items, without replacement. |\n| # of results drop-down | Select how many first items to display (10, 100, 1000, or 10,000). |\n\nTo view results, you can either:\n- ClickRun Allon the Scratchpad\n- ClickCtrl-EnterorCtrl-Din the main q or Python editor to execute the current line or selection\n\n#### Column Filter\n\nData columns, in the\nTable view\ncan be sorted in ascending and descending order by clicking on the column header.\nAdditional filtering options are available from the triple-bar menu, above the data.\n- UseAND/ORstatements alongside operatorsContains,Not contains,Equals,Not equal,Starts with, orEnds withfor more comprehensive filtering.\n- For example, the results below are filtered on theneighborhoodcolumn where itContainthe valueTot. Filter criteria can be combined usingANDandOR.\n\n### Visual\n\nThe\nVisual\ntab plots\nquery output\nas a chart.\n- You can choose betweenBubble,LineorBarcharts, from they-axis settings.\n- Visual settingsgive you options to configure the features of your chart.\nThe following example shows query results displayed in a bubble chart.\nThe following table defines the data display options:\n\n| Setting | Description |\n| --- | --- |\n| First | Displays results from the start of the data source (page 1). |\n| Last | Displays results from the end of the data source (last page). |\n| Random | Selects a random point in the data set to display results. |\n| # of results drop-down | This drop-down list lets you filter paged results by one of these number of pages results per page. |\n\n\n#### Visual Settings\n\nYou can define the following settings using the\nVisual settings\nmenu. Click the toggle button to display/hide the settings menu, and click on each setting to expand the options available.\n- X-Axis Settings\n- Y-Axis Left Settings\n- Y-Axis Right Settings\n- Chart Settings\n\n##### X-Axis settings\n\nThe\nX Axis Settings\ndefine the x-axis data variables to chart and how they are formatted and displayed.\nThese following settings are configurable:\n- Data Point: The data column to plot on the x-axis of the chart. Select from the dropdown to change the value.\n- Number of Ticks: The number of tick labels to display in the x-axis.\n- Range\n- Gridlines\n- Format\n\n###### Range\n\n\n| Setting | Description | Default |\n| --- | --- | --- |\n| Use min max | Enable the use of Min and Max to build the axis. If these are unchecked the Min and Max values, set below, are ignored. | Disabled |\n| Min | Define the minimum value for the axis. |  |\n| Max | Define the maximum values for the axis. Charted data outside of the set Min and Max range is not displayed. If all data falls outside the set range, the chart is blank. |  |\n\n\n###### Gridlines\n\n\n| Setting | Description | Default |\n| --- | --- | --- |\n| Offset Gridlines | When enabled, sets gridlines between tick values. | Disabled |\n| Gridlines Color | Set the color of the gridlines used in the chart. | #000000 |\n| Gridlines Opacity | Defines the opacity of gridlines from0(transparent) to100(opaque). | 15 |\n\n\n###### Format\n\n\n| Setting | Description | Default |\n| --- | --- | --- |\n| Display | When enabled (for x-axis), displays tick values. | Enabled for x-axis. |\n| Begin at Zero | When enabled, plotted values start at 0 for the x or y-axis. | Disabled |\n| Numeric Format | Select betweenNumber,Smart NumberandFormatted Number. | Numeric |\n| Decimal Places | Define precision of y-axis labels. | 2 |\n| Font size | Define font-size of tick labels. | 12 |\n| Prefix | Add a text element before the y-axis label. |  |\n| Suffix | Add a text element after y-axis tick label. |  |\n| Hide Trailing Zeroes | When enabled, trailing zeroes are hidden from axis labels. | Disabled |\n\n\n##### Y-Axis Settings\n\nThe left and right\nY Axis Settings\ndefine the data variables to chart and how they are formatted and displayed.\nThese following settings are configurable:\n- Enabled: The left y-axis is enabled by default. The right y-axis is disabled by default. Enable it to have two y-axes on your chart.\n- Data Point: The data columns to plot on this y-axis of the chart. Select single or multiple columns from the dropdown to change the value.\n- Chart Type: Select from one of the following chart types;Bubble,LineorBar. You can set a different chart type for the left and right y-axis. You can configure the following settings, for each chart type:Displaysettings includingColor PaletteNumber of Ticks: Define the number of tick labels to display in the y-axis.RangeGridlinesFormatAnimationOverlay\n\n###### Display\n\nThe\nDisplay\nsettings\nvary based on the chart type selected. The following screenshot shows the Display settings for a Bubble chart.\nThe following table describes the Display settings and indicates the chart types they apply to.\n\n| Setting | Description | Bubble | Bar | Line |\n| --- | --- | --- | --- | --- |\n| Radius Data | Choose between a data source variable orFixed Sizebubbles. | y |  |  |\n| Radius Scaling | Set bubble size. | y |  |  |\n| Color | Set the color of the bubble, bar, or line. | y | y | y |\n| Opacity | Set the opacity of the bubble, bar, or line in the range 0 (transparent) to 100 (opaque). | y | y | y |\n| Fill | Enable to fill the area of the line to the origin of the x-axis. |  |  | y |\n| Scale on Zoom | Enable this option forbubblesto scale on zoom. When set the bubble size increases in size on zoom in and decreases on zoom out. | y |  |  |\n| Bar Percentage | Toggle betweenPercentageorFixed Widthfor bar width. |  | y |  |\n\n\n###### Color Palette\n\nUse the\nColor Palette\nsettings\nto set the color for data points on the chart. Click on a color to change it or enter a Hex color value.\n\n##### Chart Settings\n\nThe\nChart Settings\ndefine crosshair and overlay behaviors.\nThese settings are described in the following table.\n\n| Setting | Description | Default |\n| --- | --- | --- |\n| Show Crosshairs | When enabled, this adds a crosshair to the chart. | Enabled |\n| Show Coordinates | When enabled, this displays axis values for cursor position in chart. | Enabled |\n| Snap Crosshair to Data | When enabled, the Crosshair position locks to the y-axis value relative to x-axis position. | Enabled |\n| Show all data points | When enabled, the Crosshair displays all values in a tooltip at the x-axis position. | Disabled |\n| Group tooltip by layer | When enabled, the display of chart values is grouped in the tooltip by data layers. | Disabled |\n\n\n## VSCode Integration\n\nTo work with existing code bases, versioned code, and to easily organize your code, the\nKX extension\nfor VS Code extension offers an alternative to the Insights Scratchpad Web Interface. This allows you to connect to the same q Scratchpad process accessible from the web interface, so all variables are shared between the two interfaces. It supports evaluating q (from .q, or .quke files) or Python (from .py files). The same options for querying the database are available from the extension.\n\n## Next Steps\n\n- Perform further analysis and development in the Scratchpad using q\n- Perform further analysis and development in the Scratchpad using python\n- Create advanced Visualizations\n\n## Further Reading\n\n- Query window introduction\n- Create queries using the query builder\n- Guided walkthrough on creating queries",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3518,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-7b30ed2cf210",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/use-language-interfaces.htm",
    "title": "Use Language Interfaces",
    "text": "\n# Usekdb Insights EnterpriseLanguage Interfaces\n\nThis page provides an example which demonstrates how to deploy the\nkxi-db\npackage, publish data to it, and query the data through secure, authenticated endpoints within\nkdb Insights Enterprise\n.\nThe goal is to deploy the package to the\nkdb Insights Enterprise\nbase application to enable data ingestion, persistence, and querying.\n\n## Deploying the package\n\nDeploy the package on top of the base instance of the\nkdb Insights Enterprise\nin your cluster.\nTo deploy the package, download the\nkxi-db-1.0.0.kxi\nfrom the release repository and deploy it to your cluster using the following command:\nshell\nCopy\n\n```\nkxi pm push kxi-db-1.0.0.kxikxi pm deploy kxi-db\n```\n\n\n## Client integration\n\nAfter deployment,\nkdb Insights Enterprise\nand\nkxi-db\nare ready to ingest data.\nA client must be enrolled to allow data to be published through the authenticated and secure RT entry-point. Clients can be either users or service accounts. Users and service accounts are created in Keycloak. For more information, refer to the\nKeycloak\ndocumentation.\nThe diagram below highlights the key components that allow the flow of data into the system from external sources. To do this securely it's necessary to utilize the Information Service  to register clients and to allow them to understand the endpoints. It must also authorize itself and publish the data to the appropriate location.\n\n## Client enrollment\n\n\n### Prerequisite\n\nA service account or user account must be created in\nKeycloak\nand assigned the necessary roles. Your system administrator can set up the user or service account.\nInformation\nKeycloak roles\nHere is more information on the relevant\nKeycloak roles\n. To enroll a client to publish data to an RT stream, the\ninsights.client.create\nand\ninsights.client.delete\nroles are necessary for the user or service account.\n\n### Authenticating as a user\n\nYour system administrator can use the below commands to create a user:\nbash\nCopy\n\n```\nkxi user create demouser --password demoUserPassword --admin-password myAdminPasswordINSIGHTS_ROLES=\"insights.client.create,insights.client.delete\"kxi user assign-roles demouser --roles $INSIGHTS_ROLES --admin-password myAdminPassword\n```\n\nThis user can now authenticate with the KXI CLI.\nbash\nCopy\n\n```\nkxi auth login # This command launches the browser where demouser can authenticate\n```\n\n\n### Authenticating as service account\n\nYour system administrator can use the below commands to create a service account:\nbash\nCopy\n\n```\nCLIENT_ID=svc1INSIGHTS_ROLES=\"insights.client.create,insights.client.delete\"kxi user create-client $CLIENT_ID --admin-password myAdminPasswordkxi user assign-roles service-account-$CLIENT_ID --roles $INSIGHTS_ROLES --admin-password myAdminPasswordCLIENT_SECRET=$(kxi user get-client-secret $CLIENT_ID --admin-password myAdminPassword)\n```\n\nThe admin can provide the above\nCLIENT_ID\nand\nCLIENT_SECRET\nto any user for configuring their KXI CLI using the below commands.\nInformation\nService account ID and secret\nYour $CLIENT_ID and $CLIENT_SECRET should be inputted to the 'Service account ID' and 'Service account Secret' fields.\nbash\nCopy\n\n```\nkxi configureProfile type (enterprise, microservices) [enterprise]:Hostname []: https://{INSIGHTS_HOSTNAME}/Namespace []: namespaceService account ID []: Service account Secret (input hidden):Re-enter to confirm (input hidden):Encryption enabled (true/false) [true]:CLI successfully configured, configuration stored in /home/username/.insights/cli-config\n```\n\nAfter configuring the kxi with the service account credentials, any user can authenticate as a service account\nbash\nCopy\n\n```\nkxi auth login --serviceaccount\n```\n\nFor further details on authentication, refer to the\nAuthentication\ndocumentation.\n\n### Enroll\n\nUsers who want to publish or subscribe to a RT stream from outside of their kdb Insights Enterprise cluster must enroll in order to obtain endpoints to connect to. Details on how you can enroll as a\npublisher\nor\nsubscriber\nare provided below.\n\n#### External Reference ID\n\nAs part of the enrollment step an external reference ID must be provided.\nIn this example, using the\nkxi-db package\n, the external reference ID is\next-mystream\n.\nFrom the\nkxi-db\npackage.\nshell\nCopy\n\n```\nsequencers:  mystream:    external: true    ...    topicConfig:      subTopic: ext-mystream      extSubStream: ext-sub-mystream\n```\n\nNote\nExternal Reference IDs\nThe reference ID can be identified from one of two places:\n- When using thekxi pmcommand to define a package, you can find the reference ID under thesubTopicfield in thesequencerssection of thekxi-db/databases/mydb/shards/mydb-shard.yamlfile.\n- For web interface deployments, locate the reference ID under the Database'sStream Settingstab. The external reference is displayed in the External data ingress setting. In the example below, the reference ID ismystream-ingr.\n\n#### Publish\n\n- Enroll the client using theexternal reference ID.shellCopykxi client enrol --name publisherName --insert-topic ext-mystream```shell-session{\"message\":\"success\",\"detail\":\"Client enrolled\",\"url\":\"5ed6e5b7c80c8e35d07249d12f32d9eb\",\"config_url\":\"https://{INSIGHTS_HOSTNAME}/informationservice/details/5ed6e5b7c80c8e35d07249d12f32d9eb\"}\nThe\nconfig_url\nfield must be stored as it's used by the publisher to get publish endpoints. The\nconfig_url\nis unique to the\npublisherName\nabove.\n\n#### Subscribe\n\nEnroll the client using the\nexternal reference ID\n.\nshell\nCopy\n\n```\nkxi client enrol --name subscriberName --subscribe ext-sub-mystream```shell-session{  \"message\": \"success\",  \"detail\": \"Client enrolled\",  \"url\": \"5ed6e5b7c80c8e35d07249d12f32d9eb\",  \"config_url\": \"https://{INSIGHTS_HOSTNAME}/informationservice/details/5ed6e5b7c80c8e35d07249d12f32d9eb\"}\n```\n\nThe\nconfig_url\nfield must be stored as it is used by the subscriber to get subscription endpoints. This is unique to the\nsubscriberName\nabove.\n\n## Removing a client\n\nAuthentication with the CLI is a pre-requisite for this step, as described\nhere\n.\nA client can be removed by running the CLI command below:\nshell\nCopy\n\n```\nkxi client remove --name publisherName\n```\n\nshell\nCopy\n\n```\n{  \"message\": \"success\",  \"detail\": \"Client removed\"}\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 812,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-cdd2a46a111e",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/query-scratchpad-q.htm",
    "title": "Scratchpad using q",
    "text": "\n# Development in the Scratchpad using q\n\nThis page guides you through the execution of q code and APIs using the\nScratchpad\n.\nThe q environment in the Scratchpad offers a unique, self-contained location, where you can assign variables and produce analyses that are visible only to you.\nThe following sections describe how to:\n- Execute code using qin theScratchpad\n- Develop Stream Processor pipelinesusing theStream Processor API\n- Debug pipelines\n- Interact with custom codeusingPackages\n- Develop machine learning workflowsusing theMachine Learning Core APIs\n- Include Python in your q codeusingembedPyand thePyKX q API\nFor information on data visualization and console output in the Scratchpad see\nhere\n.\nNote\nIf you are primarily developing in Python seehere.\n\n## Execute q code\n\nWhen executing code within the Scratchpad, keep the following points in mind:\n- The q language is highlighted within the Scratchpad to indicate you are using the q language version of the Scratchpad.\n- UseCtrl + EnterorCmd + Enter, depending on OS, to execute the current line or selection. You can execute the current line without selecting it.\n- ClickRun Scratchpadto execute everything in the editor.\n\n### Namespaces (or contexts)\n\nThe current namespace can be changed by adding a line such as\n\\d .myNamespace\nanywhere above the line you're executing.\nThis line does not need to be included in the selection you're executing.\nAs an example, running just the second line in the following code assigns\n1 2 3\nto\n.test.result\n.\nThe\nresult\nvariable on the last line is in the global context, and is undefined.\nq\nCopy\n\n```\n\\d .testresult: 1 2 3// The namespace remains \".test\" on all subsequent lines until it is changed by another `\\d` command.test.result ~ 1 2 3\\d .// This variable is in the global context, and is undefinedresult\n```\n\n\n## Develop Stream Processor pipelines\n\nThe Scratchpad can be used as a prototyping environment for Stream Processor pipelines, as it provides a process in which to easily publish batches to a pipeline, capture intermediate result, and step through your functions. Access to the pipeline API allows you to simulate production workflows and test code logic prior to moving development work to production environments. This is facilitated through use of the\nStream Processor API\n.\nPipeline(s) run in the Scratchpad are not listed on the\nOverview\npage, and must be managed from within the Scratchpad. They are run in the Scratchpad process, the same as when deployed using Quick Test.\nFor example, the following script creates a pipeline for enriching weather data. It contains an error that can easily be debugged in the Scratchpad.\n- Copy the following code to your Scratchpad to create the pipeline.qCopy// Only one pipeline can be run at once, so `teardown`iscalled before running a new pipeline.// Because the scratchpad process hosting the pipelineisalready running, thereisno deployment step needed, just a call to .qsp.run.qsp.teardown[]; .qsp.run// While developing a pipeline, the fromCallback reader lets you send batches one at a time.// This creates a function called `publish` to accept incoming batches..qsp.read.fromCallback[`publish].qsp.decode.csv[([] time:`timestamp$(); temp:`float$(); humidity:`float$())].qsp.map[{[op; md; data]data: update dewpoint: temp -.2* (100- humidity)fromdata;data: update heatIndex:.5* temp +61+ (1.2* temp -68) + humidity *.094fromdata;state: .qsp.get[op; md];state[`recordHigh]: max data[`heatIndex] , state`recordHigh;state[`recordLow]: min data[`heatIndex] , state`recordLow;.qsp.set[op; md; state];: update recordHigh: state`recordHigh, recordLow: state`recordLowfromdata}; .qsp.use ``state!(::; `recordHigh`recordLow!(::; ::))]// The toVariable writer appends each batch to a variableinthecurrent(scratchpad) process,// making it very convenientfordebugging.qsp.write.toVariable[`out]// Send a batch to the pipelinepublish\"time,temp,humidity\\n\",\"2024.07.01T12:00,81,74\\n\",\"2024.07.01T12:00,82,70\\n\"\n- This example is throwing the errortype - error in operator: map, so you can cache the parameters in the map node, then step through the code.\n- First, update the map node to write the parameters to a global variable, then re-run the pipeline definition and call to publish.qCopy.qsp.map[{[data]`op`md`data set' .test.cache;update dewpoint: temp -.2* (100- humidity)fromdata;update heatIndex:.5* temp +61+ (1.2* tmp -68) + humidity *.094fromdata}]\n- Next, run the following code to define the parameters while stepping through the map node.qCopy`op`md`data set' .test.cache\n- Run each line individually until you get the error on this line.qCopystate[`recordHigh]: max data[`heatIndex] , state`recordHigh;\n- Because recordHigh is a generic null, it can't be passed to max. Rewrite the initial state so the records start as negative and positive float infinity.qCopy}; .qsp.use ``state!(::; `recordHigh`recordLow!(-0w;0w))]\n- You can now redefine the pipeline, pass it multiple batches, and inspect the output.qCopypublish\"time,temp,humidity\\n\",\"2024.07.01T12:00,81,74\\n\",\"2024.07.01T13:00,82,70\\n\"publish\"2024.07.02T12:00,87,73\\n\",\"2024.07.02T13:00,90,74\\n\"outtextCopytime                          temp humidity dewpoint heatIndex recordHigh recordLow-----------------------------------------------------------------------------------2024.07.01D12:00:00.000000000 81   74       75.8     82.278    83.19      82.2782024.07.01D13:00:00.000000000 82   70       76       83.19     83.19      82.2782024.07.02D12:00:00.000000000 87   73       81.6     88.831    92.178     82.2782024.07.02D13:00:00.000000000 90   74       84.8     92.178    92.178     82.278\n\n## Debugging pipelines\n\nPipelines\nwritten in the web interface, are evaluated in the scratchpad process. Therefore, any global variables cached in a pipeline are available in the scratchpad, and any global variables defined in a scratchpad are available in a pipeline process.\nBecause the hotkeys for evaluating code in the pipeline editors, you can also step through pipeline functions in-place.\n\n## Interact with custom code\n\nBy using\nPackages\nyou can add custom code to\nkdb Insights Enterprise\nfor use in the following situations:\n- In the Stream Processor when adding custom streaming analytics.\n- In the Database for adding custom queries.\nThe Scratchpad also has access to these APIs allowing you toload custom codeand accessuser defined functionswhen developing Stream Processor pipelines or analytics for custom query APIs.\nThe following shows an example of a Scratchpad workflow which utilizes both the packages and a UDF API available within the Scratchpad.\nThis example shows a package named\nml\n, containing a function\nfilter_udf\nthat returns any rows where the first value is greater than 0.5. Passing the pipeline a table of 50 rows, containing random values between 0 and 1, it is expected that about half the rows pass through the filter node and are written to test_filter.\nInformation\nCode snippet for ML Scratchpad example.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nq\nCopy\n\n```\n.kxi.packages.list.all[]select name, function, description from .kxi.udfs.list.all[].qsp.run    .qsp.readfromCallback[`publish]    .qsp.filter[.qsp.udf[\"filter_udf\";\"ml\"]]    .qsp.write.toVariable[`test_filter]publish ([]50?1f;50?1f)count test_filter\n```\n\nXX MD - have Dev provided a working snippet\n\n## Develop machine learning workflows\n\nThe Scratchpad has access to a variety of machine learning libraries created by KX. The\nMachine Learning Core APIs\nare included with all running Scratchpads. These APIs provide you with access to the following:\n- Data preprocessing functionality and ML models/analytics designed specifically for streaming and time-series use-cases.\n- ML Model Registrywhich provides a cloud storage location for ML models generated in q/Python.\nThe example below shows how you can use this functionality to preprocess data, fit a machine learning model, and store this ephemerally within your Scratchpad session. (Note that storing of models in this manner results in them being lost upon restarting the Scratchpad pod).\nInformation\nCode snippet for ML Scratchpad example.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nq\nCopy\n\n```\nraw_data:([]asc 100?1f;100#50;100?1f;desc 100?`a`b`c;y:desc 100?1f)features:select x,x1,x2,x3 from raw_datatarget:exec y from raw_data// Run various pre-processing functions on datadata:.ml.minMaxScaler.fitTransform        .ml.lexiEncode.fitTransform[;::]        .ml.dropConstant features// Fit a linear regression modelmodel:.ml.online.sgd.linearRegression.fit[data;target;1b;`maxIter`alpha!(1000;0.001)]output:([]x:results:target;predictions:model.predict data)// Create a new ML Registry and add model to the temporary registry.ml.registry.new.registry[\"/tmp\";::].ml.registry.set.model[\"/tmp\";::;model;\"linear_model\";\"q\";::]// Retrieve and use the prediction model validating it is equivalent to persisted modelml_model:.ml.registry.get.predict[\"/tmp\";::;\"linear_model\";::]model.predict[data]~ml_model data\n```\n\n\n## Include Python in your q code\n\nYou can include Python functionality within your q code using\nembedPy\nor\nPyKX\n. Depending on your use-case or familiarity with these APIs you are free to use and interchange both APIs, however it is strongly suggested that usage of the embedPy functionality be reserved for historical code integration while any new code development targets the PyKX equivalent API.\nThe following basic example shows usage of both the embedPy and PyKX q APIs to generate and use callable Python objects.\nInformation\nCode snippet for Python DSL use in Scratchpad.\nUse the following code to replicate the behavior illustrated in the screenshot above:\nq\nCopy\n\n```\n// PyKX code example.pykx.set[`test;til 10].pykx.get[`test]`.pykx.pyexec\"import numpy as np\".pykx.pyexec\"a = np.array([1, 2, 3])\".pykx.get[`a]`// embedPy code example.p.set[`test;til 10].p.get[`test]`.p.e\"import numpy as np\".p.e\"b = np.array([1, 3, 4])\".p.get[`b]`\n```\n\nFor more comprehensive Python development, refer to the the Python development page\nhere\n.\n\n## Known issues\n\n- STDOUT and STDERR are not shown in the console, so the result of usingshow,-1, or-2is not shown.\n\n## Further reading\n\n- Data query overview\n- Create queries using the query builder\n- Perform further analysis in the Scratchpad using python\n- Create advanced Visualizations",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1375,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-d64aba5ea1bb",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/observability.htm",
    "title": "Observability",
    "text": "\n# Observability\n\nThis page provides an overview of what observability means in the context of\nkdb Insights Enterprise\nand talks about aspects like metrics and logging.\nObservability is a critical feature of any application. It allows system operators\nvisibility on how the application is performing and provides the ability to\ninvestigate and resolve issues.\nkdb Insights Enterprise\nprovides observability through\nmetrics\nand\nlogging\n.\n\n## Logging\n\nLogging is an important tool to understand what's occurring in an application by keeping track\nof those events with log messages.\nkdb Insights Enterprise\nfollows best practice when it comes to logging.\nIt emits structured log messages to STDOUT to be consumed by standard logging stacks.\nThis allows you to deploy your own logging stack, centralize the application logs,\nand easily troubleshoot issues.\nIt does not ship with a built-in logging stack but there are many available solutions.\nYou can choose to deploy your preferred one or if running in the cloud, there may\nbe an out of the box one integrated with the Kubernetes stack.\nRefer to\nLogging\nfor more information.\n\n## Metrics\n\nMetrics are a key component of observability. They allow you to capture information about\nhow your application is performing. In comparison to logging, they provide a more aggregated,\nnumerical view of the system state.\nkdb Insights Enterprise\ngenerates metrics to allow you to quickly identify the system performance and potential issues.\nThis allows you to monitor data flows, component errors, and a variety of other metrics.\nThe application is designed to work with the Prometheus stack. In order to use metrics,\nplease deploy Prometheus.\n\n## Encryption in transit metrics\n\nkdb Insights Enterprise\ncan be deployed with\nencryption of all data in transit\n.\nIstio\n, an open-source service mesh, is deployed alongside\nkdb Insights Enterprise\n. Refer to\nIstio security\nfor details on how Istio secures data in transit.\nThe Istio metrics captured are the default Istio service level metrics which are listed\nhere\n.\nThese metrics provide a way of monitoring and understanding the behavior for all service communications within a mesh and the behavior of the mesh itself.\nInformation\nCollecting the Standard Istio metrics increases the total number of records stored in Prometheus by up to 25%, mainly due to the frequency of capture recommended for the Istio metrics.\nRefer to the\nIstio Observability\ndocumentation for details on modifying which metrics are captured.\nYou can view the metrics using one of the following:\n- Kiali\n- Grafana dashboards. Refer to the followingIstio documentationfor a list of pre-configured dashboards available. Ensure that the Istio Dashboard you deploy is appropriate for the version of Istio deployed. Read therelease notesfor details on which version of Istio is been deployed withkdb Insights Enterprise.\nFor more information, refer to\nMetrics\n.\n\n## Alerts\n\nkdb Insights Enterprise\nprovides a set of pre-defined alerts, which are sent to the\nAlertmanager\n. The Alertmanager then manages the alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.\nRefer to\nAlerts reference\nfor more detail on alerts.\n\n## Dashboards\n\nkdb Insights Enterprise\nprovides a set of pre-defined dashboards, which are added to\nGrafana\n.\nFor more information, refer to\nDashboards reference\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 535,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-132719b0fa71",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/S3/s3-batch-ingest.htm",
    "title": "Batch S3 Ingest",
    "text": "\n# Batch S3 Ingest\n\nThis page describes how to read bulk data from S3 and write to an Insights SDK Database.\nArbitrary data files can be read from S3, transformed and written to downstream\nconsumers from the Stream Processor. This example illustrates how to ingest data to\nan Insights Database in Docker.\n\n## Prerequisites\n\nFollow the\ninstructions\nto\ndownload the Docker bundle and\ndocker login portal.dl.kx.com\n.\nAfter extracting the tarball, place your kdb+ license in the\nkxi-db/lic\ndirectory.\n\n## Building the walkthrough\n\nEnsure you are in the\nkxi-db\ndirectory to run these commands.\nFirstly create a Docker compose file\ncompose-batch-ingest.yaml\n.\nYAML\nCopy\n\n```\n---# include the base database bundle servicesinclude:- ./compose.yamlservices:  # define an SP controller service  kxi-controller:    image: ${kxi_img_spc}    hostname: kxi-controller    ports:    - ${kxi_port_spc}:${kxi_port_spc}    volumes:    - ${kxi_dir_logs}:/mnt/logs    - ${kxi_dir_sp}:/sp    - ${kxi_dir_lic}:/opt/kx/lic    - ${kxi_dir_config}:/mnt/config    networks: [kx]    environment:    - KDB_LICENSE_B64    command: [\"-p\", '${kxi_port_spc}']  # define an SP worker service  kxi-worker:    image: ${kxi_img_spw}    hostname: kxi-sp    ports:    - ${kxi_port_spw}:${kxi_port_spw}    volumes:    - ${kxi_dir_logs}:/mnt/logs    - ${kxi_dir_sp}:/sp    - ${kxi_dir_lic}:/opt/kx/lic    - ${kxi_dir_config}:/mnt/config    - ${kxi_dir_db}:/mnt/db    networks: [kx]    environment:    - KXI_SP_SPEC=/mnt/config/${kxi_sp_spec}                # Point to the bound spec.q file    - KXI_SP_PARENT_HOST=kxi-controller:${kxi_port_spc}     # Point to the parent controller service    - RT_TOPIC_PREFIX=rt-    - RT_REPLICAS=1    - KXI_SP_ORDINAL=1    - KXI_SP_GROUP=grp    command: [\"-p\", '${kxi_port_spw}']\n```\n\nDetails about the environment variables referenced above are available\nhere\n.\nq\nPython\nCreate a spec file called\nconfig/ingest.q\ncontaining the pipeline definition.\nIt does the following:\n- reads data from a public S3 bucket\n- uses a CSV decoder to format the data using theschemaobject\n- renames some columns\n- transforms the data to match the database schema by adding an emptyfeescolumn and reordering\n- writes the data to thetaxischema in the database using atargetofkxi-sm:10001\n- thetargetvalue corresponds to SM service's endpoint\n- directWriteis enabled to specify that data is to be written directly to the database instead of streamed\nq\nCopy\n\n```\nschema:(!) . flip (    (`pickup_datetime       ; \"p\");    (`dropoff_datetime      ; \"p\");    (`vendor                ; \"s\");    (`passengers            ; \"h\");    (`distance              ; \"e\");    (`rate_type             ; \"*\");    (`store_and_fwd         ; \"*\");    (`pickup_borough        ; \"*\");    (`pickup_zone           ; \"*\");    (`pickup_service_zone   ; \"*\");    (`dropoff_borough       ; \"*\");    (`dropoff_zone          ; \"*\");    (`dropoff_service_zone  ; \"*\");    (`payment_type          ; \"s\");    (`fare                  ; \"e\");    (`extra                 ; \"e\");    (`tax                   ; \"e\");    (`tip                   ; \"e\");    (`toll                  ; \"e\");    (`improvement_surcharge ; \"*\");    (`congestion_surcharge  ; \"*\");    (`total                 ; \"e\"));exclude:`rate_type`store_and_fwd`pickup_borough`pickup_zone`pickup_service_zone`dropoff_borough`dropoff_zone`dropoff_service_zone`improvement_surcharge`congestion_surcharge;.qsp.run    .qsp.v2.read.fromAmazonS3[\"s3://kxs-prd-cxt-twg-roinsightsdemo/ny_taxi_data_2021_12.csv\"; \"eu-west-1\"]    .qsp.decode.csv[schema; .qsp.use enlist[`exclude]!enlist exclude]    .qsp.transform.renameColumns[`pickup_datetime`dropoff_datetime`toll!`pickup`dropoff`tolls]    .qsp.map[{ :`vendor`pickup`dropoff`passengers`distance`fare`extra`tax`tip`tolls`fees`total`payment_type xcols update fees:0e from x }]    .qsp.v2.write.toDatabase[`taxi; .qsp.use `directWrite`target!(1b; \"kxi-sm:10001\")]\n```\n\nCreate a variable file\nbatch-ingest.env\nBash\nCopy\n\n```\n# imageskxi_registry=portal.dl.kx.comkxi_img_spc=${kxi_registry}/kxi-sp-controller:1.14.0kxi_img_spw=${kxi_registry}/kxi-sp-worker:1.14.0# networkingkxi_port_spc=6000kxi_port_spw=7000# pathskxi_dir_lic=\"./lic\"kxi_dir_config=\"./config\"kxi_dir_sp=\"./data/sp\"kxi_dir_logs=\"./data/logs\"kxi_dir_db=\"./data/db\"kxi_sp_spec=\"ingest.q\"\n```\n\nRead more about the\nStream Processor q API\n.\nCreate a spec file called\nconfig/ingest.py\ncontaining the pipeline definition which does the following:\n- reads data from a public S3 bucket\n- uses a CSV decoder to format the data using theschemaobject\n- renames some columns\n- transforms the data to match the database schema by adding an emptyfeescolumn and reordering\n- writes the data to thetaxischema in the database using atargetofkxi-sm:10001\n- thetargetvalue corresponds to SM service's endpoint\n- directWriteis enabled to specify that data must be written directly to the database instead of streamed\nPython\nCopy\n\n```\nimport pykx as kxfrom kxi import spschema = {    'pickup_datetime': 'p',    'dropoff_datetime': 'p',    'vendor': 's',    'passengers': 'h',    'distance': 'e',    'rate_type': '*',    'store_and_fwd': '*',    'pickup_borough': '*',    'pickup_zone': '*',    'pickup_service_zone': '*',    'dropoff_borough': '*',    'dropoff_zone': '*',    'dropoff_service_zone': '*',    'payment_type': 's',    'fare': 'e',    'extra': 'e',    'tax': 'e',    'tip': 'e',    'toll': 'e',    'improvement_surcharge': '*',    'congestion_surcharge': '*',    'total': 'e'}exclude = ['rate_type', 'store_and_fwd', 'pickup_borough', 'pickup_zone', 'pickup_service_zone', 'dropoff_borough', 'dropoff_zone', 'dropoff_service_zone', 'improvement_surcharge', 'congestion_surcharge']def reorder(data):    order = ['vendor', 'pickup', 'dropoff', 'passengers', 'distance', 'fare', 'extra', 'tax', 'tip', 'tolls', 'fees', 'total', 'payment_type']    data = kx.q.qsql.update(data, {'fees': '0e'})    return kx.q.xcols(order, data)sp.run(sp.read.from_amazon_s3('s3://kxs-prd-cxt-twg-roinsightsdemo/ny_taxi_data_2021_12.csv', region='eu-west-1', api_version=2)    | sp.decode.csv(schema, exclude=exclude)    | sp.transform.rename_columns({'pickup_datetime': 'pickup', 'dropoff_datetime': 'dropoff', 'toll': 'tolls'})    | sp.map(reorder)    | sp.write.to_database('taxi', target='kxi-sm:10001', directWrite=True, api_version=2))\n```\n\nCreate a variable file\nbatch-ingest.env\nBash\nCopy\n\n```\n# imageskxi_registry=portal.dl.kx.comkxi_img_spc=${kxi_registry}/kxi-sp-controller:1.14.0kxi_img_spw=${kxi_registry}/kxi-sp-python:1.14.0# networkingkxi_port_spc=6000kxi_port_spw=7000# pathskxi_dir_lic=\"./lic\"kxi_dir_config=\"./config\"kxi_dir_sp=\"./data/sp\"kxi_dir_logs=\"./data/logs\"kxi_dir_db=\"./data/db\"kxi_sp_spec=\"ingest.py\"\n```\n\nRead more about the\nStream Processor Python API\n.\n\n## Running the ingest\n\nRun the deployment using the following command.\nBash\nCopy\n\n```\ndocker compose -f compose-batch-ingest.yaml --env-file batch-ingest.env up\n```\n\nYou can check status of the pipeline using the command.\nBash\nCopy\n\n```\ncurl http://localhost:6000/details\n```\n\nThis outputs a JSON payload of the form\nJSON\nCopy\n\n```\n{  \"state\": \"RUNNING\",  \"error\": \"\",  \"metrics\": {    \"eventRate\": 0,    \"bytesRate\": 0,    \"latency\": -1.7976931348623157e+308,    \"inputRate\": 8746320,    \"outputRate\": 41618.86  },  \"logCounts\": {    \"trace\": 0,    \"debug\": 0,    \"info\": 63,    \"warn\": 0,    \"error\": 0,    \"fatal\": 0  },  \"readerMetadata\": []}\n```\n\nThe pipeline has completed when the\nstate\nfield is set to\nFINISHED\n.\nThis indicates all of the data has been written to the database and successfully ingested.\nYou can check the ingest session in the database using the following command.\nBash\nCopy\n\n```\ncurl http://localhost:10001/ingest\n```\n\nThe session is marked with a\nstatus\nof\npending\nwhile the SP pipeline is writing data.\nJSON\nCopy\n\n```\n[  {    \"name\": \"pipeline-482f128ffe-0\",    \"pipeline\": \"\",    \"database\": \"kdb Insights\",    \"updtype\": \"ingest\",    \"status\": \"pending\",    \"details\": [],    \"tbls\": [],    \"dates\": [],    \"progress\": {      \"cmdCurrent\": \"\",      \"cmdIndex\": null,      \"cmdTotal\": null,      \"subCurrent\": \"\",      \"subIndex\": null,      \"subTotal\": null    },    \"error\": [],    \"updated\": \"2024-11-22T12:12:25.518555535\"  }]\n```\n\nThis updates to\nprocessing\nwhen the database is ingesting the data.\nJSON\nCopy\n\n```\n[  {    \"name\": \"pipeline-482f128ffe-0\",    \"pipeline\": \"\",    \"database\": \"kdb Insights\",    \"updtype\": \"ingest\",    \"status\": \"processing\",    \"details\": {      \"kxSessionInfo.752fb820-e0db-6c26-0803-c194e45ed5a8\": {        \"pipelineName\": [],        \"pipelineID\": \"pipeline-482f128ffe\",        \"workerName\": \"spwork-kxi-sp\",        \"operatorID\": \"database.writer_taxi\",        \"ingestStartTime\": \"2024-11-22T12:12:17.580482856\",        \"ingestEndTime\": \"2024-11-22T12:14:14.123753107\"      },      \"subsessions\": [        \"752fb820-e0db-6c26-0803-c194e45ed5a8\"      ],      \"dates\": [        \"2021-12-01\",        ..        \"2021-12-31\"      ],      \"tables\": [        \"taxi\"      ]    },    \"tbls\": [      \"taxi\"    ],    \"dates\": [      \"2021-12-01\",      \"2021-12-02\",      \"2021-12-03\",      \"2021-12-04\",      \"2021-12-05\",      \"2021-12-06\",      \"2021-12-07\",      \"2021-12-08\",      \"2021-12-09\",      \"2021-12-10\",      \"2021-12-11\",      \"2021-12-12\",      \"2021-12-13\",      \"2021-12-14\"    ],    \"progress\": {      \"cmdCurrent\": \"2021.12.14\",      \"cmdIndex\": 13,      \"cmdTotal\": 31,      \"subCurrent\": \"taxi\",      \"subIndex\": 0,      \"subTotal\": 1    },    \"error\": [],    \"updated\": \"2024-11-22T12:14:14.140172780\"  }]\n```\n\nOnce the database has finished ingesting it, the\nstatus\nupdates to\ncompleted\nand the data is available for querying.\nYou can query the first 30 minutes of the dataset to verify this.\nBash\nCopy\n\n```\ncurl -X POST http://localhost:8080/data -H \"Content-Type: application/json\" -H \"Accept: application/json\" -d '{table: \"taxi\",startTS: \"2021-12-01T00:00:00.0\", endTS: \"2021-12-01T00:30:00.0\" }'\n```\n\nJSON\nCopy\n\n```\n{  \"header\": {    \"rcvTS\": \"2024-11-22T12:15:22.941000000\",    \"corr\": \"69e1aa73-f069-4854-a0b1-28c439c55e47\",    \"logCorr\": \"69e1aa73-f069-4854-a0b1-28c439c55e47\",    \"http\": \"json\",    \"api\": \".kxi.getData\",    \"agg\": \":172.18.0.7:5060\",    \"refVintage\": -9223372036854776000,    \"rc\": 0,    \"ac\": 0,    \"ai\": \"\",    \"limitApplied\": false  },  \"payload\": [    {      \"vendor\": \"Creative Mobile Tecnologies, LLC\",      \"pickup\": \"2021-12-01T00:00:07.000000000\",      \"dropoff\": \"2021-12-01T00:06:39.000000000\",      \"passengers\": 1,      \"distance\": 1.8,      \"fare\": 7.5,      \"extra\": 3,      \"tax\": 0.5,      \"tip\": 2.8,      \"tolls\": 0,      \"fees\": 0,      \"total\": 14.1,      \"payment_type\": \"Credit card\"    },    {      \"vendor\": \"Creative Mobile Tecnologies, LLC\",      \"pickup\": \"2021-12-01T00:00:19.000000000\",      \"dropoff\": \"2021-12-01T00:07:24.000000000\",      \"passengers\": 1,      \"distance\": 1.4,      \"fare\": 7,      \"extra\": 3.5,      \"tax\": 0.5,      \"tip\": 2,      \"tolls\": 0,      \"fees\": 0,      \"total\": 13.3,      \"payment_type\": \"Credit card\"    },    ..\n```\n\n\n## Next Steps\n\nRead bulk data from S3, apply scaling and preprocessing steps",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1093,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-1d3a215dbcc9",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Interfaces/cli-overview.htm",
    "title": "Command Line Interface",
    "text": "\n# Command Line Interface Overview\n\nThis page provides an overview of the kdb Insights Command Line Interface (CLI), a tool to manage your kdb Insights Enterprise application. Use it to:\n- Manage deploymentsYou can install, upgrade, and uninstall kdb Insights Enterprise.Refer tokdb Insights Enterprise installationfor more information.\n- Manage packagesYou can create and manage packages that enable you to manage the life cycle of deployments and custom logic in kdb Insights Enterprise.For more information, refer toPackaging.\n\n## Installing the CLI\n\nTo install the CLI, refer to the\nCLI installation guide\n.\n\n## Options precedence\n\nThe CLI reads each option value in the following order of precedence:\n- The command line parameter\n- Correspondingenvironment variables. This includes variables defined in a .env file.\n- The value set in your localcli-config file\n- The setting from thevalues filepassed with--filepath(in the case ofkxi install run/upgrade)\n- The setting from thevalues file already deployed on cluster(in the case ofkxi install upgrade)\n- The response to a user prompt (in an interactive session)\n- The default value\n\n### Namespace precedence\n\nThe namespace option precedence has an extra check and is read in the following order:\n- The--namespacecommand line parameter\n- Thenamespacevalue set in your localcli-config file\n- The namespace set in your activeKubernetes context\n- The response to a user prompt (in an interactive session)\n- The default value",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 224,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-bd82d7d62ea3",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/pipelines-create.htm",
    "title": "Create and Configure",
    "text": "\n# Create and Configure Pipelines\n\nThis page describes how to create and configure pipelines in the\nkdb Insights Enterprise\nweb interface.\nStream processing involves performing various actions on a continuous flow of data originating from one or more systems. Actions can include a combination of processes, such as:\n- Ingestion: Inserting the data into a downstream database.\n- Transformation: Changing a string into a date.\n- Aggregations: Calculating the sum, mean, or standard deviation.\n- Analytics: Predicting a future event based on data patterns.\n- Enrichment: Combining data with other data, whether historical or streaming, to create context or meaning.\nThe\nPipelines\nmenu, in the web interface, enables you to build and deploy stream processor pipelines to perform these actions.\nNote\nRead about the\nkdb Insights Stream Processor (SP)\n.\nThis page provides details on the following topics:\n- Index of pipelines: View a list of all pipelines.\n- Pipeline status: Check the status of all your pipelines.\n- Pipelines tab: Learn about the layout of the pipelines tab and how you use it to build pipelines.\n- Building pipelines: Instructions on how to build a pipeline, including links to anexample.\n- Pipeline settings: Manage pipeline resources, environment variables and tuning pipeline settings.\n- Deploy,testandsave: Guidance on deploying, testing and saving pipelines.\n- Teardown: Stop a running pipeline.\n- Actions: Export, rename, and delete a pipeline.\n- Python code within pipeline UI: Explains the considerations when defining the Python code used for a given node.\n\n## Pipelines Index\n\nClick on\nPipelines\nin the\nleft-hand menu\nto view a list of all your pipelines.\nThe following details are displayed:\n\n| Item | Description |\n| --- | --- |\n| Name | The pipeline name. Click on the name to view details about the pipeline in thepipeline tab. |\n| Status | The current status of your pipeline. Hover over thestatus valuefor additional information. |\n| Health | This column displays the health status of the pipeline. A value ofHealthyindicates there are no reported issues with the pipeline and is shown for status of Finished, Running. When an error occurs, a warning message is displayed indicating that attention is required. Refer towarnings and remediation actionsfor details about these errors and how they can be resolved. |\n| Package | The name and version of thepackagecontaining this pipeline. |\n| Deployed Since | The time since the pipeline was last deployed. |\n| Actions | Click the three dots, to the right of the pipeline details, to view a list of pipeline actions. You have options toExport,Rename, andDeletea pipeline. |\n\nClick\nCreate Pipeline\nto\ncreate a new pipeline\n.\n\n### Pipeline status\n\nThe status of a pipeline can be checked in two places:\n- Pipelines index\n- Recently deployedsection of the Overview tab.\nA pipeline can have one of the following status values:\n\n| Status | Description |\n| --- | --- |\n| Creating | Pipeline SP Controller is being created. |\n| Initializing controller | Pipeline SP Controller has been created. |\n| Partitioning workers | Pipeline is in the process of distributing partitions among one or more Workers. |\n| Ready to receive data | Pipeline is ready to start, but is not yet processing messages. |\n| Running | Pipeline is running and processing messages. |\n| Unresponsive | Pipeline SP Controller is not responding to HTTP requests. |\n| Errored | Pipeline errored before starting. This does not include errors encountered while processing messages. |\n| Tearing down | Pipeline is in the process of tearing down. |\n| Finished | Pipeline has finished processing messages, but has not been torn down. |\n| Not Found | Pipeline does not exist. |\n\nNote\nIf you are using the pipeline APIs such as the\ndetails\nor\nstatus\nAPI, the statuses are as follows:\n- CREATING\n- INITIALIZING\n- PARTITIONING\n- READY\n- RUNNING\n- UNRESPONSIVE\n- ERRORED\n- TEARING_DOWN\n- FINISHED\n- NOTEXIST\n\n## Pipelines tab\n\n- Use one of the following methods to open theCreate Pipelinedialog:Click+on the ribbon menu and selectPipeline.Click+besidePipelineson theleft-hand menu.ClickCreate Pipelinein thepipelines index.\n- Enter aPipeline Namethat is unique within the package, 2-32 characters long, uses lowercase letters, numbers, and hyphens, and starts and ends with an alphanumeric character.\n- ClickSelect a packageunderAdd to Package, to display a list of packages to which this pipeline can be added. This list includes packages that are currently runningClickCreate new packageif you want to add the pipeline to a new package. Enter a unique package name that is 2-32 characters long, uses lowercase letters, numbers, and hyphens, and starts and ends with an alphanumeric character.Seepackagesfor further details about packages.\n- ClickCreate.The Pipeline tab opens, as shown below.\nFrom here you can\nbuild a pipeline\n, with additional settings available on the\nSettings\ntab.\nNote\nThe terms\noperators\nand\nnodes\nare used interchangeably within this document and within the user interface.\nThe\nPipeline\ntab has a number of elements, described in the following table.\n\n| SECTION | DESCRIPTION |\n| --- | --- |\n| Package details | The top left-hand corner of the tab has the package name and version. This is the package containing this pipeline. Click on the package name to open thePackage Entitiesdialog box that displays all the entities; database(s), pipeline(s), and view(s) this package contains. |\n| Name | The pipeline name is displayed in the top left-hand corner of the screen. A default name is applied when the pipeline is created. You can change this. The name you set must be lowercase, only include letters and numbers, and exclude special characters other than hyphen (-). |\n| Operators menu | The left-hand panel shows a search box and list of operators. Operators are the building blocks for pipelines. Each operator is capable of operating on data in a stream but each has a specific function. There are differenttypes of operatorsused to perform different functions. |\n| Pipeline builder | The pipeline builder area is in the center of the screen. You build pipelines by adding operators to the workspace and joining them with the operator-anchors.Operators can be removed on a right-click, or added by click-and-drag from the entity tree, or a right-click in an empty part of the workspace.To change a connection, roll over to display the move icon and drag-connect. Remove connections with a right-click on the connection you want to delete. |\n| Graphic options | The pipeline builder contains a number of options you can use when working with pipelines.ICONFUNCTIONDETAILUndoUndo editThis icon is only active after an edit has been made.RedoRedo changeThis icon is only active after an edit has been undone.ResetReset viewLayoutRearrange the pipeline layoutUse this to enhance the readability of its elements. | ICON | FUNCTION | DETAIL | Undo | Undo edit | This icon is only active after an edit has been made. | Redo | Redo change | This icon is only active after an edit has been undone. | Reset | Reset view |  | Layout | Rearrange the pipeline layout | Use this to enhance the readability of its elements. |\n| ICON | FUNCTION | DETAIL |\n| Undo | Undo edit | This icon is only active after an edit has been made. |\n| Redo | Redo change | This icon is only active after an edit has been undone. |\n| Reset | Reset view |  |\n| Layout | Rearrange the pipeline layout | Use this to enhance the readability of its elements. |\n| Global Code | When no operator is selected, the right-hand panel contains theGlobal Codeconfiguration. Global Code is executed before a pipeline is started. This code may be necessary for Stream Processor hooks, global variables, and named functions. |\n| Operator Configuration | When an operator is selected, the right-hand panel contains the configuration details for the selected operator. For example, the following screenshot shows the configuration for the Kafka operator. |\n| Advanced Operator Configuration | Some operators support additional advanced configuration that can be accessed by clicking the gear icon in the top right corner of the operator configuration panel. For example, theMapoperator has advanced configuration options, as shown here.Operator settings dialogNodes that don't support this feature do not have the gear icon present.Refer to theq APIfor more details about advanced operator configuration. |\n| Actions | When an operator is added, the right-click menu has a number of actions that can be performed on the operator, such as Rename, Remove, and Duplicate.To undo any of these actions, click Undo.Right click menu on operator editingACTIONDESCRIPTIONRename NodeChange the name of the node. Enter a new name and clickOk.Duplicate NodeCreate a copy of the node. A duplicated operator retains the contents and configuration of the original, and has the name appended with a numeric tag; `-n`. The duplicate pipeline name must be lowercase, only include letters and numbers and have a different name to the original pipeline, while excluding special characters other than hyphen (-).Delete NodeDelete the node. Use the undo edit graph option to revert the change. | ACTION | DESCRIPTION | Rename Node | Change the name of the node. Enter a new name and clickOk. | Duplicate Node | Create a copy of the node. A duplicated operator retains the contents and configuration of the original, and has the name appended with a numeric tag; `-n`. The duplicate pipeline name must be lowercase, only include letters and numbers and have a different name to the original pipeline, while excluding special characters other than hyphen (-). | Delete Node | Delete the node. Use the undo edit graph option to revert the change. |\n| ACTION | DESCRIPTION |\n| Rename Node | Change the name of the node. Enter a new name and clickOk. |\n| Duplicate Node | Create a copy of the node. A duplicated operator retains the contents and configuration of the original, and has the name appended with a numeric tag; `-n`. The duplicate pipeline name must be lowercase, only include letters and numbers and have a different name to the original pipeline, while excluding special characters other than hyphen (-). |\n| Delete Node | Delete the node. Use the undo edit graph option to revert the change. |\n| Console | All q or Python editors in the Operator Configuration sections are executable and can be used to interactively develop complex functions. The current selection or line can be executed using 'Ctrl-Enter' or 'âEnter' and the output viewed in the Console tab in the bottom section of the pipeline tab. |\n| Data Preview | Results oftest deploysare displayed in the Data Preview tab in the lower panel of the pipeline tab.Click on an operator to see the results at each step along the pipeline. |\n\n\n## Building a pipeline\n\nTo build pipelines, start by adding and connecting operators within the pipeline builder. At a minimum, for a pipeline to produce any output, it requires a\nreader\nand a\nwriter\noperator. However, when building or debugging a complex pipeline, it can be useful to start with just a reader and test/deploy the pipeline iteratively as additional nodes are added.\nThis section provides step-by-step instructions. For an example see the\nProtocol Buffers walkthrough\n.\nNote\nIt is also possible to create a pipeline, in the Web Interface, using the\nImport Wizard\n, which guides you through the process of creating the pipeline.\nNote\nIf a database hasnât been created yet,\ncreate the database\nbefore proceeding with pipeline configuration. The initial database setup includes a default table and schema, and you can add more tables to the database schema as needed.\n- Open thepipelines taband click and add nodes to the pipeline builder.Add nodes by clicking and dragging them from the menu of operators on the left.Delete nodes by right-clicking on the node and selectingDelete Node.Join nodes using the operator-anchors. Click and drag an anchor on one operator to the anchor on another operator.To change a connection, rollover to display the move icon and drag-connect, remove with a right-click.Click a node to configure it using the configuration section on the right.Link operatorsYou can select from the following types of operators:Readersread data from an external or internal data source.Writerssend data to an internal or external data sink.Functionsprovide building blocks for transforming data by mapping, filtering or merging streams.Encodersmap data to serialize data formats that can be shared out of the system with writers.Decodersdeserialize incoming data into a kdb+ representation for manipulation.Windowsgroup data into time blocks for aggregating streams.String Utilitiesperform string operations on data.Transformoperators provide low code and no code transformations for manipulating data.Machine Learningoperators can be used to build prediction models of data in a stream.\n- Nodes that contain a main function, such asApplyandMapnodes, support customization of the parameters that are passed into that function. Seemain function parametersfor further details.\n- Configure each of the pipeline operators. Click on the node to display operator parameters in the panel on the right. The following screenshot shows the operator parameters for a Kafka node.Certain fields are required and you cannot save or deploy the pipeline without setting these field values.Development of stream processor pipelines within the UI allows for logic to be written in Python. Seepython code in pipelinesfor details.\n- Rename the pipeline by typing a new name in the name field in the top left corner.\n- ClickSaveto save the pipeline configuration.\n- It is advisable to test your pipeline before deploying it. Seetest deployfor details.\n- Ensure the database associated with this pipeline is deployed. A pipeline can be associated with a database prior to deployment, or deployed directly to an active database. Seedeploying the databasefor details.\n- When you are satisfied with your pipeline,deployit by clickingSave & Deploy.Once it's running, it shows in the list of running entities in theRecently Deployedsection, on the Overview page, and is added to the list of pipelines on thePipelines index.\nA running pipeline can then be queried. See\nquery data\nfor details.\nNote\nWhen using Kafka and Postgres pipelines, relevant charts must be installed, running, and accessible on the cluster.\nWarning\nWhen configuring pipelines, lower case must be used in all instances of names and references.\n\n### Customizing Main Function Parameters\n\nNodes that contain a main function also support customizing the parameters that are passed into that function. For most nodes, only the data from the previous node is passed into the main function as the only parameter. However, metadata and the operator definition can also be passed into the function if desired. This can be helpful when needing to action based on the data's metadata, or if needing to use Stream Processor functions like\n.qsp.set\nand\n.qsp.get\n. This works for both q and Python functions.\nWarning\nAfter changing these settings you must update your function to accept the new parameters and make use of them.\nNote\nThe 'main function' is the function defined at the top of the node configuration form. Some examples of nodes that use a 'main function' and have customizable parameters are the Apply and Map nodes. These parameter settings only apply to the main function field. Any subsequent functions defined after the main function are not affected by these settings.\n### Example **REMOVED and added xref to Protocl Buffers Walkthrough\n\nThe following example demonstrates the creation of a simple pipeline.\n\n!!!note \"For more examples take a look at some of the [guided walkthroughs.](../../../enterprise/walkthrough/index.md)\"\n\nBefore you build the pipeline, [create](../../database/configuration/ui/createdatabase.md#create-a-new-database) and [deploy](../../database/deploy/index.md) this database as follows:\n\n- Database name = **example-db** and add it to a new package called **example-package**\n- Table name = **eg-table1** with the following columns:\n<pre>\n    Column Name | Column Type | Parse Strings\n    [dashes] | [dashes] | [dashes]\n    dates        | Timestamp   | Auto\n    instance    | Symbol      | Auto\n    sym         | Symbol      | Auto\n    count         | Integer     | Auto\n</pre>\n1. Create a new pipeline called **example-pipeline**, as described [above](#building-a-pipeline), and add it to the **example-package** package.\n\n1. In the [pipelines tab](#pipelines-tab) drag the **Reader** node **Expression** into the pipeline builder.\n3. Click on the **Expression** node and add the following code to the operator configuration section on the right and click **Apply**.\n\n    <div class=\"code-snippets\"><div class=\"code-snippet\"><p class=\"snippet-selector\">q</p><MadCap:codeSnippet><MadCap:codeSnippetCopyButton /><MadCap:codeSnippetBody MadCap:useLineNumbers=\"False\" MadCap:lineNumberStart=\"1\" MadCap:continue=\"False\" xml:space=\"preserve\" style=\"mc-code-lang: Python;\">    n:2000;\n    ([] dates:n?(reverse .z.d-1+til 10);\n        instance:n?`inst1`inst2`inst3`inst4;\n        sym:n?`USD`EUR`GBP`JPY;\n        cnt:n?10)\n</MadCap:codeSnippetBody></MadCap:codeSnippet></div></div>\n    This q code sample generates a table with 2000 rows, each containing a random date from the last 10 days, a random instance from four options, a random currency symbol from four options, and a random count between 0 and 9.\n\n1. Drag the **Transform** node **Apply Schema** into the pipeline builder.\n1. Click on the Apply Schema node:\n\n    - leave the **Data Format** as **Any** (required).\n    - Click the **Load Schema** icon and select **example-db** as the database and **table-1** as the table.\n    - Click **Load**, to load the schema details and click **Apply**.\n\n1. Join the **Expression** node to the **Apply Schema** node.\n\n1. Drag the **Writer** node **kdb Insights Database** into the pipeline builder and connect it to the **Apply Schema** node.\n1. Click on the **kdb Insights Database** node and configure it as follows, then click **Apply**.\n\n        property           | setting\n        [dashes] | [dashes]\n        Select Database| **example-db**\n        Select Table | **table1**\n\n        Accept the default values for the other settings.\n\n1. Review the completed pipeline\n\n    ![Completed pipeline](images/ui/pipeline/pipeline-example.png)<br>\n\n1. Click **Save & Deploy**.\n1. Check the [status of your pipeline](#pipeline-status) and once it is `Finished` you can query the data.\n1. Click **+** beside **Queries** in the left-hand menu and enter the following in the **SQL** tab of the [Query and Load Data](../../analysis/query.md#query-using-sql) panel. \n\n    ```  \n    Select * from table1\n\n1. Click **Run Query**.\n\n    The results are displayed in the **Console** and show the data generated by the pipeline.\n\n## Copy pipeline into Package\n\nYou can copy a pipeline into another package.\n- ClickCopy into Packagetop open theCopy Pipeline into Packagedialog.\n- ThePipeline Nameis auto-filled withpipeline-name-copy. Change the name if required.\n- ClickSelect a PackageunderAdd a Package. Either select an existing package, to add this copy of the pipeline to, or clickCreate new packageto add the pipeline copy to a new package.\n- ClickCreate. The pipeline (including settings and nodes) is created in the selected or newly created package.The copied pipeline is opened.\n\n## Save, Test, Deploy and Teardown\n\n\n### Save\n\nIn the\npipeline tab\nclick\nSave\nto save the pipeline. A pipeline must be\ndeployed\nto ingest data.\n\n### Quick test\n\nTest deploys allow you to validate the output without any data being written from the writers.\nRun a quick test deploy in the scratchpad to validate that the pipeline works. See\ntest deploys\nfor more information.\n\n### Full test\n\nTest deploys allow you to validate the output without any data being written from the writers.\nRun a full test deploy using worker and controller pods to validate that the pipeline works. See\ntest deploys\nfor more information.\n\n### Deploy\n\nIt's advisable to\ntest deploy\nyour pipelines before performing a full deployment.\nWarning\nIf entitlements are enabled, but you donât have entitlements to a database, any pipeline you deploy that tries to read from that database will fail.\nBefore deploying a pipeline, ensure you have entitlements to all the databases that pipeline needs to access, especially if you're using a Database Reader.\nTo deploy your pipeline, click\nSave  Deploy\n. The pipeline initializes and starts running.\n- Seepipeline statusfor details on how to check if your pipeline has deployed successfully.\n- Deployed pipelines are deployed indefinitely.\n- If a deployed pipeline errors, it remains deployed.\n- To stop a running pipelinetear it down\n\n### Teardown\n\nTearing down a pipeline stops data processing.\n- ClickTeardownon thepipeline tabfor the active pipeline.\n- If you want to delete all pipeline checkpoints and user states, tick theClean up resources after teardowncheck box before you clickTeardown Pipeline.\n- ClickTeardown Pipelineto stop data processing.The status of the pipeline is updated toInactiveas soon as the teardown is complete.\n\n## Pipeline Actions\n\nThe pipeline actions menu can be accessed by:\n- Clicking on the three dots to the right of the pipeline details in thePipeline Index.\n- Opening thePipelinesdrop-down on the left-hand menu and clicking on the three dots beside the pipeline name.\n\n### Export a pipeline\n\nTo export a pipeline as a .yaml file:\n- ClickExportin the pipeline actions menu.A .yaml file is created with the name of the exported pipeline.\n\n### Rename a pipeline\n\nTo rename a pipeline:\n- ClickRenamein the pipeline actions menu.\n- Replace the current name with the new name.\n- ClickOK.\n\n### Delete a pipeline\n\nTo delete a pipeline:\n- ClickDeletein the pipeline actions menu.\n- ClickDeleteto confirm you want to proceed.The pipeline is deleted.\n\n## Python code within pipeline UI\n\nDevelopment of stream processor pipelines\nwithin the UI allows for logic to be written in Python. The following sections outline the requirements and caveats that you should bear in mind when developing such nodes.\n\n### Writing Python code within the pipeline UI\n\nWhen defining the code which is to be used for a given node you need to take the following into account:\n- The parsing logic used to determine the function used within a Python node works by retrieving for use the first function defined within the code block. The entire code block will be evaluated but it is assumed that the first function defined is that is retrieved is that used in execution.\n- Only Python functions are currently supported for definition of the execution function logic. You should not attempt to uselambdasor functions within classes to define the execution logic for a node. Additionally, while functions/variables defined within the global code block can be used within individual nodes, they cannot be referenced by name as the definition of the function to be used.\n- All data entering and exiting an executed Python node block receives and emits its data as aPyKXobject. ThePyKX documentationoutlines how these objects can be converted to common Python types and, additionally, includes notes on performance considerations that should be accounted for when developing code that interacts with PyKX objects.\nThe following provides an example of a map and filter node logic which could be used on a pipeline:\nSimple map node\nSimple filter node\nPython\nCopy\n\n```\nimport pykx as kxdef map_func(data):# compute a three-window moving average for a tablereturn kx.q.mavg(3, data)\n```\n\nPython\nCopy\n\n```\n# Function expected to return a list of booleans# denoting the positions within the data which are to# be passed to the next nodedef filter_func(data):return kx.q.like(data[0], b\"trades\")\n```\n\n\n## Next Steps\n\n- Read more about pipeline operators\n- Configure pipeline settings\n- Test pipeline configuration\n- Troubleshoot pipelines\n\n## Further Reading\n\n- Databases\n- Query data\n- Create visualizations\n- Guided walkthroughs",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3769,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-3098a1a52b3e",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Interfaces/vscode-overview.htm",
    "title": "VS Code Overview",
    "text": "\n# Visual Studio Code Extension\n\nThe kdb Visual Studio Code extension is a companion extension for kdb developers to edit q files, connect to kdb processes, and run queries. This VS code extension can be used alongside\nkdb Insights Enterprise\nwhen using a shared kdb process.\nYou can access this extension and its documentation on the\nVisual Studio Marketplace\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 60,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-0b04ba03c3f3",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/initial-import-overview.htm",
    "title": "kdb+ Database Initial Import Overview",
    "text": "\n# kdb+ Database Initial Import Overview\n\nThis page explains how to import an existing kdb+ database into\nkdb Insights Enterprise\n.\nkdb Insights Enterprise\nallows you to work with existing kdb+ databases. To do this, the Storage Manager (SM) needs to adjust the database to its own format. Once adjusted, this guarantees atomicity during write-down; and at the same time ensures that a database is mountable by a kdb+ process at any point in time. To achieve this, SM uses symbolic links to represent a standard kdb+ segmented database, while keeping the backing data in a proprietary structure.\nThree scenarios are supported:\n- Partitioned database on disk\n- Partitions only in object storage\n- Partitions on disk and partitions in object storage (the same date partition can't exist in both)\nNote\nSplayed and basic tables are also supported. These can only exist on local storage.\nNote\nData in object storage is excluded from this transformation, and kept in standard kdb+ format.\n\n## Import Phases\n\nThe initial import has two phases:\n- Copy kdb+ to target the data\n- Configure and deploy the database\n\n### Copy kdb+ data to target\n\nFirst, you must copy the kdb+ database to the target filesystem. In many cases this is a PVC on a Kubernetes cluster. The example below shows how the data is easily migrated with the aid of the PVC and POD creation yaml.\n\n### Configure and deploy kdb+ Insights database\n\nAfter the data has been copied to the target filesystem, the next step is to configure a database, assume ownership of that PVC and allow kdb Insights to start managing it.\nAfter deploying the database, the data is now available for query.\n\n## Next steps\n\n- To quickly try out this method, follow ourquick start.\n- Alternatively, follow theinitial import process.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 299,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-6ad1316a1517",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/import-wizard.htm",
    "title": "Import Wizard",
    "text": "\n# Import Wizard\n\nThis page describes the\nImport Wizard\n, a guided process for creating a pipeline to import data from a number of different sources, including cloud and relational data services.\nNote\nCreate database\nThe Import Wizard makes reference to a database and table, in the\nreader\nand\nwriter\nsteps of the process. Therefore, before proceeding with creating a pipeline using the Import Wizard, you must\ncreate a database\n.\nInformation\nGuided Walkthroughs using the Import Wizard\nThis page describes the elements of the Import Wizard which assist you in the pipeline building workflow. For detailed examples of using the Import Wizard to create pipelines for ingesting different types of data, refer to the relevant\nguided walkthroughs\n.\nInformation\nBuild pipelines manually in web interface\nAs an alternative to using the Import Wizard, you can build a pipeline manually through the web interface using the\nPipelines\nmenu.\n\n## Using the Import Wizard\n\nThis section describes the workflow for creating a pipeline using the\nImport Wizard\n.\n- To open the Import Wizard, use one of the following methods:ClickImport Data, underDatabasesin theQuick Actionspanel on theOverviewpage.Click on a database name, under theDatabasesin the left-hand menu, to open the database screen. ClickOther Actionsand clickImport Data.Click on the three dots beside the database name, underDatabasesin the left-hand menu and clickImport DataIn each case, the Import Wizard opens as shown below.\nThe wizard guides you through the following workflow to create a pipeline.\n- Select and configure a reader to ingest data into your kdb Insights Enterprise instance\n- Select and configure a decoder to convert the data into a format suitable for processing\n- Configure the schema to convert data to a type compatible with a kdb+ database\n- Configure the writer to transform data to the kdb Insights Enterprise database\n- Open the pipeline\n\n## Select and configure reader\n\nReaders enable both streaming and batch data to be ingested into\nkdb Insights Enterprise\n.\n- When you open theImport Wizard, as described in the previous section, theImport your datascreen is displayed.This contains the following set of readers which you can use to import data.KX ExpressionGoogle Cloud StorageKafkaMQTTMicrosoft Azure StorageParquetAmazon S3PostgreSQLSQL Server\nClick on the reader that matches the type of data you are ingesting. The wizard presents the configuration screen for the selected reader.\nClick on the relevant tab below for details.\nKX Expression\nGoogle Cloud Storage (GCS)\nKafka\nMQTT\nMicrosoft Azure Storage\nAmazon S3\nPostgreSQL\nSQL Server\nThe\nKX Expressions\nreader enables you to import data directly using a kdb+ expression.\nThe Kdb Expression Reader starts with a code editor. Add the following kdb+ sample data generator in the node editor:\nNote\ntext\nCopy\n\n```\nn:2000;([] date:n?(reverse .z.d-1+til 10);    instance:n?`inst1`inst2`inst3`inst4;    sym:n?`USD`EUR`GBP`JPY;    num:n?10)                  \n```\n\nRefer to\nExpression\nfor further details on configuring this reader.\nThe\nGoogle Cloud Storage\nreader enables you to take advantage of object storage by importing data from Google Cloud Storage directly into\nkdb Insights Enterprise\n.\nDefine the Reader node with GCS details. Default values can be used for all but\nPath\nand\nProject ID\n.\nProject ID\nmay not always have a customizable name, so its name is contingent on what's assigned when creating your GCS account.\nNote\nSample GCS Reader Node\n- Path:gs://mybucket/pricedata.csv\n- Project ID:ascendant-epoch-999999\nOther required properties can be left at default values.\nRefer to\nGoogle Cloud Storage\nfor further details on configuring this reader.\nRefer to the\nObject storage walkthrough\nfor an example using Google Cloud Storage.\nThe\nKafka\nreader enables you to connect to an Apache Kafka distributed event streaming platform.\n- Define the Kafka Broker details, port information and topic name.\nNote\nSample Kafka Reader Node\n- Broker:kafka:9092(note: not a working example)\n- Topic:trades\n- Offset:Start\n- Use TLS:No\nRefer to\nKafka\nfor further details on configuring this reader.\nRefer to the\nKafka guided walkthrough\nfor an example using Kafka.\nThe\nMQTT\nreader enables you to subscribe to a MQTT topic.\nRefer to\nMQTT\nfor details on configuring this reader.\nRefer to the\nManufacturing tutorial\nfor an example using MQTT.\nThe\nMicrosoft Azure Storage\nreader enables you to take advantage of object storage by importing data from Microsoft Azure Storage directly into\nkdb Insights Enterprise\n.\nDefine the Microsoft Azure Storage node details. Default values can be used for all but\nPath\nand\nProject ID\n.  A customizable account name is supported for ACS.\nNote\nSample ACP Reader Node\n- Path:ms://mybucket/pricedata.csv\n- Account:myaccountname\nOther required properties can be left at default values.\nRefer to\nMicrosoft Azure Storage\nfor further details on configuring this reader.\nRefer to the\nGuided walkthrough\nfor an example using Microsoft Azure Storage.\nThe\nAmazon S3\nreader enables you to take advantage of object storage by importing data from Amazon S3 Storage directly into\nkdb Insights Enterprise\n.\nDefine the Reader node with the S3 path details for how the file is to be read and optionally, the Kubernetes secret for authentication. Default values can be used for all but\nPath\nand\nRegion\n.\nNote\nSample Amazon 3 Reader Node\n- Path:s3://bucket/pricedata.csv\n- Region:us-east-1\nRefer to\nAmazon S3\nfor further details on configuring this reader.\nRefer to the\nObject Storage guided walkthrough\nfor an example using Amazon S3.\nThe\nPostgreSQL\nreader executes a query on a PostgreSQL database.\nDefine the Reader node, including any required authentication, alongside server and port details.\nNote\nSample Postgre Reader Node\n- Server:postgresql\n- Port:5432\n- Database:testdb\n- Username:testdbuser\n- Password:testdbuserpassword\n- query:select * from cars\nWhere Server is\npostgresql.default.svc\n, the\ndefault\nis the namespace. For example,\ndb\nis\npostgresql.db.svc\nNote\nNode properties and queries are case sensitive\nand should be lower case.\nRefer to\nPostgreSQL\nfor further details on configuring this reader.\nThe\nSQL Server\nreader executes a query on a SQL Server database.\nRefer to\nSQL Server\nfor  details on configuring this reader.\nRefer to the\nSQL Database guided walkthrough\nfor an example using SQL Server.\nNext, select and configure a decoder.\n\n## Select and configure a decoder\n\nDecoders convert data to a format that can be processed.\nOnce you have\nselected and configured a reader\n, select a decoder from the list of options:\n- No Decoder\n- Arrow\n- CSV\n- JSON\n- Pcap\n- Protocol Buffers\nRefer to\nDecoders\nfor details on configuring each of these decoders.\nIn addition, some readers have specific decoder requirements. Click on the tabs below for details.\nKX Expression\nGoogle Cloud Storage\nKafka\nMicrosoft Azure Storage\nAmazon S3\nNo decoder is required for\nKX Expression\n.\nThis data requires a\nCSV decoder\nnode.  Google Cloud Storage data requires a\ntimestamp\ncolumn to parse the data.\nNote\nTypical CSV decoder Node\n- Delimiter:,\n- Header:First Row\n- Encoding Format:UTF8\nEvent data on Kafka is of JSON type, so a\nJSON decoder\nis required to transform the data to a kdb+ friendly format. Data is converted to a kdb+ dictionary. There is no need to change the default setting for this.\nNote\nJSON decode\n- Decode Each:false\nThis data requires a\nCSV decoder\nnode.  Microsoft Azure Storage data requires a timestamp column to parse the data.\nNote\nTypical CSV decoder Node\n.\n- Delimiter:,\n- Header:First Row\n- Encoding Format:UTF8\nThis data requires a\nCSV decoder\nnode. Amazon S3 data requires a timestamp column to parse the data.\nNote\nTypical CSV decoder Node\n- Delimiter:,\n- Header:First Row\n- Encoding Format:UTF8\nOther required properties can be left at default values.\nNext, configure the schema.\n\n## Configure the schema\n\nOnce you have\nselected and configured a decoder\n, configure the schema. The schema converts data to a type compatible with a kdb+ database. Every imported data table requires a schema; and every data table must have a timestamp key to be compatible with kdb's time series columnar database.\nThe schema configuration can be done manually, or preferably, loaded from a database. Click on a tab below for details on each method.\nManually define schema\nLoad Schema from a database\n- Click+in the sectionAdd schema columns belowand add each column setting a value forColumn NameandColumn Type.Parse Stringscan be left atAuto.\nThe following table provides a sample schema configuration.\n\n| Column name | Column type | Parse strings |\n| --- | --- | --- |\n| Date | Timestamp | Auto |\n| instance | Symbol | Auto |\n| symbol | Symbol | Auto |\n| num | Integer | Auto |\n\nA schema can also be loaded from a database by clicking\n.\nRefer to\nApply Schema\nfor details on the configuration options for this node.\nRefer to\nschemas\nfor further information on schema configuration.\nNext, configure the writer.\n\n## Configure the writer\n\nThe writer writes transformed data to the\nkdb Insights Enterprise\ndatabase.\nAfter you have selected and configured a schema, configure the writer.\nThe writer configuration can be done manually, or preferably, by selecting an existing database. Click on a tab below for details on each method.\nManually define schema\nSelected from existing database\n- Click+ Databaseto go to theCreate Databasescreen.\n- Following database creation instructions described inDatabases.\n- ClickSelect Databaseto choose the database to write data to.\n- ClickSelect Tableto choose the table, in the selected database, to write data to.\nThese are usually the same database and table from the\nschema creation\nstep.\nRefer to\nkdb Insights Database writer\nfor further details.\nNote\nSchema Column Parsing\nParsing is done automatically as part of the import process. If you are defining a manual parse, ensure parsing is enabled for all time, timestamp, and string fields unless your input is IPC or RT.\nNext, open the pipeline.\n\n## Open the pipeline\n\nAfter you have\nselected and configured a writer\n, open the pipeline.\n- ClickOpen Pipelineto open theCreate Pipelinedialog.\n- Enter a uniquePipeline Namethat is 2-32 characters long, uses lowercase letters, numbers, and hyphens, and starts and ends with an alphanumeric character.\n- ClickSelect a packageunderAdd to Package, to display the list of packages this pipeline can be added to.ClickCreate new packageif you want to add the pipeline to a new package. Enter a unique package name that is 2-32 characters long, uses lowercase letters, numbers, and hyphens, and starts and ends with an alphanumeric character.Seepackagesfor further details about packages.\nTip\nPipeline editor\nOnce a pipeline has been created, using the Import Wizard, it can be modified using the\nPipeline editor\n.\nNote\nAdvanced Settings\nEach pipeline created by the\nImport wizard\nhas an option for Advanced pipeline settings. The Advanced settings include\nGlobal Code\nwhich is executed before a pipeline is started; this code may be necessary for Stream Processor hooks, global variables and named functions.\nSee\nPipeline Settings\nfor further details.\nNext, if you are configuring a\nKafka\npipeline you need to add and configure a map node. Refer to the\nadditional step for Kafka\nfor details.\nOtherwise you can proceed to\ndeploying the pipeline\n.\n\n## Additional step for Kafka\n\nFor Kafka pipelines, built using the\nImport Wizard\n, a\nMap\nfunction node is required to convert the data from a kdb+ dictionary to a kdb+ table.\n- In the search box of the pipeline screen, enterMapto locate theMapfunction. Click and drag this node onto the pipeline workspace.\n- Insert theMapbetween theDecoderand theTransformSchema as shown below:\n- Click on theMapnode and in theConfigure Map Nodepanel replace the existing code with the following.NoteMap functiontextCopy{[data]enlist data}\n- ClickApply.\n- ClickSave.\nNext, you can deploy the pipeline.\n\n## Deploy the pipeline\n\nTo deploy the pipeline:\n- In the pipeline template screen, clickSave & Deploy.The pipeline is deployed when the status isRunning. This is indicated with a green tick beside the pipeline name in the left-handPipelineslist.Readpipeline statusfor further details on pipeline status values.\n\n## Next steps\n\nAfter you have completed the steps outlined in the sections above to create a pipeline using the Import Wizard you are ready to:\n- Query data\n- Visualize the data\n\n## Further reading\n\n- Guided walkthroughs\n- Database schemas\n- Create pipelines manually in the web interface",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1980,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-ee5f2c569453",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/architecture.htm",
    "title": "Architecture",
    "text": "\n# kdb Insights EnterpriseArchitecture\n\nThis page explores how the underlying components of\nkdb Insights Enterprise\nwork together.\nkdb Insights Enterprise\nis a powerful platform designed to capture, process, and analyze time-series data. At its core, the system brings together three key components that work together to create data workflows:\n- Databases: (Storage Manager and Data Access) Responsible for storing data and managing data access.\n- Streams: (Reliable Transport) Provide reliable data transport.\n- Pipelines: (Stream Processor) Process the data in real time.\nThese are grouped into a\npackage\n. You can deploy as many packages as you need, whether the data is related or comes from entirely different sources.\n\n## Dynamic workload creation\n\nThe life cycle of each package deployed into\nkdb Insights Enterprise\nis managed by an internal Kubernetes operator. The operator takes care of creation and placement of all required configuration files, deployments, and services, to bring an end-to-end data workflow online. The configuration of these workloads can be changed dynamically, and\nkdb Insights Enterprise\nautomatically updates the underlying workflow components.\n\n## Data capture\n\nThe Stream Processor is used for getting data in, streaming data out, and creating streaming analytics such as derived data and alerts. Multiple input and output sources are built-in. For anything not built-in, data can be written directly to a Stream (Reliable Transport) via the\nC or Java RT SDK\n. The Stream Processor can then read the data from that stream if required, or it can be written directly into a database.\n\n## Data exploration\n\nEach database is associated with a label set (a key/value set) of metadata which associates it with other databases that have an overlapping subset of labels. This allows queries to target a single database, or aggregate across multiple databases by specifying the common label set between databases.\n\n## Scalability\n\nkdb Insights Enterprise\nis built on top of Kubernetes, allowing scaling and orchestration to be handled by the underlying cloud platform.\nkdb Insights Enterprise\nleverages this capability by allowing scaling for a number of purposes.\n\n### Scaling for query load\n\nEach database takes the form of a single-writer, many-reader paradigm, with each of these being kdb+ processes.\nData is split into temporal tiers and automatically migrated between tiers (the most immediate data is held in memory, daily data is held on disk partitioned by arrival bucket, and historical data is held on disk and in object storage partitioned by arrival date). Each of these ranges is surfaced for queries by a separate class of data access process (RDB, IDB, and HDB respectively). Each of these allows multiple dynamic replicas to be set, and this number can change as required.\nData is written to fast shared storage, allowing query processes to span multiple nodes.\n\n### Scaling for increased ingestion\n\nAs data sizes grow for a given dataset, or if new datasets are added to an existing deployment,\nkdb Insights Enterprise\ncan scale horizontally to accommodate the additional data.\nSince each database is a single writer,\nkdb Insights Enterprise\nscales for additional ingestion by creating more databases.\n\n## Fault tolerance\n\nkdb Insights Enterprise\nis designed with process level redundancy and node assignment in mind; this allows for individual process, node, and availability zone failure without impacting primary data functionality.\nAll package components allow multiple redundant replicas:\n- Multiple Stream (Reliable Transport) replicas making use of RAFT for coordination\n- Optional multiple DAP database read replicas\n- Optional multiple Stream Processor replicas with output deduplication handled by the receiving stream\nThe exception is the database writer (Storage Manager), which is a single replica within each database. However, since the writer is not visible to queries, a failure or rescheduling of the writer does not impact ingestion and query workloads. This is because Kubernetes and the kdb Insights Operator bring the writer back online on a working node. During this time, additional data is held in memory in the real-time (RDB) data tier. When the writer comes back online, it continues to write data from the point where it left off.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 668,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-5d9673329974",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/import-data.htm",
    "title": "Import Data into kdb Insights Enterprise",
    "text": "\n# Import Data intokdb Insights Enterprise\n\nThis page explains the different ways you can import data into\nkdb Insights Enterprise\nand in which situations they are recommended.\nYou can import data into\nkdb Insights Enterprise\nin one of two ways:\n- Initial import- allows you to import data from an existing kdb+ database into a new, empty kdb+ Insights database.\n- Batch ingest- allows you to backfill static data directly to the historical tier of an existing database.\nIt is important to draw a clear distinction between an initial import and a batch ingest.\n\n## Initial import\n\nImporting data using initial import can only be used with a\ndatabase that has never been initialized\nbefore and consequently does not contain any data.\nTo import data using this method, refer to the\ninitial import documentation\n.\n\n## Batch ingest\n\nImporting data using batch ingest is recommended for adding data to an\nalready initialized and running database\nthat may also contain data.\nTo import data using this method, refer to the\nbatch ingest documentation\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 173,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-09b415ac2e58",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/streaming-kafka-ingest.htm",
    "title": "Streaming Kafka Ingest",
    "text": "\n# Streaming Kafka Ingest\n\nThis page describes how to read data from Kafka and visualize it in a kdb Insights Enterprise View.\nApache Kafka is an event streaming platform that seamlessly integrates with\nkdb Insights Enterprise\n, enabling real-time data processing through pipelines connected to Kafka data sources. For this example, we have provided a Kafka subway feed that generates live alerts for NYC Subway trains, including arrival times, station coordinates, direction, and route details.\nThis example shows how to use the\nCLI\nto do the following:\n- Download the packagecontaining the pipeline and view.\n- Deploy the packageto read subway data from Kafka, decode the data using a JSON decoder, and write the data to thesubwaysubscriber.\n- View datain the web interface.\n- Check progressof the running pipeline.\n- Teardowna previously installed version of the package.\nBefore you begin, you must complete the\nprerequisites\ndescribed in the following section.\nNote\nStreaming subway data\nRefer to the\nStreaming subway data to a data grid in Views\nwalkthrough for instructions on setting up and deploying a package to ingest and view Kafka data using the web interface.\n\n## Prerequisites\n\n- Configure a username and bearer token, for the KX Downloads Portal, as environment variables. To do this:Click theKX Downloads Portallink and follow the steps to login.Click your login name, in the top right-hand corner, and clickToken Management.ClickAdd New Token.Copy the value in theBearerfield and save it for use later.Use the following commands to set the environment variables, whereKX_USERis the username you used to login to the KX Downloads Portal, andKX_DL_BEARERis the bearer token you just generated.bashCopyexportKX_USER=<USERNAME>exportKX_DL_BEARER=<TOKEN>\n- A runningkdb Insights Enterprisesystem with the hostname URL configured as the value for theKX_HOSTNAMEenvironment variable.bashCopyexportKX_HOSTNAME=<URL>\n- TheCLIconfigured for your deployment with authentication credentials\n\n## Downloading the package\n\n- Run the following command to setKX_VERSIONenvironment variable in your terminal.bashCopyexportKX_VERSION=<VERSION>\n- Download the sample package with the command below.bashCopycurl -s --fail-with-body -D /dev/stderr -u${KX_USER}:${KX_DL_BEARER}-L -OJ https://portal.dl.kx.com/assets/raw/package-samples/${KX_VERSION}/kafka-ingest-${KX_VERSION}.kxi\n- Run the following command to unpack and view the contents of the kafka-ingest sample package.bashCopyexportPKG=kafka-ingestkxi package unpack$PKG-$KX_VERSION.kxiThekafka-ingestsample package contains the files listed below and described in the following table:bashCopykafka-ingest/âââ manifest.yamlâââ pipelinesâ   âââ subway.yamlâââ srcâ   âââ ml.pyâ   âââ ml.qâ   âââ subway.pyâ   âââ subway.qâââ viewsâââ subway.yaml3 directories, 7 filesArtefactDescriptionmanifest.yamlUsed by the CLI for package management.Do not modifythe contents of this file.pipelinesThe pipeline configuration files,subway.yaml.srcPipeline spec files.subway.q: this is default pipeline spec, written in q. This is located underkafka-ingest/src/subway.qin the unpacked package.subway.py: The equivalent Python pipeline spec is located underkafka-ingest/src/subway.py.ml.q/ml.py: ML q and Python examples are also contained within the package for reference.viewsThe view configuration files,subway.yaml.\nClick on the q or Python tab below to view the contents of the q and Python versions of the\nsubway\npipeline spec.\nq\nPython\nkafka-ingest/src/subway.q\nq\nCopy\n\n```\nsubway: ([] trip_id: `symbol$(); arrival_time: `timestamp$(); stop_id: `symbol$(); stop_sequence: `long$(); stop_name: `symbol$(); stop_lat: `float$(); stop_lon: `float$(); route_id: `long$(); trip_headsign: `symbol$(); direction_id: `symbol$(); route_short_name: `symbol$(); route_long_name: `symbol$(); route_desc: `char$(); route_type: `long$(); route_url: `symbol$(); route_color: `symbol$()).qsp.run    .qsp.read.fromKafka[.qsp.use (!) . flip (        (`brokers ; \"kafka.trykdb.kx.com:443\");        (`topic   ; \"subway\");        (`options; (!) . flip (            (`security.protocol ; \"SASL_SSL\");            (`sasl.username     ; \"demo\");            (`sasl.password     ; \"demo\");            (`sasl.mechanism    ; \"SCRAM-SHA-512\"))))]    .qsp.decode.json[]    .qsp.map[{ enlist x }]    .qsp.transform.schema[subway]    .qsp.write.toSubscriber[`subway;`trip_id]                    \n```\n\nkafka-ingest/src/subway.py\nPython\nCopy\n\n```\nschema = {'trip_id': pykx.SymbolAtom,        'arrival_time': pykx.TimestampAtom,        'stop_id': pykx.SymbolAtom,        'stop_sequence': pykx.ShortAtom,        'stop_name': pykx.SymbolAtom,        'stop_lat': pykx.FloatAtom,        'stop_lon': pykx.FloatAtom,        'route_id': pykx.ShortAtom,        'trip_headsign': pykx.SymbolAtom,        'direction_id': pykx.SymbolAtom,        'route_short_name': pykx.SymbolAtom,        'route_long_name': pykx.SymbolAtom,        'route_desc': pykx.List,        'route_type': pykx.ShortAtom,        'route_url': pykx.SymbolAtom,        'route_color': pykx.SymbolAtom}sp.run(sp.read.from_kafka('subway',                        brokers='kafka.trykdb.kx.com:443',                        options={'sasl.username': 'demo',                                'sasl.password': 'demo',                                'sasl.mechanism': 'SCRAM-SHA-512',                                'security.protocol': 'SASL_SSL'})    | sp.decode.json()    | sp.transform.schema(schema)    | sp.write.to_subscriber('subway', 'trip_id'))                    \n```\n\nThe\nsubway\npipeline spec file can be summarised as follows:\n\n| Object/Node | q Object | Python Object | Description |\n| --- | --- | --- | --- |\n| Schema | table | pykx.Dict | The schema definition used for type conversion when parsing incoming data. |\n| Read from Kafka | .qsp.read.fromKafka | sp.read.from_kafka | Setup Kafka reader with topic, broker and security config. |\n| Decode JSON | .qsp.decode.json | sp.decode.json | Use JSON decoder to decode incoming data. |\n| Transform Schema | .qsp.transform.schema | sp.transform.schema | Parse the incoming data using the defined schema. |\n| Write to Subscriber | .qsp.write.toSubscriber | sp.write.to_subscribe | Write the data to a subscriber named subway with trip_id key column. |\n\n\n## Deploying the package\n\nNext, authenticate with\nkdb Insights Enterprise\nand deploy the package to begin reading the subway data from Kafka and writing it to the subscriber/web-socket.\n- Run the following command to authenticate withkdb Insights Enterprise:bashCopykxi auth login\n- Perform the following setup for Python (skip to 3 for q):Unpack the sample package.bashCopykxi package unpack$PKG-$KX_VERSION.kxiReplace the values ofbaseandspecinpipelines/subway.yamlto use the Python spec file instead of the q one.yamlCopy# base: qbase: pyyamlCopy# spec: src/subway.qspec: src/subway.py\n- Run the following command to deploy the package:bashCopykxi pm push$PKGkxi pm deploy$PKGNoteCleaning up resourcesIt may be necessary toteardownany previously installed version of the package and clean up resources before deploying a new version of the package.\n\n## Viewing output\n\nThe\nsubway\nView is included in the kafka-ingest sample package.\nTo view data for all inbound trains on the 6, 7 or 8\nth\nAvenue Express routes:\n- Log into thekdb Insights EnterpriseWeb Interface\n- Navigate to theViewstab and click on thesubwayView.\n\n## Checking progress\n\nTo check the progress of the running pipeline, use the\nkdb Insights Enterprise\nREST APIs.\n- Get the token generated when you authenticated against the system.bashCopyexportKX_TOKEN=$(kxi authprint-token)NoteRegenerate expired tokenIf this token expires, you can regenerate it by runningkxi auth loginagain and re-storing with the command above.\n- Check the progress of the SP pipeline using the details API, as follows:bashCopycurl -H\"Authorization: Bearer$KX_TOKEN\"${KX_HOSTNAME}/streamprocessor/detailsA pipelinestateofRUNNINGindicates that it is processing data.bashCopy[{\"id\":\"kafka-ingest-subway\",\"name\":\"subway\",\"group\":\"g-794319749428\",\"mode\":\"default\",\"created\":\"2025-03-03T12:22:29.000000000\",\"lastSeen\":\"2025-03-03T12:27:11.389548559\",\"state\":\"RUNNING\",\"error\":\"\",\"metrics\":{\"eventRate\":0,\"bytesRate\":0,\"latency\":null,\"inputRate\":1328.754,\"outputRate\":2.745616},\"logCounts\":{\"trace\":0,\"debug\":0,\"info\":157,\"warn\":3,\"error\":0,\"fatal\":0},\"packageName\":\"kafka-ingest\",\"reportedMetadata\":[{\"id\":1,\"name\":\"kafka-ingest-subway-1-spwork\",\"reportedMetadata\":[{\"operatorID\":\"subscriber_subway\",\"plugin\":\"subscriber\",\"cacheLimit\":2000,\"keyCol\":\"trip_id\",\"publishFrequency\":null,\"table\":\"subway\"}],\"pipeID\":\"kafka-ingest-subway\"}]}]\n\n## Teardown\n\nYou must teardown a previously installed version of the package and clean up resources before you can deploy a new version of the package.\n- To teardown packages and their resources, run the command below.bashCopykxi pm teardown --rm-data$PKG\n\n## Further reading\n\n- Command line interface overview\n- Web Interface\n- Guide to building visualizations with Views\n- Stream Processor APIs",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 992,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-4440673376ea",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/postgre-sql-ingest.htm",
    "title": "Postgre SQL Ingest",
    "text": "\n# PostgreSQL Ingestion\n\nThis page demonstrates how to setup a simple PostgreSQL database and query it from the kdb Insights Stream\nProcessor\n.\nPostgreSQL\nis an open-source relational database system. This example demonstrates how to setup a simple PostgreSQL database and query it from the kdb Insights Stream Processor.\nThis example is split into a\ninstall\nand\nquery\n. If you already have a running\nPostgreSQL database, consider jumping to the query section.\n\n## Install a PostgreSQL database\n\nThis example uses scripts provided by\nBitnami\nto install a PostgreSQL database.\nDocker\nKubernetes\nThe following\ninitdb.sql\nis provided as an example to setup a database with mock data.\nSQL\nCopy\n\n```\nCREATE DATABASE finance;\\c financeCREATE TABLE stocks (    id int,    sym varchar(10),    market varchar(10),    name varchar(255),    cap varchar(20));INSERT INTO stocks VALUES (1, 'AAPL', 'NASDAQ', 'Apple Inc.', '$2.47T');INSERT INTO stocks VALUES (2, 'MSFT', 'NASDAQ', 'Microsoft', '$2.32T');\n```\n\nTo launch the PostgreSQL database in Docker, add a PostgreSQL service to a\ndocker-compose.yaml\nyaml\nCopy\n\n```\nversion: \"3.3\"services:  psql:    image: docker.io/bitnami/postgres:latest    ports:      - 5432:5432    environment:      POSTGRES_USER: postgres      POSTGRES_PASSWORD: iamsecure    restart: always    volumes:      - ./initdb.sql:/docker-entrypoint-initdb.d/initdb.sql\n```\n\nFinally, run\ndocker-compose up\nto launch the database.\nBeing by installing a PostgreSQL database into the current Kubernetes cluster.\nbash\nCopy\n\n```\nhelm install postgresql bitnami/postgresql                    \n```\n\nThis results in a\npostgresql\nimage being launched in the current Kubernetes context.\nbash\nCopy\n\n```\nkubectl get pods | grep postgresql                    \n```\n\nbash\nCopy\n\n```\nNAME                      READY   STATUS    RESTARTS   AGEpostgresql-postgresql-0   1/1     Running   0          4h16m                    \n```\n\nThe PostgreSQL image will generate a random password. Extract the password from the cluster and store it in\nand environment variable called\nPGPASSWORD\nfor later use.\nbash\nCopy\n\n```\nexport PGPASSWORD=$(kubectl get secret postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)                    \n```\n\nAdding mock data\nNow that the database is running, add some mock data by interacting with a PostgreSQL client. Launch\nan interactive client by running the following.\nbash\nCopy\n\n```\nkubectl run postgresql-client --rm --tty -i --restart='Never' \\    --image docker.io/bitnami/postgresql:latest \\    --env=\"PGPASSWORD=$PGPASSWORD\" \\    --command -- psql --host postgresql -U postgres -d postgres -p 5432                    \n```\n\nTo add a mock\nstocks\ntable with some mock data, run each of the following commands in the client shell from above.\nSQL\nCopy\n\n```\nCREATE DATABASE testdb;CREATE USER testdbuser WITH PASSWORD 'testpass';GRANT ALL PRIVILEGES ON DATABASE testdb TO testdbuser;CREATE TABLE stocks (id int, sym varchar(10), market varchar(10), name varchar(255), cap varchar(20));INSERT INTO stocks VALUES (1, 'AAPL', 'NASDAQ', 'Apple Inc.', '$2.47T');INSERT INTO stocks VALUES (2, 'MSFT', 'NASDAQ', 'Microsoft', '$2.32T');                    \n```\n\nThe end result should have a table with the mock data added\nSQL\nCopy\n\n```\nSELECT * FROM stocks                    \n```\n\ntext\nCopy\n\n```\n id | sym  | market |    name    |  cap----+------+--------+------------+--------  1 | AAPL | NASDAQ | Apple Inc. | $2.47T  2 | MSFT | NASDAQ | Microsoft  | $2.32T                    \n```\n\nFor next steps, see the guide on issuing a PostgreSQL query from the Stream Processor, continue to the\nquerying\nguide.\n\n### Data examples\n\nAnother example using car sample data follows:\nSQL\nCopy\n\n```\nCREATE TABLE cars (id int, Name varchar(250), \"Miles_per_Gallon\" smallint, \"Cylinders\" smallint, \"Displacement\" smallint, \"Horsepower\" smallint, \"Weight_in_lbs\" smallint NOT NULL, \"Acceleration\" smallint, \"Year\" date NOT NULL, \"Origin\" character varying(60));INSERT INTO cars VALUES (1, 'chevrolet chevelle malibu', 18, 8, 307, 130, 3504, 12, '1970-01-01', 'USA');INSERT INTO cars VALUES (2, 'volkswagen 1131 deluxe sedan', 26, 4, 97, 46, 1835, 21, '1970-01-01', 'Europe');                    \n```\n\nThe final table data should be as follows:\nSQL\nCopy\n\n```\nSELECT * FROM cars                    \n```\n\ntext\nCopy\n\n```\nid |             name             | Miles_per_Gallon | Cylinders | Displacement | Horsepower | Weight_in_lbs | Acceleration |    Year    | Origin----+------------------------------+------------------+-----------+--------------+------------+---------------+--------------+------------+--------  1 | chevrolet chevelle malibu    |               18 |         8 |          307 |        130 |          3504 |           12 | 1970-01-01 | USA  2 | volkswagen 1131 deluxe sedan |               26 |         4 |           97 |         46 |          1835 |           21 | 1970-01-01 | Europe\n```\n\n\n## PostgreSQL queries\n\nThis section provides an overview of integrating the Stream Processor with a PostgreSQL database.\nNote\nThis example uses a mock database which can be setup by following\nthe PostgreSQL setup guide\nThe Stream Processor provides a reader interface for issuing queries on a PostgreSQL database. The\nPostgreSQL Reader API\ncan be used as a data source in a pipeline.\nThe following\nspec.q\nwill run a select query against a \"finance\" database and write it to\nthe console. The details of the database will be configured during deployment.\nq\nCopy\n\n```\n.qsp.run    .qsp.read.fromPostgres[\"SELECT * FROM stocks\"; \"finance\"]    .qsp.write.toConsole[]                  \n```\n\nNote\nThe example below requires a PostgreSQL database to be running in the cluster as setup\nin the Kubernetes section of\nthis tutorial\n.\nbash\nCopy\n\n```\njobname=$(curl -X POST http://localhost:5000/pipeline/create -d \\    \"$(jq -n  --arg spec \"$(cat spec.q)\" --arg pass \"$PGPASSWORD\" \\    '{        name     : \"psql\",        type     : \"spec\",        config   : { content: $spec },        settings : { minWorkers: \"1\", maxWorkers: \"10\" },        env      : {            KXI_SP_POSTGRES_SERVER   : \"postgresql\",            KXI_SP_POSTGRES_PORT     : \"5432\",            KXI_SP_POSTGRES_DATABASE : \"finance\",            KXI_SP_POSTGRES_USERNAME : \"postgres\",            KXI_SP_POSTGRES_PASSWORD : $pass        }    }' | jq -asR .)\" | jq -r .id)                    \n```\n\nOnce deployed, check the console output of the deployed\nspwork\npod to see the result of the query.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 848,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-67dfdb3ce09e",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/package-overview.htm",
    "title": "Package Overview",
    "text": "\n# Package Overview\n\nThis page provides an overview of\nkdb Insights Enterprise\npackages.\nkdb Insights Enterprise\npackages provide a comprehensive solution by bundling tools and resources into a single, easy-to-use format. Packages streamline the management of various elements within the\nkdb Insights Enterprise\nplatform. They simplify the complexities of data management by providing instructions on how to ingest, store, and query datasets, making the process accessible and actionable for those who require access.\nNote\nkdb Insights Enterprise\nhas both a\nweb interface\nand\ncommand line interface (CLI)\n. The web interface is a visual, browser-based tool for creating, querying, and visualizing data, while the CLI is a text-based tool for managing installations, configurations, and deployments programmatically. You can use either one to manage your packages.\n\n## Package structure\n\nThe package structure describes the set of files and sub-folders which represent components that control different aspects of the overall package.\nThe files and sub-folders for a newly created package look like the listing below:\nbash\nCopy\n\n```\n$ kxi package init test-package$ tree test-packagetest-packageâââ init.qâââ manifest.yaml\n```\n\nYou can extend this to contain additional components. The following more complex package contains a database, a pipeline, and arbitrary code:\nbash\nCopy\n\n```\nâââpkg-a âââ databases â âââ mydb â âââ shards â â âââ mydb-shard.yaml â âââ tables âââ init.q âââ manifest.yaml âââ pipelines â âââ pipe1.yaml âââ src âââ init.q âââ pipe1.q\n```\n\n\n## Package manifest file\n\nThe\nmanifest.yaml\nfile serves as the packages index, describing configurable sections and dependencies. The file is crucial for the package's use by\nkdb Insights Enterprise\n.\nWhen a package is created, the\nmanifest.yaml\nfile has the following structure.\nbash\nCopy\n\n```\nkxi package -q init test-package --force && cat test-package/manifest.yaml\n```\n\nconsole\nCopy\n\n```\n# yaml-language-server: $schema=https://code.kx.com/insights/enterprise/packaging/schemas/package.json#/$defs/Manifestuuid: 9e7a570c-07d5-451f-9da7-9c748a3f1c37name: test-packageversion: 0.0.1dependencies: []metadata: {}entrypoints:  default: init.qtables: {}databases: {}pipelines: {}views: {}patches: []\n```\n\nNote\nEditing the manifest file directly is not recommended, unless you are directed to do so.\nThe manifest is designed to reflect what is in the package. You should manipulate the manifest file through the\nkxi package\ntool, where possible. For example, if you change a pipeline name in the manifest and then run the\nkxi package refresh\ncommand, the manifest file changes the name back to the original name again.\n\n### Package manifest file sections\n\nWarning\nDo not delete any fields defined within the\nmanifest.yaml\nfile as part of the creation of a package!\nThe following table provides a brief description of each configurable section within the\nmanifest.yaml\nand whether the definition of its content is required.\n\n| section | description | required |\n| --- | --- | --- |\n| uuid | The unique identifier for the package. | yes |\n| name | The default name assigned to the package when building it. | yes |\n| version | The default version number associated with the package when building it. | yes |\n| dependencies | Any explicit dependencies on additional packages. For more information, refer toPackage dependencies. | no |\n| metadata | Information about the package contents and the users who have contributed to it. | no |\n| entrypoints | The set of possible q/Python files used as the component for a package. For more information, refer toPackage entrypoints. | no |\n| build | Build, clean, and extra commands for a package. |  |\n| system | Information about the system conditions under which the package was generated, including the version of the CLI which was used to create the package. | no |\n| license | The relative path to the license file under which the defined package is intended to be released. | no |\n| tables | Tables linked to databases to deploy onkdb Insights Enterprise. | no |\n| databases | The databases to deploy onkdb Insights Enterprise. | no |\n| pipelines | Any Stream Processor pipeline definitions to deploy onkdb Insights Enterprise. | no |\n| routers | AnyRoutersto deploy onkdb Insights Enterprise. | no |\n| reports | AnyViewsto deploy onkdb Insights Enterprise. | no |\n| deployment_config | Path to deployment_config to deploy onkdb Insights Enterprise. | no |\n| udf_namespaces | This denotes the tagged names which are searched when parsing the package text foruser-defined functions. | no |\n| patches | Patchesto apply to this package. | no |\n\n\n## Next steps\n\nLearn how to\nconfigure a package\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 733,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-bb91b63b78da",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Query_Data/rest-qipc.htm",
    "title": "REST and qIPC",
    "text": "\n# Rest and qIPC Query\n\nThis page compares REST and qIPC query interfaces in\nkdb Insights Enterprise\n, highlighting their performance characteristics and suitable use cases.\nkdb Insights Enterprise\nincludes services for persisting, and accessing data.\nThe Service Gateway offers an authenticated, secure and OpenAPI compatible API to retrieve data from the system.\nAn operator is used to dynamically provision data access processes, and storage manager nodes.\n\n## Deployment\n\nTo query data, ensure a database is configured and data publishers are deployed.\nSDK\nEnterprise\nTo configure and deploy a database, refer to the\nconfiguration\nand\ndeployment\nguides in the kdb Insights documentation.\nTo download one of the sample packages and deploy it to\nkdb Insights Enterprise\nrefer to the\ndownloading packages\nguide.\nNote\nSQL Usage\nTo use SQL, you must augment the database to set\nqueryEnvironment\n. For more information, refer to the\nSQL\ndocumentation.\n\n## Role based access\n\nAll service gateway endpoints starting with\n/kxi\nuse a singular\ninsights.query.data\nrole.\nThis role must be applied to the user or service account before acquiring a new bearer token.\nInvoking a UDA requires the\ninsights.query.custom\nrole.\nNote\nTokens, users, and Service Accounts\nFor information on acquiring a token and instructions on querying as a user or service account, refer to the\nAuthentication and Authorization\ndocumentation.\n\n## Querying data\n\nAll DA processes come equipped with an API for simple data retrieval, called\n.kxi.getData\n.\nTo query data using this API, you can make a REST API call to\nservicegateway/kxi/getData\n.\nA query minimally includes the name of the table, start timestamp, end timestamp, and one or more user-defined labels.\nFor an example user-defined label\nassetClass\n:\nbash\nCopy\n\n```\nSTART=$(date \"+%Y.%m.%dD00:00:00.000000000\")END=$(date \"+%Y.%m.%dD23:59:59.999999999\")# Set $INSIGHTS_TOKEN to your OAuth2 Tokencurl -X POST --header \"Content-Type: application/json\"\\    --header \"Accept: application/json\"\\    --header \"Authorization: Bearer $INSIGHTS_TOKEN\"\\    --data \"{\\\"table\\\":\\\"trades\\\",\\\"startTS\\\":\\\"$START\\\",\\\"endTS\\\":\\\"$END\\\",\\\"assetClass\\\": \\\"manufacturing\\\"}\"\\    \"https://${INSIGHTS_HOSTNAME}/servicegateway/kxi/getData\"\n```\n\nNote\nTokens, users, and Service Accounts\nFor information on how to acquire a token, and instructions on querying as a user or service account, refer to\nAuthentication and authorization\n.\nThe\ngetData\nAPI supports additional parameters for reducing the columns returned, and basic filtering.\nFor more details, refer to the\ngetData API\npage.\nWarning\nCase-sensitive labels\nLabels are case sensitive. Ensure that the label key/value pairs provided match the labels assigned when the database was applied.\nFor an overview of the REST API, refer to the\nREST API\npage.\n\n### Using QIPC responses\n\nBy including the HTTP\nAccept\nheader \"application/octet-stream\", you can get query results as a serialized QIPC byte array.\nThis header allows for significantly reduced overhead and faster response times at the cost of some minor complexity when handling the results.\nBy using any of the\nkdb+ as client interfaces\n, you can deserialize the responses, and then process as normal.\nTip\nAdded bonus\nUsing this strategy has the additional benefit of preserving type information.\nJSON\nresponses have the disadvantage of converting all numbers to floats, and\nmay truncate the precision of timestamps.\nEach of the following examples assumes you have\nINSIGHTS_TOKEN\nand\nINSIGHTS_HOSTNAME\ndefined in your environment.\ncurl and kdb Insights\nkdb Insights REST client\nJavaScript\nbash\nCopy\n\n```\n# Save results to results.datcurl -X POST --header \"Content-Type: application/json\"\\    --header \"Accept: application/octet-stream\"\\    --header \"Authorization: Bearer $INSIGHTS_TOKEN\"\\    -o results.dat\\    --data \"{\\\"table\\\":\\\"trades\\\"}\"\\    \"https://${INSIGHTS_HOSTNAME}/servicegateway/kxi/getData\"\n```\n\nStart\nq\nand deserialize the response:\nq\nCopy\n\n```\n-9!read1`:results.dat\n```\n\nq\nCopy\n\n```\nURL:\"https://\",getenv[`INSIGHTS_HOSTNAME],\"/servicegateway/kxi/getData\";headers:(\"Accept\";\"Content-Type\";\"Authorization\")!(    \"application/octet-stream\";    \"application/json\";    \"Bearer \",getenv `INSIGHTS_TOKEN);body:.j.j enlist[`table]!enlist \"trades\";resp:.kurl.sync (URL; `POST; `binary`headers`body!(1b;headers;body));if[200 <> first resp; 'last resp];show -9!last resp\n```\n\nEnsure your copy of\nc.js\nhas decompression support:\n// 2021.04.05 added decompress support\nJavaScript\nCopy\n\n```\nconst https = require('https');const c = require('./c');let TOKEN = process.env.INSIGHTS_TOKEN;const options = {    host    : process.env.INSIGHTS_HOSTNAME,    path    : '/servicegateway/kxi/getData',    method  : 'POST',    headers : {        'Accept'      : 'application/octet-stream',        'Content-Type'  : 'application/json',        'Authorization' : 'Bearer ' + TOKEN    }};let body = {'table' : 'trades'};let request = https.request(options, (res) => {    res.setEncoding('binary');    if (res.statusCode !== 200) {        console.error(`Non 200 error code ${res.statusCode}`)        res.resume();        return;    }    let chunks = [];    res.on('data', (chunk) => {        chunks.push(Buffer.from(chunk, 'binary'));    });    res.on('end', () => {        let b = Buffer.concat(chunks);        console.log(c.deserialize(b));    });    });request.write(JSON.stringify(body));request.end();request.on('error', (err) => {    console.error(`Encountered an error trying to make a request: ${err.message}`);});\n```\n\nNote\nkdb Insights EnterpriseREST client\nFor more details on using the rest client, refer to\nAuthentication and authorization\n.\n\n## User defined analytics\n\nUser Defined Analytics (UDAs) enable you to define new APIs that are callable through the Service Gateway (SG). UDAs augment the standard set of\nAPIs\navailable in the kdb Insights system with application logic specific to your business needs.\nSDK\nEnterprise\nFor instructions, refer to\nInstalling UDAs\nin the kdb Insights documentation.\nFor instructions, refer to\nInstalling UDAs\nin the\nkdb Insights Enterprise\ndocumentation.\n\n### Calling UDAs\n\nUDAs are callable through the\nservicegateway\n. For configuration details, refer to the\nUsing UDAs\ndocumentation.\nIf UDAs are configured, they are included in a\ngetMeta\nrequest.\nTo call a UDA named\nexample/api\nusing REST, use the following format:\nAggregation functions for UDAs are defined on registration, if none is specified then the default method is to\nraze\nthe results from the UDA query function.\nbash\nCopy\n\n```\n# Example that uses UDA on data within the current hourstartTS=$(date -u '+%Y.%m.%dD%H:00:00')endTS=$(date -u '+%Y.%m.%dD%H:%M%:%S')curl -X POST --header \"Content-Type: application/json\"\\    --header \"Accepted: application/json\"\\    --header \"Authorization: Bearer $INSIGHTS_TOKEN\"\\    --data \"{\\\"table\\\": \\\"trades\\\", \\\"columns\\\":[\\\"sym\\\",\\\"price\\\"], \\\"startTS\\\": \\\"$startTS\\\", \\\"endTS\\\": \\\"$endTS\\\"}\"\\    \"https://${INSIGHTS_HOSTNAME}/servicegateway/example/api\"\n```\n\nBy default, the aggregation method is\nraze\n, unless using\nscope.tier\nto specify a single DAP as the target of the request, in which case no aggregation is performed. Defining a custom\naggregation\nfunction overrides this default.\nbash\nCopy\n\n```\n# Example that uses custom aggregation API on `getData` within the current hourstartTS=$(date -u '+%Y.%m.%dD%H:00:00')endTS=$(date -u '+%Y.%m.%dD%H:%M%:%S')curl -X POST --header \"Content-Type: application/json\"\\    --header \"Accepted: application/json\"\\    --header \"Authorization: Bearer $INSIGHTS_TOKEN\"\\    --data \"{\\\"table\\\": \\\"trades\\\", \\\"columns\\\":[\\\"sym\\\",\\\"price\\\"],  \\\"startTS\\\": \\\"$startTS\\\", \\\"endTS\\\": \\\"$endTS\\\", \\\"opts\\\": {\\\"aggFn\\\":\\\"avPrice\\\"}}\"\\    \"https://${INSIGHTS_HOSTNAME}/servicegateway/example/api\"\n```\n\nNote\nIf the\nscope\nparameter is also defined you must set it to the name of the package that the UDA is defined in. This ensures the appropriate aggregator is used when running the query.\nbash\nCopy\n\n```\nSTART=$(date \"+%Y.%m.%dD00:00:00.000000000\")END=$(date \"+%Y.%m.%dD23:59:59.999999999\")PKG=\"mypackage\"# Set $INSIGHTS_TOKEN to your OAuth2 Tokencurl -X POST --header \"Content-Type: application/json\"\\    --header \"Accept: application/json\"\\    --header \"Authorization: Bearer $INSIGHTS_TOKEN\"\\    --data \"{\\\"table\\\":\\\"table\\\",\\\"startTS\\\":\\\"$START\\\",\\\"endTS\\\":\\\"$END\\\",\\\"scope\\\":{\\\"assembly\\\":$PKG}\\}\"\\    \"https://${INSIGHTS_HOSTNAME}/servicegateway/namespace/name\"\n```\n\n\n## Data tiers and life-cycle\n\nDatabases in insights are distributed across tiers. Data migrates across tiers as the data ages.\nData tiers are configured in the database specification, including mounts and data retention lifecycle settings.\nNewly received data can be made available in-memory for a number of days, before being migrated to on-disk storage or cloud storage.\nThis enables a faster response time for recent data.\nAn example mount description detailing that the IDB/HDB are to be kept in a\nRook CephFS\npartition, under the root\n/data/db\n.\nYAML\nCopy\n\n```\n  mounts:    rdb:      type: stream      baseURI: none      partition: none    idb:      type: local      baseURI: file:///data/db/idb      partition: ordinal      volume:        storageClass: \"rook-cephfs\"        accessModes:          - ReadWriteMany    hdb:      type: local      baseURI: file:///data/db/hdb      partition: date      dependency:      - idb      volume:        storageClass: \"rook-cephfs\"        accessModes:          - ReadWriteMany\n```\n\nAn example showing corresponding data tiering configuration, saved under the Storage Manager elements.\nIntra-day data would migrate from memory, to on disk every ten hours, again every midnight, and be retained for 3 months.\nYAML\nCopy\n\n```\nsm:  source: south  tiers:    - name: streaming      mount: rdb    - name: interval      mount: idb      schedule:        freq: 00:10:00    - name: recent      mount: hdb      schedule:        freq: 1D00:00:00        snap:   01:35:00      retain:        time: 3 Months\n```\n\nFor a full detail description of data tiering, such as data compression, refer to the\nElements section of the Database configuration page\n.\nNote\nQuerying is tier agnostic. Do not specify a tier when accessing data, instead use labels to query data.\n\n## Troubleshooting\n\nFor troubleshooting information, refer to\nTroubleshooting\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1273,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-f94752ddaeb3",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/stream-data-views.htm",
    "title": "Visualize Streaming Data",
    "text": "\n# Visualize Streaming Data\n\nThis page guides you through the steps to  visualize streaming data in real-time in a View, by streaming the latest co-ordinates of a set of NY City subway trains to a data grid.\nThis walkthrough contains the following sections:\n- Creating a pipelineto ingest and transform the subway data and add it to a package\n- Create a Viewto visualize the subway data in a Data Grid\n- Adding a mapto your View to display the subway data\nA Kafka subway feed has been provided for use in this walkthrough. This feed generates live alerts for NYC subway trains, tracking which trains are stopped at stations. For each train it provides arrival time, station location coordinates, direction and route details.\nNote\nRefer to the\nStreaming Kakfa Ingest\nwalkthrough for details of to ingest and visualize Kakfa streaming data using the CLI.\n\n## Create a pipeline\n\nTo create the pipeline:\n- Click+on the ribbon menu and clickPipeline, as shown below.\n- In theCreate Pipelinedialog, set the following:SettingValuePipeline Namesubway-streamingSelect a Packageinsights-demoTo add the new view to theinsights-demopackage the package must not be deployed. If it is you musttear it downto proceed.\n- ClickCreate.The pipeline editor opens, as shown below.The following sections guide you through the process of adding the following nodes to build your pipeline:Kafka Nodeto ingest the dataDecoder Nodeto decode the dataMap Nodeto convert the decoded data to a kdb+ tableTransform Nodeto convert data fields to kdb+ compatible typesSubscriber Nodeto provide real-time updates of data\n- Once these are configured you can deploy the package.\n- You can then:Create a ViewAdd filtering to the data gridAdd a map to your view to display the streaming data\n\n### Add a Kafka node\n\nYou begin your pipeline with a Kafka node to read the data from the\nsubway\nfeed we have provided.\n- Click-and-drag aKafkanode, from theReaders, into the workspace.\n- Select theKafkanode and add the following connection details to theConfigure Kafka Nodepanel:SettingValueBrokerkafka.trykdb.kx.com:443TopicsubwayOffsetEndUse TLSUncheckedUse Schema RegistryUnchecked\n- Expand theAdvancedparameters section and selectAdvanced Broker Options.\n- Click the+to add an advanced configuration and enter the following key value pairs:KeyValuesasl.usernamedemosasl.passworddemosasl.mechanismSCRAM-SHA-512security.protocolSASL_SSL\n- ClickApply.\n\n### Add a Decoder node\n\nKafka event data is in JSON and must be decoded to a kdb+ friendly format (a kdb+ dictionary).\n- Click-and-drag aJSONdecoder node, from theDecoders, into the workspace, and connect it to theReadernode.\n- ClickApply, to keep the default JSON decoder settings.\n\n### Add a Map node\n\nThe decoded data needs to be converted to a kdb+ table. The Map node converts an incoming dictionary to a table using an\nenlist\n.\n- Click-and-drag aMapnode, from theFunctions, into the workspace, and connect it to theDecodernode.\n- Select theMapnode and replace the code in the right-handConfigure Map Nodeproperty panel with the following code:qCopy{[data]enlist data}\n- ClickApplyto apply this code to the Map node.\n\n### Add a Transform node\n\nThe next step is to add a\nTransform\nnode which transforms the\nsubway\ndata fields to kdb+ compatible types and drops the fields that are not going to be displayed in the View.\nNote\nThe data from Kafka that is used in this example includes strings. The Subscriber node requires any columns defined as keyed columns to be symbols, integers, longs or GUIDs. This step is required to convert the strings in our data to symbols. Refer to the note on current limitations in the section on\nsetting up a data source for views\nfor further details.\nIf you are using a Subscriber node with your own data and the data in your keyed columns are already symbols, integers, longs or GUIDs this step is not necessary. See the\nnext section\nfor details on keyed columns.\n- Click-and-drag theApply Schemanode from the list ofTransformnodes, and connect it to theDecodernode.\n- In theSchemasection click the+icon under theAdd schema columns belowtext and add the following columns, leavingParse Stringsset toAutofor all columns:Column nameColumn typearrival_timeTimestampstop_nameSymbolstop_latFloatstop_lonFloatdirection_idSymbolroute_long_nameSymbolroute_colorSymbol\n- ClickApply.\n\n### Add a Subscriber node\n\nA\nSubscriber\nnode emits a summarized view of the input data, with one record per unique combination of values in the list of keyed columns. The remaining columns are set to their latest values.\nOnce deployed, the emitted stream of data is available as a data source in a View. This is described in the\nCreate a View\nsection later.\nIn this example the node is configured so that it emits updates to the co-ordinates of trains per route and direction of travel (i.e. inbound or outbound).\n- Click-and-drag theSubscribernode, from the list ofWritersinto the central workspace, and connect it to theApply Schemanode.\n- Select theSubscribernode and set theTabletosubway.\n- Add the followingKeyed Columnvalues:Valueroute_long_namedirection_idDefining these keyed columns ensures that:The output of the Subscriber node has one record per train route and direction.\n- Set thePublish Frequencyto100. This determines how regularly the subscriber updates the web-socket in milliseconds.\n- ClickApply.\n- ClickSave.\n- ClickPackagesin the left-hand menu to open thePackagesindex\n- Click the three dots to the right of theinsights-demopackage and clickDeploy.\nOnce the database and pipeline are deployed you can create a View of the data.\n\n## Create a View\n\nNow that the data is being streamed to a pipeline containing a Subscriber node, you can create a View with a Data Grid to display the locations of the subway trains that are stopped at stations.\n- SelectCreate NewunderViewson theOverviewpage.\n- In theCreate Viewdialog set the following:ValueSettingView Nameinsights-streaming-viewSelect a Packageinsights-demo\n- ClickCreate.\n- Click-and-drag aData Grid, which is the first component on the list, from the icon menu into the central workspace.\n- In the Data Grid component,Click to populate Data Sourceto open the Data editor.\n- ClickNewto create a new data source and name itstreaming.\n- Click on theStreamingradio button and set the following properties:ValueSettingpipelineThis is automatically populated with any running pipelines with a Subscriber node. Selectsubway-streaming.tableThis is automatically populated with tables associated with the pipeline selected. Choosesubway.filtersleave empty\n- ClickExecuteto view the results in theResultspanel at the bottom of the screen, as shown below. The streaming position values are displayed with updates visible as data is streamed.\n- ClickApplyand thenSelect Item.\n- With the Data Grid selected, set theSort Column, in the Basics properties, toroute_long_name.\n- Save the View by clicking theSave Dashboardicon at the top of the workspace.\nThe Data Grid now displays a record per train showing the most recent stop along the NY City subway.\n- If no data is displayed ensure that the package containing the database and pipeline are deployed. Go to thePackageindex, click the three dots to the right of theinsights-demopackage and clickDeploy. This deployes the database and pipeline associated with your view.\n\n### Filtering the Data Grid\n\nNow that you have a View that is being updated with streamed data you can add filters, based on any of the columns, to show a subset of the data.\n- With the Data Grid selected in the new View, click onData Sourcevalue to open the Data dialog and choose thestreamingData Source from the list on the left-hand side.\n- Enter one of the example filters below:filterfilter description{\"direction_id\":\"inbound\"}All inbound trains{\"route_long_name\":[\"6 Avenue Express\",\"7 Avenue Express\",\"8 Avenue Express\"],\"direction_id\":\"inbound\"}All inbound trains on the 6,7 or 8 Avenue Express routesNoteFor details on the current limitations refer to the Streaming section of thesetting up a data source for Viewsguide.\n- ClickExecuteto check that the results, displayed in theResultspanel at the bottom of the screen, have applied the filter to the data.\n- ClickApplyand thenSelect Item.\nThe Grid now lists records that match the filter being applied. The following example shows the grid being updated with data for all inbound trains only.\n\n#### Filtering based on a drop-down\n\nHaving used a static filter to limit the Data Grid to a specific value in any column, we can enhance this by using a drop-down to filter the Data Grid to one or more specific values from a list.\n- With the View open, selectDrop Down Listfrom the list of components. Drag and drop it above the Data Grid.\n- With the Drop Down List selected, check theMulti-selectcheckbox in the Basics properties.\n- Expand theItemsproperties in the Basics properties.\n- Click+to add an item withValueandTextproperties equal toinbound. Repeat foroutbound.\n- Create a View State parameter for the selected value in the Drop Down List:Click on theSelected Valueitem of the Drop Down List.ClickNewto add a new view state and clickRenameto change the name toselected_direction.Set theTypevalue tolistSet theDefaultandValuefields toinboundClickSelect Item.Theselected_directionView State is now populated with the value chosen in the drop-down.\n- Update the Filter in the Data Grid to use the newselected_directionView State value.Select the Data Grid and click on theData Sourceto open the Data dialog.Update theFilterfield to{\"direction_id\":{{mydirection}}}.Click on the eye in the far right of the newmydirectiontext box below theFilterfield and selectselected_directionfrom the list of View States displayed.ClickSelect Item.\n- ClickApplyand thenSelect Itemto close the Data Source screen.\nIn Preview mode, choose the direction from the Drop-down List and the data grid is filtered appropriately. In the following example, the\nDirection\nselected is\ninbound\nand so only inbound trains are displayed:\n\n### Add a map\n\nIf you have an access to an API Key for Google Maps Platform you can add a Map component to your View to display the subway data.\nNote\nThis step is optional. If you want to proceed you must create an API Key for Google Maps Platform\nas described here\n.\nTo add a map to your View:\n- In the design mode for your View, search for theMapcomponent in the list of components, on the left, and drag it into the workspace.\n- Click on theMapcomponent and configure it as follows:Click\"set Google Maps JavaScript API Key in Map Key Property\". The Map Key is saved as a view state inside your view.ClickNewand name the nodemapkey.In the Properties section formapkey, leaveTypeset toSymbol. Enter your API Key for Google Maps into theDefaultandValueproperties and clickSelect Item.Centralize the map on New York by setting the values, in the following table, in the Map properties. These are illustrated in the screenshot below the table.settingvalueCenterX40.75CenterY-73.98\n- Define theData Sourcefrom which the component gets data:Click onData Sourcein thePointsproperties.Create a new data source and name itstreaming-map.NoteIf you want to share your filtering options for theData Grid, select the existingstreamingdata source.Click on theStreamingradio button and set the following properties:valuesettingpipelinesubway-streamingtablesubwayfiltersleave empty\n- ClickExecuteto check the results are displayed in theResultspanel at the bottom of the screen, as shown below. This displays the streaming position values with updates visible as data is streamed.\n- ClickApplyand thenSelect Item.\n- To ensure the trains are named,  positioned and colored on the map based on the streaming data set thePointsproperties as follows:propertiesvalueSelected Attrroute_long_nameLatitude Datastop_latLongitude Datastop_longShapetrainShape Colour (From DB)route_colourClusterUnchecked\n- Save the View by clicking theSave Dashboardicon at the top of the workspace.Your View now contains a data grid and a map updating for all inbound and outbound trains, as illustrated below.\n\n## Further reading\n\n- Views overview\n- Quickstart guide to building Views\n- Detailed guide to creating Views\n- Guided walkthrough on adding a Map component to a View\n- Read more about View States\n- Full guide to KX Dashboards",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1832,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-2d9579fcd14a",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/maps.htm",
    "title": "Add a Map to a View",
    "text": "\n# Add a Map to a View\n\nThis page guides you  through the steps to add a Map to your View to visualize\nweather\ndata.\n\n## Prerequisite\n\nYou need a Google Maps API Map Key to use the Maps component. Learn more about\nGoogle Map Keys\n.\n\n## 1.Add a Map component\n\nTo add a Map to a View:\n- Create the viewinsights-demo-view, as described in theViews walkthrough.\n- Search for the Map component in the left-hand list of components and drag into the workspace.\n- With the map component selected, (it has a light blue border when active), update the properties of theMapsection:Clickset Google Maps JavaScript API Key in 'Map Key' property. This allows you to save the map key as a View State inside your Map component.Provide aGoogle Maps API Map Key.  This is saved as a View State parameter in your view.Centralize the map on New York with the followingMapproperties:SettingValueCenterX40.75CenterY-73.98\n\n## 2.Configure the Data Source\n\nExpand the\nPoints\nproperties and click\nData Source\n.\n- Create aNewdata source.\n- ClickAPIin the data editor, and set:SettingValuetableweatherstartTSmidnight of deployment dateendTS23:59 of deployment dategroupByname,sensor,borough. Must be of type ListaggSee next step to create a View StateWithinagg, create a View State by rolling over the text input and clicking on the small, blue eye icon to the right.In the view state dialog, create aNewview state and set type toList.SetDefaulttolastlat;last;latitudeandlastlon;last;longitude; tab across to add the second filter.Click the refresh icon next toValueto copy the default values to theValueproperty.ClickSelect Itemto apply the View State.\n- ClickApplyandSelect Itemto apply the data source change to the map.\n\n## 3.Configure the Map component\n\n- In thePointsproperties set:SettingValueLatitude DatalastlatLongitude DatalastlongThe following Map showing sensor locations in New York is displayed.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 283,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-15fc54b9e2f8",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/sign-in.htm",
    "title": "Log in to kdb Insights Enterprise",
    "text": "\n# Log in tokdb Insights Enterprise\n\nThis page provides instructions on how to log in, log out, and reset your password for the\nkdb Insights Enterprise\nWeb Interface.\nAccess to\nkdb Insights Enterprise\nweb interface is restricted by robust\nauthentication and authorization\n. Permissions to use system features are defined in user accounts. If you cannot login, or do not have access to features described in this documentation contact your system administrator.\n\n## Log in\n\nWhen you access\nkdb Insights Enterprise\nweb interface URL, the sign in screen is displayed.\n- Enter theUsername or emailandPasswordassigned to you by your administrator.\n- ClickSIGN IN.Thekdb Insights Enterprise web interfaceis displayed.\nThe options available to you vary depending on your\nassigned role\n. For example, refer to\nviews-only user\nfor details on how the web interface appears for this type of user.\nNote\nClick the\nicon to see your password.\nNote\nTake a look at\nAuthentication and authorization\nfor more information on controlling access to the web interface.\n\n## Reset your password\n\nClick\nForgot Password?\nand follow the on-screen instructions to receive an email and reset your password.\nNote\nThe\nForgot Password?\nlink is available only if your administrator has configured the password reset service.\n\n## Log out\n\n- SelectLogoutfrom the menu option in your user profile of thekdb Insights Enterprisescreen.\n- Confirm that you want to log out of the application.\n\n## Explore the web interface\n\nThe next step is to explore the functionality provided by the\nkdb Insights Enterpriseweb interface.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 249,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-a0845c1a772f",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Interfaces/web-interface-overview.htm",
    "title": "Web Interface",
    "text": "\n# Web Interface Overview\n\nThis page provides a high-level overview of the workflow for importing data, writing it to a database, then querying and visualizing that data in\nkdb Insights Enterprise\nWeb Interface.\nTo get started with the\nkdb Insights Enterprise\nweb interface, it is recommended that you:\n- Explore theweb interface overviewto familiarize yourself with its layout and functionality.\n- Review the high-level workflow for importing data, writing it to a database, and then querying and visualizing that data inkdb Insights Enterprise. For details, refer to theWorkflowbelow.\n- Use theGuided walkthroughsto gain hands-on experience with this workflow.\n\n## Workflow\n\nThe workflow for importing, querying, and analyzing data is summarized in the following steps.\n- Begin by creating a database and adding it to a package. Data is stored inkdb Insights Enterpriseusing kdb+, column-based, relational time series database technology.Refer toCreate a databasefor details.\n- Configure the database schema to store your data. The schema contains table definitions to ensure imported data is compatible with kdb+ data types.Refer toCreate a schemafor details.\n- Create a pipeline to ingest data from a source into kdb+. This can be done using the Import Wizard or by building a pipeline of nodes.Refer toImport Wizardandbuild a pipelinefor details.\n- Deploy the package that contains the database and pipeline to ingest data.Refer toDeploy a packagefor details.\n- Create queries to analyze the data using Basic query,q, SQL, or User Defined Analytics (UDAs).Refer toCreate Queryfor details.\n- Perform further ad-hoc queries on data using q or python in the Scratchpad.Refer toCreate ad-hoc queries in Scratchpadfor details.\n- Create visualizations of your data using Views to incorporate data into charts, maps, and more, and share these views with others.Refer toCreate a Viewfor details.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 285,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-14c2b1f27206",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/views-quick-start.htm",
    "title": "Quick-Start Guide to Creating Views",
    "text": "\n# Quick-Start Guide to Creating Views\n\nThis page provides a high-level guide to building\nkdb Insights Enterprise\nViews.\nThe steps to building Views are listed below.\n- Create a Database and Pipeline\n- Create a View\n- Add and configure components\n- Configure the data source\n- Format the components data\n- Enhance your View by adding more components\n- Save your View and share it with other users\n\n## Create database and pipeline\n\nBefore you can build your Views you must create and deploy a database to store data and a pipeline to import data into the database. You can then add this data to components on your view.\n- Click here for details of how to deploy a database\n- Click here for details of how to create and deploy a pipeline with streaming data\n\n## Create a View\n\nTo create a view:\n- On theOverviewpage in theViewstab clickCreate New, or use one of the other methods describedhere.\nIn the\nCreate View\ndialog, enter a name for the view, select the\npackage\nit belongs to, and optionally add\nTags\n.\n- Click here to read a more detailed guide to creating a view.\n\n## Add and configure a component\n\n- Click and drag a component from the list of icons on the left of the workspace, into the center as shown in the following animation. The first component, Data Grid, is a good one to start with.\n- Open theDataeditor by clickingClick to populate Data Sourceinside the selected component.\n- ClickNewto add a new data source.\n- If you are new tokdb Insights Enterprise, we recommend starting with the getData API query to get the data, so click theAPIradio button.\n- Click here to read more details about  the components available.\n- Click here for full details on configuring data sources.\n\n## Format the components data\n\nYou can format the data inside the component; adding colors, highlight rules, actions and/or filters.\nRead the\nthe guided walkthrough on building Views\nfor more details.\n\n## Add more components\n\nYou can continue to build the view by adding more components and data queries.\nRead more about the\nfull set of components\nhere.\n\n## Preview a View\n\nOnce you have designed your view, you can interact with it in the preview panel.\nClick\nPreview\nat the top of the workspace. Preview Mode enables you to test the dashboard with the end-user level of interactivity.\n\n## Save and share\n\nOnce your view is configured you can save it and share the URL link with friends and colleagues. Click on the triple dot menu beside a View and click\nShare\n.\nClick here for further details on saving and sharing views\n\n## Further reading\n\n- Views overview\n- Detailed guide to creating Views\n- Guided walkthrough on setting up Views\n- Guided walkthrough on adding a map component to a View",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 477,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-02cd40790ef7",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/views-guide.htm",
    "title": "Guide to Building Views",
    "text": "\n# Guide to Building Views\n\nThis page provides a detailed guide to building visualizations using Views.\nkdb Insights Enterprise\nViews enable you to build\nvisualizations\nthat help you to understand complex data. This page follows on from the\nquick-start guide\nand provides more detailed instructions on setting up Views. For examples of the types of views that you can access see\ndemo views\n.\nThe following sections explain how to create, configure, and share Views.\nNote\nIn the context of\nkdb Insights Enterprise\nweb interface the terms Dashboards and Views are synonymous.\n\n## Views-only users\n\nFor users with the\nviews-only role\n, Views provide a secure way to explore and interpret data without the ability to alter it. These users can open and interact with the Views, such as filtering or drilling down into data but cannot create, edit, or delete any visualizations or underlying queries. Refer to the\nweb interface for views-only users\nfor details.\n\n## Create a View and add a component\n\n- Before you can create a View, you must deploy the following:a pipeline that ingests the data used by your View. Readpipelinefor details.a database that stores the data used by your View. Readdatabasefor details.\n- Create a new View in one of the following ways:click+on theribbon menuand then click onViewclick the+besideViewson theleft-hand panelclickCreate Viewin theViews indexclickVisualizeon theOverview page, as shown belowTheCreate Viewdialog opens.\n- In theCreate Viewdialog:settingdescriptionView NameEnter a name for the View. The name must meet the following criteria; be unique inside the package, be between two and 32 characters long, have a combination of lowercase, uppercase, hyphens and spaces, and start and end with an alphanumeric character.Select a PackageChoose thepackageto add this view to. Alternatively, clickCreate new packagein the drop-down list.Select tagsOptionally add tags to your View. While not required, tags help organize multiple Views more effectively. For more information, seeTags.Click the dropdown to open the full list of available tags and scroll to browse through them.Click on a tag to add it to the View. Once selected, a tag cannot be selected againAlternatively start typing in the field. Matching tags appear in a filtered list for quick selection.If no matching tag exists, continue typing your desired tag name and press Enter to create the new tag. Tags must be between 2 and 20 characters long, start with an alphanumeric character or emoji, and may only contain letters (uppercase or lowercase), numbers, hyphens (-), underscores (_), or emojis.Repeat the process to add multiple tags as needed.Tags are displayed in the header of each View. In the example below, thederivative-positionsView shows the tagsderivative,positions, andoptionsin its header.\n- ClickCreateto open a new View workspace.\n- ClickDesignat the top of the workspace to design and edit your view.\n- Click-and-drag acomponentinto the central workspace. A Data Grid is a good component to start with, as shown here.Breadcrumbs at the top of the workspace indicate the selected item and the package its contained in, for example, Data Grid. Click on the package name to open the package entities dialog, showing all the entities in the same package.\n- With the new component selected,Click to populate Data Sourceto open the data editor.For this example, chooseAPIand select atablegenerated by a pipeline.  Define the date range of the data withstartTSandendTS.NoteThestartTSandendTSrange are from theTimestampcolumn as set in the table schema.Readhere for full details on data source configuration.\n- Continue to build your view by adding morecomponents, defining data sources, and arranging the workspace as required.\n- ClickPreviewat the top of the workspace to interact with your dashboard.\n- Click theUndoicon, in the top left-hand corner of the workspace, at any stage to undo any actions taken.\n- Click theSave Dashboardicon, in the top left-hand corner of the workspace, and name theView.NoteSaved Views persist across sessions, but any required update reset all changes.\n\n### Save a View as a pdf\n\nTo save a view as a pdf:\n- Click theGenerate pdf documenticon, in the top left-hand corner of the workspace. APrint<View Name>dialog is displayed, allowing you to choose  the pdf print size.\n- Enter a file name, select a pdf width, and clickPrint.OptiondescriptionFile NameEnter a name for the pdf file.PDF WidthChoose a width for the pdf:Standard- Generates a pdf with the dimensions 1920 x 1080.Current preview- Generates a pdf with the same dimensions as the current view. Use this option to ensure the exported PDF matches the on-screen view width.A status indicator is displayed showing the progress of the PDF generation. The generated PDF file opens in a new browser tab where you can view or save it.\nNote\nTo save dashboards as PDF and export data to CSV, Excel, and PNG, you need the\ninsights.builder.report.export\nrole, included in the Viewer preconfigured role. For more details on roles, refer to\nkeycloak permissions\n.\n\n## View Actions\n\nClick on the three dots beside the View name on the left-hand menu or on the\nViews index\n, to access the view actions:\n\n### Share a View\n\nTo share any of the Views you have created:\n- Click on the three dots beside the View name on the left-hand menu or on theViews index, and clickShare.\n- The View link is displayed in theShare Viewdialog. Copy the link displayed, or open the link in a new tab. Anyone you share with must have read access to view the Views you share with them.\n- ClickOKwhen done.\n\n### Rename a View\n\nTo rename any of the Views you have created:\n- Click on the three dots beside the View name on the left-hand menu or on theViews index, and clickRename.\n- Enter a new name in theRename Viewdialog and clickOK.\n\n### Edit Tags\n\nTo edit the\nTags\nassigned to a view:\n- Click on the three dots beside the View name on the left-hand menu or on theViews index, and clickEdit Tags.\n- To add a new tag, you can:Start typing a new tag name that meets the following conditions: contains only lettersAâZ, aâz, numbers0â9, emojis, hyphens-, or underscores_, or select an existing tag from the dropdown list.To remove a tag, click thexicon next to it.To remove all tags, clickClear all.ClickSaveto apply your changes.\n\n### Delete a View\n\nTo delete any of the Views you have created:\n- Click on the three dots beside the View name on the left-hand menu or on theViews index, and clickDelete.\n- ClickDeletein theConfirm Deletedialog.\n\n## Configure View properties\n\nWhen no component is selected in the workspace, the properties of the view are displayed in the right-hand panel.\nThese are referred to as dashboard properties, as shown below.\nThe dashboard properties are described in the following sections.\n\n### Basics\n\nThese are the basic properties for the dashboard. You can define a name for the dashboard and then link and setup the workspace area using the properties described in the following table.\n\n| Property | Description |\n| --- | --- |\n| Hash | Name of dashboard view. |\n| Link | URL reference for the view. |\n| Hostname(s) access | List of hostnames (comma separated) permissioned to access the view; if not set, all hostnames have view access. |\n| Enable \"Share Dashboard\" | Not used bykdb Insights Enterprise. |\n| Share Viewer State | When enabled, the current view states are saved for use in the next decision. If disabled, default view state values are used. |\n| Unsaved Viewer Prompt | Not used bykdb Insights Enterprise. |\n| Fill Height | When enabled, view scales to height of browser. When disabled, view height is Column Count * Row Count in pixels. |\n| Floatable | When enabled, components can be locked anywhere in the workspace grid. When disabled, components lock from top-down of workspace. |\n| Column Count | Defines the number of column anchor points. |\n| Row Count | Defines the number of row anchor points. |\n\nClick here to learn more dashboards basic properties.\n\n### Style\n\nStyle properties allow you to set border colors, background color, and spacing between components on the dashboard.\n\n| Property | Description |\n| --- | --- |\n| Dashboard Theme | Defines whether the view is to be a Light or Dark Theme. |\n| Theme Switchable | Check if you want to be able to switch between light and dark themes; uncheck to lock color to selected Dashboard Theme. |\n| Widgets Spacing | Set distance between components in pixels. |\n| Widget Shadow | Enable for component shadow. |\n| Background Color | Defines the background color of a component. |\n| Transparent Background | Check to enable transparency; uncheck to use Background Color. |\n| Border Size | Sets the width of border in pixels. |\n| Border Color | Sets the color of border. |\n| Border Rounding | Applies a rounding to border corners;0for no rounding. |\n| Border Spacing | Defines the amount of space between components; set in pixels. |\n| Worksheet Padding | The padding in pixels around each component. |\n| Show Loading Indicators | When Checked the dashboard shows a loading icon when components are accessing data. |\n| Advanced CSS | Any Advanced CSS component can be used to define the CSS for any component in a dashboard. Seeherefor details. |\n| CSS Classes | CSS Classes can be used to style components. ReadCSS classesfor details. |\n\nClick here to learn more about dashboard styling properties.\n\n### Popups\n\nYou can create a popup and then edit it to add content into the popup element.\nClick here to learn more about popups.\n\n### Shortcuts\n\nYou can assign a keyboard key to the data source and execute when pressed.\nClick here to learn more about shortcuts.\n\n### Notifications\n\nYou can specify whether notification messages, for example a failed login attempt, appear on the screen.\nClick here to learn more about notifications.\n\n### Screens\n\nEach dashboard has a main screen; other screens can be added and linked using the Button component.\nClick here to learn more about screens.\n\n## Components\n\nViews support over 40 configurable components.  When you click on a component in the workspace, its configurable properties are displayed on the right-hand panel. A selected component has a light-blue border.\nTo add a component to your view:\n- Click-and-drag a component from the left-hand menu into the workspace.  When dragging a component into a workspace, be careful not to drop it on top of another as this replaces the underlying component. Any such change (or others) can be undone by clicking theundoicon in the report menu bar next to the save icon.\nClick to view the full set of components.\n\n| Component | Description |\n| --- | --- |\n| Data Grid | Tabular data display and handling. |\n| Pivot Grid | OLAP drilldown data display. |\n| ChartGL | Hardware accelerated charts using WebGL; supports line, bar, bubble, waterfall, bounds, candlestick, heatmap and baseline charts. |\n| Pie Chart | Circular statistical graphic. |\n| Radar Chart | 2D multivariate data graphic. |\n| Canvas Chart | Basic line, bar, bubble, candlestick, violin, boxplot, and waterfall charts. |\n| 3D Chart | 3D charting (with highlight rule support for 4D); dot, surface, bar, grid, and line charts. |\n| Gauge | Display a single value of quantitative value. |\n| Layout Panel | Container for components. |\n| Flex Panel | Flexible, two-component container. |\n| Tab Control | Tabbed component container. |\n| Accordion | Collapsible component container. |\n| Overlay Panel | A container to overlay components. |\n| Data Form | Input display for query parameters; text, dropdown, and date picker selectors. |\n| Form Builder | Create a custom form for users. |\n| Drop Down List | Single or multi-select dropdown component. |\n| Button | Single button supporting multi-action select. |\n| Text Input | A text input component. |\n| Text | WYSIWYG text editor with HTML support. |\n| Selection Controls | Radio or check-box selection control. |\n| Date Picker | Date or date-time input. |\n| Navigation Menu | A navigation bar. |\n| Breadcrumbs | Navigation element for OLAP data. |\n| Tree View | A tree-view navigation selection component. |\n| Data Filter | User generated queries; text and dropdown filters (no coding required). |\n| Visual Query Builder | Graph-based visual query builder; support for multi-data sources with update, group-by, join, filter and custom functions. |\n| Editable List | A custom dropdown menu. |\n| Range Slider | A data range selector; interval or time. |\n| Playback | A multi-control stream player suitable for video content, interval, or time sequence data playback. |\n| Upload | File transfer to/from server. |\n| Map | Map component featuringGoogle maps. |\n| Mapbox | Map component with 3D overlay driven byMapbox. |\n| Treemap | Treemap or heatmap data graphic. |\n| Sunburst | Sunburst (ring/doughnut) chart for hierarchical data. |\n| Graph | Network/Flow relationship map. |\n| Sankey | A flow diagram where the width is proportional to the rate. |\n| Bipartite Chart | A chart of two independent data sets, with relationship connections between the data sets. |\n| Vega Chart | Support forVisualization Grammarcharts. |\n| Financial Chart | International financial charting component; OHLC, Line, Candlestick, Mountain, Heikin Ashi, Kagi, and over 20 technical indicators and user annotation support. |\n| Trade | Add a Forex trade panel with TWAP and VWAP support and tick updates. |\n| Contour | Add a contour graphic; plotting 3D data in 2D. |\n| Blob Download | Download files from byte array in database. |\n| Bitmap | Bitmap generated chart. |\n| Video | Add video content to a dashboard. |\n| Analyst Visual | Embed a KX Developer graphic into a chart. |\n| Scatter | Add a quartet or matrix graphic. |\n| Code Editor | Embeddable code editor with support forjson,plaintext,handlebars,html,java,javascript,markdown,python,r,sql,typescript,xml, andq. |\n\n\n## Configure the data source for a component\n\nEach component on your View must have a data source.  The following sections describe how to:\n- Setup a data source using each of the methods;API,SQL,kdb/q,StreamingandVirtual\n- Setup the subscription type\n- Verify results of the data source queries\n\n### Set up a data source for Views\n\n- Open theData Editorby doing one of the following:Click onClick to populate Data Sourcein the component on the workspace.SelectData SourceinBasicsProperties for the component, as shown below.\n- ClickNewto add a new data source.\n- Click on the relevant radio button to select the type of query you are adding. The following query options are available:API- an API (getData) orUDAsSQL- an SQL querykdb/q- a kdb/q queryStreaming- a pipeline setup to subscribe to streaming dataVirtual- a query using javascriptThe configuration for each of these query methods is described in the following tabs.\nAPI\nSQL\nkdb+/q\nStreaming\nVirtual\nYou can query data directly through the API option, without needing to write any code.\nWhen you add a\nNew\nnode, the default is to use the\ngetData API\n. Alternatively, you can use any UDAs that have been\nloaded\ninto your deployment. These appear as\nnamespace/name\nin the list of available APIs in the data source window, to the left.\nSet values for the\nAPI\nparameters described in the following table. Required properties are denoted with an asterisk (*).\n\n| Parameters | Description |\n| --- | --- |\n| table | Select a table from the list of active tables. |\n| startTS | Start time of returned data.  If a preset date range is required for a View, settemporalityproperty tosliceand define thesliceproperty for start time. Note that it can be helpful to use thescratchpadto get the date range. |\n| endTS | End time of returned data; defaults to today's date. If a preset date range is required for a View, settemporalityproperty tosliceand define thesliceproperty for end time. |\n| filter | define by (function;columnname;parameter), for example>;valuecolumn;100to filter \"valuecolumn\" to values above 100.  Selected functions include>,=or<and can include text values; for example=;name;Jimreturn rows of data where thenamecolumn containsJim. |\n| groupBy | Select data column(s) for grouping data. |\n| agg | Define columns to select:`price`size, an aggregation`col1`avg`price, or a dict(ionary) aggregation`col1`avg`price;`col2`sum`size. |\n| sortCols | Sort results by selected column. |\n| slice | Sets the time range forstartTSandendTswhensliceis set fortemporality. |\n| fill | How to handle nulls in the data.  Select betweenzeroto treat nulls as zeroes, orforwardto carry forward a previous, non-null value. |\n| temporality | Set tosliceif data returned is between a definedstartTSandendTs; use thesliceproperty to set the date range.  Set tosnapshotfor a continuous range of data. |\n| labels | Where data columns are shared across different tables, an (optional) label can be applied to restrict returned data.  For example,table1has aregionproperty for 'north america' andtable2has aregionproperty for 'europe', then a label ofregion:europereturns data fromtable2only.  Data can be returned from tables in different databases. |\n\nTo manage memory usage effectively with large datasets, set the Query Limit parameters outlined in the table below:\n\n| Parameters | Description | Default |\n| --- | --- | --- |\n| Apply Limit | Applies query limits based on the Row Limit specified below. When unchecked, no query limits are applied. | Checked |\n| Row Limit | The number of rows returned from the API. | 2,000 |\n| First/Last | Use this toggle to indicate whether the database returns the first or the last rows based on the Row Limit value. | Last |\n\nNote\nYou can query data with\nAPI\nLabels\n. Labels are defined in the\ndatabase\n. You can use a label to select data from a particular table when data column names are shared across multiple tables.\nNote\nWhen your datasource uses a UDA, if the\nscope\nparameter is defined you must set it to the name of the package that the UDA is defined in. This ensures the appropriate aggregator is used when running the query.\n- Click the+icon below thescopefield.\n- The key is automatically set toassembly.\n- Set the value of the assembly to the name of your package.\n- ClickExecute.\nFor information on creating UDAs, refer to the\nUDA\ndocumentation.\nUse the code editor to write\nSQL\nqueries. The following select query is illustrated in the screen below.\nSQL\nCopy\n\n```\nselect * from tablename\n```\n\nUse the code editor to write\nq/sql\n.\nNote\nThis option only works when Query Environment(s) are enabled\nRefer to\nSystem Information\nfor details on how to check the status.\nq\nCopy\n\n```\nselect from tablename\n```\n\nTip\nqsql\nbypasses entitlements. You must disable qsql to enforce database and row level entitlements. Refer to the\ndisable qsql\ninstructions.\nA Streaming data source requires a pipeline that writes to a subscriber node (a generic named target that can be subscribed to later). The View component is then linked to this pipeline to listen to this subscriber and receive real-time data updates.\nThe following screen shows the data source setup for streaming data.\nTip\nFor\nentitlements\n, only\npackage entitlements\napply to streaming data.\nData entitlements\ndo not apply because the data is streamed and not from an existing database/table.\nThe data source parameters you need to configure for streaming components are defined here.\n\n| Parameter | Description |\n| --- | --- |\n| pipeline | This field provides a list of running pipelines with subscriber nodes. Select the pipeline that contains the data you want to stream to your component. |\n| table | This is a dropdown list of available tables. Select the table containing the data you want streamed to your component. |\n| filters | You can filter streaming data, based on any column, to show a subset of the data. It is also possible to set filters using view states. |\n\nNote\nCurrent limitations\n- The filter cannot use wildcards, but can support lists of values for a column and must be defined using the following JSON format:{\"column1\":\"value1\",\"column2\":[\"value2\",\"value3\"]}. Multiple values listed for a single column are ORed together and the values for each different column are ANDed together.\n- The keyed columns can be symbols, integers, longs or GUIDs, but they cannot be strings. If the keyed columns are strings, the filtering does not work. The columns need to be converted to symbols.\n- For data entitlements, only package entitlements apply to streaming data. Database and row level entitlements do not apply because the data is streamed and not from an existing database/table.\nUse the code editor to write Javascript to meld data from multiple data sources, view state parameters or text inserts.  Virtual queries run against data stored on the client.\nJavaScript\nCopy\n\n```\nfunction (source, selectedName, callback) { var filtered = _.clone(source); filtered.rows = source.rows.filter(r => r.name === selectedName).map(r => _.cloneDeep(r)); callback(filtered);}\n```\n\nUse these links to find our more information about:\n- Data Source configuration\n- View state parameters\n- The getData API\n- Writing q code\n- SQL code\n- Virtual queries\n- ClickExecuteto generate results to the lower panel, as shown below.\n- ClickApplyto add data to the component. The following screenshot shows data displayed in a Data Grid component.\n\n### Subscription\n\nThe subscription type specifies whether the query subscribes to the database in one of the following methods:\n- Static\n- Polling\nNote\nThese settings are not relevant to the\nstreaming\nquery method.\nThe options, illustrated below, along with the\nAuto-execute\nand\nForce Reset\nare displayed at the bottom of the data source configuration screen.\n\n#### Static\n\nA static subscription involves a single request for data when you run the database query.\n\n#### Polling\n\nA polling subscription involves a client-side poll of the database when you execute the database query. When selected, there are two parameters to be set as described in the following table:\n\n| Option | Effect |\n| --- | --- |\n| Interval | Set the time between each poll request (in seconds) |\n| Key | Select which data source column to define subscription handling. For example, by column in situ, or by time for a streaming update |\n\n\n#### Auto-execute\n\nThe Auto-execute option controls whether the query runs when there is a parameter change.\n- When this is checked, the query runs whenever there is a parameter change or on load if datamappingis used. This is the default option.\n- When this is unchecked, parameter changes don't run the query unless its associated with a component, e.g. adata gridor as anactiontied to abutton.\n\n#### Force Reset\n\nThe Force Reset option specifies what happens to the existing dataset on each update.\n- When this is checked, the existing dataset is cleared, on each update from the database, regardless of whether a parameter has changed or not.  This is the default option.\n- When this is unchecked, updates from the database merge with the existing dataset, unless a parameter is changed in which case the existing dataset is cleared.\n\n### Results\n\nOnce you have added a data source to a component as described\nhere\nyou click\nExecute\n. The results of the query are displayed in the main\nResults\ntab, at the bottom of the screen, with additional information in the following tabs:\n- Mapping\n- Input\n- Raw output\n\n#### Mapping\n\nThe Mapping tab takes a\nValue\nfrom a column,\nKey\n, and maps it to a\nview state\nas shown here.\nAdditional mapping actions can be added with by clicking the\n+\nicon.\nClick the reset icon to create a map of values for all available keys.\n\n#### Input\n\nThe Input tab shows the raw output of the query as shown in the following example.\ntext\nCopy\n\n```\n{\"table\":\"weather\",\"startTS\":\"2022-12-05T00:00:00.000000000,\"endTS\":\"2022-12-05T23:59:00.000000000\"}\n```\n\n\n#### Raw output\n\nThe Raw Output tab shows the raw output of the results. This includes kdb+ type information.\ntext\nCopy\n\n```\n{\"primaryKey\":\"_rowIndex\",\"columns\":{\"collection\":[{\"id\":\"timestamp\",\"index\":0,\"kdbType\":12},{\"id\":\"sensor\",\"index\":1,\"kdbType\":11},{\"id\":\"airtemp\",\"index\":2,\"kdbType\":9},{\"id\":\"name\",\"index\":3,\"kdbType\":11},{\"id\":\"borough\",\"index\":4,\"kdbType\":11},{\"id\":\"longitude\",\"index\":5,\"kdbType\":9},{\"id\":\"latitude\",\"index\":6,\"kdbType\":9},{\"id\":\"color\",\"index\":7,\"kdbType\":11}],\"reset\":[{\"id\":\"timestamp\",\"index\":0,\"kdbType\":12}\n```\n\n\n## View states\n\nView states are used to store dynamic values and states that can be used by components and queries. They can be used to store user preferences or pass values into queries.\nThis\nfollowing section\nsummarizes how to use view states in Views. For comprehensive details about using view states, see the\nDashboards documentation\n.\n\n### Using view states in data editor\n\nIn the code view of the data editor, view states are used to add variable inputs; for example, from a drop down, date picker or data form component.\nTo create a view state in the data editor:\n- Click on the generate view state iconin the relevant field of the data editor.\n- ClickNewto add a new view state. The following screen shot shows aSelect View Statedialog with properties that must be configured.\nTip\nView states can be assigned to properties where the generate view state icon\nis hidden; such properties reveal the icon on rollover.\nNote\nWhen you select one of the following options from the\nType\ndrop-down list:\ndate\n,\ndatetime\n,\nminute\n,\nmonth\n,\nsecond\n,\ntime\n,\ntimestamp\n, a\nRolling\ncheckbox appears above the\nDefault\nfield. Refer to\nRolling\nfor more details.\nThe following tabs provide examples of using view states for SQL, q and virtual query methods.\nSQL\nq\nVirtual\nYou can map view states with\n{{}}\n; for example:\nSQL\nCopy\n\n```\nselect * from mytable where datavariable = {{viewstate}}\n```\n\nThe following screenshot shows an example of adding a viewstate to an SQL query.\nThe following screenshot shows a mapped view state.\n[CDATA[\n]]\nYou can map view states as a\nq\nfunction; for example:\nq\nCopy\n\n```\n{[viewstate] select from mytable where datavariable = viewstate}\n```\n\nThe following screenshot shows an example of adding a view state to a q query.\n[CDATA[\n]]\nYou can map view states with a Javascript function; for example:\nJavaScript\nCopy\n\n```\nfunction (source, selectedName, callback) {var filtered = _.clone(source);filtered.rows = source.rows.filter(r => r.name === selectedName).map(r => _.cloneDeep(r));callback(filtered);}\n```\n\nThe following screenshot shows an example of adding a view state to a virtual query.\n\n### Pass user information as a viewstate\n\nYou can pass user information as a viewstate into your Views. To create a new viewstate variable go to the\n_settings\ngroup and select\ndashboardUser\nas illustrated below.\nFor further information about view states, refer to:\n- Selected values\n- View state routing\n- Component linking\n\n## Actions\n\nAn action is a common property set that you can use to configure a view state, run a query, or open an URL. For further information on actions see here.\n\n## Highlight rules\n\nYou can configure highlight rules, a common property utilizing color and/or icon markers, to notify users of value changes. Highlight rules are best used with real-time streaming, or polling data. For further information on highlight rules see here.\n\n## Load a ready made view\n\nYou can load a pre-made view, which must be saved in a JSON  file.\n- Drag a JSON file over a View.\n- When the workspace is highlighted in blue andDrop Dashboards to Importis displayed in the center of the workspace you can drop the file.\n- TheCreate Viewscreen is dsplayed. The name of the view you are loading is populated in theView Namefield, but you can change this. Select a package to add this view to or create a new package, as per details in thecreate a view section.\nThe ready-made View is displayed in a new tab.\nRefer to the\nfinance tutorial\nfor an example of this in action using\nthis example view\n..\n\n## Further reading\n\n- Views overview\n- Quickstart guide to building Views\n- Guided walkthrough on setting up Views\n- Guided walkthrough on adding a Map component to a View\n- Full guide to KX Dashboards",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 4594,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-7c4a0b40cb6c",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Views-only_Users.htm",
    "title": "View-Only User",
    "text": "\n# Views-Only Users\n\nThis page provides a high-level overview of the web interface available to views-only users in kdb Insights Enterprise.\nAs a views-only user, you can access existing\nViews\nthat you are entitled to see, enabling you to explore and analyze data without the risk of making unintended changes.\n\n## Web interface introduction\n\nAfter you\nlogin\nto the kdb Insights Enterprise web interface, you are presented with the Overview page.\nThe interface includes the following elements:\n- Left-hand menu:You can browse and open existing Views. While other menus are visible and clickable, they do not display any content.\n- Ribbon menu:You have access only to the System Information option.\n- Home/Overview page:\n\nShown above, contains the following panels:\n- Quick Actions: contains the option toView Demos.\tThesedemonstrationexamples illustrate the power and versatility of the Views in kdb Insights Enterprise.\n- Recently Deployed: appears without data.\n[CDATA[\n]]\nNote\nKeyboard navigation\n[CDATA[\n]]\nMenu items can be navigated using the keyboard:\n- Tab to move focus.\n- Enter to select.\n- Esc to close menus.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 175,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-77a3bf205d34",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Get_Started/free-trial-tour.htm",
    "title": "Product Tour",
    "text": "\n# kdb Insights EnterpriseProduct Tour\n\nThis product tour lets you try out selected features of\nkdb Insights Enterprise\nsuch as querying sample data, configuring code in an easy-to-use interface and visualizing your data.\nImportant\nThe product tour is only available with our free trial. If you don't have the free trial, it's easy to\nsign up\n.\nPlease note that resources in free trial are only provisioned for the activities described in these instructions. Be aware that if you perform additional analysis you may encounter issues and unexpected behavior due to these constraints.\n\n## Aim\n\nThe aim of this tour is to show you the power and speed of the data analytic capabilities provided by\nkdb Insights Enterprise\n. It guides you on how to gain insights from your data by running queries on the NYC taxi dataset showcasing the use of SQL, q, and Python.\nThis tour is divided into the following sections:\n- The dataset- which introduces the data being analyzed.\n- The Query tab- which introduces you to theQuerytab and how it is used to explore the data.\n- Analyzing the data- which guides you through querying the data to answer questions relating to daily trips, average fares, passenger counts, tipping behavior and more.\n- Next steps- which provides links to other useful resources.\n\n## The dataset\n\nThis product tour uses a taxi dataset which contains historical data from New York's yellow taxi network, provided by the\nNYC Taxi & Limousine Commission\n. Using this data you will gain insights into the journeys being taken.\nThe data contains approximately 3 million records from trips taken during the month of December 2021. This data records attributes such as pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts.\n\n## The Query tab\n\nIn\nkdb Insights Enterprise\ndata analysis is done in the\nQuery\ntab. This is accessible by clicking [+] on the ribbon bar and selecting\nQuery\n, or by clicking\nQuery\non the Overview tab, as shown below.\nNote\nTheQuerytab explained\n- Get Data: The top part of the screen is used for extracting the data for analysis. You can extract the data using SQL, q, or a set of controls in the UI.\n- Scratchpad: The middle section of the screen is used for analyzing the extracted data. You can use python or q, whichever you prefer, choosing from the language option at the bottom right of this section.\n- View Data: The lower part of the screen is used for visualizing the analyzed data. The data can be visualized in three different ways using theConsole,Table, orVisualtabs.\nThe following screenshot displays the\nQuery\ntab.\n\n### Explore the data's shape\n\nIn this section we query how many records there are in the dataset.\n- Open theQuerytab and select eitherSQLorQ, in the top part of the screen. Add the appropriate code depending on your query method.SQLqSQLCopy-- how many records are there?SELECTCOUNT(*)FROMtripsWhen using theQoption, first selectdatabase=taxiandinstance=hdbbefore runningGet Data.qCopy// how many records are there?count tripsThere around 3 million records in the dataset.NotePlease note the first query you run withGet Datamay take a little longer while the session is initialized.\n- Enter a text name for theOutput variable.\n- ClickGet Data.\nNext, we query how many columns are in the dataset and what are the datatypes.\n- In the Get Data section of the screen, choose theQtab;  selectdatabase=taxiandinstance=hdb, and add the following code.qCopy// what are the columnsanddatatypes?meta tripsNoteSyntax explanationThemetaoperator allows you to inspect the schema of the table.\nThis returns a table of the available columns, their types, and other information.\nThe following columns are returned:c: column namet:column typef:foreign keysa:attributes: modifiers applied for performance characteristics\n- Enter a text name for theOutput variable.\n- ClickGet Data.\nThe following screenshot displays the output of meta trips run using\nq\n.\nYou can see the columns that are part of the dataset and their types. At this stage you have a better understanding of the size and shape of the data, which will assist with the later analysis.\n\n## Analyzing the data\n\nYou are now ready to start running some queries on this data. You will find out:\n- What is the daily trip count?\n- What is the daily average fare?\n- What is the hourly passenger count?\n- What is the passengers tipping behavior?\n- What is the relationship between tips and distance?\n- What is the breakdown of tips by rate type?\n- How many trips where there to and from the airport each day?\n\n### Daily trip count\n\nIn the Get Data part of the\nQuery\ntab choose either\nSQL\nor\nQ\nand enter the relevant query to find out the daily trip count.\nSQL\nq\nSQL\nCopy\n\n```\n-- how many trips per day?SELECT date, COUNT(*) AS num_tripsFROM tripsWHERE date BETWEEN '2021-12-01' AND '2021-12-31'GROUP BY date\n```\n\nThe following screenshot shows the daily trip count query run using\nSQL\n.\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\n// how many trips per day?select num_trips:count i by date from trips where date within (2021.12.01;2021.12.31)\n```\n\nNote\nSyntax explanation\nThe above\nvirtual columni\nmaps to a record index in the table.\nA simple aggregation can be obtained by taking the count of this virtual column.\nTo assign the resulting aggregation to a new column name in the resulting table, we simply use a colon\n:\n.\nThe following screenshot shows the daily trip count query run using\nq\n.\nTip\nHow many trips occur each day?\nSwitching to the visual tab, in the lower part of the screen, you see that around 100,000 trips were taken per day during the month and that trips taken reduced towards the end of the month, with the lowest day being Christmas day.\nThis analysis could help taxi companies decide how many drivers are required on different days of the year.\n\n### Daily average fare\n\nIn the Get Data part of the\nQuery\ntab choose either\nSQL\nor\nQ\nand enter the relevant query to find out the daily average fare.\nSQL\nq\nSQL\nCopy\n\n```\n-- what is the average fare each day?SELECT date, avg(fare)FROM tripsWHERE date BETWEEN '2021-12-01' AND '2021-12-31'GROUP BY date\n```\n\nThe following screenshot shows the daily average fare using\nSQL\n.\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\n// what is the average fare each day?select avg fare by date from trips where date within (2021.12.01;2021.12.31)\n```\n\nThe following screenshot shows the daily average fare using\nQ\n.\nTip\nWhat is the average fare each day?\nAgain using the visual tab, it is clear the number of trips per day was lower around Christmas, however, you can also see that the average fare amount was higher for these days.\nThis analysis could help taxi companies to determine which days are most lucrative possibly indicating higher demand.\n\n### Hourly passenger count\n\nIn the Get Data part of the\nQuery\ntab choose\nQ\nand enter the following query to find out the hourly passenger count.\nq\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\n// 1. group the pickup time into 60 minute buckets using xbar// 2. get the average number of passengers for the trips in each bucketselect avg passengers by buckets:60 xbar pickup_datetime.minute from trips where date within (2021.12.01;2021.12.31)\n```\n\nNote\nSyntax explanation\nThe\nxbar\nfunction is used to group the data on the pickup datetime column into 60 minute buckets.\nTo assign the resulting buckets to a new column name in the resulting table, we simply use a colon\n:\n.\nThe following screenshot shows the hourly passenger count query using\nq\n.\nTip\nWhat is the average number of passengers per trip each hour?\nIn the later hours of the day, trips have a higher average number of passengers than in the early morning hours.\nThis analysis could help taxi companies optimize the deployment of their drivers based on the seat capacity of the cars in their fleet.\n\n### Tipping behavior\n\nIn the Get Data part of the\nQuery\ntab choose\nQ\nand enter the following query to find out the tipping behavior.\nq\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\n// what is the average tip for each 15-minute timespan?select avg tip by buckets:15 xbar pickup_datetime.minute from trips where date within (2021.12.01;2021.12.31)\n```\n\nThe following screenshot displays the tipping behavior query.\nTip\nWhen are people giving the most tips?\nThroughout the day, there is usually a consistent amount of tips, with a slight decline in the middle of the night. This decline is followed by a massive increase during the hours of 4am to around 7am.\nThis analysis could help taxi companies encourage their drivers to work shifts that include those early morning hours.\n\n### Compare tips and distance\n\nFor this example you can switch to the scratchpad for analyzing the extracted data in further detail.\n- In the top part of theQuerytab chooseQand enter the following query and save it as a variable calledtip_buckets.qSelectdatabase=taxiandinstance=hdb.qCopy// get the average tipsin15minute intervals0!select avg tip by buckets:15xbar pickup_datetime.minutefromtrips where datewithin(2021.12.01;2021.12.31)\n- In the Get Data part of theQuerytab chooseQand enter the following query and save it as a variable calleddistance_buckets.qSelectdatabase=taxiandinstance=hdb.qCopy// get the average distancesin60minute intervals0!select avg distance by buckets:60xbar pickup_datetime.minutefromtrips where datewithin(2021.12.01;2021.12.31)\n- In the Scratchpad, in the middle part of  the Query screen, selectPython.  Add the following python code to the scratchpad editor:PythonCopy# join the tables 'tip_buckets' and 'distance_buckets' on the time column 'buckets'# this is joining tables with varying time granularityimportpandasaspdpd.merge_asof(tip_buckets.pd(), distance_buckets.pd(), left_on=\"buckets\", right_on=\"buckets\")NoteYou may need to move the cursor to the final line of code and runCtrl+Enterto visualize in Python.\n- In the Visual tab, switch the chart type toLine.\n- ClickRun Scratchpad. The line chart showing the results of the python query, on tip and distance traveled, is displayed, as shown in the following screenshot.NoteSyntax explanationWe need to import thepandasmodule to access thepandas.merge_asoffunction to merge the tables together. When importing this module, we alias it aspd.Thispd.merge_asoffunction does not work on pykx tables so we need to convert this data structure into a pandas dataframe. This is done by calling the.pd()function on the pykx table.In this example, we use thepd.merge_asoffunction to join two tables together of varying temporal granularity.\n- The same output can be generated usingQin the scratchpad:qCopy// join the tables'tip_buckets'and'distance_buckets'on the time column'buckets'// thisisjoining tableswithvarying time granularityaj[`buckets;tip_buckets;distance_buckets]NoteSyntax explanationajis a powerful timeseries join, also known as an asof join, where a time column in the first argument specifies corresponding intervals in a time column of the second argument.In this example, we use it to join two tables together of varying temporal granularity.\nTip\nDoes the distance of journey have an impact on tips earned?\nTrips that are a longer distance tend to earn higher tips for drivers.\nThis analysis might influence drivers to choose to take longer distance fares as they can earn a higher tip.\n\n### Breakdown trips by rate type\n\nIn the top part of the\nQuery\ntab choose either\nSQL\nor\nQ\nand enter the relevant query to find out the breakdown of trips by rate type.\nSQL\nq\nSQL\nCopy\n\n```\n-- how many trips fell under each rate type?SELECT rate_type, COUNT(*) AS num_tripsFROM tripsWHERE date BETWEEN '2021-12-01' AND '2021-12-31'GROUP BY rate_typeORDER BY num_trips\n```\n\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\n// how many trips fell under each rate type?`num_trips xdesc select num_trips:count i by rate_type from trips where date within (2021.12.01;2021.12.31)\n```\n\nTip\nHow many trips occur for each area?\nYou can see here that most of the trips fall into\nStandard rate\n, while there are also a good number of trips falling under the airport rates\nJFK\n,\nNewark\n, and\nNassau or Westchester\n.\nThis analysis could help taxi companies gauge how many of the trips their drivers did of each rate type so they can manage the fare rates associated with each rate type.\nThe following screenshot shows the number of trips under each rate type, as returned by the q query.\n\n### Daily airport trips\n\nIn the Get Data part of the\nQuery\ntab choose either\nSQL\nor\nQ\nand enter the relevant query to find the number of daily airport trips.\nSQL\nq\nSQL\nCopy\n\n```\n-- how many trips to and from the airport were there each day?SELECT date, rate_type, COUNT(*) AS num_tripsFROM tripsWHERE date BETWEEN '2021-12-01' AND '2021-12-31' AND rate_type in ('JFK', 'Newark', 'Nassau or Westchester')GROUP BY date, rate_type\n```\n\nThe following screenshot shows the number of daily airport trips as generated by a\nSQL\nquery.\nSelect\ndatabase=taxi\nand\ninstance=hdb\n.\nq\nCopy\n\n```\nselect num_trips:count i by date, rate_type from trips where date within (2021.12.01;2021.12.31), rate_type in `$(\"JFK\"; \"Newark\"; \"Nassau or Westchester\")\n```\n\nThe following screenshot shows the number of daily airport trips as generated by a\nq\nquery.\nTip\nHow many trips to and from the airport were there each day?\nYou can see here that the number of trips to and from the airport was relatively constant and that Christmas had a minimal effect on the daily airport journeys.\nThis analysis could help taxi companies ensure that they have enough drivers working during this period despite the holidays.\nNote\nIf you are using the\nQuery\nget data filters, please be aware that the taxi data covers the period from 2021.12.31D00:00:00.000000000, for duration of the day.\n\n## Next steps\n\n- Learn how to build databases and pipelines to ingest, query and visualize data using theguided walkthrough.\n- Build a real-time trading application or learn how to apply predictive analytics for manufacturing using theindustry tutorials.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2249,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-1186ae15774c",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Examples/ingest-parquet.htm",
    "title": "Ingest Data from Parquet Files",
    "text": "\n# Ingest Data from Parquet Files\n\nThis page demonstrates how to ingest Apache Parquet files into\nkdb Insights Enterprise\nusing Pipelines in the Web Interface.\nParquet\nis an efficient data storage format. This tutorial demonstrates how to ingest data from Apache Parquet files stored in an AWA S3 bucket to the\nkdb Insights Enterprise\ndatabase. The parquet files used in this tutorial are the New York City taxi ride data from February 2022. These files are provided by the\nNYC Taxi and Limousine Commission\n.\nIn cases where you wish to ingest your own data, stored in Parquet file format, you can use the pipeline created as part of this tutorial as a template.\nThis walkthrough guides you through the process to:\n- Create and deploy the database,\n- Create and deploy a pipeline\n- Query the data.\n\n## Create and deploy the database\n\nNote\nHow to determine your database schema\nIf you are unsure about the data your Parquet files contain, you may need to query these files to investigate the available fields before defining your schema. To help with this analysis, use the available\nParquet Reader q API\n.\n- On theOverviewpage, in theQuick Actionspanel underDatabases, chooseCreate New.NoteA detailed walkthrough of how to create and deploy a database is availablehere.\n- Name the databasetaxidb.\n- Open theSchema Settingstab and clickon the right-hand-side.\n- Paste the following JSON schema into the code editor:Paste into the code editorJSONCopy[{\"name\":\"taxitable\",\"type\":\"partitioned\",\"primaryKeys\": [],\"prtnCol\":\"lpep_dropoff_datetime\",\"sortColsDisk\": [\"VendorID\"],\"sortColsMem\": [\"VendorID\"],\"sortColsOrd\": [\"VendorID\"],\"columns\": [{\"type\":\"int\",\"attrDisk\":\"parted\",\"attrOrd\":\"parted\",\"name\":\"VendorID\",\"attrMem\":\"\",\"foreign\":\"\"},{\"type\":\"timestamp\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"lpep_pickup_datetime\",\"foreign\":\"\"},{\"type\":\"timestamp\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"lpep_dropoff_datetime\",\"foreign\":\"\"},{\"type\":\"string\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"store_and_fwd_flag\",\"foreign\":\"\"},{\"type\":\"long\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"RatecodeID\",\"foreign\":\"\"},{\"type\":\"int\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"PULocationID\",\"foreign\":\"\"},{\"type\":\"int\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"DOLocationID\",\"foreign\":\"\"},{\"type\":\"long\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"passenger_count\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"trip_distance\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"fare_amount\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"extra\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"mta_tax\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"tip_amount\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"tolls_amount\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"ehail_fee\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"improvement_surcharge\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"total_amount\",\"foreign\":\"\"},{\"type\":\"long\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"payment_type\",\"foreign\":\"\"},{\"type\":\"long\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"trip_type\",\"foreign\":\"\"},{\"type\":\"float\",\"attrDisk\":\"\",\"attrMem\":\"\",\"attrOrd\":\"\",\"name\":\"congestion_surcharge\",\"foreign\":\"\"}]}]\n- Applythe JSON.\n- Savethe database.\n- Deploythe database.\nWhen the database status changes to\nActive\n, it is ready to use.\n\n## Create and deploy a Parquet reader pipeline\n\nTo create the pipeline:\n- Hover overPipelinesin the menu on the left pane and click the+symbol to add a pipeline.\n- Name this pipelinetaxipipeline\n- To add aParquetreader node:Typeparquetin the search boxDrag and drop the Parquet node into the pipeline pane.The screen updates as illustrated:\n- Click on Parquet node in the pipeline pane to open theConfigure Parquet Nodescreen where you can input the values described in the following table.variablevaluedetailsVersion2Select from dropdownPath TypeAWS S3Select from dropdownParquet URls3://kxs-prd-cxt-twg-roinsightsdemo/green_tripdata_2024-02.parquetDecode ModalityTableThe format to which the parquet file is decoded, select from dropdownRegioneu-west-1The format to which the parquet files is decoded, select from dropdownUse MetaNoUse WatchingNoWatch for new storage bucket objects. This is unchecked here as the pipeline is intended to pick up a fixed set of parquet files\n- To add aDatabase Writernode to the pipeline:Typewritersin the search box.Drag and drop thekdb Insights Databasenode into the pipeline pane.Connect the parquet reader node to the database writer node by clicking and drag the dot that represents the data output terminal on the Parquet node to the dot that represents the data input terminal on the kdb Insights Database node.You screen updates as illustrated:\n- Configure theDatabase Writerto point to the database and table created previously. Use the variables described in the following table to complete theConfigure kdb Insights Database Nodescreen.variablevaluedetailsDatabasetaxidbThe kdb Insights Database to write the data to.TabletaxitableThe table to write the data to in the selected database.Write Direct to HDBYesWhen enabled, data is directly written to the database.Deduplicate StreamNoData is deduplicated. Useful if running multiple publishers that are generating the same data.Set Timeout ValuableNoOverwriteYesOverwrites content within a each date partition with the new batch ingest table data.InformationRefer to thekdb Insights Database writerfor more details on configuration of this node.\n- Configure the pipeline memory. You must allocate sufficient memory to your pipeline to ensure it can ingest the parquet files.InformationParquet files can have varying levels of compression. When the file is read by the pipeline, the data held in memory is uncompressed. The level of compression determines the memory allocation required. If you're unsure of the memory required, allocate an amount that is 8 times the size of the largest parquet file you plan to ingest.Click thePipeline Settingstab.Scroll down to locate the memory settings and set the values as shown below:\n- Start the pipeline by clickingSave & Deploy\n- The pipeline can be torn down once all the parquet files are processed.\n\n## Query the data\n\n- Create a newQuerywindow.\n- Adjust the time range to any date-time range in February 2022, and clickGet Data.\n- Data is displayed in the console as shown below.\nThe steps above show how easy it is to populate a database with a fixed set of parquet files using a\nParquet Reader\nnode.\n\n## Ingestion of parquet files as they are delivered\n\nYou can use wildcards to facilitate ingestion of parquet files as they are delivered to an object store bucket. One use case for this would be to ingest the data files from the previous day once they are copied into the bucket at a particular time each morning..\n- On yourParquet Readernode:Update the URL to include wildcards. Using a * in the URL will match any string in a directory or file name.Glob patternsprovide a way to specify a path that can match one or more files.Turn on theWatchingfeature to ensure new files that match the file pattern are ingested as they are delivered to the bucket.\n- On yourDatabase Writernode, turn offWrite Direct to HDBas this option is not supported when using the File Watcher.\n\n## Further reading\n\n- Web interface overviewexplaining the functionality of the web interface\n- Create & manage databasesto store data\n- Create & manage pipelinesto ingest data\n- Create & manage queriesto interrogate data\n- Create & manage viewsto visualize data",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 906,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-0d83537d0e0c",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Reference/glossary.htm",
    "title": "Glossary",
    "text": "\n# Glossary\n\n\n## Agg\n\nShort for Aggregator.\n\n## Assembly\n\nVersions of\nkdb Insights Enterprise\nbefore v1.5 use assemblies; and assembly is comprised of:\n- Adatabaseto store and access your data.\n- Aschemato convert imported data to a format compatible withkdb Insights Enterprise(using kdb+ technology).\n- Astreamto push event data into a database.\n- Pipelinesto read data from source and write it to akdb Insights Enterprisedatabase.\nEvery assembly is\nlabeled\nwith its name.\n\n## Build a Database\n\nBuild a database is accessible from a tile under \"Discover\nkdb Insights Enterprise\n\" on the Overview page. Simply name and save the database to get started.\n\n## Ceph\n\nAn open-source, scalable, simplified storage solution for data pipelines.\n\n## C-SDK\n\nA software development kit to support C/C++ applications.\n\n## CLI\n\n\"CLI\" is the command line interface. The CLI runs Insights processes and is an alternative to the Web Interface for power users.\n\n## Console (UI)\n\nThe console is where results from ad hoc queries run in the Scratchpad are presented.\nThe console is part of the Query window.\n\n## Context\n\nThe request context is a store of stateful information for the current ongoing request. This is used primarily in User Defined Analytics to persist information before and after a defer/resume operation (see\nhelper functions\n).\n\n## DAP\n\nData Access Process.\n\n## Dashboards\n\nDashboards is an interactive visualization tool that runs in your browser. You can query, transform, share and present live data insights. Dashboards is integrated into\nkdb Insights Enterprise\nas\nViews\n.\nLearn more about KX Dashboards.\n\n## Database\n\nA database is a data store built on kdb+ technology. A database offers rdb (real-time database storage), idb (interval data storage) and at least one hdb (historic database storage) - sub-tiers of a hdb may be on the database too.\nA database also includes:\n- A schema to convert imported data to a kdb+ compatible format.\n- A stream to help push event data to the database.\n- Optional pipelines to import data to the platform\nBuild your own database.\n\n## Decode\n\nOne of the functions available in\npipeline\nis decode. The decode\nnode\nconverts data to a format that can be directly processed within the\nStream Processor\n.\nLearn more about decode nodes.\n\n## Diagnostics\n\nkdb Insights Enterprise\nincludes diagnostic and logging tools to report on the status of\ndatabase\nand\npipeline\ndeployments.\nLearn more about diagnostics.\n\n## Docker\n\nDocker\nis a platform-as-a-service, delivering software to consumers via \"containers\".\n\n## Entity-tree\n\nThe entity-tree is a dynamic menu, always available in the left margin of the\nkdb Insights Enterprise\nuser interface. The content of the menu changes depending on the interaction in the platform. On the Overview page, for example, the entity-tree shows a list of databases, pipelines, queries and views. On the pipeline page, the entity-tree lists the nodes used to build data pipelines to import data from source and transform it to a format compatible with a\nkdb Insights Enterprise\ndatabase.\n\n## hdb\n\nA hdb is a mount for storing historic data on a database. A historic database is the final destination for interval data.\n\n## idb\n\nAn idb is a mount for storing interval data on a database. It takes data from a real-time database (rdb), stores this data for a set period, e.g. 10 minutes, before the data is written to a historic database.\n\n## Import Wizard\n\nA step-by-step process for building a pipeline to import, transform and write data to your database.\nLearn more about the import wizard\n\n## Java-SDK\n\nA software development kit for developing applications in Java.\n\n## kdb+\n\nKdb+ is an ultra-fast time series columnar database.\n\n## Keycloak\n\nKeycloak is an open-source, single-sign-on authentication service and management tool. It offers enhanced user security built from existing protocols and can support authentication via social platform providers like Google, Facebook or GitHub.\n\n## kodbc\n\nAn open database connectivity driver for connecting kdb+ databases.\n\n## Kubernetes\n\nKubernetes is an open-source tool for bundling and managing clusters of containerized applications.\n\n## Kurl\n\nKurl is an easy-to-use cloud integration, registering Azure, Amazon, and Google Cloud Platform authentication information.\nLearn more about Kurl.\n\n## Label\n\nA Label is required by a database. Every created database has a default label, kxname; additional labels can be added to the database. Labels are a filter option in the\nquery\ntab.\nLearn more about labels.\n\n## Language interfaces\n\nkdb Insights language interfaces are libraries created in different languages for developers to integrate with that help you publish, subscribe and query data stored in an kdb Insights.\nLanguage interfaces are available for\nC\n,\nJava\n,\nq\nand\nPython\n.\nNote\nThese interfaces were called SDKs prior to 1.7.0\n\n## Machine Learning\n\nMachine learning is a branch of artificial intelligence (AI) that focuses on the use of data and algorithms to imitate how people learn, with the goal of improving accuracy.\nI want to learn more about Stream Processor machine learning.\n\n## Mount\n\nA mounted database is ready for use; a database can have a hdb, idb and/or rdb mounts.\n\n## Nodes\n\nNodes are used by pipelines to read, write and transform data from its source to the database.\nEach node has a defined function and set of properties to edit. Some nodes allow for\nq\nor\npython\ncode.\nMachine Learning nodes offer more advanced manipulations of imported data before writing to the database.\n\n## Object Storage\n\nA data storage system managing data as objects, compared to a file hierarchy or block based storage architecture. Object storage is used for unstructured data, eliminating the scaling limitations of traditional file storage. Limitless scale is the reason object storage is the storage of the cloud; Amazon, Google and Microsoft all employ object storage as their primary storage.\n\n## Output Variable\n\nResults from a database query are written to an Output Variable. The Output Variable can be queried in the scratchpad using\nq\nor\npython\n.\n\n## Packages\n\nA package is a storage location for code, metadata and information for describing an application.\nLearn more about packages\n\n## Partitioning\n\nWhen a data table is written to a database it must be partitioned to be compatible with a kdb+ time series database.\nPartitioning is handled by a Timestamp column, and defined in a\nschema\n. Every table must have a Timestamp column.\n\n## Pipeline\n\nPipelines are a linked set of processes to read data from its source, transform it to a format compatible with\nkdb Insights Enterprise\n, then write it to a database for later querying.\nPipelines can be created using the Import Wizard or a visual pipeline builder. The pipeline builder offers a set of nodes to help read, writer or transform data; nodes are connected together in a workspace to form a linked chain of events or\npipeline template\n. Additional machine learning nodes are available for more advanced data interactions.\nPipelines can be deployed individually or associated with a\ndatabase\n; pipelines associated with a database will be deployed and activated when the database is deployed.\nLearn more about pipelines.\n\n## Pipeline template\n\nThe Pipeline Template is the layout of the nodes that together make a Pipeline.\n\n## Postgres\n\nPostgres\n(PostgreSQL), is an open-source relational database management system.\n\n## pgwire\n\npgwire is a PostgresSQL client library, used to implement a Postgres wire protocol server that connects to kdb Insights Core.\nLearn more about pgwire.\n\n## Protocol Buffers\n\nProtocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data - think XML, but smaller, faster, and simpler. Data structure is first defined before specially generated source code read-and-writes structured data, to-and-from a variety of data streams, using a variety of programming languages.\n\n## PyKX\n\nPyKX is the interface between the programming language, q, the time-series columnar database, kdb+, data, and Python.\nLearn more about PyKX.\n\n## q\n\nq is the programming language used to query a kdb+ database.\n\n## q/SQL\n\nq/SQL is a collection of\nSQL\n-like functions for interacting with a kdb+ database.\nLearn more about q-sql.\n\n## Query\n\nInteract with your data by building queries in the Query window. Build queries with filters, or execute with q, or SQL code. Queries reference the name of the table generated by the\npipeline\nand results written to an Output Variable for use by the Scratchpad.\nAd hoc queries in the scratchpad use q or python with the Output Variable; results are outputted to the console as a table or chart.\nLearn more about data exploration.\n\n## Reader\n\nA reader is typically the first\nnode\nin a\npipeline\n. It feeds or imports data from an external data source to\nkdb Insights Enterprise\n. Data read from an external source needs to be decoded (in most cases), and transformed, before it can written to a\nkdb Insights Enterprise\ndatabase.\nLearn more about readers.\n\n## Reliable Transport\n\nThe kdb Insights Reliable Transport (RT) is a microarchitecture for ensuring the reliable streaming of messages.\nLearn more about kdb Insights Reliable Transport (RT).\n\n## REST\n\nREST, Representational State Transfer, is a software architectural style that describes the architecture of the web.\nLearn more about REST.\n\n## rdb\n\nReal-time event data is stored on an rdb mount of the database, before it's written to the interval database (idb).\n\n## RT Interface\n\nThe RT Interface provides a set of interfaces that allow you to read and write data to an RT stream.\n\n## RT stream\n\nAn RT stream is the\nkdb Insights Enterprise\ndeployment of a Reliable Transport cluster.\n\n## Service Discovery\n\nThe Service Discovery microservice has been deprecated and is no longer be available. Older versions of Service Discovery will be available as part of\npast releases\n. Patches will be issued as required for critical issues and security vulnerabilities in accordance with KX's Security Standards.\n\n## Schema\n\nA schema is how data is converted from its source format, to a\nformat compatible\nwith a kdb+ database. Every data table has its own schema.\nLearn more about schemas\n.\n\n## Scratchpad\n\nScratchpad is part of the Query window. With scratchpad you can make ad hoc queries against an Output Variable generated by a query against a table in the database.\nYou can also create data tables directly in the scratchpad editor. The scratchpad editor supports\nq\nor\npython\ncode.\nResults from a scratchpad query are presented in the console, or as a table or chart.\n\n## SDK\n\nkdb Insights Enterprise\nSDKs have been renamed\nlanguage interfaces\n.\n\n## SQL\n\nSQL (Structured Query Language) is a standard language for accessing databases, and is supported by kdb+ databases.\n\n## Stream\n\nStreams is how event data is written to a database. Event data is typically real-time as may be generated by a price or sensor feed.\nReal-time data is stored on an real-time database (rdb), moved to an interval database (idb), before the data is written to an historic (hdb) database.\n\n## Stream Processor\n\nThe KX Stream Processor is a steam processing service for transforming, validating, processing, enriching and analyzing real-time data in context.\nLearn more about the KX Stream Processor.\n\n## Terraform\n\nTerraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\n\n## Transform\n\nA Transform node is required for most pipelines. A transform node takes imported data and transforms it to a kdb+ format suitable for storage on the database.\n\n## Upgrades\n\nUpgrades lists overloaded and orphaned\nstreams\n, and orphaned\nschemas\n, created in earlier versions of\nkdb Insights Enterprise\n.\n\n## View\n\nViews is how you build visualizations in\nkdb Insights Enterprise\n. Views are powered by KX Dashboards technology.\n\n## Web Interface\n\nWeb Interface is the User Interface for\nkdb Insights Enterprise\n.\n\n## Writer\n\nA Writer node is an essential part of any pipeline.  The writer nodes takes the transformed (kdb+) data you have read from its source and writes it to a kdb+ database.\nLearn more about writers.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1971,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-54cebb0fa99f",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Entitlements/package.htm",
    "title": "Package Entitlements",
    "text": "\n# Package Entitlements\n\nThis page describes how package entitlements work and how to configure them.\nPackage entitlements control the operations a user can perform in relation to a specific package. Packages contain databases, pipelines, and views.\nNote\nBy receiving entitlements to a package, you can connect to any worker in this package and access anything the owner of this package would be entitled to access. In contrast, by receiving  entitlements to a database, you can access only data through the API and you would not be able to hop on to a worker.\nAll packages have their own set of entitlements and each user can have different entitlements for different packages.\nPackage entitlements include four access levels - All, Read, Write, Execute (ARWX):\n- All- No restrictions. Full access to read, write, execute, and delete the package.\n- Read- Users can view the package and download its configuration.\n- Write- Users can edit pipelines, databases, and views in this package.\n- Execute- Users can deploy and teardown this package.\nIf a user is granted either the Write or Execute access level, they also automatically receive Read access.\nNote\nTo modify entitlements you need to either:\n- have theAdministratorrole, or\n- have theMaintainerrole and own the package that contains the database.\nWhen you create and push a package, the corresponding entitlements records are created automatically if they didn't already exist.\n\n## Implicit entitlements\n\nIn certain deployment scenarios, entitlements granted to the owner of one package can implicitly allow access to a package with a different owner. This behavior is referred to as implicit entitlements, and it applies at the package level, not the database level.\n\n#### Example:\n\nIf the owner of Package A receives entitlements to read from or publish to Package B, they also implicitly âinheritâ the entitlements of Package Bâs owner. This is not exact inheritance â it simply means that the connection between Package A and Package B is no longer blocked. As a result, code can be deployed from Package A into Package Bâs workers, and that code will run within Package B, inheriting its entitlements.\n\n## Use package entitlements\n\nAfter you complete the\nprerequisites\n, you can begin providing entitlements to user groups. To do this, either follow the\nquickstart\nguide or use the\nconfiguration\ndetails.\n\n### Package entitlements in the web interface\n\nCertain actions are only visible on packages in the web interface if you have specific entitlements:\n- The package itself is only visible in the web interface if you have Read access to the package\n- TheSavebuttons only appear if you have Write access to the package\n- TheDeploy/Teardownbuttons only appear if you have Execute access to the package\n- TheDeletebutton only appears if you have All access to the package\n\n### Execute code with entitlements\n\nWhen you execute a code through\n.kxi.packages.load\n,\nkdb Insights Enterprise\nchecks entitlements. You must have\nexecute\naccess level to load a package; without the correct entitlements on the package being loaded, the process fails.\nEntitlements are applied to code loaded this way when you use the\nScratchpad\n, a temporary q process, where you can execute\nUDFs\n, and when defining\nUDAs\n.\n\n### Logs for package entitlements\n\nYou can view logs for deployed databases and pipelines in packages that you are entitled to.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 548,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-8e63736c3416",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/decoders.htm",
    "title": "Decoders",
    "text": "\n# Decoders\n\nThis page explains how to set up decoder operators for\nkdb Insights Enterprise\npipelines using the Web Interface.\nDecoding allows data to be converted into a format that can be processed directly within the Stream Processor. Decoders need to be used when ingesting data from an external data format before performing other transformations.\nTip\nBoth\nq and Python interfaces\ncan be used to build pipelines programmatically.\nThe pipeline builder uses a drag-and-drop interface to link together operations within a pipeline. For details on how to wire together a transformation, see the\nbuilding a pipeline guide\n.\nThese decoders are used by the\nImport Wizard\nto configure a decoder for pipelines created using the wizard.\n\n## Arrow\n\n(Beta Feature) The Arrow operator decodes Arrow encoded data.\nNote\nBeta - For evaluation and trial use only\nThis feature is currently in beta.\n- Referhere to the standard termsrelated to beta features\n- We invite you to use this beta feature and to provide feedback using theIdeas portal\n- During deployment, the entitlements feature is disabled by default, meaning no restrictions are applied and you can manage all databases, pipelines, and views as well as query all data in akdb Insights Enterprisedeployment\n- When you enable the feature, youdo nothave access to query data in a database unless you have been given a data entitlement to query the database in question\nNote\nSee\nq and Python API\nfor more details.\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| As List | If checked, the decoded result is a list of arrays, corresponding only to the Arrow stream data. Otherwise, by default the decoded result is a table corresponding to both the schema and data in the Arrow stream. | No |\n\n\n## Avro\n\nThe Avro decoder processes messages in either binary or JSON Avro format, producing a kdb+ dictionary that conforms to the configured Avro schema. The schema must be supplied as a JSON string.\nNote\nBeta - For evaluation and trial use only\nThis feature is currently in beta.\n- Referhere to the standard termsrelated to beta features\n- We invite you to use this beta feature and to provide feedback using theIdeas portal\n- During deployment, the entitlements feature is disabled by default, meaning no restrictions are applied and you can manage all databases, pipelines, and views as well as query all data in akdb Insights Enterprisedeployment\n- When you enable the feature, youdo nothave access to query data in a database unless you have been given a data entitlement to query the database in question\nNote\nSee\nq and Python API\nfor more details.\nRequired Parameters:\n\n| name | description | default |\n| --- | --- | --- |\n| Schema | The schema used by the incoming avro messages, in JSON format. |  |\n\nOptional Parameters:\n\n| name | description | default |\n| --- | --- | --- |\n| Encoding | Encoding of incoming avro messages, eitherBinaryorJSON. | Binary |\n| Offset | Offset to begin decoding each message from, in bytes. Useful for trimming magic bytes or schema registry IDs. | 0 |\n\n\n## CSV\n\nThis operator parses CSV data to a table.\nNote\nSee\nq and Python API\nfor more details.\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Delimiter | Field separator for the records in the encoded data | , |\n\nOptional Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Header | Defines whether source CSV file has a header row, eitherAlways,FirstandNever. When set toAlways, the first record of every batch of data is treated as being a header. This is useful when decoding a stream (ex. Kafka) and each message has a header.Firstindicates that only the first batch of data has a CSV header. This must be used when processing files that have a header row. Lastly,Noneindicates that there is no header row in the data. | First |\n| Schema | A table with the desired output schema. |  |\n| Columns to Exclude | A list of columns to exclude from the output. |  |\n| Encoding Format | How the data is expected to be encoded when being consumed. Currently supportsUTF8andASCII. | UTF8 |\n| Newlines | Indicates whether newlines may be embedded in strings. Can impact performance when enabled. | 0b |\n\n\n### Expected type formats\n\nThe\nparse\noption allows for string representations to be converted to typed values. For numeric values to\nbe parsed correctly, they must be provided in the expected format. String values in unexpected formats may\nbe processed incorrectly.\n- Strings representing bytes are expected as exactly two base 16 digits, e.g.\"ff\"\n- Strings representing integers are expected to be decimal, e.g.\"255\"\n- Strings representing boolean values have a number of supported options, e.g.\"t\",\"1\"More information on the availableformats.\n\n## GZIP\n\n(Beta Feature) The GZIP operator inflates (decompresses) gzipped data.\nBeta Features\nBeta feature are included for early feedback and for specific use cases. They are intended to work but have not\nbeen marked ready for production use. To learn more and enable beta features, see\nenabling beta features\n.\nNote\nSee\nq and Python API\nfor more details.\nWarning\nFault tolerance\nGZIP decoding is currently not fault tolerant which is why it is marked as a beta feature. In the event\nof failure, the incoming data must be entirely reprocessed from the start. The GZIP decoder is only\nfault tolerant when you are streaming data with independently encoded messages.\nGZIP requires no additional configuration. On each batch of data, this operator decodes as much data as it\ncan passing it down the pipeline and buffering any data that cannot be decoded until the next batch arrives.\n\n## JSON\n\nThe JSON operator parses JSON data.\nNote\nSee\nq and Python API\nfor more details\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Decode Each | By default messages passed to the decoder are treated as a single JSON object. SettingdecodeEachto true indicates that parsing must be done on each value of a message. This is useful when decoding data that has objects separated by newlines. This allows the pipeline to process partial sets of the JSON file without requiring the entire block to be in memory. | No |\n\n\n## Pcap\n\nThe Pcap operator decodes Pcap Data.\nNote\nSee\nq and Python API\nfor more details.\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Columns to Include | The columns to include. If none of the options are selected, the output includes every available column. |  |\n\n\n## Protocol Buffers\n\nThe Protocol Buffers operator decodes Protocol Buffer encoded data.\nNote\nSee\nq and Python API\nfor more details.\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Message Name | The name of the Protocol Buffer message type to decode |  |\n| Message Definition | A.protodefinition containing the expected schema of the data to decode. This definition must include a definition of the Message Name referenced above. |  |\n\nOptional Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| As List | If checked, the decoded result is a list of arrays, corresponding only to the Protocol Buffer stream data. Otherwise, by default the decoded result is a table corresponding to both the schema and data in the stream. | No |\n\nDefinition Example\n:\nIn this example, the operator is configured to read the\nPerson\nmessage and decode data with the\ndefined fields. Because As List is unchecked, the resulting data is a table with the columns\nname\n,\nid\nand\nemail\n.\nJSON\nCopy\n\n```\nmessage Person { string name = 1;int32 id = 2;string email = 3; }                  \n```\n\n\n## Further reading\n\n- Building pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1343,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-77290b3dfa68",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Packages/load-package.htm",
    "title": "Load a Package",
    "text": "\n# Load a Package\n\nThis page explains how to use the\nkxi pm load\ncommand to load a package in\nkdb Insights Enterprise\nusing the command line interface (CLI).\n\n## Overview\n\nThe\nkxi pm load\ncommand allows you to load a package into running Data Access Processes (DAPs) and aggregators without requiring a redeployment. This applies for both a deployed package and the global aggregators, and is especially useful when developing User-Defined Analytics (UDAs).\n\n## Usage\n\nThe\nkxi pm load\ncommand loads a package into a running deployment or global process.\nRun the\n--help\ncommand as follows to learn about the options you can use with the\nkxi pm load\ncommand:\nbash\nCopy\n\n```\nkxi pm load --help\n```\n\nconsole\nCopy\n\n```\n Usage: kxi pm load [OPTIONS] PACKAGE Load a package into running deployment or global processesâ­â Authentication option overrides âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â --hostname,--url                  TEXT  Insights URL                                                                                       ââ --realm                           TEXT  Realm                                                                                              ââ --client-id                       TEXT  Client id                                                                                          ââ --client-secret                   TEXT  Client secret                                                                                      ââ --auth-enabled/--auth-disabled          Retrieve Bearer Token                                                                              ââ°âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯â­â Options âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â --deployment        -d  NAME_GLOB_OR_UUID                                                                                                  ââ --targets           -t  TEXT               Comma-separated targets: daps,qe_daps,pkg_agg,pkg_qe_agg                                        ââ --global-processes  -g  TEXT               Comma-separated global processes: agg,qe_agg                                                    ââ --version               TEXT               Version of the package to load.                                                                 ââ --server-timeout        INTEGER            Timeout for Insights server calls                                                               ââ --yes,--assume-yes  -y                     Automatic yes to prompts; assume \"yes\" as answer to all prompts and run non-interactively.      ââ --output-format     -o  [json|table]       Output format for the command. default: 'table'.                                                ââ --help                                     Show this message and exit.                                                                     ââ°âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯\n```\n\n\n## Examples\n\nThis section provides a couple of usage examples for the\nkxi pm load\ncommand.\n- Load a UDA package (uda-pkg) into the QE DAPs of a deployed package calledmypackage. For example:bashCopykxi pm load uda-pkg -d mypackage -t qe_dapsâ­âââââââââââââ¬ââââââââââââââââââââââââââââââââââ¬ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â deployment â packages                        â statuses                                                                âââââââââââââââ¼ââââââââââââââââââââââââââââââââââ¼ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¤â mypackage  â â­ââââââââââ¬ââââââââââ¬ââââââââââ® â â­ââââââââââââââââââââââââââââââ¬ââââââââââ¬ââââââââââââââââââââââââââââââ® ââ            â â name    â version â targets â â â name                        â status  â details                     â ââ            â âââââââââââ¼ââââââââââ¼ââââââââââ¤ â âââââââââââââââââââââââââââââââ¼ââââââââââ¼ââââââââââââââââââââââââââââââ¤ ââ            â â uda-pkg â 0.0.1   â qe_daps â â â mypackage-qe-dap-da-0 (hdb) â SUCCESS â Package loaded successfully â ââ            â â°ââââââââââ´ââââââââââ´ââââââââââ¯ â â mypackage-qe-dap-da-0 (idb) â SUCCESS â Package loaded successfully â ââ            â                                 â â mypackage-qe-dap-da-0 (rdb) â SUCCESS â Package loaded successfully â ââ            â                                 â â°ââââââââââââââââââââââââââââââ´ââââââââââ´ââââââââââââââââââââââââââââââ¯ ââ°âââââââââââââ´ââââââââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯\n- Load a package onto a deployed package and a global aggregator at the same time. For example:bashCopykxi pm load uda-pkg -d mypackage -g aggâ­âââââââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â deployment â packages                                                   â statuses                                                                           âââââââââââââââ¼âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¤â mypackage  â â­ââââââââââ¬ââââââââââ¬âââââââââââââââââââââââââââââââââââââ® â â­ââââââââââââââââââââââââââââââ¬ââââââââââ¬ââââââââââââââââââââââââââââââ®            ââ            â â name    â version â targets                            â â â name                        â status  â details                     â            ââ            â âââââââââââ¼ââââââââââ¼âââââââââââââââââââââââââââââââââââââ¤ â âââââââââââââââââââââââââââââââ¼ââââââââââ¼ââââââââââââââââââââââââââââââ¤            ââ            â â uda-pkg â 0.0.1   â daps, qe_daps, pkg_agg, pkg_qe_agg â â â mypackage-dap-da-0 (hdb)    â SUCCESS â Package loaded successfully â            ââ            â â°ââââââââââ´ââââââââââ´âââââââââââââââââââââââââââââââââââââ¯ â â mypackage-dap-da-0 (idb)    â SUCCESS â Package loaded successfully â            ââ            â                                                            â â mypackage-dap-da-0 (rdb)    â SUCCESS â Package loaded successfully â            ââ            â                                                            â â mypackage-qe-dap-da-0 (hdb) â SUCCESS â Package loaded successfully â            ââ            â                                                            â â mypackage-qe-dap-da-0 (idb) â SUCCESS â Package loaded successfully â            ââ            â                                                            â â mypackage-qe-dap-da-0 (rdb) â SUCCESS â Package loaded successfully â            ââ            â                                                            â â°ââââââââââââââââââââââââââââââ´ââââââââââ´ââââââââââââââââââââââââââââââ¯            ââ __GLOBAL__ â â­ââââââââââ¬ââââââââââ®                                      â â­âââââââââââââââââââââââââââââââââââââââââ¬ââââââââââ¬ââââââââââââââââââââââââââââââ® ââ            â â name    â version â                                      â â name                                   â status  â details                     â ââ            â âââââââââââ¼ââââââââââ¤                                      â ââââââââââââââââââââââââââââââââââââââââââ¼ââââââââââ¼ââââââââââââââââââââââââââââââ¤ ââ            â â uda-pkg â 0.0.1   â                                      â â nightly-pakx-aggregator-0 (aggregator) â SUCCESS â Package loaded successfully â ââ            â â°ââââââââââ´ââââââââââ¯                                      â â°âââââââââââââââââââââââââââââââââââââââââ´ââââââââââ´ââââââââââââââââââââââââââââââ¯ ââ°âââââââââââââ´âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯\nNote\nIf\n--targets\nis omitted, the command defaults to all applicable targets.\n\n## Configuration requirements\n\nTo enable dynamic package loading, the following environment variables must be set:\n- ALLOW_PACKAGE_LOAD\n- ALLOW_PACKAGE_LOAD_GLOBAL_PROCESSES\nThese can be set at install time in the\nvalues\nyaml, as follows:\nYAML\nCopy\n\n```\nkxi-package-manager:  env:    ALLOW_PACKAGE_LOAD: true    ALLOW_PACKAGE_LOAD_GLOBAL_PROCESSES: true\n```\n\nAlternatively, you can set them using the\nkxi pm config set\ncommand, as follows:\nbash\nCopy\n\n```\nkxi pm config set ALLOW_PACKAGE_LOAD truekxi pm config set ALLOW_PACKAGE_LOAD_GLOBAL_PROCESSES true\n```\n\nImportant\nRunning this command requires that you have the\ninsights.admin.package.system\nrole.\nTo use\nkxi pm load\n, you must have the Execute (X) entitlement on the package being loaded. The UDA package needs to exist on the package manager server before it can be loaded into your package. For example:\nbash\nCopy\n\n```\nkxi pm push uda-pkgkxi pm load uda-pkg -d mypackage -t qe_daps\n```\n\nYour package must be deployed before you can run the\nkxi pm load\ncommand, otherwise it errors out.\n\n## Considerations\n\nWhen running the\nkxi pm load\ncommand, consider the following:\n- Reload failures: If the new code contains errors (for example, syntax issues), the reload fails. This may result in the removal of previously working UDAs.\n- Pod restarts: Dynamically loaded code is not persisted across pod restarts unless explicitly loaded throughinit.qor similar startup scripts.\n\n## Next steps\n\n- Learn how totear down a package",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 798,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-f5501f87d86b",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Query_Data/query-resilience.htm",
    "title": "Query Resilience",
    "text": "\n# Query Resilience\n\nThis page explains query resilience in\nkdb Insights Enterprise\noutlines the recommended practices for establishing redundant connections across components, describes configuration methods, and provides guidance on using ordinals.\nThere are four process types in the query path: Gateway (GW), Resource Coordinator (RC), Data Access Process (DAP), and Aggregator (Agg). Each process can be configured with multiple replicas for resiliency.\nProcess connections are as follows.\n- GWs connect to multiple RCs. Each GW distributes requests round-robin across all known RCs.\n- DAPs and Aggs connect to exactly one RC each. Hence, every RC owns its set of DAPs/Aggs.\n- RCs can connect to each other.\nIn general, it is best practice to allocate multiple of each resource at each connection point. That is:\n- Allocate multiple RCs, so that if one dies, the GWs can distribute to the remaining ones. If no RCs remain, requests return the following error:\"No Resource Coordinator connections are available and ready for service\"Refer toTroubleshootingfor more details.\n- Allocate multiple DAPs (of each type RDB/IDB/HDB) for eachlabel setto each RC. Multiple DAPs increase query throughput as RCs can distribute queries to several DAPs in parallel. If a DAP dies, the RC continues to distribute to the remaining ones. If no DAPs for a particular tier/label set are available, requests queue up in the RCs. Refer toQueueingfor details.\n- Allocate multiple Aggs to each RC. Multiple Aggs increase query throughput as RCs can allocate queries across several Aggs. If an Agg dies, the RC allocates to the remaining ones. If no Aggs remain for a particular RC, requests received by this RC return a\"No aggregator available\"error. Refer toTroubleshootingfor more details.\n\n## Configuration\n\nLearn how to configure:\n- kdb Insights\n- kdb Insights Enterprise\n- Kubernetes q\n- Ordinal connections\n\n### kdb Insights\n\nUsing kdb Insights offers the greatest degree of flexibility around process connection at the cost of extra configuration. All processes connect to the RCs. The details for how to configure each process type are described below.\n\n#### Gateway\n\nYou can configure the Gateway to connect to the RC(s) in one of three ways. They are listed below in decreasing order of decreasing precedence.\n- Environment variableThe simplest configuration method is to explicitly define the RC address(es) using theKXI_SG_RC_ADDRenvironment variable in the GW container. This variable supports one or more RC addresses.bashCopyKXI_SG_RC_ADDR=\"<rc_host1>:<rc_port1>,<rc_host2>:<rc_port2>\"Note that this method restricts the GW to connect to a single RC.\n- Kubernetes control plane.If using Kubernetes, configure the GW to connect to RCs usingKubernetes labels. For this method, the GW pod requiresKubernetes RBACpermissions for the \"get\", \"watch\", and \"list\" verbs of the \"pods\" resource.Note that GWs connect to all RCs it is configured to discover. The GWs round-robin between them on each request. Moreover, the GW can target a specific set of RCs usingscope.Below is an example configuration.YAMLCopy# GW pod.apiVersion: apps/v1kind: Podmetadata:name: insights-gatewayspec:serviceAccountName: insights-serviceAccountcontainers:# GW container- ...env:# The following environment variables control what RCs the GW will find. Shown here are the default values; they# are used if the corresponding environment is not defined. These can be overwritten to allow for fine-tuned# controlled over GW-RC connections.#    - RC_LABEL_SELECTOR    Label selector to identify/filter RC pods (`kubectl get pods -l '...').#    - RC_CONTAINER_NAME    Name of the RC's container within the RC pod.- name: KXI_RC_LABEL_SELECTORvalue: app.kubernetes.io/name=resource-coordinator- name: KXI_RC_CONTAINER_NAMEvalue: resource-coordinator---# GW service account.apiVersion: v1kind: ServiceAccountmetadata:name: insights-gateway-service-account---# RBAC role.apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:name: insights-gateway-rolerules:- apiGroups: [\"\"]resources: [\"pods\"]verbs: [\"get\",\"watch\",\"list\"]---# RoleBinding RBAC role to GW's ServiceAccount.apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:name: insights-gateway-role-bindingsubjects:- kind: ServiceAccountname: insights-gateway-service-accountapiGroup:\"\"roleRef:kind: Rolename: insights-gateway-roleapiGroup:\"\"The GW(s) connect to all RCs with the corresponding metadata labels:YAMLCopykind: Podmetadata:name: insights-resource-coordinatorlabels:app.kubernetes.io/name:\"resource-coordinator\"# Must match GW's KXI_RC_LABEL_SELECTORspec:containers:- resource-coordinator:# Must match GW's KXI_RC_CONTAINER_NAMEports:- ...containerPort:5050# Must set a port for the GW to connect toprotocol: TCP\n\n#### Data access process\n\nDAPs connect to their respective RCs in one of three ways. They are listed here in order of decreasing precedence.\n- Environment variableConfigure a DAP to explicitly connect to a particular RC by defining the RC address in theKXI_SG_RC_ADDRenvironment variable.bashCopyKXI_SG_RC_ADDR=\"<rc_host>:<rc_port>\"\n- Kubernetes discoveryIf using Kubernetes, configure the DAPs to connect to RCs usingKubernetes labels. For this method, the DAP pods requireKubernetes RBACpermissions for the \"get\", and \"list\" verbs of the \"pods\" resource.Refer toKubernetes configurationfor an example configuration.Note that, while DAPs may discover multiple RCs (all those matching the configured label selector), each DAP connects to one RC usingordinals.\n\n#### Aggregator\n\nAggs connect to to their respective RCs in one of below methods. They are listed here in order of decreasing precedence.\n- Environment variableConfigure an Agg to explicitly connect to a particular RC by defining the RC address in theKXI_SG_RC_ADDRenvironment variable.bashCopyKXI_SG_RC_ADDR=\"<rc_host>:<rc_port>\"\n- Kubernetes discoveryIf using Kubernetes, configure the Aggs to connect to RCs usingKubernetes labels. For this method, the Agg pods requireKubernetes RBACpermissions for the \"get\", and \"list\" verbs of the \"pods\" resource.Refer toKubernetes q configurationfor an example configuration.Note that, while Aggs may discover multiple RCs (all those matching the configured label selector), each Agg connects to one RC usingordinals.\n\n#### Resource coordinator\n\nRCs connect to each other so they can enlist each other for help when the RC receiving the request does not contain the required DAPs to be able to complete the request on its own, Refer to\nRouting\nfor details. RCs can only connect to each other using\nKubernetes labels\n. The RC pods require\nKubernetes RBAC\npermissions for the \"get\", and \"list\" verbs of the \"pods\" resource.\nRefer to\nKubernetes q configuration\nfor an example configuration.\nNote that\nglobal\nRCs connect to all RCs, and\ndedicated\nRCs connect only to other RCs within their respective packages. Refer to\ndedicated/global RCs\nfor more information.\n\n### kdb Insights Enterprise\n\nIf you use\nkdb Insights Enterprise\n, no extra configuration is needed. DAP-RC and Agg-RC connection is done\nby ordinal\n.\n\n### Kubernetes q configuration\n\nThe following is an example configuration for Kubernetes-based discovery for q containers (DAP, RC, Agg).\nYAML\nCopy\n\n```\n# Connecting pod.# Connecting pod.apiVersion: apps/v1kind: Podmetadata:  name: connecting-podspec:  serviceAccountName: insights-serviceAccount  containers:  # Connecting container.  - ...    env:    # Enable Kubernetes discovery.    - name: KXI_DISC_MODE      value: kubernetes    # The following environment variables control what RCs the connecting container will find. Shown here are the    # default values; they are used if the corresponding environment is not defined. These can be overwritten to allow    # for fine-tuned controlled over RC connections.    #    - RC_LABEL_SELECTOR    Label selector to identify/filter RC pods (`kubectl get pods -l '...').    #    - RC_CONTAINER_NAME    Name of the RC's container within the RC pod.    - name: KXI_RC_LABEL_SELECTOR      value: app.kubernetes.io/name=resource-coordinator    - name: KXI_RC_CONTAINER_NAME      value: resource-coordinator---# Service account.apiVersion: v1kind: ServiceAccountmetadata:  name: insights-service-account---# RBAC role.apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: insights-rolerules:- apiGroups: [\"\"]  resources: [\"pods\"]  verbs: [\"get\", \"list\"]---# RoleBinding RBAC role to GW's ServiceAccount.apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: insights-role-bindingsubjects:- kind: ServiceAccount  name: insights-service-account  apiGroup: \"\"roleRef:  kind: Role  name: insights-role  apiGroup: \"\"\n```\n\nThe Pod/container above discovers all RCs configured as shown below.\nYAML\nCopy\n\n```\nkind: Podmetadata:  name: resource-coordinator  labels:    app.kubernetes.io/name: \"resource-coordinator\" # Must match connecting pod's KXI_RC_LABEL_SELECTORspec:  containers:  - resource-coordinator: # Must match connecting pod's KXI_RC_CONTAINER_NAME    ports:    - ...      containerPort: 5050 # Must set a port      protocol: TCP\n```\n\nRCs connect to all discovered RCs, whereas DAPs and Aggs connect to one of the discovered RCs using\nordinals\n.\n\n### Ordinal connection\n\nIf you use Kubernetes-based discovery to connect to RCs, DAPs and Aggs, use ordinals. A process's ordinal is the number following the last\n\"-\"\nor\n\"_\"\nin the process's host name. If a process's host name has no number following the last\n\"-\"\nor\n\"_\"\n, then its ordinal is\n0\n. For example:\n\n| host name | ordinal |\n| --- | --- |\n| resource-coordinator-3 | 3 |\n| dap-hdb_11 | 11 |\n| aggregatorOne | 0 |\n\nIt is important to use properly numbered RC, DAP, and Agg replicas with sequential ordinals. Use\nKubernetes StatefulSets\nor\nDocker compose replicas\nto do this.\nIn kdb Insights, in order for this method to work, RCs MUST set the following the\nKXI_RC_STS_SIZE\nenvironment variable to the total number of RCs.\nbash\nCopy\n\n```\nKXI_RC_STS_SIZE=<total_number_of_RCs>\n```\n\nIn\nkdb Insights Enterprise\n, this environment variable is automatically set.\nA DAP or Agg with ordinal\nn\nconnects to the (unique) RC whose ordinal is congruent to\nn\nmodulo\nKXI_RC_STS_SIZE\n. For example, in a system with 6 DAPs and\nKXI_RC_STS_SIZE=3\n:\n\n| DAP | RC |\n| --- | --- |\n| dap-0 | rc-0 |\n| dap-1 | rc-1 |\n| dap-2 | rc-2 |\n| dap-3 | rc-0 |\n| dap-4 | rc-1 |\n| dap-5 | rc-2 |\n\nNote\n- You must have at least as many DAPs and Aggs as RCs so that each RC has at least one DAP and one Agg.\n- We recommend that you have a number of DAPs and Aggs equal to a multiple of the number of RCs so that each RC has equal query throughput capacity.\n- We recommend using ordinal-based connections as described above (i.e. letting DAPs and Aggs determine the correct RC via modulo arithmic). However, for specialized setups, it may be desirable to override an individual pod's ordinal. This can be achieved using theKXI_ORDINALenvironment variable. E.g.KXI_ORDINAL=\"1\"sets the pod's ordinal to1regardless of the pod's actual ordinal in the stateful set.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1515,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-3610ac7fae0a",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/beta-feature-standard-terms.htm",
    "title": "Beta Features Terms",
    "text": "\n# Beta Feature Standard Terms\n\nThis page outlines the standard terms related to beta features of\nkdb Insights Enterprise\n.\nYou acknowledge that the following features of the Licensed Software are made available to\nyou on a beta/early access basis, ahead of general availability to the public:\n- Stats - Stream Processor\n- Delete Rows in a Table:Insights SDKInsights Enterprise\n- Scratchpad log capture and retrieval\n- Avro EncoderandDecoder\nWarranty disclaimer\n:\nTHE LICENSEE ACKNOWLEDGES THAT THE ALPHA/BETA PRODUCT IS NOT A PUBLICLY RELEASED PRODUCT, AND THAT AS SUCH THE ALPHA/BETA PRODUCT IS NOT A FULLY FUNCTIONING PRODUCT, IS SUBJECT TO CHANGE AND MAY HAVE LIMITATIONS, UNEXPECTED RESULTS, BUGS AND OTHER ERRORS. KX MAKES NO REPRESENTATION OR WARRANTY, EXPRESS OR IMPLIED, THAT THE ALPHA/BETA PRODUCT WILL EVER BE A KX PRODUCT. FOR PURPOSES OF THIS AGREEMENT, THE ALPHA/BETA PRODUCT IS LICENSED âAS ISâ AND KX MAKES NO WARRANTY, EXPRESS, IMPLIED, OR STATUTORY, INCLUDING BUT NOT LIMITED TO ALL WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT OR FITNESS FOR A PARTICULAR PURPOSE.\nLimitation of Liability\n:\nREGARDLESS OF WHETHER ANY REMEDY SET FORTH HEREIN FAILS OF ITS ESSENTIAL PURPOSE, IN NO EVENT WILL KX BE LIABLE TO LICENSEE FOR ANY SPECIAL, CONSEQUENTIAL, INDIRECT OR SIMILAR DAMAGES, INCLUDING ANY LOST PROFITS OR LOST DATA ARISING OUT OF THE USE OR INABILITY TO USE THE KX SERVICE EVEN IF KX HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 233,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-8401dc48d0c9",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/encoders.htm",
    "title": "Encoders",
    "text": "\n# Encoders\n\nThis page explains how to set up encoder operators for\nkdb Insights Enterprise\npipelines using the Web Interface.\nEncoding allows data to be converted into a format that can be passed to an external system either by writing the content to a static file or by streaming the data to a different system.\nTip\nSee APIs for more details\nBoth q and Python interfaces can be used to build pipelines programmatically. See\nthe q and Python APIs\nfor details.\nThe pipeline builder uses a drag-and-drop interface to link together operations within a pipeline. For details on how to wire together a transformation, see the\nbuilding a pipeline guide\n.\n\n## Arrow\n\n(Beta Feature) The Arrow operator encodes kdb Arrow data.\nNote\nBeta - For evaluation and trial use only\nThis feature is currently in beta.\n- Referhere to the standard termsrelated to beta features\n- We invite you to use this beta feature and to provide feedback using theIdeas portal\n- During deployment, the entitlements feature is disabled by default, meaning no restrictions are applied and you can manage all databases, pipelines, and views as well as query all data in akdb Insights Enterprisedeployment\n- When you enable the feature, youdo nothave access to query data in a database unless you have been given a data entitlement to query the database in question\nNote\nq and Python APIs:\n.qsp.encode.arrow\nOptional Parameters:\n\n| name | description | default |\n| --- | --- | --- |\n| Payload Type | Indicates the message payload that is passed in the stream. This is used as an optimization if the data shape is known ahead of time. Otherwise, leave set asAutomatic. | Automatic |\n\n\n## Avro\n\nThe Avro encoder converts kdb+ objects into Avro data messages.\nYou must provide an Avro schema object as an argument and optionally the desired message encoding.\nNote\nBeta - For evaluation and trial use only\nThis feature is currently in beta.\n- Referhere to the standard termsrelated to beta features\n- We invite you to use this beta feature and to provide feedback using theIdeas portal\n- During deployment, the entitlements feature is disabled by default, meaning no restrictions are applied and you can manage all databases, pipelines, and views as well as query all data in akdb Insights Enterprisedeployment\n- When you enable the feature, youdo nothave access to query data in a database unless you have been given a data entitlement to query the database in question\nNote\nq and Python APIs:\n.qsp.encode.avro\nRequired Parameters:\n\n| name | description | default |\n| --- | --- | --- |\n| Schema | A schema definition indicating the Avro message format to be encoded. |  |\n\nOptional Parameters:\n\n| name | description | default |\n| --- | --- | --- |\n| Encoding | Indicates whether to encode data as AvroBinaryorJSON | Binary |\n\n\n## CSV\n\nThis operator converts data into CSV format.\nNote\nq and Python APIs:\nCSV\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Delimiter | Field separator for the records in the encoded data | , |\n\nOptional Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Header | Indicates whether encoded data should start with a header row. Options areNever,First Row, orAlways. | First Row |\n\n\n## JSON\n\nThe JSON operator serializes data into JSON format.\nNote\nq and Python APIs:\nJSON\nOptional Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Split | By default, batches are encoded as single JSON objects. Split encodes each value in a given batch separately. When the input is a table, this encodes each row as its own JSON object. | No |\n\n\n## Protocol Buffers\n\nThe Protocol Buffers operator serializes data into\nProtocol Buffers\n.\nNote\nq and Python API:\nProtocol Buffer\nRequired Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Message Name | The name of the Protocol Buffer message type to decode |  |\n| Message Definition | A.protodefinition containing the expected schema of the data to decode. This definition must include a definition of theMessage Namereferenced above. |  |\n\nOptional Parameters\n:\n\n| name | description | default |\n| --- | --- | --- |\n| Payload Type | Indicates the message payload that will be passed in the stream. This is used as an optimization if the data shape is known ahead of time. Otherwise, leave set asAutomatic. | Automatic |\n\n\n## Further reading\n\n- Building pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 777,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-d4beb56cd84f",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Releases/KXI/1.17.2](https:/github.com/rook/rook/releases/tag/v1.17.2",
    "title": "Secure Login",
    "text": "A modern browser is required to view this site. Please use the latest version of Chrome, Firefox, Safari, or Edge.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 20,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-e7cacaaf2610",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/entitlements.htm",
    "title": "Entitlements",
    "text": "\n# Entitlements\n\nThis page describes how you can use entitlements in\nkdb Insights Enterprise\n.\n\n## Entitlements overview\n\nEntitlements manage a group of usersâ access to specific entities in the system. When entitlements are enforced, a user can access the specific functionality provided by their\nRoles\n, but not across the entire system. Functionality is available only to users on the specific packages or data they are entitled to access.\nFor example:\n- You may have a scenario where you want to allow an end user to query data in the database, but you don't want them to see all the business logic for data transformation or analytics.\n- A pipeline builder needs the capability to edit and deploy a pipeline, but not all pipelines.\nEntitlements help you achieve this. When using the entitlements feature, you must enable\nEncryption in transit\nfor extra security.\nNote\nIf another user gains entitlements to a package that contains a pipeline, that pipeline's workers also gain delegated entitlements to that pipelineâs runtime entitlements. For more information, refer to\nEntitlement delegation in pipeline execution context\n\n## Types of entitlements\n\nThere are three types of entitlements you can use to control user permissions in\nkdb Insights Enterprise\n:\n- Data entitlements- control who is entitled to query data within a deployed database, this includes Stream Processor and pipelines.\nWarning\nData and Row Entitlements are applied only on the query path. They are not currently applied when streaming data from a Stream Processor or over a WebSocket. Refer to\nVisualize Streaming Data\n.\n- Row-level data entitlements- control who is entitled to query specific rows within a table, based on a policy. Note you must have data entitlement enforced on the system to be able to enable row-level entitlements on a database.\n- Package entitlements- control the entitlement each user has to interact with packages that contain databases, pipelines, and views. Each package has its own entitlements.\n\n## Policies\n\nPolicies enable more granular, row-level data access control for users. They  can be applied to each table in a database to manage row-level data entitlements. Find out more how these are configured in\nrow-level data entitlements.\n\n## Entitlement delegation in pipeline execution context\n\nWhen working with pipelines in\nkdb Insights Enterprise\n, entitlement enforcement behaves differently depending on how a pipeline is deployed and accessed. This section explains how entitlements are delegated during pipeline execution, and how access to packages and workers can implicitly broaden a userâs access to data.\n\n### Pipeline execution context is tied to the deployer\n\nEvery pipeline runs with the entitlements of the user who deployed it, not necessarily the owner of the associated package. This distinction is important:\n- The pipeline inherits the data access entitlements of the deployer.\n- These entitlements are enforced at runtime across all workers in the pipeline.\n\n### Entitlements are delegated with package access\n\nIf another user gains entitlements to a package, they also gain delegated access to the entitlements the deployer of the pipeline has within the context of this pipeline.\n- These entitlementsare not directlygranted to the other user.\n- Instead, these entitlements are implicitly delegated through the pipeline itself.\n- As soon as the user connects to a worker within that pipeline, they enter the same security context as the deployer.\nThis means:\n- Workers from another package the user deploys can connect into this pipeline and run within its entitlement context.\n- Even if a user doesn't have direct entitlements to a database, they can access it indirectly through the pipeline deployer's entitlements, if they can deploy or connect to a worker in that pipeline.\n\n### The security context\n\nPipelines establish a security context based on the deployer's entitlements. This context governs what the workers in the pipeline can reach: typically databases or other infrastructure components.\n- As more entitlements are given to the deployer, the pipelineâs security context grows.\n- Any additional users with entitlements to the pipeline can now leverage the expanded entitlements implicitly.\nFor example:\nA user with no entitlement to a sensitive salary database might still reach it indirectly, if they gain entitlements to a pipeline who's deployer has that database in its entitlement scope.\n\n### Data querying via APIs vs. pipelines\n\nEntitlement enforcement differs between querying as a user and running code within a pipeline:\n- If a user queries a database through the REST API, entitlements are strictly enforced. They cannot access restricted data without explicit entitlements.\n- However, if they deploy additional workers (for example, to q processes) or connect to a pipeline, they can potentially access sensitive resources that fall within the pipeline's security context.\n\n### Summary\n\n\n| Scenario | Entitlement Enforcement |\n| --- | --- |\n| Deploying a pipeline | Inherits entitlements from the deployer |\n| Connecting to a pipeline worker | Gains access to the pipelineâs entitlement context |\n| Querying through REST API | Requires direct user entitlements |\n| Accessing a database through a pipeline | Indirect access based on the entitlements of the deployer of the pipeline |\n\n\n## Next steps\n\nYou can configure entitlements using the following guides:\n- Prerequisites- conditions you must meet to be able to use entitlements\n- Configuration- detailed information on how to use and configure entitlements features\n- Data entitlements quickstart- a step-by-step guide to configuring data entitlements\n- Row-level data entitlements- a step-by-step guide to configuring row-level data entitlements\n- Row-level data entitlements quickstart- a step-by-step guide to configuring row-level data entitlements",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 911,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-4f5f3581a7a4",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Reference/CLI_Reference/cli-reference_-_kxi_pm.htm",
    "title": "CLI - kxi pm",
    "text": "\n# kxi pm\n\nInsights Package Manager Service commands.\nUsage:\ntext\nCopy\n\n```\nkxi pm [OPTIONS] COMMAND [ARGS]...\n```\n\nOptions:\ntext\nCopy\n\n```\n  --version                       Display the version of the kdb+ Insights                                  Enterprise Package Manager service and                                  client.  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm backup\n\nDownload a backup of all entitled packages from kdb+ Insights Enterprise Package Manager service\nAdmin users can download all packages, while non-admin users can only download packages they have READ access to.\nUsage:\ntext\nCopy\n\n```\nkxi pm backup [OPTIONS] [TARGET]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm config\n\nServer configuration options.\nUsage:\ntext\nCopy\n\n```\nkxi pm config [OPTIONS] COMMAND [ARGS]...\n```\n\nOptions:\ntext\nCopy\n\n```\n  --help  Show this message and exit.\n```\n\n\n### kxi pm config get\n\nGet the current configuration of the kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm config get [OPTIONS]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n### kxi pm config set\n\nSet the current configuration of the kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm config set [OPTIONS] OPTION VALUE\n```\n\nOptions:\ntext\nCopy\n\n```\n  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm convert\n\nConvert V1/V2 assemblies into Packages with the server.\nOptionally keep the package installed on the server.\nUsage:\ntext\nCopy\n\n```\nkxi pm convert [OPTIONS] ASSEMBLY_FILE\n```\n\nOptions:\ntext\nCopy\n\n```\n  --install                       Additionally installs the resulting package                                  on the server.  -o, --output-format [json|table]                                  Output format for the list command  --spec-files                    This flag converts embedded pipeline specs                                  to files  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm deploy\n\nDeploy a package to an insights instance.\nPACKAGE: package-name\nVERSION: package-version\nUsage:\ntext\nCopy\n\n```\nkxi pm deploy [OPTIONS] PACKAGE [VERSION]\n```\n\nOptions:\ntext\nCopy\n\n```\n  --env TEXT                      Inject environment variables to the deployed                                  package. `[component_name:]VAR=value`  -o, --output-format [json|table]                                  Output format for the list command  --db TEXT                       Deploy a database in the package  --pipeline TEXT                 Deploy a pipeline in the package  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm edit\n\nEdit the runnable component of the kdb+ Insights Enterprise Package on the service.\nUsage:\ntext\nCopy\n\n```\nkxi pm edit [OPTIONS] PACKAGE_NAME {pipeline|database} COMPONENT_NAME\n```\n\nOptions:\ntext\nCopy\n\n```\n  --field TEXT                    Only edit a specific field of the entity.  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm info\n\nGet info about a package stored on the kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm info [OPTIONS] PACKAGE_NAME [VERSION]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -v, --version TEXT              Select package version. Default to latest.  -i, --include [udfs|policies]   Include additional information in the                                  output. Only applies when no component type                                  is given.  -o, --output-format [json|table]                                  Output format for the list command  -f, --filter TEXT               Key-pattern pairs to filter the returned                                  components using case-sensitive wildcard                                  matching (e.g. name=*test). Multiple                                  patterns ANDed together. Only applies when                                  component type (e.g. table) is given.  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm list\n\nList objects stored on  kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm list [OPTIONS] [[package|deployment|pipeline|view|database|udf]]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -f, --filter TEXT               Key-pattern pair to filter the returned                                  packages on. Applies case-sensitive                                  filtering using wildcard patterns. Multiple                                  patterns ANDed together. Key names may                                  include dot notation for nested values                                  (e.g., owner.name=John*).  --fields TEXT                   [default: name, package, version, function,                                  status, data, access, owner, id, info]  -o, --output-format [json|table|csv|simple]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm pull\n\nDownload a package from kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm pull [OPTIONS] NAME [VERSION]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -O, --output TEXT               Specify the output directory/archive name                                  for the downloaded package. Defaults to                                  package-name or packagename-version.kxi  -A, --artifact                  Download the package into a compressed                                  archive.  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm push\n\nPublish  a package to   kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm push [OPTIONS] [PATH]\n```\n\nOptions:\ntext\nCopy\n\n```\n  --deploy                        Deploy the package after pushing.  --force                         Force push: overwrite existing packages  -o, --output-format [json|table]                                  Output format for the list command  --lock-q-files                  Lock the contents of the q files in the                                  package. Files whose first line is /dnc or                                  //dnc will be ignored.  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm restore\n\nRestore a previously downloaded backup to kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm restore [OPTIONS] PATH\n```\n\nOptions:\ntext\nCopy\n\n```\n  --force                         Force restore: overwrite existing packages  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm rm\n\nRemove   a package from kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm rm [OPTIONS] ID [VERSION]\n```\n\nOptions:\ntext\nCopy\n\n```\n  --teardown                      Teardown the package before removing.  --rm-data                       Remove the data associated with the                                  deployment  -o, --output-format [json|table]                                  Output format for the list command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm teardown\n\nTeardown a deployed package running on an insights instance\nUsage:\ntext\nCopy\n\n```\nkxi pm teardown [OPTIONS] PACKAGE\n```\n\nOptions:\ntext\nCopy\n\n```\n  --rm-data                       Remove the data associated with the                                  deployment  -o, --output-format [json|table]                                  Output format for the list command  --db TEXT                       Teardown a database in the package  --pipeline TEXT                 Teardown a pipeline in the package  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n\n\n## kxi pm validate\n\nValidate a package in kdb+ Insights Enterprise Package Manager service.\nUsage:\ntext\nCopy\n\n```\nkxi pm validate [OPTIONS] [PATH]\n```\n\nOptions:\ntext\nCopy\n\n```\n  -o, --output-format [json|table]                                  Output format for the validate command  --hostname, --url TEXT          Insights URL  --realm TEXT                    Realm  --client-id TEXT                Client id  --client-secret TEXT            Client secret  --auth-enabled / --auth-disabled                                  Retrieve Bearer Token  --server-timeout INTEGER        Timeout for Insights server calls  -y, --yes, --assume-yes         Automatic yes to prompts; assume \"yes\" as                                  answer to all prompts and run non-                                  interactively.  --help                          Show this message and exit.\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1693,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-74f66b1995d2",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/data-entitlements.htm",
    "title": "Data Entitlements",
    "text": "\n# Data Entitlements\n\nThis page describes how data entitlements work and how to configure them.\nData entitlements restrict user access to data. When setting up user access, you must explicitly entitle a group of users to query a specific database.\nkdb Insights Enterprise\ncontrols querying of data in databases deployed using the\nCLI\nand the\nweb interface\n, using a combination of:\n- Role based access controls- this determines which users can create databases and ingest, analyze, and view data. For details on the available roles, refer toRoles.\n- Data entitlements-if entitlements are enforced,and the user has the required Role to query, the data entitlement determines which groups of users can query which databases.\nWarning\nLimitations\n- Entitlements are applied on the query path only. Streaming data from a stream processor or over a web socket does not currently apply entitlements. ReadStreaming to Views.\nOnce data entitlements are enforced, you have the option to additionally enable\nRow-Level Entitlements\nfor a specific database. When enabled, access is restrictive by default. This means that only the\nOwner\nof the database and users with the\ninsights.entitlements.admin\nrole can query any rows in any tables in the database. To grant access to rows for end users, you must add a\nrow policy\nthat allows a group of users to query a specific set of rows in a table, based on a filter you specify.\nWhen data entitlements are enforced but row-level entitlements are disabled, the following examples illustrate the outcomes for users with and without entitlements when querying data in\nkdb Insights Enterprise\n. These examples assume the users have at least the Viewer (\ninsights.role.viewer\n) role assigned to them:\n- If a query scope selects a database for which the user is not entitled, a permission error is displayed.\n- If a query spans multiple databases, they only receive data from the databases for which they are entitled.\nWhen row-level entitlements is enabled, the data a user receives is a subset of the data based on the row policy associated with the group they are part of. A user does not receive any data if there are no row policies associated with the group they are part of.\nNote\nEntitlement owners and users with the\ninsights.entitlements.admin\nrole are entitled to query every row in a database, no matter what specific entitlements are set.\n\n## Next steps\n\nAfter you complete the\nprerequisites\nand have enabled entitlements, you can begin providing entitlements to user groups. To do this, you can either follow the\nquickstart\nguide or use the\nconfiguration\ndetails.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 426,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-67eb73de9f90",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Deployment/Standalone/k8s-managed-prereqs.htm",
    "title": "Prerequisites",
    "text": "\n# Managed K8S Prerequisites\n\nThis page details the infrastructure prerequisites required to deploy\nkdb Insights Enterprise\non the Kubernetes container orchestration system.\n\n## Managed Kubernetes cluster\n\nkdb Insights Enterprise\ncurrently supports the managed Kubernetes offerings below.\n- AmazonEKSversion <=1.33\n- GoogleGKEversion <=1.33\n- Microsoft AzureAKSversion <=1.33\n\n## Cluster Node Pools\n\nThe number of node pools required for the cluster depends on the\nrook-ceph\nconfiguration used.\n- Host Storage ClusterForrook-cephwithHost Storage Clusterconfiguration two separate node pools are required. SeeHost Storage Clusterfor more details.\n- PVC ClusterForrook-cephwithPVC Clusterconfiguration a single node pool is sufficient. SeePVC Clusterfor more details.\nNote\nThe default configuration provided by the\nInfrastructure as Code example scripts\nuses a\nHost Storage Cluster\nconfiguration with separate compute and storage node pools.\n\n## Ingress Controller\n\nAn ingress controller such as\ningress-nginx\nis required to access the\nkdb Insights Enterprise\ndashboards and APIs from outside the cluster.\nIn order to use the NGINX Ingress Controller a valid SSL certificate is required for the ingress endpoint. For details on how certificates are used in\nkdb Insights Enterprise\nsee\nhere\n.\n\n## Certificate Manager\n\nThe\ncert-manager\ninstallation is required to add certificates and certificate issuers as resource types in the Kubernetes cluster.\nEach deploy of\nkdb Insights Enterprise\nwill create a namespaced certificate issuer to provide mTLS between microservices.\nA ClusterIssuer such as\nletsencrypt\ncan be used with the NGINX Ingress Controller above to provide a certificate for the API endpoints.\nNB air-gapped deploys: cert-manager requires outbound access to the internet as well as unrestricted inbound HTTP access to the cluster and so cannot be used in an air-gapped environment. Instead the NGINX Ingress Controller should reference a Kubernetes secret which contains a certificate for the API endpoints.\n\n## Distributed storage system\n\nThe data tier in\nkdb Insights Enterprise\nrequires a shared filesystem such as\nRook Ceph\nwhich can be mounted with read/write permission from multiple pods. Choose from one of the\nsupported file systems\nbased on your performance and availability requirements.\nNote\nkdb Insights Enterprise\nrequires a\nstorage class\nnamed\nsharedfiles\nto provision shared file storage instances.\n\n## DNS record which points to your Kubernetes Ingress\n\nIn order to access your cluster, a DNS record should be created which resolves to the external IP address of the clusterâs NGINX Ingress Controller. For more information see\nDNS Setup\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 385,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-c04bd6c56ee3",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Ingest/pipeline-k8s-secrets.htm",
    "title": "Secure Pipelines with Kubernetes Secrets",
    "text": "\n# Secure Pipelines with Kubernetes Secrets\n\nThis page outlines how to manage sensitive data within pipelines by using Kubernetes Secrets and mounting them into pipelines as environment variables. By storing sensitive information in Kubernetes Secrets, it separates them from your pipeline definition and reduces the risk of exposure.\nThe following sections describe how to read from an authenticated Kafka broker using Secrets to store the authentication credentials:\n- What is a Kubernetes Secret\n- How to create a Secret\n- Mount Secret keys into environment variables\n- Use those environment variables in your pipeline nodes\n\n## What is a Kubernetes Secret\n\nKubernetes Secrets are objects that allow you to store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. Using Secrets is a more secure way to handle sensitive data than putting it directly in your pipeline.\n\n## Create a Kubernetes Secret\n\nTo create a Kubernetes Secret, define it in a YAML manifest and apply it with\nkubectl\n. For example:\nYAML\nCopy\n\n```\napiVersion: v1kind: Secret    metadata:        name: kafka-secret        type: Opaque    data:        username: <base64-encoded-username>        password: <base64-encoded-password>\n```\n\nKubernetes YAML manifest details\n- apiVersion: v1- The API group and version to use.\n- kind: Secret- Tells Kubernetes that you are creating a Secret.\n- metadata:This section provides the information:name: kafka-secret- The name of the secret. In this example, we are creating a secret to be used by the Kafka reader so have named itkafka-secret. Any name can be used.type: Opaque- Tells Kubernetes this is a generic user-defined type.\n- data:This contains the actual data for the secret, and says that the values must be base64 encoded strings, not plain text.username: <base64-encoded-username>- Specify the actual username here.password: <base64-encoded-password>- Specify the actual password here.\nNote\nWhen creating secrets, the data values must be base64 encoded.\nTo create the Secret in your cluster, run:\nbash\nCopy\n\n```\nkubectl apply -f kafka-secret.yaml                 \n```\n\nCommand detail:\n- kubectl apply- Kubernetes command to create or update a resource from a configuration file.\n- -f kafka-secret.yaml- Specifies the filekafka-secret.yamlthat contains the secret key definition.\nInformation\nIf you don't have access to the Kubernetes cluster to create a Secret, talk to your system administrator.\nTip\nSecrets managers\nExternal secrets managers (like AWS Secrets Manager, Google Secret Manager, or HashiCorp Vault) are dedicated systems designed to securely store, manage, and distribute sensitive information. They are often used in place of manual Kubernetes storage because they provide enhanced security through centralized control, auditing capabilities, and often offer features like secret rotation, fine-grained access policies. This offloads the responsibility of secure secret handling from Kubernetes' default mechanisms, allowing for more robust and scalable credential management across various environments and applications.\n\n## Mount Secrets as environment variables\n\nOnce a Kubernetes Secret is created (either directly, as described above, or provisioned from a secrets manager), you can mount its keys as environment variables into your pipeline(s). This is the recommended way for pipelines to consume credentials.\nUsing the\nkafka-secret\ncreated in the\nprevious section\n:\n- Mount theusernameandpasswordkeys into the pipeline as environment variables. For this example, call these variablesKAFKA_USERNAMEandKAFKA_PASSWORD.The names of these environment variables must match the valuesused in the node parameters later.\nThe way you do this depends on how you build your pipelines.\nWeb Interface\ncode\n- Navigate to theEnvironment Variablessection of thePipeline Settingspanel.\n- Add the two environment variables referencingkafka-secret, and theusernameandpasswordkeys.\nIf you're deploying a code pipeline, using either q or python, add the following to your pipeline YAML definition to define the environment variables.\nyaml\nCopy\n\n```\nbase: qname: kafkaspec: file://src/kafka.qenv: - name: KAFKA_USERNAME    valueFrom:        secretKeyRef:            name: kafka-secret            key: username - name: KAFKA_PASSWORD    valueFrom:        secretKeyRef:            name: kafka-secret            key: password                 \n```\n\nyaml details\nThis defines two environment variables:\n- KAFKA_USERNAMEwhose value is pulled securely from a Kubernetes Secret namedkafka-secret, using the keyusername.\n- KAFKA_PASSWORDwhose value is pulled from the same secret, using the keypassword.\n\n## Use environment variables in pipeline nodes\n\nOnce the secret is created and mounted to environment variables, you can then use it as a parameter in your pipeline nodes. The Kafka broker the pipeline is connecting to requires SASL authentication which require the username and password configured already.\nTo use the environment variable, follow the relevant steps depending on whether you're using a Web Interface pipeline, q code, or Python code pipeline. The environment variables referenced below must match those\ncreated earlier\n.\nWeb Interface\nq code\nPython code\n- Open theKafka reader node. EnableAdvanced Broker Options, and set the followingKey/Valuepairssasl.username=${KAFKA_USERNAME}sasl.password=${KAFKA_PASSWORD}In the Web Interface environment variables are supported using${VAR NAME}syntax.\nWhen creating a q code pipeline:\n- Use the.qsp.useVarfunction to set an environment variable as a node parameter.\n- In the code snippet below, the$KAFKA_USERNAMEand$KAFKA_PASSWORDenvironment variables are specified using this API.\nThe variables are resolved to their values at runtime.\nq\nCopy\n\n```\nusername : .qsp.useVar \"KAFKA_USERNAME\"password : .qsp.useVar \"KAFKA_PASSWORD\"options : `sasl.username`sasl.password!(username; password);.qsp.run    .qsp.read.fromKafka[topic; broker; .qsp.use enlist[`options]!enlist options]                  \n```\n\nSnippet detail\n- username : .qsp.useVar \"KAFKA_USERNAME\"- Creates an object to resolve the $KAFKA_USERNAME environment variable.\n- password : .qsp.useVar \"KAFKA_PASSWORD\"- Creates an object to resolve the $KAFKA_PASSWORD environment variable.\n- options : `sasl.username`sasl.password!(username; password);- Builds a q dictionary with two keys;sasl.usernameandsasl.password, whose values come from the environment variable objects.\n- .qsp.run- Launches the pipeline.\n- .qsp.read.fromKafka[topic; broker;- Creates the Kafka reader with a specific topic and broker.\n- .qsp.use enlist[`options]!enlist options]- Pulls the username and password into the Kafka reader.\nWhen creating a Python code pipeline:\n- Use thekxi.sp.use_varfunction to set an environment variable as a node parameter.\n- In the code snippet below, the$KAFKA_USERNAMEand$KAFKA_PASSWORDenvironment variables are specified using this API.\nThe variables are resolved to their values at runtime.\npython\nCopy\n\n```\nfrom kxi import spusername = sp.use_var(\"KAFKA_USERNAME\")password = sp.use_var(\"KAFKA_PASSWORD\")options = {\"sasl.username\": username, \"sasl.password\": password};sp.run(    sp.read.from_kafka(topic; broker; {\"options\": options})\n```\n\nSnippet detail:\n- from kxi import sp- Imports the SP API.\n- username = sp.use_var(\"KAFKA_USERNAME\")- Creates an object to resolve the $KAFKA_USERNAME environment variable.\n- password = sp.use_var(\"KAFKA_PASSWORD\")- Creates an object to resolve the $KAFKA_PASSWORD environment variable.\n- options = {\"sasl.username\": username, \"sasl.password\": password};- Builds the Kafka authentication options.\n- sp.run- Starts the pipeline.\n- sp.read.from_kafka(topic; broker; {\"options\": options}- Defines the Kafka reader using the environment variables in the Kafka authentication options.\n\n## Further Reading\n\n- Build Pipelines using the Web Interface\n- Stream Processor APIs",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1036,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-0d0061e64583",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/How_To/Observe/pm-journal.htm",
    "title": "Package Manager Journaling",
    "text": "\n# Package Manager Journaling\n\nThis page explains how to use the\nkxi pm journal\ncommand in\nkdb Insights Enterprise\nusing the Command Line Interface (CLI) for observability purposes.\nThe\nkxi pm journal\ncommand provides administrators with a server-wide view of all successful events executed in the\nkdb Insights Enterprise\nPackage Manager\n.\n\n## Usage\n\nUnlike package-specific commands, the\nkxi pm journal\ncommand is designed as a server observability feature, enabling you to:\n- View historical records of executed actions in the Package Manager\n- Audit actions across all users and packages.\n- Support auditing, troubleshooting, and operational oversight\nImportant\nThe\nkxi pm journal\ncommand requires the Package Admin role.\nRun the command as follows:\nbash\nCopy\n\n```\nkxi pm journal\n```\n\nRunning the\n--help\ncommand displays all available options that you can use with the\nkxi pm journal\ncommand:\nbash\nCopy\n\n```\nkxi pm journal --help\n```\n\nconsole\nCopy\n\n```\n Usage: kxi pm journal [OPTIONS] Get the current journal history of the kdb+ Insights Enterprise Package Manager service.â­â Authentication option overrides ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â --hostname,--url                  TEXT  Insights URL                                                                                                          ââ --realm                           TEXT  Realm                                                                                                                 ââ --client-id                       TEXT  Client id                                                                                                             ââ --client-secret                   TEXT  Client secret                                                                                                         ââ --auth-enabled/--auth-disabled          Retrieve Bearer Token                                                                                                 ââ°ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯â­â Options ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®â --server-timeout        INTEGER                                         Timeout for Insights server calls                                                     ââ --yes,--assume-yes  -y                                                  Automatic yes to prompts; assume \"yes\" as answer to all prompts and run               ââ                                                                         non-interactively.                                                                    ââ --output-format     -o  [json|table]                                    Output format for the command. default: 'table'.                                      ââ --fields                TEXT                                            [default: timestamp, action, username, package, component_type, component, message,   ââ                                                                         params]                                                                               ââ --action            -a  TEXT                                                                                                                                  ââ --package           -p  TEXT                                                                                                                                  ââ --username          -u  TEXT                                                                                                                                  ââ --component         -c  TEXT                                                                                                                                  ââ --limit             -l  INTEGER                                                                                                                               ââ --offset                INTEGER                                                                                                                               ââ --from                  [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]                                                                                        ââ --to                    [%Y-%m-%d|%Y-%m-%dT%H:%M:%S|%Y-%m-%d %H:%M:%S]                                                                                        ââ --help                                                                  Show this message and exit.                                                           ââ°ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯\n```\n\n\n## Example commands\n\n- Retrieve journal entries for a specific user:bashCopykxi pm journal --username alice\n- Limit results to the last 10 entries in JSON format:bashCopykxi pm journal --limit10 --output-format json\n- Filter by time range:bashCopykxi pm journal --from 2025-01-01 --to 2025-01-31\n- Filter by action type (for example, install):bashCopykxi pm journal --action install",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 341,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-d690a750a003",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Deployment/Standalone/deployment-overview.htm",
    "title": "Deployment Overview",
    "text": "\n# Standalone Deployment ofkdb Insights EnterpriseOverview\n\nThis page details an example infrastructure architecture for deploying\nkdb Insights Enterprise\non the Kubernetes container orchestration system.\n\n## Example infrastructure\n\nTo support the deployment of\nkdb Insights Enterprise\n, we provide an Infrastructure as Code (IaC) bundle. The bundle contains a\nDockerized\nenvironment with\nTerraform\ncode and the tools required to create a managed Kubernetes cluster in any of the three major cloud providers.\nThe infrastructure provisioned satisfies the base requirements for deploying\nkdb Insights Enterprise\nand provides additional integrations including Cloud Logging and Metrics. The result is a secure, but not production hardened infrastructure, with the Kubernetes cluster deployed into a private network and is only accessible through a VPN.\nThe following diagram shows a high-level overview of the cloud infrastructure provisioned by the IaC bundle.\n\n### Terraform\n\nHashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share.\nWe chose Terraform as it is multi-cloud, with plugins for AWS, Azure, GCP and others. This allows you to spin up the same architecture regardless of your chosen cloud provider, with only minor difference in the configuration steps.\n\n## Resources provisioned\n\nThe IaC bundle provisions resources at both the cloud service provider and Kubernetes level.\n\n### Cloud Provider resources\n\n- Virtual Private Cloud (VPC)\n- Public subnets used by the bastion host and any resources created by the Kubernetes cluster (for example, Load Balancer)\n- Private subnets to deploy the Kubernetes worker nodes\n- Bastion host running on a compute instance which acts as a VPN server to provide access to the Kubernetes cluster\n- Managed Kubernetes Cluster (AKS,EKS,GKE).\n\n### Kubernetes resources\n\n- cert-manager1.19.1- generates certificates for any endpoint exposed by nginx-ingress\n- nginx-ingress4.14.0- exposeskdb Insights Enterpriseendpoint which can be either REST API or Web Interface\n- rook-ceph1.18.7- provides a shared filesystem forkdb Insights Enterprise\n- A StorageClass namedsharedfilesthat is backed by shared storage. A recommended option is to deploy the cloud provider's CSI driver that implements network file storage (AWS,Azure,GCP)\n\n### Architecture profile\n\nThe IaC bundle offers the following three different Architecture Profiles:\n\n| Profile | Compute Node vCPUs | Compute Node RAM (GB) | Compute Node Count | Availability Zones |\n| --- | --- | --- | --- | --- |\n| High Availability | 16 | 128 | Min 3 / Max 10 | 3 |\n| Performance Optimized | 30 (GKE) / 32 (AKS/EKS) | 240 (GKE) / 256 (AKS/EKS) | Min 3 / Max 10 | 1 (AKS/GKE) / 2 (EKS) |\n| Cost Optimized | 8 | 64 | Min 3 / Max 10 | 1 (AKS/GKE) / 2 (EKS) |\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 452,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-677ea58e322e",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Deployment/Standalone/upgrade-third-party.htm",
    "title": "Upgrade Third-party Dependencies",
    "text": "\n# Upgrade Third-Party Dependencies\n\nThis page outlines the process for upgrading third-party dependencies for Terraform-based infrastructure deployments.\nThird-party dependencies refer to the software that is installed by the Terraform scripts to support the deployment of\nkdb Insights Enterprise\n.\n\n## When to upgrade\n\nEach release of\nkdb Insights Enterprise\nincludes  a\nThird-party Dependencies\nsection in the release notes. This section lists the specific versions of the third-party infrastructure components that have been tested and verified for compatibility with that specific\nkdb Insights Enterprise\nrelease and provides links to download the required files.\nTo ensure continued stability, compatibility, and support, it is recommended that you update these dependencies in conjunction with the corresponding upgrade of\nkdb Insights Enterprise\n.\n\n### Upgrade the cert-manager\n\n- Using the download links in thekdb Insights Enterpriserelease notes, get the latest version of thecert-managerhelm chart for the specific release ofkdb Insights Enterprise.\n- Switch to your localkxi-terraformdirectory.\n- Open the Terraformvariables.tffile for your cloud provider, for example, AWS (Amazon Web Services),  ACS (Azure Cloud Services), or GCP (Google Cloud Platform), and update thecert-manager helm chart versionvariable to  the new version.AWSACSGCPconsoleCopyvi terraform/k8s_config_aws/variables.tfvariable\"cert_manager_helm_version\"{....default =\"1.16.3\"}consoleCopyvi terraform/k8s_config_azure/variables.tfvariable\"cert_manager_helm_version\"{....default =\"1.16.3\"}consoleCopyvi terraform/k8s_config_gcp/variables.tfvariable\"cert_manager_helm_version\"{....default =\"1.19.1\"}\n- Run themanage-cluster.shscript.LinuxWindowsconsoleCopy./scripts/manage-cluster.shconsoleCopy.\\scripts\\manage-cluster.bat\n- Run theterraform initcommand.LinuxWindowsconsoleCopy./scripts/terraform.sh init configconsoleCopy.\\scripts\\terraform.bat init config\n- Run theterraform plancommand.LinuxWindowsconsoleCopy./scripts/terraform.sh plan configconsoleCopy.\\scripts\\terraform.bat plan config\n- Run theterraform applycommand.LinuxWindowsconsoleCopy./scripts/terraform.sh apply configconsoleCopy.\\scripts\\terraform.bat apply config\n- Verify thecert-manager helm chart version.consoleCopyhelm ls -n cert-manager\n\n### Upgrade the ingress-nginx\n\n- Using the download links in thekdb Insights Enterpriserelease notes, get the latest version of thengress-nginxhelm chart for the specific release ofkdb Insights Enterprise\n- Switch to your localkxi-terraformdirectory.\n- Open the Terraformvariables.tffile for your cloud provider (for example, AWS, ACS, or GCP), and update thecert-manager helm chart versionvariable to  the new version.AWSACSGCPconsoleCopyvi terraform/k8s_config_aws/variables.tfvariable\"ingress_nginx_helm_version\"{....default =\"4.11.5\"}consoleCopyvi terraform/k8s_config_azure/variables.tfvariable\"ingress_nginx_helm_version\"{....default =\"4.11.5\"}consoleCopyvi terraform/k8s_config_gcp/variables.tfvariable\"ingress_nginx_helm_version\"{....default =\"4.11.5\"}\n- Run themanage-cluster.shscript.LinuxWindowsconsoleCopy./scripts/manage-cluster.shconsoleCopy.\\scripts\\manage-cluster.bat\n- Run theterraform initcommand.LinuxWindowsconsoleCopy./scripts/terraform.sh init configconsoleCopy.\\scripts\\terraform.bat init config\n- Run theterraform plancommand.LinuxWindowsconsoleCopy./scripts/terraform.sh plan configconsoleCopy.\\scripts\\terraform.bat plan config\n- Run theterraform applycommand.LinuxWindowsconsoleCopy./scripts/terraform.sh apply configconsoleCopy.\\scripts\\terraform.bat apply config\n- Verify theingress-nginx helm chart version.consoleCopyhelm ls -n ingress-nginx",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 325,
    "metadata": {}
  },
  {
    "id": "kx-official_docs-81b6d52a4af1",
    "origin": "kx",
    "source_type": "official_docs",
    "url": "https://docs.kx.com/insights-enterprise-latest/insights/enterprise/Learn/Web_Interface/pipeline-settings.htm",
    "title": "Pipeline Settings",
    "text": "\n# Pipeline Settings\n\nThis page describes how to configure pipeline settings in the Web Interface.\nWhen creating a pipeline, the\nSettings\ntab lets you configure resources, environment variables, and other tuning options. The following sections detail these settings.\nPipeline Settings screen\nThis is a large screen,  you must scroll to see all the options shown below.\n\n## Runtime\n\n\n| item | description |\n| --- | --- |\n| Protected Execution | Enabled for greater granularity in error reporting. When enabled, operating performance may be impacted. |\n| Log Format | Define debug reporting format; select betweenJSONorTextformats. It is recommended to leave this set to JSON for more structured logs. |\n| Log Level | Select betweenTrace,Debug,Info,Warning,ErrororFatal. Lower log levels include higher ones. For example, the default level isInfowhich includesWarning,ErrorandFatal. |\n\n\n## Resources\n\nResources are the amount of computational resources that this pipeline is allowed to consume. When allocating resources, it is important to remember that only the worker is responsible for processing data. To increase throughput, increase resources on the worker. The controller resources only needs to be modified when a high degree of parallelism is used.\n\n### Controller\n\n\n#### CPU\n\n\n| item | description |\n| --- | --- |\n| Minimum CPU | Minimum amount of CPU for the controller;1 CPUor1000 mCPUfor one core,0.5 CPUor500 mCPUis half time for one core. Must be  >=0.1 CPU. |\n| Maximum CPU | Maximum amount of CPU for the controller; must be <=4 CPU. |\n\n\n#### Memory\n\n\n| item | description |\n| --- | --- |\n| Minimum Memory | Minimum memory to allocate to the controller and always be available; >=50 MiB. |\n| Maximum Memory | Maximum memory to allocate to the controller; once reached it returnsout-of-memoryerror; <=10 GiB. |\n\n\n### Worker\n\n\n#### CPU\n\n\n| item | description |\n| --- | --- |\n| Minimum CPU | Minimum amount of CPU for for the worker;1 CPUor1000 mCPUfor one core,0.5 CPUor500 mCPUis half time for one core. Must be  >=0.1 CPU. |\n| Maximum CPU | Maximum amount of CPU for for the worker. |\n\n\n#### Memory\n\n\n| item | description |\n| --- | --- |\n| Minimum Memory | Minimum memory to allocate to the worker and always be available; >=100 MiB. |\n| Maximum Memory | Maximum memory to allocate to the worker; once reached it returnsout-of-memoryerror. Must be <=500GiB. |\n\n\n#### Config\n\n\n| item | description |\n| --- | --- |\n| Minimum number of workers | Define the minimum number of workers that can be created to run the pipeline; >=1. |\n| Maximum number of workers | Define the maximum number of workers that can be created to run the pipeline; <=10. |\n| Maximum number of worker threads | Maximum number of worker threads; value between1and16. |\n\n\n## Metrics\n\nCustomize or disable pipeline metrics.\n\n### Record counting\n\nPipeline record counting allows pipelines to calculate an average input and output data rate, which is a good indication of how much data the pipeline is ingesting and outputting. Disabling record counting increases pipeline performance by a small amount.\n\n| item | description |\n| --- | --- |\n| Readers and Writers | Enable record counting for all readers and writers, allowing input and output data rates to be calculated. |\n| Disabled | Disable record counting. This increases performance but doesn't allow input and output data rates to be calculated. |\n\n\n## Persistence\n\n\n### Controller\n\nKubernetes persistence configuration.\n\n| item | description |\n| --- | --- |\n| Disabled | Enabled by default, click to disable. |\n| Storage Class | Kubernetes storage class name; e.g.standard. |\n| Storage Size | Size volume allocated to each controller; defaults to20Gi. |\n| Checkpoint Frequency | Check frequency in milliseconds, defaults to5,000. |\n\nNote\nEnsure\nStorage Class\nis set to a valid class.  This is defined in the cluster and may be different across cloud providers i.e.\nGCP: standard, AWS: gp2, Azure: default\n.  This information can be retrieved by running:\nshell\nCopy\n\n```\nkubectl get storageclass\n```\n\n\n### Worker\n\nKubernetes persistence configuration.\n\n| item | description |\n| --- | --- |\n| Disabled | Enabled by default, click to disable. |\n| Storage Class | Kubernetes storage class name; e.g.standard. |\n| Storage Size | Size volume allocated to each Worker; defaults to20Gi. |\n| Checkpoint Frequency | Check frequency in milliseconds, defaults to5,000. |\n\n\n## Kubernetes\n\n\n| item | description |\n| --- | --- |\n| Label | Add a label and value to created resources. This is a Kubernetes label and can be used to identify resources associated with this pipeline. |\n| Image Pull Secrets | Add Kubernetes secret reference name; for example,kx-access. Secrets can be used for mounting in passwords, TLS certificates or just protected data. |\n\n\n## Environment Variables & Secrets\n\nUse this section to define environment variables and Kubernetes Secrets.\nVariables\nUse environment variables to define key-value pairs that customize pipeline behavior and support environment-specific configuration.\nEnvironment variables have the following attributes:\n\n| item | description |\n| --- | --- |\n| Name | The name of the environment variable. |\n| Value | The variable value. |\n\nSee the\nlist of environment variables\nfor available options.\nSecrets\nReference Kubernetes secrets to securely inject sensitive values, which are resolved at runtime during pipeline execution.\nSecrets have the following attributes:\n\n| item | description |\n| --- | --- |\n| Name | The name of the environment variable that stores the secret. |\n| Secret | The kubernetes secret name. This is used to find the object in the cluster. |\n| Key | The specific values in the secret. For example this could be username or password. See settingKubernetes Secretsfor an example. |\n\n\n## Advanced\n\n\n| Field | description |\n| --- | --- |\n| Replicas | The number of pipeline replicas to deploy. Default value of1means only a single pipeline is deployed with no replicas. |\n| Worker Image | A registry URL that points to a custom Stream Processor worker image. This can be used for extending the core set of functionality of the Stream Processor with custom code or libraries. |\n| Controller Image | A registry URL that points to a custom Stream Processor controller image. |\n| Unsafe Mode | This enables unsafe mode, which disablesdeterminismand may lead to duplicate or lost data in failover scenarios. The options are:Automatic: Inherits the system default (default setting)Disabled: Enables determinism (safe mode)Enabled: Disables determinism (unsafe mode) |\n\n\n## Test timeout\n\nThis setting defines the maximum amount of time a test is allowed to run before it automatically times out. It ensures that tests do not hang indefinitely if expected data does not arrive in time.\nA timeout is especially useful when the timing of incoming data is unpredictable, such as when the pipeline involves multiple readers. Setting a timeout improves the overall testing experience and removes the need to either use a full deployment for testing or stub out data readers with a callback function.\nThis timeout applies when running both\nquick and full tests\n, with an additional 20-second startup buffer for full tests.\n\n| setting | description |\n| --- | --- |\n| Test Timeout | Enter the timeout duration in seconds. The default value is 10 seconds |\n\n\n## Further Reading\n\n- Building pipelines",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1231,
    "metadata": {}
  }
]