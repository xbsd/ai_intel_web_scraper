[
  {
    "id": "kx-github_release-a7311b9a0573",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.4.1-rc.1",
    "title": "Release Release candidate for 1.4.1",
    "text": "# Release Release candidate for 1.4.1 (1.4.1-rc.1)\n\n**Note: the 1.4.1-rc.1 arrowkdb package was built against Apache Arrow version 9.0.0. If you have a different version of the libarrow runtime installed, it may be necessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nArrow only supports a single string array containing up to 2GB of data.  If the kdb string/symbol list contains more than this amount of data then it has to be populated into an Arrow chunked array.  Chunked arrays were already support by `arrowkdb` when writing Arrow IPC files or streams, but not when when writing Parquet files.\r\n\r\nTherefore, in order to support the used of chunked arrays when writing Parquet files, the `ARROW_CHUNK_ROWS` option has been added to:\r\n* pq.writeParquet\r\n* pq.writeParquetFromTable\r\n\r\nNote: This only applies to how kdb lists are chunked internally to the Parquet file writer.  This is different to the row groups configuration (set using `PARQUET_CHUNK_SIZE`) which controls how the Parquet file is structured when written.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 174,
    "metadata": {
      "tag_name": "1.4.1-rc.1",
      "release_name": "Release candidate for 1.4.1",
      "is_prerelease": false,
      "created_at": "2024-05-21T16:53:39Z",
      "published_at": "2024-05-21T17:34:56Z"
    }
  },
  {
    "id": "kx-github_release-479a75cbe6e1",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.4.0-rc.1",
    "title": "Release Release candidate for 1.4.0",
    "text": "# Release Release candidate for 1.4.0 (1.4.0-rc.1)\n\n**Note: the 1.4.0-rc.1 arrowkdb package was built against Apache Arrow version 9.0.0. If you have a different version of the libarrow runtime installed, it may be necessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nEnhancements include:\r\n1. New `COMPRESSION` option to specify the codec to use when writing Parquet files, IPC files or IPC streams.\r\n2. Bug fix for handling float32 and float64 nulls when mapping to/from `0n` and `0nf`.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 91,
    "metadata": {
      "tag_name": "1.4.0-rc.1",
      "release_name": "Release candidate for 1.4.0",
      "is_prerelease": false,
      "created_at": "2023-08-24T18:00:33Z",
      "published_at": "2023-09-12T13:55:46Z"
    }
  },
  {
    "id": "kx-github_release-20d561704cf6",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.3.0-rc.1",
    "title": "Release Release candidate for 1.3.0",
    "text": "# Release Release candidate for 1.3.0 (1.3.0-rc.1)\n\n**Note: the 1.3.0-rc.1 arrowkdb package was built against Apache Arrow version 9.0.0. If you have a different version of the libarrow runtime installed, it may be necessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nEnhancements include:\r\n1. [New APIs](https://github.com/KxSystems/arrowkdb/blob/main/docs/reference.md#apache-orc-files) for reading and writing Apache ORC files (Linux and macOS only).  This includes NULL support via the `NULL_MAPPING` and `WITH_NULL_BITMAP` options.\r\n2. When building from source, arrrowkdb detects your libarrow version and selects c++14 (libarrow < 10.0) or c++17 (libarrow >= 10.0) as appropriate.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 104,
    "metadata": {
      "tag_name": "1.3.0-rc.1",
      "release_name": "Release candidate for 1.3.0",
      "is_prerelease": false,
      "created_at": "2023-03-30T17:01:50Z",
      "published_at": "2023-04-04T09:03:02Z"
    }
  },
  {
    "id": "kx-github_release-3a1790f04d50",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.2.0-rc.1",
    "title": "Release Release candidate for 1.2.0",
    "text": "# Release Release candidate for 1.2.0 (1.2.0-rc.1)\n\n**Note: the 1.2.0-rc.1 arrowkdb package was built against Apache Arrow version 9.0.0. If you have a different version of the libarrow runtime installed, it may be necessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nEnhancements include:\r\n\r\n1. [Support](https://github.com/KxSystems/arrowkdb/blob/main/docs/null-mapping.md) for converting kdbs null to arrow nulls when reading and writing via a new `NULL_MAPPING` option when:\r\n* Reading and writing Parquet files\r\n* Reading and writing Arrow IPC files\r\n* Reading and writing Arrow IPC streams\r\n\r\n2. [Support](https://github.com/KxSystems/arrowkdb/blob/main/docs/null-bitmap.md) for reading the arrow bitmap as a separate structure via a new `WITH_NULL_BITMAP` option when:\r\n* Reading Parquet files\r\n* Reading Arrow IPC files\r\n* Reading Arrow IPC streams\r\n\r\n3. Arrow IPC files and streams can be written with chunking via a new `ARROW_CHUNK_ROWS` option",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 143,
    "metadata": {
      "tag_name": "1.2.0-rc.1",
      "release_name": "Release candidate for 1.2.0",
      "is_prerelease": false,
      "created_at": "2023-03-14T15:10:19Z",
      "published_at": "2023-03-15T08:56:27Z"
    }
  },
  {
    "id": "kx-github_release-c64b8268a58a",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.1.0-rc.1",
    "title": "Release Release candidate for 1.1.0",
    "text": "# Release Release candidate for 1.1.0 (1.1.0-rc.1)\n\n**Note: the 1.1.0-rc.1 arrowkdb package was built against Apache Arrow version 9.0.0. If you have a different version of the libarrow runtime installed, it may be necessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nEnhancements include:\r\n* Support multithreaded use of arrowkdb with peach\r\n* Add support for reading Parquet files with row groups (chunking)\r\n* Upgrade build to use libarrow and libparquet 9.0.0\r\n* Support latest v2 Parquet file formats\r\n\r\nNew functions:\r\n* pq.readParquetNumRowGroups\r\n* pq.readParquetRowGroups\r\n* pq.readParquetRowGroupsToTable\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 100,
    "metadata": {
      "tag_name": "1.1.0-rc.1",
      "release_name": "Release candidate for 1.1.0",
      "is_prerelease": false,
      "created_at": "2022-11-01T14:52:59Z",
      "published_at": "2022-11-01T15:04:25Z"
    }
  },
  {
    "id": "kx-github_release-2fbccdee1dcb",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.0.0-rc.1",
    "title": "Release Release candidiate for 1.0.0",
    "text": "# Release Release candidiate for 1.0.0 (1.0.0-rc.1)\n\n**Note: the 1.0.0-rc.1 arrowkdb package was built against Apache Arrow version 5.0.0.  If you have a different version of the libarrow runtime installed, it may be nessary to build arrowkdb from source in order to support that version (instructions to build arrowkdb from source are in the README.md).**\r\n\r\nArrowkdb enhancements:\r\n\r\n1. Make the API more future proof and extensible by adding an options parameter to the read and write functions where it was not already present:\r\n- pq.readParquetColumn\r\n- ipc.writeArrow\r\n- ipc.writeArrowFromTable\r\n- ipc.serializeArrow\r\n- ipc.serializeArrowFromTable\r\n- ipc.parseArrowData\r\n- ipc.parseArrowToTable\r\n- ar.prettyPrintArray\r\n- ar.prettyPrintArrayFromList\r\n- tb.prettyPrintTable\r\n- tb.prettyPrintTableFromTable\r\n\r\n2. Support mapping the Arrow decimal128 datatype to and from a kdb+ 9h list via the new option DECIMAL128_AS_DOUBLE.\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 124,
    "metadata": {
      "tag_name": "1.0.0-rc.1",
      "release_name": "Release candidiate for 1.0.0",
      "is_prerelease": false,
      "created_at": "2021-07-29T15:42:26Z",
      "published_at": "2021-07-29T16:03:54Z"
    }
  },
  {
    "id": "kx-github_release-734bd0dee91d",
    "origin": "kx",
    "source_type": "github_release",
    "url": "https://github.com/KxSystems/arrowkdb/releases/tag/1.0.0-alpha",
    "title": "Release Initial alpha release for version 1.0.0",
    "text": "# Release Initial alpha release for version 1.0.0 (1.0.0-alpha)\n\nApache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in memory. This data format has a rich datatype system (included nested data types) designed to support the needs of analytic database systems, dataframe libraries, and more.\r\n\r\nThe `arrowkdb` integration enables kdb+ users to read and write Arrow tables created from kdb+ data using:\r\n* Parquet file format\r\n* Arrow IPC record batch file format\r\n* Arrow IPC record batch stream format\r\n\r\nCurrently Arrow supports over 35 datatypes including concrete, parameterized and nested datatypes. Each Arrow datatype is mapped to a kdb+ type and `arrowkdb` can seamlessly convert between both representations.\r\n\r\nSeparate APIs are provided where the Arrow table is either created from a kdb+ table using an inferred schema or from an Arrow schema and the tableâ€™s list of array data.\r\n* Inferred schemas.  If you are less familiar with Arrow or do not wish to use the more complex or nested Arrow datatypes, `arrowkdb` can infer the schema from a kdb+ table where each column in the table is mapped to a field in the schema.\r\n* Constructed schemas.  Although inferred schemas are easy to use, they support only a subset of the Arrow datatypes and are considerably less flexible.  Where more complex schemas are required then these should be manually constructed using the datatype/field/schema constructor functions which `arrowkdb` exposes, similar to the C++ Arrow library and PyArrow.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 245,
    "metadata": {
      "tag_name": "1.0.0-alpha",
      "release_name": "Initial alpha release for version 1.0.0",
      "is_prerelease": true,
      "created_at": "2021-02-25T13:14:09Z",
      "published_at": "2021-02-25T13:49:36Z"
    }
  }
]