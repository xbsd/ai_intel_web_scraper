[
  {
    "id": "kx-github_issue-69113ad70ae9",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/31",
    "title": "how can we get symbols to be written as dictionary encoded strings",
    "text": "# how can we get symbols to be written as dictionary encoded strings\n\nit seems the mechanism for doing this is in the library:\r\nfor instance given a symbol vector:\r\n\r\n```q\r\nsym:`a`b`c`a`a`c\r\ndvalues:distinct sym\r\nindices:dvalues?sym\r\n/ideally we would use the smallest type that can support the number of distinct symbols:\r\nmt:(.arrowkdb.dt[`int8`int16`int32`int64])!im:floor 2 xexp 0 7 15 31\r\nmkt:4 5 6 7h!im\r\nindextype:mt bin c:count dvalues\r\nindexktype:mkt bin c\r\ndatatype_symbol:.arrowkdb.dt.dictionary[.arrowkdb.dt.utf8[];indextype[]]\r\n/we can even pretty print the type we want:\r\n.arrowkdb.ar.prettyPrintArray[datatype_symbol;(dvalues;indexktype$indices);::]\r\n```\r\n\r\nbut what's not clear is how to enhance the current inferSchema to do this calculation, this means that currently tables that have symbols are not the same after the round trip and all the symbols are cast to type string\r\n\n\n## Top Comments\n\n**nmcdonnell-kx**: Yeah, that's a tricky one\r\nOne simple solution would be to have an option to decode utf8 as symbols but it would apply to all columns in the table which I suspect is not what you want.\r\nAnother possibility would be to have a utf8_as_symbol option which only applies to dictionary keys.  I think that's possible but again would apply to all dictionary columns in the table.\n\n---\n\n**pporter-jumptrading**: I suspect, that in many use cases treating dictionary encoded strings as symbols always, is exactly the behavior you want. If you have other dictionary encoded types, (ie if you have Uint64 but only use a few of them so they are dictionary encoded that would probably end up turning into the Uint64 but that seems fine.\n\n---\n\n**nmcdonnell-kx**: I agree that dictionary utf8 values as symbol is a reasonable use case.  However, I'm not sure it could be the default - generally users don't like decoding to symbols as the default because of symbol bloat.  But I think a DICTIONARY_UTF8_VALUES_AS_SYMBOL option could work for you.\n\n---\n\n**nugend**: A fun extra you can add to your dict encoding  is to exclude the null from the dictionary and use a null bit mask to override the find result. These will convert out on the other side to a dictionary with a nullity mask for the index. So you can have separate nullity checks for symbols and character vecs that are isomorphic to the KDB types and your types will all round trip as you expect.\r\n\r\nThis is also nice because it means the plain utf8 type can skip a nullity vector.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 393,
    "metadata": {
      "issue_number": 31,
      "state": "open",
      "labels": [],
      "comments_count": 4,
      "created_at": "2023-08-03T20:28:53Z",
      "updated_at": "2023-08-16T19:21:01Z",
      "closed_at": null,
      "author": "pporter-jumptrading",
      "top_comments": [
        "**nmcdonnell-kx**: Yeah, that's a tricky one\r\nOne simple solution would be to have an option to decode utf8 as symbols but it would apply to all columns in the table which I suspect is not what you want.\r\nAnother possibility would be to have a utf8_as_symbol option which only applies to dictionary keys.  I think that's possible but again would apply to all dictionary columns in the table.",
        "**pporter-jumptrading**: I suspect, that in many use cases treating dictionary encoded strings as symbols always, is exactly the behavior you want. If you have other dictionary encoded types, (ie if you have Uint64 but only use a few of them so they are dictionary encoded that would probably end up turning into the Uint64 but that seems fine.",
        "**nmcdonnell-kx**: I agree that dictionary utf8 values as symbol is a reasonable use case.  However, I'm not sure it could be the default - generally users don't like decoding to symbols as the default because of symbol bloat.  But I think a DICTIONARY_UTF8_VALUES_AS_SYMBOL option could work for you.",
        "**nugend**: A fun extra you can add to your dict encoding  is to exclude the null from the dictionary and use a null bit mask to override the find result. These will convert out on the other side to a dictionary with a nullity mask for the index. So you can have separate nullity checks for symbols and character vecs that are isomorphic to the KDB types and your types will all round trip as you expect.\r\n\r\nThis is also nice because it means the plain utf8 type can skip a nullity vector."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-5c26fd93bbde",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/29",
    "title": "Separate nulls for nested char arrays and symbols.",
    "text": "# Separate nulls for nested char arrays and symbols.\n\nDon’t believe it’s currently possible. Poked in at the current mappings and it seems tractable given the separation of the writing code paths between symbols and nested char arrays. \r\n\r\nWould be nice!\n\n## Top Comments\n\n**nmcdonnell-kx**: I think it should be possible to have separate null mappings when writing utf8+kdb symbol vs utf8+kdb string.\r\nHowever, when reading back, utf8 are always decoded as strings so would follow the utf8+kdb string null mapping.\r\nAlthough another issue https://github.com/KxSystems/arrowkdb/issues/31 is asking about a special case (dictionary utf8 keys) for decoding utf8s as symbols so may be some overlap.\n\n---\n\n**nugend**: I just meant writing, really. Reading *probably* isn't going to be a problem.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 119,
    "metadata": {
      "issue_number": 29,
      "state": "open",
      "labels": [],
      "comments_count": 2,
      "created_at": "2023-07-20T15:02:16Z",
      "updated_at": "2023-08-10T04:53:44Z",
      "closed_at": null,
      "author": "nugend",
      "top_comments": [
        "**nmcdonnell-kx**: I think it should be possible to have separate null mappings when writing utf8+kdb symbol vs utf8+kdb string.\r\nHowever, when reading back, utf8 are always decoded as strings so would follow the utf8+kdb string null mapping.\r\nAlthough another issue https://github.com/KxSystems/arrowkdb/issues/31 is asking about a special case (dictionary utf8 keys) for decoding utf8s as symbols so may be some overlap.",
        "**nugend**: I just meant writing, really. Reading *probably* isn't going to be a problem."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-95d739d19a13",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/17",
    "title": "Docs should be included with release",
    "text": "# Docs should be included with release\n\nNew docs folder wont currently be added to a release build - should be included in the future while docs are there.\r\n.travis.yml example area\r\ne.g.\r\nelif [[ $TRAVIS_OS_NAME == \"windows\" ]]; then\r\n7z a -tzip $FILE_NAME README.md install.bat LICENSE q examples;\r\nelif [[ $TRAVIS_OS_NAME == \"linux\" || $TRAVIS_OS_NAME == \"osx\" ]]; then\r\ntar -zcvf $FILE_NAME README.md install.sh LICENSE q examples;\n\n## Top Comments\n\n**vgrechin-kx**: Hi Simon - I'm including your Travis CI patch into the next docs update as a separate commit\n\n---\n\n**vgrechin-kx**: Closing the ticket because the patch is merged.\r\nSimon, thanks for pointing out.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 105,
    "metadata": {
      "issue_number": 17,
      "state": "closed",
      "labels": [],
      "comments_count": 2,
      "created_at": "2023-03-03T09:08:59Z",
      "updated_at": "2023-03-09T11:03:18Z",
      "closed_at": "2023-03-09T11:03:17Z",
      "author": "sshanks-kx",
      "top_comments": [
        "**vgrechin-kx**: Hi Simon - I'm including your Travis CI patch into the next docs update as a separate commit",
        "**vgrechin-kx**: Closing the ticket because the patch is merged.\r\nSimon, thanks for pointing out."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-4577f852e0ef",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/2",
    "title": "Handle nulls when reading (didn't test writing)",
    "text": "# Handle nulls when reading (didn't test writing)\n\nThe library is not respecting the null bitmaps. These could probably be handled using the Compute API to fill in the null KDB values.\n\n## Top Comments\n\n**nugend**: Before Arrow 6.0, this can be done with the fill_null compute function, 6.0 and greater, it needs to be done with the coalesce compute function.\n\n---\n\n**vgrechin-kx**: Delivered in Release 1.2.0",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 67,
    "metadata": {
      "issue_number": 2,
      "state": "closed",
      "labels": [],
      "comments_count": 2,
      "created_at": "2022-01-21T01:53:24Z",
      "updated_at": "2023-03-16T12:02:10Z",
      "closed_at": "2023-03-16T12:02:10Z",
      "author": "nugend",
      "top_comments": [
        "**nugend**: Before Arrow 6.0, this can be done with the fill_null compute function, 6.0 and greater, it needs to be done with the coalesce compute function.",
        "**vgrechin-kx**: Delivered in Release 1.2.0"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-19b4de40ab60",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/40",
    "title": "Support null mapping for DECIMAL128 types",
    "text": "# Support null mapping for DECIMAL128 types\n\nThe Parquet file described below (exported from Snowflake) stores floating point numbers as `FIXED_LEN_BYTE_ARRAY` / `DECIMAL128`. Unfortunately it does not look like I can upload a sample here; I'll also raise a support ticket and attach there.\r\n\r\nSchema:\r\n```\r\n>>> from pyarrow.parquet import ParquetFile\r\n>>> ParquetFile(\"sample.parquet\").schema\r\n<pyarrow._parquet.ParquetSchema object at 0x71ae68081bc0>\r\nrequired group field_id=-1 schema {\r\n  optional binary field_id=-1 ID (String);\r\n  optional int64 field_id=-1 DATETIME (Timestamp(isAdjustedToUTC=true, timeUnit=milliseconds, is_from_converted_type=true, force_set_converted_type=false));\r\n  optional fixed_len_byte_array(16) field_id=-1 BIDPRICE (Decimal(precision=38, scale=10));\r\n  optional fixed_len_byte_array(16) field_id=-1 BIDSIZE (Decimal(precision=38, scale=0));\r\n  optional fixed_len_byte_array(16) field_id=-1 ASKPRICE (Decimal(precision=38, scale=10));\r\n  optional fixed_len_byte_array(16) field_id=-1 ASKSIZE (Decimal(precision=38, scale=0));\r\n}\r\n\r\n>>> import pyarrow.parquet as pq\r\n>>> pq.read_schema('sample.parquet')\r\nID: string\r\nDATETIME: timestamp[ms, tz=UTC]\r\nBIDPRICE: decimal128(38, 10)\r\nBIDSIZE: decimal128(38, 0)\r\nASKPRICE: decimal128(38, 10)\r\nASKSIZE: decimal128(38, 0)\r\n```\r\nReading this with Python, it correctly parses the null values on the second row:\r\n```\r\n>>> import pandas as pd\r\n>>> pd.read_parquet('sample.parquet')\r\n    ID                         DATETIME       BIDPRICE BIDSIZE       ASKPRICE ASKSIZE\r\n0  ABC 2022-02-14 22:00:00.591000+00:00  77.5000000000      20  78.6100000000       3\r\n1  ABC 2022-02-14 22:00:01.695000+00:00  77.5000000000      20           None    None\r\n```\r\n\r\nReading in with KDB, it instead returns zeros:\r\n\r\n```\r\n.pq.nullTypes: `bool`uint8`int8`uint16`int16`uint32`int32`uint64`int64`float16`float32`float64`date32`date64`month_interval`day_time_interval`timestamp`time32`time64`duration`utf8`large_utf8`binary`large_binary`fixed_size_binary!(0b;0x00;0x00;0Nh;0Nh;0Ni;0Ni;0N;0N;0Nh;0Ne;0Nf;0Nd;0Np;0Nm;0Nn;0Np;0Nt;0Nn;0Nn;\"\";\"\";`byte$\"\";`byte$\"\";`byte$\"\")\r\n.pq.PARQUET_READ_OPTNS_CONVERT_NULLS: (``PARQUET_MULTITHREADED_READ`DECIMAL128_AS_DOUBLE`NULL_MAPPING)!(::; 1; 1; .pq.nullTypes);\r\ntbl: .arrowkdb.pq.readParquetToTable[\"sample.parquet\"; .pq.PARQUET_READ_OPTNS];\r\ntbl\r\nID    DATETIME                      BIDPRICE BIDSIZE ASKPRICE ASKSIZE\r\n---------------------------------------------------------------------\r\n\"ABC\" 2022.02.14D22:00:00.591000000 77.5     20      78.61    3      \r\n\"ABC\" 2022.02.14D22:00:01.695000000 77.5     20      0        0     \r\n```\r\n\r\nI believe this is due to the lack of support for DECIMAL128 here: https://github.com/KxSystems/arrowkdb/blob/main/src/KdbOptions.cpp#L57, after which it could be added to the `NULL_MAPPING` parameter.\r\n\r\nCould this be supported?\r\n\r\nPerhaps this is also related to the `DECIMAL128_AS_DOUBLE` flag, albeit without this the data is returned as hex.\r\n\r\nMany thanks\n\n## Top Comments\n\n**erichards97**: It turned out we just needed to add `decimal : 0Nf` to the `NULL_MAPPING`.\r\n\r\nIf there is a reason this wasn't included in the sample list [here](https://github.com/KxSystems/arrowkdb/blob/main/docs/null-mapping.md#implementation) please let me know. Otherwise closing the issue.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 297,
    "metadata": {
      "issue_number": 40,
      "state": "closed",
      "labels": [],
      "comments_count": 1,
      "created_at": "2024-06-14T15:58:14Z",
      "updated_at": "2024-06-16T11:45:00Z",
      "closed_at": "2024-06-16T11:44:59Z",
      "author": "erichards97",
      "top_comments": [
        "**erichards97**: It turned out we just needed to add `decimal : 0Nf` to the `NULL_MAPPING`.\r\n\r\nIf there is a reason this wasn't included in the sample list [here](https://github.com/KxSystems/arrowkdb/blob/main/docs/null-mapping.md#implementation) please let me know. Otherwise closing the issue."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-46480080c932",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/37",
    "title": "Any way to support dictionary encoded columns with ARROW_CHUNK_ROWS?",
    "text": "# Any way to support dictionary encoded columns with ARROW_CHUNK_ROWS?\n\nAt the moment this throws an unequal length arrays error when attempting to pass the dictionary and indices in for a column represented that way.\r\n\r\nThe functionality is definitely supported by the AtrowStream format. It seems like the issue is that the MakeDictionary function and the MakeChunkedArray function don’t play nicely together. I’m not sure what the preferred solution is. I’m happy to handle preparing the value array manually and passing the indices in with an explicit reference if that’s what’s needed.\r\n\r\nIf you want to handle it in the library, my guess is you could handle the values and indices in separate passes? ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 114,
    "metadata": {
      "issue_number": 37,
      "state": "open",
      "labels": [],
      "comments_count": 0,
      "created_at": "2023-11-14T17:28:29Z",
      "updated_at": "2023-11-14T17:28:29Z",
      "closed_at": null,
      "author": "nugend",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-75dcd53db481",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/arrowkdb/issues/32",
    "title": "Infinite handling",
    "text": "# Infinite handling\n\nHi, \r\n\r\nI see the last release of the arrowkdb package contained new feature that supports null handling. Is there any plans to introduce a similar feature for infinities? Ideally, we would like any of the infinity values in our table to be set to nulls when writing to parquet files using .arrowkdb.pq.writeParquetFromTable.\r\n\r\nThanks,",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 56,
    "metadata": {
      "issue_number": 32,
      "state": "open",
      "labels": [],
      "comments_count": 0,
      "created_at": "2023-08-08T21:31:17Z",
      "updated_at": "2023-08-08T21:31:17Z",
      "closed_at": null,
      "author": "kmeaney-ccl",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  }
]