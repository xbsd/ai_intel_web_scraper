[
  {
    "id": "kx-github_issue-679337fd2fb9",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/111",
    "title": "One message lag",
    "text": "# One message lag\n\nGood morning \r\nI am facing a strange issue \r\nOur consumer program consumes all the messages and leaves out the last message for all the partitions present in the Kafka broker\r\n\r\nSay We have 0, 1 and 2 partitions , for all of them. I can see 1 lag. At the end of consuming messages the Current offset value is always one number less than log-end-offset\r\n\r\nI am using kfx.assign to subscribe and kfx.consumecb function to receive the messages \r\n\r\nPls let me know how to proceed \n\n## Top Comments\n\n**chunaiarun**: I am using .kfx.CommitOffsets function to commit offset\r\n\r\nIf I try to print .kfx.CommittedOffsets it shows the offset of the last but one offset\n\n---\n\n**chunaiarun**: Config parameters\r\nmetadata.broker.list\r\ngroup.id\r\nfetch.wait.max.ms 10\r\nstatistics.interval.ms 10000\r\nenable.auto.commit false \r\nenable.auto.offset.store false\r\nmessage.max.bytes 10000000\n\n---\n\n**chunaiarun**: ![Uploading IMG_1027.jpeg…]()\r\n\n\n---\n\n**sshanks-kx**: Will look to recreate & report back when done so. Thanks.\n\n---\n\n**chunaiarun**: Thank you for your response\r\n\r\nOn Tue, 16 Apr 2024 at 11:51 PM, Simon Shanks ***@***.***>\r\nwrote:\r\n\r\n> Will look to recreate & report back when done so. Thanks.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/KxSystems/kafka/issues/111#issuecomment-2059686600>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BFO7CEWAZJ3WNTWNZS2RONDY5VT2HAVCNFSM6AAAAABGIIJJBGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANJZGY4DMNRQGA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n\n\n---\n\n**sshanks-kx**: PositionOffsets uses rd_kafka_position (https://github.com/KxSystems/kafka/blob/c6de3f88803072d5ed778d8c39699b9a4752e325/kfk.c#L664C22-L664C39)\r\n\r\nThe documentation on librdkafka for this says \"The offset field of each requested partition will be set to the offset of the last consumed message + 1\"\r\nhttps://docs.confluent.io/platform/7.5/clients/librdkafka/html/rdkafka_8h.html#a6e9e36bd9e6bf84a9f3092fcbfa3a9ac\r\n\r\nUsing our examples...\r\n\r\nrunning test_producer.q to send msgs to topic1 ...\r\n```\r\n\"Sending 2024.04.17D15:41:09.772013000\"\r\n\"Sending 2024.04.17D15:41:10.772013000\"\r\n\"Sending 2024.04.17D15:41:11.772013000\"\r\n```\r\n\r\nrunning test_offsetc.q to sub, record msgs to `data` table & print offsets/commits....\r\n\r\nlooking at `data` table:\r\n```\r\nmtype topic client partition offset msgtime                       data                            key      headers                 rcvtime                      \r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n      test1 0      0         28     2024.04.17D15:41:09.776000000 \"2024.04.17D15:41:09.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:09.781294000\r\n      test1 0      0         29     2024.04.17D15:41:10.774000000 \"2024.04.17D15:41:10.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:10.780654000\r\n      test1 0      0         30     2024.04.17D15:41:11.776000000 \"2024.04.17D15:41:11.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:11.780235000\r\n```\r\nlooking at details printed after everything received/committed:\r\n```\r\n\"Position:\"\r\ntopic partition offset metadata\r\n-------------------------------\r\ntest1 0         31     \"\"      \r\n\"Before commited:\"\r\ntopic partition offset metadata\r\n-------------------------------\r\ntest1 0         30     \"\"      \r\n\"After commited:\"\r\ntopic partition offset metadata\r\n-------------------------------\r\ntest1 0         30     \"\" \r\n```\r\n\r\nfrom the above, I can see it consumed the last message (recorded/commited at position 30) and the PositionOffsets reports offset 31. Based on the librdkafka doc, this would appear to be correct.\r\n\r\nI can change our docs to reflect that PositionOffsets provides \"last consumed message + 1\" as per librdkafkas rd_kafka_position documentation.\r\n\r\nDoes that explain what you are seeing or do you believe that your missing a message or some other issue?\r\n\r\nThanks\n\n---\n\n**chunaiarun**: Thank you for getting back on this so quickly\r\nYes I have similar scenario, but when i restart the consumer the last\r\ncommitted message is getting read again\r\n\r\nAlso when I try with manual commit option the lag is 0\r\n\r\n\r\n\r\nOn Wed, 17 Apr 2024 at 9:23 PM, Simon Shanks ***@***.***>\r\nwrote:\r\n\r\n> PositionOffsets uses rd_kafka_position (\r\n> https://github.com/KxSystems/kafka/blob/c6de3f88803072d5ed778d8c39699b9a4752e325/kfk.c#L664C22-L664C39\r\n> )\r\n>\r\n> The documentation on librdkafka for this says \"The offset field of each\r\n> requested partition will be set to the offset of the last consumed message\r\n> + 1\"\r\n>\r\n> https://docs.confluent.io/platform/7.5/clients/librdkafka/html/rdkafka_8h.html#a6e9e36bd9e6bf84a9f3092fcbfa3a9ac\r\n>\r\n> Using our examples...\r\n>\r\n> running test_producer.q to send msgs to topic1 ...\r\n>\r\n> \"Sending 2024.04.17D15:41:09.772013000\"\r\n> \"Sending 2024.04.17D15:41:10.772013000\"\r\n> \"Sending 2024.04.17D15:41:11.772013000\"\r\n>\r\n> running test_offsetc.q to sub, record msgs to data table & print\r\n> offsets/commits....\r\n>\r\n> looking at data table:\r\n>\r\n> mtype topic client partition offset msgtime                       data                            key      headers                 rcvtime\r\n> ----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n>       test1 0      0         28     2024.04.17D15:41:09.776000000 \"2024.04.17D15:41:09.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:09.781294000\r\n>       test1 0      0         29     2024.04.17D15:41:10.774000000 \"2024.04.17D15:41:10.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:10.780654000\r\n>       test1 0      0         30     2024.04.17D15:41:11.776000000 \"2024.04.17D15:41:11.772013000\" `byte$() (`symbol$())!`symbol$() 2024.04.17D15:41:11.780235000\r\n>\r\n> looking at details printed after everything received/committed:\r\n>\r\n> \"Position:\"\r\n> topic partition offset metadata\r\n> -------------------------------\r\n> test1 0         31     \"\"\r\n> \"Before commited:\"\r\n> topic partition offset metadata\r\n> -------------------------------\r\n> test1 0         30     \"\"\r\n> \"After commited:\"\r\n> topic partition offset metadata\r\n> -------------------------------\r\n> test1 0         30     \"\"\r\n>\r\n> from the above, I can see it consumed the last message (recorded/commited\r\n> at position 30) and the PositionOffsets reports offset 31. Based on the\r\n> librdkafka doc, this would appear to be correct.\r\n>\r\n> I can change our docs to reflect that PositionOffsets provides \"last\r\n> consumed message + 1\" as per librdkafkas rd_kafka_position documentation.\r\n>\r\n> Does that explain what you are seeing or do you believe that your missing\r\n> a message or some other issue?\r\n>\r\n> Thanks\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/KxSystems/kafka/issues/111#issuecomment-2061626390>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BFO7CEUEW7CQE7F5ZOX5NPLY52LGDAVCNFSM6AAAAABGIIJJBGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANRRGYZDMMZZGA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n\n\n---\n\n**sshanks-kx**: Thanks for the details. Will investigate & report back.\n\n---\n\n**sshanks-kx**: If you haven't already, the issue might be that you need to add 1 received offset when using CommitOffsets ?\r\nI will update the docs & add to example program.\r\nref: https://docs.confluent.io/platform/7.5/clients/librdkafka/html/rdkafka_8h.html#ab96539928328f14c3c9177ea0c896c87 \"offset should be the offset where consumption will resume, i.e., the last processed offset + 1\"\r\n\r\n### Recreation\r\n\r\n#### Using automatic commits:\r\n\r\n3 messages published, received with offsets **182,183,184**. \r\n\r\nThe subscriber with enable.auto.commit set to true, I eventually see offsetcb firing, reporting offset **185**:\r\n\r\n```q\r\n0i\r\n\"Success\"\r\n,`topic`partition`offset`metadata!(`test1;0i;185;\"\")\r\n```\r\n\r\nInspecting with kafka-consumer-groups program\r\n```bash\r\nkafka-consumer-groups --bootstrap-server localhost:9092 --describe --group 0\r\nGROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\r\n0               test1           0          185             185             0               -               -               -\r\n```\r\n\r\n#### Using manual commits:\r\n\r\n3 messages published, received with offsets **185,186,187**\r\n\r\nThe subscriber with enable.auto.commit set to false, with .kfk.CommitOffsets called with current offset+1, I eventually see offsetcb firing, reporting offset **188**:\r\n\r\n```q\r\n0i\r\n\"Success\"\r\n,`topic`partition`offset`metadata!(`test1;0i;188;\"\")\r\n```\r\n\r\nInspecting with kafka-consumer-groups program\r\n```bash\r\nGROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\r\n0               test1           0          188             188             0               -               -               -\r\n```\r\n\r\n#### Example Client\r\n\r\nUsing this basic subscriber to show issue \r\n```q\r\n\\l ../kfk.q\r\n\r\nkfk_cfg:(!) . flip(\r\n    (`metadata.broker.list;`localhost:9092);\r\n    (`group.id;`0);\r\n    (`fetch.wait.max.ms;`10);\r\n    (`statistics.interval.ms;`10000);\r\n    (`enable.auto.commit;`false);\r\n    (`enable.auto.offset.store;`true)\r\n    );\r\nclient:.kfk.Consumer[kfk_cfg];\r\n\r\n// Topics to be published to\r\ntopic1:`test1\r\ndata:();\r\n\r\n// Default callback function overwritten for managing of consumption from all topics\r\n.kfk.consumetopic[`]:{[msg]\r\n    0N!string msg`offset;\r\n    0N!string msg`data;\r\n    msg[`data]:\"c\"$msg[`data];\r\n    msg[`rcvtime]:.z.p;\r\n    data,::enlist msg;\r\n    .kfk.CommitOffsets[client;msg`topic;(enlist msg`partition)!(enlist 1+msg`offset);0b];\r\n    0N!\"kfk.consumetopic done\";}\r\n\r\n// Define Offset callback functionality\r\n.kfk.offsetcb:{[cid;err;offsets]0N!\"offsetcb\";show (cid;err;offsets);}\r\n\r\n// Subscribe to relevant topics from a defined client\r\n.kfk.Assign[client;(enlist `test1)!enlist 0]\r\n```\r\n\r\n(I comment out **.kfk.CommitOffsets** line & change **enable.auto.commit** to **true** to compare with automatic commits)\n\n---\n\n**chunaiarun**: Thank you again for the detailed explanation\r\nWill try this out\r\n\r\nOn Mon, 22 Apr 2024 at 10:38 PM, Simon Shanks ***@***.***>\r\nwrote:\r\n\r\n> If you haven't already, the issue might be that you need to add 1 received\r\n> offset when using CommitOffsets ?\r\n> I will update the docs & add to example program.\r\n> ref:\r\n> https://docs.confluent.io/platform/7.5/clients/librdkafka/html/rdkafka_8h.html#ab96539928328f14c3c9177ea0c896c87\r\n> \"offset should be the offset where consumption will resume, i.e., the last\r\n> processed offset + 1\"\r\n> Recreation Using automatic commits:\r\n>\r\n> 3 messages published, received with offsets *182,183,184*.\r\n>\r\n> The subscriber with enable.auto.commit set to true, I eventually see\r\n> offsetcb firing, reporting offset *185*:\r\n>\r\n> 0i\"Success\",`topic`partition`offset`metadata!(`test1;0i;185;\"\")\r\n>\r\n> Inspecting with kafka-consumer-groups program\r\n>\r\n> kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group 0\r\n> GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\r\n> 0               test1           0          185             185             0               -               -               -\r\n>\r\n> Using manual commits:\r\n>\r\n> 3 messages published, received with offsets *185,186,187*\r\n>\r\n> The subscriber with enable.auto.commit set to false, with\r\n> .kfk.CommitOffsets called with current offset+1, I eventually see offsetcb\r\n> firing, reporting offset *188*:\r\n>\r\n> 0i\"Success\",`topic`partition`offset`metadata!(`test1;0i;188;\"\")\r\n>\r\n> Inspecting with kafka-consumer-groups program\r\n>\r\n> GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\r\n> 0               test1           0          188             188             0               -               -               -\r\n>\r\n> Example Client\r\n>\r\n> Using this basic subscriber to show issue\r\n>\r\n> \\l ../kfk.q\r\n> kfk_cfg:(!) . flip(\r\n>     (`metadata.broker.list;`localhost:9092);\r\n>     (`group.id;`0);\r\n>     (`fetch.wait.max.ms;`10);\r\n>     (`statistics.interval.ms;`10000);\r\n>     (`enable.auto.commit;`false);\r\n>     (`enable.auto.offset.store;`true)\r\n>     );client:.kfk.Consumer[kfk_cfg];\r\n> // Topics to be published totopic1:`test1data:();\r\n> // Default callback function overwritten for managing of consumption from all topics.kfk.consumetopic[`]:{[msg]\r\n>     0N!string msg`offset;\r\n>     0N!string msg`data;\r\n>     msg[`data]:\"c\"$msg[`data];\r\n>     msg[`rcvtime]:.z.p;\r\n>     data,::enlist msg;\r\n>     .kfk.CommitOffsets[client;msg`topic;(enlist msg`partition)!(enlist 1+msg`offset);0b];\r\n>     0N!\"kfk.consumetopic done\";}\r\n> // Define Offset callback functionality.kfk.offsetcb:{[cid;err;offsets]0N!\"offsetcb\";show (cid;err;offsets);}\r\n> // Subscribe to relevant topics from a defined client.kfk.Assign[client;(enlist `test1)!enlist 0]\r\n>\r\n> (I comment out *.kfk.CommitOffsets* line & change *enable.auto.commit* to\r\n> *true* to compare with automatic commits)\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/KxSystems/kafka/issues/111#issuecomment-2070273053>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BFO7CETJVFMCZ75NW6T7EO3Y6U7ZXAVCNFSM6AAAAABGIIJJBGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANZQGI3TGMBVGM>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 1446,
    "metadata": {
      "issue_number": 111,
      "state": "closed",
      "labels": [],
      "comments_count": 17,
      "created_at": "2024-04-15T23:48:56Z",
      "updated_at": "2024-05-16T10:38:29Z",
      "closed_at": "2024-05-15T10:35:30Z",
      "author": "chunaiarun",
      "top_comments": [
        "**chunaiarun**: I am using .kfx.CommitOffsets function to commit offset\r\n\r\nIf I try to print .kfx.CommittedOffsets it shows the offset of the last but one offset",
        "**chunaiarun**: Config parameters\r\nmetadata.broker.list\r\ngroup.id\r\nfetch.wait.max.ms 10\r\nstatistics.interval.ms 10000\r\nenable.auto.commit false \r\nenable.auto.offset.store false\r\nmessage.max.bytes 10000000",
        "**chunaiarun**: ![Uploading IMG_1027.jpeg…]()\r\n",
        "**sshanks-kx**: Will look to recreate & report back when done so. Thanks.",
        "**chunaiarun**: Thank you for your response\r\n\r\nOn Tue, 16 Apr 2024 at 11:51 PM, Simon Shanks ***@***.***>\r\nwrote:\r\n\r\n> Will look to recreate & report back when done so. Thanks.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/KxSystems/kafka/issues/111#issuecomment-2059686600>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BFO7CEWAZJ3WNTWNZS2RONDY5VT2HAVCNFSM6AAAAABGIIJJBGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANJZGY4DMNRQGA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-620e808015bc",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/48",
    "title": "message header and message type",
    "text": "# message header and message type\n\n\r\n1. as a producer, is it possible to add message type and message header in the message if we are publishing multiple message types in the same topic\r\n\r\n2. as a consumer, is it possible to include the message header and message type in the message so we can distinguish the message type/header information separately without going into the data payload. Currently, the consumer only sees certain information on the call back function.\r\n\r\n\n\n## Top Comments\n\n**sshanks-kx**: Thanks for creating an issue.\r\nWe will review possible changes & report back in due course. \r\nThanks\n\n---\n\n**hungchinchang**: Just some background to the issue:\r\n\r\nIf we want to send different message types through the same topic, it seems the library doesn't allow us to specify the message type/header in the producer publish function.\r\n\r\nIn addition, when other processes publish message type/header, it is not shown in our consumer call back function (e.g. mtype is always empty)\r\n\r\nI assume the way to get around this is to have the message type embedded in the data payload (JSON format) if we want to publish to the same topic - partition.\r\n\r\nAnother method I can think of is to publish/consume different message events within the same topic but on different partitions. \r\n\r\nI am not sure how other fellow kdb+ uses this library or i am missing something. I like to know what is the best practice and whether message type and headers can be included.\r\n\r\nThanks,\r\n\r\nNIon\n\n---\n\n**sshanks-kx**: Mtype is the internal librdkafka message representation (e.g. end of partition msg) , rather than a user defined type that is sent/received (i.e. kafka can internally send message objects with the error code set which are not messages sent by a client). So you may see the same type for all client messages (normal action) and end-of-partition type when all current messages consumed. \r\nIts actually coming from the kafka libs message object error code rather than a message type - its prob not well named - but error codes might make a user thing something bad has happened when it can be normal (e.g. end-of-partition err msg)\r\n\r\nKafka internal header has the key (for partitioning/etc) and general internal kafka processing (message length/etc), rather than user defined types or business applications (Ref: https://docs.confluent.io/2.0.0/clients/librdkafka/classRdKafka_1_1Message.html https://kafka.apache.org/documentation/#record ). \r\nUser content is placed in payload (e.g. a user could define their own seperated header/content within the payload section). So an architect of a given system may come up with the best solution for their use-case (topic organistion,payload style,etc) but that may be beyond the facilities of the general interface to get data to/from Kafka itself.\r\n\r\nthanks\n\n---\n\n**hungchinchang**: Thanks,\r\n\r\nI understand what mtype is now. Our company has implemented kafka 0.11 version and it seems to have headers in the message sent.\r\n\r\n#define RD_KAFKA_V_HEADERS(HDRS)                                        \\        _LRK_TYPECHECK(RD_KAFKA_VTYPE_HEADERS, rd_kafka_headers_t *, HDRS), \\                (rd_kafka_headers_t *)HDRS\r\n12:30\r\nhttps://github.com/edenhill/librdkafka/blob/master/src/rdkafka.h\r\n\r\nline 1161 and line 1176.\r\n\r\nThey are going to put information in the message so it has header information. Unfortunately, i don't think the current kdb kafka implementation has this? Hence in the message received/published, we can't specify it?\r\n\r\nIs this a functionality that can be added into the general interface?\r\n\r\nThanks,\n\n---\n\n**sshanks-kx**: Thanks, see it all now.\r\nLooks it requires checks for at least librdkafka >0.11.4 (luckily redhat7/etc default package is 0.11.4) and broker support (will have to add checks for that also in potential builds/calls when users might have older installs).\r\n\r\nFurther reference material:\r\nhttps://github.com/edenhill/librdkafka/releases/tag/v0.11.4\r\nhttps://docs.confluent.io/5.0.0/clients/librdkafka/rdkafka_8h.html\r\n\r\nWill have to run some tests/etc & update issue when done of what might be possible, and potential changes that might have to occur.\r\n\r\nThanks\n\n---\n\n**sshanks-kx**: Added some basic code to publish with header names&values. With added code can see subscriber getting the names&values.\r\nTesting further. To create appropriate function/types & version checks, before creating pull request to add code.\n\n---\n\n**hungchinchang**: looks good! looking forward to use it once its ready - will you also update the code.kx.com site and give some examples either there or here so i can do some testing myself too.\n\n---\n\n**hungchinchang**: Just looking at the pull requests\r\n\r\nthe consumer function accepts a headers field where key is sym and value is bytes /char arrray\r\n\r\nthe publisher function can publish header with sym!bytes (why not char array as well?)\r\n\r\nIs the kafka c library not support types i assume?\n\n---\n\n**sshanks-kx**: Publish should be be able to use byte array or char array for header values on publish. It'll convert either to bytes on the send\r\nThe underlying API sends/gets data as bytes.\r\nDid you see something that showed a problem & it couldnt? Thanks\n\n---\n\n**hungchinchang**: currently you have two PR, is it possible to get them into one so i can potentially make it and test it myself if the review is going to take a long time? ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 805,
    "metadata": {
      "issue_number": 48,
      "state": "closed",
      "labels": [],
      "comments_count": 14,
      "created_at": "2020-06-18T06:18:39Z",
      "updated_at": "2021-03-18T10:10:45Z",
      "closed_at": "2021-03-18T10:10:45Z",
      "author": "hungchinchang",
      "top_comments": [
        "**sshanks-kx**: Thanks for creating an issue.\r\nWe will review possible changes & report back in due course. \r\nThanks",
        "**hungchinchang**: Just some background to the issue:\r\n\r\nIf we want to send different message types through the same topic, it seems the library doesn't allow us to specify the message type/header in the producer publish function.\r\n\r\nIn addition, when other processes publish message type/header, it is not shown in our consumer call back function (e.g. mtype is always empty)\r\n\r\nI assume the way to get around this is to have the message type embedded in the data payload (JSON format) if we want to publish to the same topic - partition.\r\n\r\nAnother method I can think of is to publish/consume different message events within the same topic but on different partitions. \r\n\r\nI am not sure how other fellow kdb+ uses this library or i am missing something. I like to know what is the best practice and whether message type and headers can be included.\r\n\r\nThanks,\r\n\r\nNIon",
        "**sshanks-kx**: Mtype is the internal librdkafka message representation (e.g. end of partition msg) , rather than a user defined type that is sent/received (i.e. kafka can internally send message objects with the error code set which are not messages sent by a client). So you may see the same type for all client messages (normal action) and end-of-partition type when all current messages consumed. \r\nIts actually coming from the kafka libs message object error code rather than a message type - its prob not well named - but error codes might make a user thing something bad has happened when it can be normal (e.g. end-of-partition err msg)\r\n\r\nKafka internal header has the key (for partitioning/etc) and general internal kafka processing (message length/etc), rather than user defined types or business applications (Ref: https://docs.confluent.io/2.0.0/clients/librdkafka/classRdKafka_1_1Message.html https://kafka.apache.org/documentation/#record ). \r\nUser content is placed in payload (e.g. a user could define their own seperated header/content within the payload section). So an architect of a given system may come up with the best solution for their use-case (topic organistion,payload style,etc) but that may be beyond the facilities of the general interface to get data to/from Kafka itself.\r\n\r\nthanks",
        "**hungchinchang**: Thanks,\r\n\r\nI understand what mtype is now. Our company has implemented kafka 0.11 version and it seems to have headers in the message sent.\r\n\r\n#define RD_KAFKA_V_HEADERS(HDRS)                                        \\        _LRK_TYPECHECK(RD_KAFKA_VTYPE_HEADERS, rd_kafka_headers_t *, HDRS), \\                (rd_kafka_headers_t *)HDRS\r\n12:30\r\nhttps://github.com/edenhill/librdkafka/blob/master/src/rdkafka.h\r\n\r\nline 1161 and line 1176.\r\n\r\nThey are going to put information in the message so it has header information. Unfortunately, i don't think the current kdb kafka implementation has this? Hence in the message received/published, we can't specify it?\r\n\r\nIs this a functionality that can be added into the general interface?\r\n\r\nThanks,",
        "**sshanks-kx**: Thanks, see it all now.\r\nLooks it requires checks for at least librdkafka >0.11.4 (luckily redhat7/etc default package is 0.11.4) and broker support (will have to add checks for that also in potential builds/calls when users might have older installs).\r\n\r\nFurther reference material:\r\nhttps://github.com/edenhill/librdkafka/releases/tag/v0.11.4\r\nhttps://docs.confluent.io/5.0.0/clients/librdkafka/rdkafka_8h.html\r\n\r\nWill have to run some tests/etc & update issue when done of what might be possible, and potential changes that might have to occur.\r\n\r\nThanks"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-2d68a25b652e",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/30",
    "title": "Bottleneck in publishing on a tight while loop",
    "text": "# Bottleneck in publishing on a tight while loop\n\n**Internally raised issue**\r\n\r\n**Describe the bug**\r\n* Publishing data on a tight while loop without polling for delivery report functions as expected (hitting queue full at the 100k messages sent)\r\n* Polling after every publish however blocks on C function call (rd_kafka_poll) within 3k messages. Increasing system buffer size does not appear to change how quickly this behaviour arises.\r\n\r\n**To Reproduce**\r\n\r\n`consumer script`\r\n\r\n```\r\n\\l kfk.q\r\n\r\nkfk_cfg:(!) . flip(\r\n  (`metadata.broker.list;`localhost:9011);\r\n  (`group.id;`0);\r\n  (`queue.buffering.max.ms;`2);\r\n  (`enable.partition.eof;`0)\r\n  );\r\nclient:.kfk.Consumer[kfk_cfg];\r\n\r\ndata:();\r\nkfk.consumecb:{[msg]\r\n  msg[`data]:\"c\"$msg[`data];\r\n  msg[`rcvtime]:.z.p;\r\n  data,::enlist msg;}\r\n\r\n.kfk.Sub[client;`random;enlist .kfk.PARTITION_UA];\r\n```\r\n\r\n`producer script`\r\n\r\n```\r\n\\l kfk.q\r\nkfk_cfg:(!) . flip(\r\n  (`metadata.broker.list; `localhost:9011);\r\n  (`queue.buffering.max.ms;`10)\r\n  );\r\nproducer:.kfk.Producer[kfk_cfg];\r\n\r\nrandom:.kfk.Topic[producer;`random;()!()];\r\n\r\nn:0;\r\nrun:{\r\n  .kfk.Pub[random;.kfk.PARTITION_UA; raze string -8!n+:1;\"\"];\r\n  //.kfk.Poll[producer; 1; 100];\r\n  show .kfk.OutQLen producer;\r\n  };\r\n\r\nshow \"Publishing...\";\r\nwhile[1b; run[]];\r\n```\r\n\r\n**Expected behavior**\r\nProducer should not block in this scenario.\r\n\r\n**Screenshots**\r\nNo applicable screenshots to explain this scenario further\r\n\r\n**Desktop (please complete the following information):**\r\nBehaviour has been seen in a variety of Linux environments and on MacOS so should be reproducible across multiple environments\r\n\r\n**Additional context**\r\nNo applicable additional context\n\n## Top Comments\n\n**sshanks-kx**: Re-created. Remember to change scripts broker connection details on both scripts.\r\nUncommenting the poll line shows issue.\n\n---\n\n**sshanks-kx**: Thread with issue\r\n```\r\n#0  0x00007f8c9d42154d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n#1  0x00007f8c9d423d3c in _L_cond_lock_847 () from /lib64/libpthread.so.0\r\n#2  0x00007f8c9d423bd1 in __pthread_mutex_cond_lock () from /lib64/libpthread.so.0\r\n#3  0x00007f8c9d41eeb4 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#4  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96e2052e in pollClient (rk=0x17ee7a0, timeout=1, maxcnt=<optimized out>) at kfk.c:531\r\n```\r\n\r\nAll threads\r\n```\r\n(gdb) thread apply all bt\r\n\r\nThread 4 (Thread 0x7f8c95b11700 (LWP 82)):\r\n#0  0x00007f8c9d41ede2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b5d424 in rd_kafka_thread_main () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#6  0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 3 (Thread 0x7f8c95310700 (LWP 83)):\r\n#0  0x00007f8c9d41ede2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b8c9d0 in rd_kafka_q_pop_serve () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b7475f in rd_kafka_broker_ops_serve () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96b74821 in rd_kafka_broker_serve () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b74cdf in rd_kafka_broker_ua_idle () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96b7637b in rd_kafka_broker_thread_main () from /lib64/librdkafka.so.1\r\n#7  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#8  0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 2 (Thread 0x7f8c94b0f700 (LWP 84)):\r\n#0  0x00007f8c9d4216fd in write () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96b6644b in rd_kafka_q_io_event () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b6f8c9 in rd_kafka_dr_msgq () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b959a4 in rd_kafka_handle_Produce () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96b88af6 in rd_kafka_buf_callback () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b6ca0d in rd_kafka_recv () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96b85f28 in rd_kafka_transport_io_event () from /lib64/librdkafka.so.1\r\n#7  0x00007f8c96b748b8 in rd_kafka_broker_serve () from /lib64/librdkafka.so.1\r\n#8  0x00007f8c96b761f1 in rd_kafka_broker_thread_main () from /lib64/librdkafka.so.1\r\n#9  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#10 0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#11 0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 1 (Thread 0x7f8c9df56000 (LWP 81)):\r\n#0  0x00007f8c9d42154d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n#1  0x00007f8c9d423d3c in _L_cond_lock_847 () from /lib64/libpthread.so.0\r\n#2  0x00007f8c9d423bd1 in __pthread_mutex_cond_lock () from /lib64/libpthread.so.0\r\n#3  0x00007f8c9d41eeb4 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#4  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96e2052e in pollClient (rk=0x17ee7a0, timeout=1, maxcnt=<optimized out>) at kfk.c:531\r\n#7  0x00007f8c96e20656 in kfkPoll (x=0x7f8c99002a30, y=0x7f8c99000bf0, z=0x7f8c99001500) at kfk.c:564\r\n```\n\n---\n\n**sshanks-kx**: Retrying with latest librdkafka...\n\n---\n\n**sshanks-kx**: Tried with latest Kafka release - no difference\r\n\r\nRef: current redhat 7 version \r\nyum install librdkafka-devel\r\n.kfk.Version\r\n722175i\r\n.kfk.VersionSym[]\r\n`0.11.4\r\n\r\nNewest release\r\ncd /source/kafka\r\nwget https://github.com/edenhill/librdkafka/archive/v1.4.2.tar.gz\r\ntar xvf v1.4.2.tar.gz --strip-components=1\r\n./configure\r\nmake \r\nmake install\r\n\r\n.kfk.Version\r\n17040127i\r\n.kfk.VersionSym[]\r\n`1.4.2\r\n\n\n---\n\n**sshanks-kx**: Ref: https://github.com/edenhill/librdkafka/blob/master/examples/producer.c\r\nInstalled as part of a build. Change code to pub on infinite loop - Runs ok. Run example\r\n./producer broker:29092 random\n\n---\n\n**sshanks-kx**: Temp removing \r\nrd_kafka_queue_io_event_enable(rd_kafka_queue_get_main(rk),spair[1],\"X\",1);\r\nstop the hang\r\n\n\n---\n\n**sshanks-kx**: Think I can see the potential root of the problem now.\r\nIts like the opposite of #37.\r\n\r\nIn this occasion, thread is consuming all of the thread operations (main kdb thread). The poll is letting Kafka events happen. When Kafka events happen, they try to inform kdb to do stuff via a file descriptor (this happens every time the queue gets emptied). Queue is not able to be consumed by KDB for housekeeping & react to data to be read cos its spending 100% of its time publishing.\r\n\r\nI expect if the above example was changed to do a certain amount on a timer/etc each time it'd cause it not to hang (though this isn't the desired action the user wants to do).\r\n\r\nWill work on a change.\n\n---\n\n**sshanks-kx**: To check whether return code error from pub is appropriate. Code can present queue full/etc were user could take action.\r\nref: \r\nhttps://librdkafka.dpldocs.info/deimos.rdkafka.rd_kafka_produce.html\r\nhttps://github.com/edenhill/librdkafka/blob/master/examples/producer.c\n\n---\n\n**sshanks-kx**: linked to https://github.com/edenhill/librdkafka/issues/2932",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 791,
    "metadata": {
      "issue_number": 30,
      "state": "closed",
      "labels": [],
      "comments_count": 9,
      "created_at": "2020-06-05T14:27:47Z",
      "updated_at": "2020-06-15T18:30:54Z",
      "closed_at": "2020-06-15T18:30:54Z",
      "author": "cmccarthy1",
      "top_comments": [
        "**sshanks-kx**: Re-created. Remember to change scripts broker connection details on both scripts.\r\nUncommenting the poll line shows issue.",
        "**sshanks-kx**: Thread with issue\r\n```\r\n#0  0x00007f8c9d42154d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n#1  0x00007f8c9d423d3c in _L_cond_lock_847 () from /lib64/libpthread.so.0\r\n#2  0x00007f8c9d423bd1 in __pthread_mutex_cond_lock () from /lib64/libpthread.so.0\r\n#3  0x00007f8c9d41eeb4 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#4  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96e2052e in pollClient (rk=0x17ee7a0, timeout=1, maxcnt=<optimized out>) at kfk.c:531\r\n```\r\n\r\nAll threads\r\n```\r\n(gdb) thread apply all bt\r\n\r\nThread 4 (Thread 0x7f8c95b11700 (LWP 82)):\r\n#0  0x00007f8c9d41ede2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b5d424 in rd_kafka_thread_main () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#6  0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 3 (Thread 0x7f8c95310700 (LWP 83)):\r\n#0  0x00007f8c9d41ede2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b8c9d0 in rd_kafka_q_pop_serve () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b7475f in rd_kafka_broker_ops_serve () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96b74821 in rd_kafka_broker_serve () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b74cdf in rd_kafka_broker_ua_idle () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96b7637b in rd_kafka_broker_thread_main () from /lib64/librdkafka.so.1\r\n#7  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#8  0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 2 (Thread 0x7f8c94b0f700 (LWP 84)):\r\n#0  0x00007f8c9d4216fd in write () from /lib64/libpthread.so.0\r\n#1  0x00007f8c96b6644b in rd_kafka_q_io_event () from /lib64/librdkafka.so.1\r\n#2  0x00007f8c96b6f8c9 in rd_kafka_dr_msgq () from /lib64/librdkafka.so.1\r\n#3  0x00007f8c96b959a4 in rd_kafka_handle_Produce () from /lib64/librdkafka.so.1\r\n#4  0x00007f8c96b88af6 in rd_kafka_buf_callback () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b6ca0d in rd_kafka_recv () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96b85f28 in rd_kafka_transport_io_event () from /lib64/librdkafka.so.1\r\n#7  0x00007f8c96b748b8 in rd_kafka_broker_serve () from /lib64/librdkafka.so.1\r\n#8  0x00007f8c96b761f1 in rd_kafka_broker_thread_main () from /lib64/librdkafka.so.1\r\n#9  0x00007f8c96bc1b47 in _thrd_wrapper_function () from /lib64/librdkafka.so.1\r\n#10 0x00007f8c9d41aea5 in start_thread () from /lib64/libpthread.so.0\r\n#11 0x00007f8c9d1438dd in clone () from /lib64/libc.so.6\r\n\r\nThread 1 (Thread 0x7f8c9df56000 (LWP 81)):\r\n#0  0x00007f8c9d42154d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n#1  0x00007f8c9d423d3c in _L_cond_lock_847 () from /lib64/libpthread.so.0\r\n#2  0x00007f8c9d423bd1 in __pthread_mutex_cond_lock () from /lib64/libpthread.so.0\r\n#3  0x00007f8c9d41eeb4 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#4  0x00007f8c96bc1dd5 in cnd_timedwait_ms () from /lib64/librdkafka.so.1\r\n#5  0x00007f8c96b8cbf2 in rd_kafka_q_serve () from /lib64/librdkafka.so.1\r\n#6  0x00007f8c96e2052e in pollClient (rk=0x17ee7a0, timeout=1, maxcnt=<optimized out>) at kfk.c:531\r\n#7  0x00007f8c96e20656 in kfkPoll (x=0x7f8c99002a30, y=0x7f8c99000bf0, z=0x7f8c99001500) at kfk.c:564\r\n```",
        "**sshanks-kx**: Retrying with latest librdkafka...",
        "**sshanks-kx**: Tried with latest Kafka release - no difference\r\n\r\nRef: current redhat 7 version \r\nyum install librdkafka-devel\r\n.kfk.Version\r\n722175i\r\n.kfk.VersionSym[]\r\n`0.11.4\r\n\r\nNewest release\r\ncd /source/kafka\r\nwget https://github.com/edenhill/librdkafka/archive/v1.4.2.tar.gz\r\ntar xvf v1.4.2.tar.gz --strip-components=1\r\n./configure\r\nmake \r\nmake install\r\n\r\n.kfk.Version\r\n17040127i\r\n.kfk.VersionSym[]\r\n`1.4.2\r\n",
        "**sshanks-kx**: Ref: https://github.com/edenhill/librdkafka/blob/master/examples/producer.c\r\nInstalled as part of a build. Change code to pub on infinite loop - Runs ok. Run example\r\n./producer broker:29092 random"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-cd85df4bb8a8",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/119",
    "title": "libkfk fails to connect back automatically to broker on azure cloud after certain inactivity time. ",
    "text": "# libkfk fails to connect back automatically to broker on azure cloud after certain inactivity time. \n\n**Describe the bug**\r\nI see that we are making kafka connection to brokers on azure cloud . It looks like after a while of no activity the connection goes stale and it disconnects from that broker connection. It seems it does retry to connect back but fails giving error that all brokers are down. \r\n\r\n**To Reproduce**\r\nTrying connecting to a broker on cloud, do not do activity on the topic on which you connect. The connection would go stale or break after some time. \r\n\r\n**Expected behavior**\r\nIt should have been able to reconnect back if it goes down. But upon retrial it seems to fail. \r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Redhat 7 \r\n - KDB+ banner information [e.g. KDB+ 3.5 2017.09.06 Copyright (C) 1993-2017 Kx Systems l64/ 8()core 64303MB]\r\n - Repository version [e.g. 0.1.0]\r\n\r\n**Additional context**\r\nWhat are the parameters to be used which can help in keeping connection enable without timeout ( keep connection alive ) \r\n\n\n## Top Comments\n\n**sshanks-kx**: Is it using a consumer or producer client type? Thanks\n\n---\n\n**sarritesh**: @sshanks-kx It is a Producer, I also tried parameters **socket.keepalive.enable**  as true and **conections.max.ideal.ms** as 0 but even after this did not work. Wondering if there could be firewall etc. that can be issue  ? \n\n---\n\n**sshanks-kx**: @sarritesh \r\n[.kfk.Poll](https://github.com/KxSystems/kafka/blob/master/docs/reference.md#poll)  calls the underlying kafka lib [rd_kafka_poll](https://docs.confluent.io/platform/current/clients/librdkafka/html/rdkafka_8h.html#ad50c431e3a29d14da534db49bd0682a4 ) which has a comment\r\n\r\n> \"The timeout_ms argument specifies the maximum amount of time (in milliseconds) that the call will block waiting for events. For non-blocking calls, provide 0 as timeout_ms. To wait indefinitely for an event, provide -1.....An application should make sure to call poll() at regular intervals to serve any queued callbacks waiting to be called.\"\r\n\r\nSlightly confusingly, the consumer (non-producer) version [rd_kafka_consumer_poll](https://docs.confluent.io/platform/current/clients/librdkafka/html/rdkafka_8h.html#a65cc6cb9bd72c4084f074af0361ceddf) has \r\n\r\n> \"...must call poll at least every max.poll.interval.ms to remain a member of the consumer group...\"\r\n\r\nwhere max.poll.interval.ms is for consumers only.\r\n\r\nIf you can experiment, can you call [.kfk.Poll ](https://github.com/KxSystems/kafka/blob/master/docs/reference.md#poll)(passing 0 as the timeout value), using the kdb+ timer ( [.z.ts](https://code.kx.com/q/ref/dotz/#zts-timer) )? If you have any success/failure with that, can you let me know. _NOTE: be careful that your not already using .z.ts for something else, which may effect that logic if you change the timer frequency._\r\n\r\nAlso, do you have statistics.interval.ms set in your producer config? \r\n\r\nI'll have to get a moment to replicate here.\r\n \r\n\r\n\n\n---\n\n**sshanks-kx**: There is a connections.max.idle.ms that can be set on both brokers and clients.\r\nIn the kafka lib (librdkafka) connections.max.idle.ms ( https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md ) has\r\n\r\n> Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info). \r\n\r\nLinks to [issue#3109](https://github.com/confluentinc/librdkafka/issues/3109)\r\n\r\nOther ref https://stackoverflow.com/questions/78033060/keep-kafka-connection-alive-even-if-it-is-idle-for-long-time\n\n---\n\n**sarritesh**: Thanks @sshanks-kx  . I had tried with socket.keepalive.enable as true and connections.max.idle.ms as 0 on the producer side, but that did not work. What I will do is also ask the relevant broker side team to set connections.max.idle.ms as 0 on their side and then come back and update to you if this works. Thanks for coming back on this.\n\n---\n\n**sarritesh**: By the way one more question @sshanks-kx , while this would help in keeping connection alive, should not this be able to reconnect once disconnected. Or this is because of connection going stale ? I do see in logs same error multiple time about not able to connect to broker which does means that it tries to but is enable to reconnect automatically.  \n\n---\n\n**sshanks-kx**: tried docker-compose instance to test a disconnect\r\n```\r\n---\r\nversion: '3'\r\nservices:\r\n  zookeeper:\r\n    image: confluentinc/cp-zookeeper:7.0.1\r\n    container_name: zookeeper\r\n    ports:\r\n      - \"2181:2181\"\r\n    environment:\r\n      ZOOKEEPER_CLIENT_PORT: 2181\r\n      ZOOKEEPER_TICK_TIME: 2000\r\n\r\n  broker:\r\n    image: confluentinc/cp-kafka:7.0.1\r\n    container_name: broker\r\n    ports:\r\n    # To learn about configuring Kafka for access across networks see\r\n    # https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/\r\n      - \"9092:9092\"\r\n    depends_on:\r\n      - zookeeper\r\n    environment:\r\n      KAFKA_BROKER_ID: 1\r\n      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\r\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT\r\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092\r\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\r\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\r\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\r\n      KAFKA_TRANSACTION_STATE_LOG_RETENTION_MINUTES: 1\r\n```\r\nrunning \r\n`docker compose up`\r\nthen\r\n`q test_producer.q`\r\nfrom our examples after env started & connected.\r\nI ctrl-c'd the docker env, and ran `docker compose stop`. I could see the producer disconnected.\r\nI restarted the env using `docker compose up` & once env started I could see producer connected again. I could run the test_consumer & start publishing, with consumer receiving.\r\n\r\nWould need more info on your connection/logs/etc to know more.\n\n---\n\n**sshanks-kx**: Please reopen if can recreate locally, still an issue, etc. Thanks",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 758,
    "metadata": {
      "issue_number": 119,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2024-09-11T11:46:47Z",
      "updated_at": "2024-10-03T16:41:48Z",
      "closed_at": "2024-10-03T16:41:47Z",
      "author": "sarritesh",
      "top_comments": [
        "**sshanks-kx**: Is it using a consumer or producer client type? Thanks",
        "**sarritesh**: @sshanks-kx It is a Producer, I also tried parameters **socket.keepalive.enable**  as true and **conections.max.ideal.ms** as 0 but even after this did not work. Wondering if there could be firewall etc. that can be issue  ? ",
        "**sshanks-kx**: @sarritesh \r\n[.kfk.Poll](https://github.com/KxSystems/kafka/blob/master/docs/reference.md#poll)  calls the underlying kafka lib [rd_kafka_poll](https://docs.confluent.io/platform/current/clients/librdkafka/html/rdkafka_8h.html#ad50c431e3a29d14da534db49bd0682a4 ) which has a comment\r\n\r\n> \"The timeout_ms argument specifies the maximum amount of time (in milliseconds) that the call will block waiting for events. For non-blocking calls, provide 0 as timeout_ms. To wait indefinitely for an event, provide -1.....An application should make sure to call poll() at regular intervals to serve any queued callbacks waiting to be called.\"\r\n\r\nSlightly confusingly, the consumer (non-producer) version [rd_kafka_consumer_poll](https://docs.confluent.io/platform/current/clients/librdkafka/html/rdkafka_8h.html#a65cc6cb9bd72c4084f074af0361ceddf) has \r\n\r\n> \"...must call poll at least every max.poll.interval.ms to remain a member of the consumer group...\"\r\n\r\nwhere max.poll.interval.ms is for consumers only.\r\n\r\nIf you can experiment, can you call [.kfk.Poll ](https://github.com/KxSystems/kafka/blob/master/docs/reference.md#poll)(passing 0 as the timeout value), using the kdb+ timer ( [.z.ts](https://code.kx.com/q/ref/dotz/#zts-timer) )? If you have any success/failure with that, can you let me know. _NOTE: be careful that your not already using .z.ts for something else, which may effect that logic if you change the timer frequency._\r\n\r\nAlso, do you have statistics.interval.ms set in your producer config? \r\n\r\nI'll have to get a moment to replicate here.\r\n \r\n\r\n",
        "**sshanks-kx**: There is a connections.max.idle.ms that can be set on both brokers and clients.\r\nIn the kafka lib (librdkafka) connections.max.idle.ms ( https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md ) has\r\n\r\n> Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info). \r\n\r\nLinks to [issue#3109](https://github.com/confluentinc/librdkafka/issues/3109)\r\n\r\nOther ref https://stackoverflow.com/questions/78033060/keep-kafka-connection-alive-even-if-it-is-idle-for-long-time",
        "**sarritesh**: Thanks @sshanks-kx  . I had tried with socket.keepalive.enable as true and connections.max.idle.ms as 0 on the producer side, but that did not work. What I will do is also ask the relevant broker side team to set connections.max.idle.ms as 0 on their side and then come back and update to you if this works. Thanks for coming back on this."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-9c42bc4d682a",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/105",
    "title": "Metadata on consumer can present UNKNOWN_TOPIC_OR_PART error",
    "text": "# Metadata on consumer can present UNKNOWN_TOPIC_OR_PART error\n\nRunning `.kfk.Metadata[client];` from consumer can present callback msg as follows:\r\n```\r\nmtype                 topic client partition offset msgtime data                                                                        key      headers                 rcvtime                      \r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nUNKNOWN_TOPIC_OR_PART test1 0      0         -1001          \"Subscribed topic not available: test1: Broker: Unknown topic or partition\" `byte$() (`symbol$())!`symbol$() 2024.01.22D15:46:39.829536000\r\n```\r\nwhen doing this after doing a subscribe, it can make it look like the subscription had failed (when it hasn't).\r\n\r\nTemp removing `.kfk.Metadata[client];` after any subscription, the callback no longer fires with a UNKNOWN_TOPIC_OR_PART. It doesn't appear to stop the subscription from occurring, whether its there or not.\r\n\r\nWill find out why & change this effect\n\n## Top Comments\n\n**sshanks-kx**: Occurs every time .kfk.Metadata[client] called after subscription is made (e.g. call twice, get 2 UNKNOWN_TOPIC_OR_PART msgs). \r\nDoesn't occur when called prior to any subscription.\r\n\n\n---\n\n**sshanks-kx**: Created librdkafka issue https://github.com/confluentinc/librdkafka/issues/4589 (can see same occurrence without using kdb+). \r\nCurrent workaround is that if you need to use `.kfk.Metadata` to see topics/etc, call it prior to subscription.\n\n---\n\n**sshanks-kx**: Update now on linked issue. May be issue with librdkafka. To try possible fix.\n\n---\n\n**sshanks-kx**: Tested with the pending librdkafka pull request - bug still appears to be there. Informed librdkafka dev. Current workaround is to do any metadata request prior to subscription.\n\n---\n\n**sshanks-kx**: bug doesnt occur with v2.0.2 of librdkafka. will await response from librdkafka devs.\n\n---\n\n**sshanks-kx**: looks like its working again on latest pr change to librdkafka (issue linked above). tested with kdb+. will await future librdkafka release to see if it makes it in.\n\n---\n\n**sshanks-kx**: Fixed in underlying librdkafka [2.4.0](https://github.com/confluentinc/librdkafka/releases/tag/v2.4.0) ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 263,
    "metadata": {
      "issue_number": 105,
      "state": "open",
      "labels": [],
      "comments_count": 7,
      "created_at": "2024-01-22T15:52:19Z",
      "updated_at": "2024-09-13T15:17:51Z",
      "closed_at": null,
      "author": "sshanks-kx",
      "top_comments": [
        "**sshanks-kx**: Occurs every time .kfk.Metadata[client] called after subscription is made (e.g. call twice, get 2 UNKNOWN_TOPIC_OR_PART msgs). \r\nDoesn't occur when called prior to any subscription.\r\n",
        "**sshanks-kx**: Created librdkafka issue https://github.com/confluentinc/librdkafka/issues/4589 (can see same occurrence without using kdb+). \r\nCurrent workaround is that if you need to use `.kfk.Metadata` to see topics/etc, call it prior to subscription.",
        "**sshanks-kx**: Update now on linked issue. May be issue with librdkafka. To try possible fix.",
        "**sshanks-kx**: Tested with the pending librdkafka pull request - bug still appears to be there. Informed librdkafka dev. Current workaround is to do any metadata request prior to subscription.",
        "**sshanks-kx**: bug doesnt occur with v2.0.2 of librdkafka. will await response from librdkafka devs."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-2c4250c38c26",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/118",
    "title": "Timed out OffsetCommitRequest in flight (after 60628ms, timeout #0)\\\")\"",
    "text": "# Timed out OffsetCommitRequest in flight (after 60628ms, timeout #0)\\\")\"\n\n**Describe the bug**\r\n32 consumers started to process 7 million messages from 32 partitions ( from the kafka queue)\r\nWe are doing manual async commit\r\n\r\n**To Reproduce**\r\n\r\n- Have 7 million messages produced and segregated equally into 32 partitions \r\n-  Configure consumer code to do \"manual async commit\"\r\n- then start 32 consumers at the same time\r\n- we saw below timeout error in almost all consumer logs.\r\n\r\n**Expected behavior**\r\n\"(5i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out OffsetCommitRequest in flight (after 60628ms, timeout #0)\\\")\"\r\n\"(5i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out OffsetCommitRequest in flight (after 60628ms, timeout #1)\\\")\"\r\n\"(5i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out OffsetCommitRequest in flight (after 60628ms, timeout #2)\\\")\"\r\n\"(5i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out OffsetCommitRequest in flight (after 60628ms, timeout #3)\\\")\"\r\n\"(5i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out OffsetCommitRequest in flight (after 60628ms, timeout #4)\\\")\"\r\n\"(4i;\\\"REQTMOUT\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator/2: Timed out 66792 in-flight, 0 retry-queued, 95763 out-queue, 0 partially-sent requests\\\")\"\r\n\"(3i;\\\"FAIL\\\";\\\"[thrd:GroupCoordinator]: GroupCoordinator: <broker host>:443: 162555 request(s) timed out\r\n \r\nLooks like internally the library retries to commit and does it successfully, and I see the LAG goes down fine, '**but how to avoid this...** What is the max retry ms it does ? what is the retry related librdkafka configuration (https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md) \r\n **to be on the safer side should I increase/adjust any configuration** \r\n\r\nFYI\r\n    (`fetch.wait.max.ms;`10);\r\n    (`statistics.interval.ms;`10000);\r\n    (`enable.auto.commit;`false);\r\n    (`enable.auto.offset.store;`false);\r\n    (`message.max.bytes;`1000000000)\r\n    );\r\n\r\n.kfk.CommitOffsets[x[`client];x[`topic];((enlist x[`partition])!(enlist 1+x[`offset]));**1b**];] We are using async commit here\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux \r\n - 12 CPUs\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 237,
    "metadata": {
      "issue_number": 118,
      "state": "closed",
      "labels": [],
      "comments_count": 6,
      "created_at": "2024-08-13T12:19:29Z",
      "updated_at": "2024-09-13T15:09:27Z",
      "closed_at": "2024-09-13T15:09:26Z",
      "author": "chunaiarun",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-564b8f789e37",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/103",
    "title": "Kfk.sub",
    "text": "# Kfk.sub\n\nWhether consumer can subscribe at topic level without specifying the partition? If so can you provide me an example\r\n\r\nif partition should be always specified always , how should I specify \r\n\r\nI was trying to specific subscriptions with callback function\r\n.kfk.Subcribe[client;`topic1;`id`err`leader`replicas`isrs!(0i;`Success;0i;0 1 2i;2 1 0i);callbck] here partition is dictionary. It is not working. Whether as my python code is working at topic level.\r\n\r\nhelp needed\r\n\r\n\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 67,
    "metadata": {
      "issue_number": 103,
      "state": "closed",
      "labels": [],
      "comments_count": 6,
      "created_at": "2024-01-19T18:16:39Z",
      "updated_at": "2024-03-07T10:27:34Z",
      "closed_at": "2024-03-07T10:27:22Z",
      "author": "chunaiarun",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-39f9f8e04b7d",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/94",
    "title": "Failed to get current position offset",
    "text": "# Failed to get current position offset\n\n**Describe the bug**\r\nWhen using the function kfk.PositionOffsets, it is not returning the current offset on the topic from kafka until kdb have processed the message.\r\n\r\n**To Reproduce**\r\n```\r\n/- note we do not commit offset\r\n/- check the current position offset on the topic - which should be 1 because there is 1 message.\r\n.kfk.PositionOffsets[kfk_consumer;`queue;0i]\r\ntopic partition offset metadata\r\n-------------------------------\r\nqueue 0         -1001  \"\"      \r\n/- subscribe to the topic and process the message\r\n{[x] .kfk.Subscribe[kfk_consumer;x;enlist .kfk.PARTITION_UA;upd x]} each kfk_consumer_topics;\r\n/- check the current position offset on the topic\r\n.kfk.PositionOffsets[kfk_consumer;`queue;enlist 0i]\r\ntopic partition offset metadata\r\n-------------------------------\r\nqueue 0         1      \"\"      \r\n\r\n```\r\n\r\n**Expected behavior**\r\nThe function should be able to provide the current position offset on the topic without having to subscribe/process the message so we can determine where the current offset is on the topic and able to be in replay mode until we catch up to the current message.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS:  \r\nNo LSB modules are available.\r\nDistributor ID:\tDebian\r\nDescription:\tDebian GNU/Linux 11 (bullseye)\r\nRelease:\t11\r\nCodename:\tbullseye\r\n - KDB+ banner information \r\n KDB+ 4.1t 2021.11.04 Copyright (C) 1993-2021 Kx Systems\r\nl64/ 8()core 31578MB nion debian 127.0.1.1 EXPIRE 2023.04.16 carta.com DEV TMP #76484\r\n - Repository version [e.g. 0.1.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 231,
    "metadata": {
      "issue_number": 94,
      "state": "closed",
      "labels": [
        "enhancement"
      ],
      "comments_count": 6,
      "created_at": "2022-12-19T02:02:13Z",
      "updated_at": "2022-12-20T23:45:52Z",
      "closed_at": "2022-12-20T23:45:51Z",
      "author": "hungchinchang",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-65efa6402195",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/84",
    "title": "Getting error when trying to load kfk.q",
    "text": "# Getting error when trying to load kfk.q\n\n**Describe the bug**\r\nInstalled kdb kafka on WIndows using WSL and Ubuntu - everything looked ok.  When running q and using the load command\r\n\\l kfk.q\r\n\r\nI get the following error\r\nKDB+ 3.6 2019.04.02 Copyright (C) 1993-2019 Kx Systems\r\nw32/ 8()core 4095MB wlee use7410wlee2 192.168.1.196 NONEXPIRE\r\n\r\nWelcome to kdb+ 32bit edition\r\nFor support please see http://groups.google.com/d/forum/personal-kdbplus\r\nTutorials can be found at http://code.kx.com\r\nTo exit, type \\\\\r\nTo remove this startup msg, edit q.q\r\nq)\\l kfk.q\r\n'The specified module could not be found.\r\n  [2]  \\\\wsl$\\Ubuntu-20.04\\home\\wlee\\kdb_kafka\\kfk.q:74: .kfk,:(`$3_'string funcs[;0])!LIBPATH@/:funcs\r\n                                                                                               ^\r\n  [0]  (<load>)\r\n\r\n**Expected behavior**\r\ndon't expect error\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04.2 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n\r\nAPologies, new to KDB and Kafka - but I would have expected this to work out of the box.  Any help would be appreciated.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 153,
    "metadata": {
      "issue_number": 84,
      "state": "closed",
      "labels": [],
      "comments_count": 6,
      "created_at": "2021-06-04T18:55:05Z",
      "updated_at": "2021-06-21T20:18:51Z",
      "closed_at": "2021-06-21T08:12:57Z",
      "author": "wlee-git-sys",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-7af5186df50e",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/56",
    "title": "Deleting one of the many producers in a process results in segfault",
    "text": "# Deleting one of the many producers in a process results in segfault\n\nYou hit a segfault if you try to delete a producer in a process with more than one producer. \r\n\r\n```\r\nproducer :.kfk.Producer enlist[`metadata.broker.list]!enlist `localhost:9012;\r\nproducer2:.kfk.Producer enlist[`metadata.broker.list]!enlist `localhost:9013;\r\n\r\n.kfk.ClientDel producer;\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 43,
    "metadata": {
      "issue_number": 56,
      "state": "closed",
      "labels": [
        "bug"
      ],
      "comments_count": 6,
      "created_at": "2020-07-09T13:59:00Z",
      "updated_at": "2021-03-18T14:04:22Z",
      "closed_at": "2021-03-18T14:04:21Z",
      "author": "rpoply1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "kx-github_issue-9054d8afeb09",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/32",
    "title": "Failure to connect to a broker does not throw a q error",
    "text": "# Failure to connect to a broker does not throw a q error\n\n**Internally raised issue**\r\n\r\n**Describe the bug**\r\nIf the q process fails to connect to an invalid kafka broker does not throw a q error but prints out kafka errors and returns the client id that would have been used. This is invalid behaviour.\r\n\r\n**To Reproduce**\r\nRun the following in a valid q session initialised with `q kfk.q`\r\n\r\n```\r\nq).kfk.Consumer[`metadata.broker.list`group.id!`foobar`0]\r\n```\r\n \r\n**Expected behavior**\r\nEvaluation of the above should error out with an appropriate error indicating that failure was unsuccessful.\r\n\r\n**Screenshots**\r\nThere are no relevant screenshots necessary for this\r\n\r\n**Desktop (please complete the following information):**\r\nThis issue is os independent and a result of implementation decisions\r\n\r\n**Additional context**\r\nThere is no additional context for this issue necessary\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 128,
    "metadata": {
      "issue_number": 32,
      "state": "closed",
      "labels": [],
      "comments_count": 5,
      "created_at": "2020-06-05T14:52:09Z",
      "updated_at": "2021-03-18T16:27:24Z",
      "closed_at": "2021-03-18T16:27:24Z",
      "author": "cmccarthy1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-a04672a56df5",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/29",
    "title": "Support for rd_kafka_produce_batch",
    "text": "# Support for rd_kafka_produce_batch\n\n**Internal Feature Request**\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nWithin the librdkafka C api there appears to be functionality to allow the batched send/receive of data. In many use cases this is preferable to the continuous consumption/sending of data.\r\n\r\n**Describe the solution you'd like**\r\nProvide a logical mechanism for q to expose the use of some of the below librdkafka C functions \r\n* rd_kafka_produce_batch\r\n\r\n**Describe alternatives you've considered**\r\nThere isn't currently an alternative mechanism within the structure of the interface to provide this functionality\r\n\r\n**Additional resource**\r\n* librdkafka documentation for [produce_batch](https://docs.confluent.io/current/clients/librdkafka/rdkafka_8h.html#a7ad15c71f228c47946500a0e5c6f88ed)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 99,
    "metadata": {
      "issue_number": 29,
      "state": "closed",
      "labels": [],
      "comments_count": 5,
      "created_at": "2020-06-05T14:16:11Z",
      "updated_at": "2020-06-17T08:30:14Z",
      "closed_at": "2020-06-17T08:28:55Z",
      "author": "cmccarthy1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-2b29e3c10060",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/122",
    "title": "kfkBarchPub - calloc Arguments",
    "text": "# kfkBarchPub - calloc Arguments\n\nGot a warning when compiling on RHEL9, reporting that line 504 potentially has incorrect order of arguments?\n\n'Calloc' sizes specified with 'sizeof' in the earlier argument and not in the later argument\n\nAdmittedly my c skills are not up to scratch here, but wanted to raise in case of potential issue.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 56,
    "metadata": {
      "issue_number": 122,
      "state": "closed",
      "labels": [],
      "comments_count": 4,
      "created_at": "2025-10-31T20:22:37Z",
      "updated_at": "2026-01-04T13:38:03Z",
      "closed_at": "2025-12-27T13:00:21Z",
      "author": "dannicholls244",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-ee8a117a605a",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/101",
    "title": "Bad config can cause consumer not to get data",
    "text": "# Bad config can cause consumer not to get data\n\n\r\nRecreation \r\n\r\nRun test_consumer.q and subscribe to data. Now quit & edit test_consumer.q to have an extra config \r\n```(`queue.buffering.max.ms;`1);```\r\n\r\nyou will get no data & when exit you will see\r\n```\"(4i;\\\"CONFWARN\\\";\\\"[thrd:app]: Configuration property queue.buffering.max.ms is a producer property and will be ignored by this consumer instance\\\")\"```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 55,
    "metadata": {
      "issue_number": 101,
      "state": "closed",
      "labels": [],
      "comments_count": 4,
      "created_at": "2023-09-08T10:35:48Z",
      "updated_at": "2023-09-08T15:53:37Z",
      "closed_at": "2023-09-08T15:53:36Z",
      "author": "sshanks-kx",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-4fb777555e26",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/85",
    "title": "Issue with connecting AWS MSK kafka broker URL",
    "text": "# Issue with connecting AWS MSK kafka broker URL\n\n**Describe the bug**\r\nI was using kdb kafka wrapper to send data to AWS MSK. When we create broker on AWS MSK it will give the URL. This URL generated by AWS MSK contains \"-\" in the URL. For e.g **b-1.aws.east-us.awsmks.com** Due to hyphen in URL when we try to pass broker URL in config Q throws error.\r\n**To Reproduce**\r\nPlease use any URL which have - in it as a broker url. For e.g\r\n`kfk_cfg:(!)` . flip(  (`metadata.broker.list;`b-1.aws.east-us.awsmks.com:9092);  (`statistics.interval.ms;`10000);  (`queue.buffering.max.ms;`1);  (`fetch.wait.max.ms;`10)  );`\r\n\r\n**Expected behavior**\r\nQ should not restrict such URL to be used as Kafka broker URL\r\n\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 107,
    "metadata": {
      "issue_number": 85,
      "state": "closed",
      "labels": [],
      "comments_count": 4,
      "created_at": "2021-06-08T07:39:50Z",
      "updated_at": "2021-06-21T08:14:05Z",
      "closed_at": "2021-06-21T08:14:05Z",
      "author": "joshisagar92",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-f2fb27111d50",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/77",
    "title": ".kfk.Metadata returns topic information as lists of dictionaries instead of tables",
    "text": "# .kfk.Metadata returns topic information as lists of dictionaries instead of tables\n\nWhen you invoke .kfk.Metadata, the topics return is a list of dictionaries that have aligned keys but is not typed as `98h`. This makes it impossible to do `select` or `exec` on that data to filter out the topic you might be interested in. \r\n\r\n ```q\r\nclient:.kfk.Consumer[`metadata.broker.list`group.id!(`$\"localhost:9092\";`0)];\r\ninfo: .kfk.Metadata client\r\ntype info `topics\r\n/=> 0h\r\ntype each info`topics\r\n/=> 99h 99h\r\nkey each info`topics\r\n/=> topic err partitions\r\n/=> topic err partitions\r\n```\r\n\r\nThe same is then true for the `partitions` key that is nested under the topics table. I am using version 1.5.0 in kdb+ 3.6 2019.11.13 on macOS 10.15.7.\r\n\r\n```\r\nKDB+ 3.6 2019.11.13 Copyright (C) 1993-2019 Kx Systems\r\nm64/ 4()core 8192MB\r\n```\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 125,
    "metadata": {
      "issue_number": 77,
      "state": "open",
      "labels": [],
      "comments_count": 4,
      "created_at": "2021-03-15T14:25:16Z",
      "updated_at": "2021-03-18T11:18:33Z",
      "closed_at": null,
      "author": "mattmaynes",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-995ef839f22c",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/75",
    "title": "SASL/Kerberos authentication ",
    "text": "# SASL/Kerberos authentication \n\nI am looking to connect to a topic that requires SASL authentication.\r\nWould I be able to do this using this library? if yes, what would an example look like?\r\nThank you\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 35,
    "metadata": {
      "issue_number": 75,
      "state": "closed",
      "labels": [],
      "comments_count": 4,
      "created_at": "2020-12-29T18:32:12Z",
      "updated_at": "2021-03-18T10:16:28Z",
      "closed_at": "2021-03-18T10:16:28Z",
      "author": "BogdanS",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-1c04be8bda63",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/37",
    "title": "Thread starvation when consuming persisted messages",
    "text": "# Thread starvation when consuming persisted messages\n\n**Client Raised Issue**\r\n\r\n**Describe the bug**\r\nWhen consuming persisted data with large numbers of messages, thread starvation can occur as the main thread is blocked until consumption is completed.\r\n\r\n**To Reproduce**\r\n- Create a Kafka topic with say few hundred thousand messages persisted. This will create q main thread starvation.\r\n- Set consumer to read from the begining -> (`auto.offset.reset;`earliest);\r\n- make group_id random string. It has to be unique string. We do not use consumer groups in many use cases so random is fine.\r\n\r\nSettings:\r\n```\r\ngroup_id:20?.Q.a\r\nkfk_cfg:(!) . flip(\r\n    (`metadata.broker.list;`localhost:9100);  \r\n    (`group.id;group_id);\r\n    (`statistics.interval.ms;`10000);\r\n    (`enable.auto.commit;`false);\r\n    (`auto.offset.reset;`earliest);\r\n    (`queue.buffering.max.ms;`1);\r\n    (`fetch.wait.max.ms;`10)\r\n    );\r\n```\r\n\r\n**Expected behavior**\r\nThere should be a mechanism by which to interrupt the consumption process or an option to allow this consumption to take place in a more controlled manner.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 136,
    "metadata": {
      "issue_number": 37,
      "state": "closed",
      "labels": [],
      "comments_count": 3,
      "created_at": "2020-06-08T13:43:30Z",
      "updated_at": "2020-06-15T08:10:00Z",
      "closed_at": "2020-06-15T08:10:00Z",
      "author": "cmccarthy1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-9a82387a6945",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/121",
    "title": "Timeout happened , no reconnect seen",
    "text": "# Timeout happened , no reconnect seen\n\nYesterday in our production we had an timeout issue, we did not see any reconnect that is the issue\n \nWe are the consumer side\n\nLocal: Timed out\n\nErrors seen\nTimed out MetadataRequest in flight(after 6060 ms,timeout #0\n\nTimed out FindCoordinatorRequest in flight(after 6060 ms,timeout #0\n\nTimed out 2 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent-requests\n\n2 request(s) timed out: disconnect ( after 60061ms in state UP\n\n\nAre we missing any configs\n\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 78,
    "metadata": {
      "issue_number": 121,
      "state": "open",
      "labels": [],
      "comments_count": 2,
      "created_at": "2025-05-08T14:35:51Z",
      "updated_at": "2025-05-08T17:40:02Z",
      "closed_at": null,
      "author": "chunaiarun",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-7f465df7b3c8",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/76",
    "title": "Stale consumers after commit offsets",
    "text": "# Stale consumers after commit offsets\n\n**Internally raised issue**\r\n\r\n**Describe the bug**\r\nCommitting an offset as a member of a consumer group during a group rebalance event for that group can cause the consumer to become stale thus resulting in the consumer no longer receiving messages\r\n\r\n**To Reproduce**\r\nThe following scripts can be used to reproduce the issue (Note that the localhost/port need to be added in accordance with your kafka installation)\r\n```\r\ncat stale_con.q\r\n//load kafka\r\nOFFSET_LOG:() ; MSGS:()\r\n\\c 5000 5000\r\ncommit:{ .kfk.CommitOffsets[0i;`test1;;1b] exec partition!offset from MSGS where offset = (max;offset)fby partition ; `COMMITED set .z.p ;  }\r\n.kfk.offsetcb: {[cid;err;offsets] if[not err like \"Success\" ; 0N!\"offsetcb not success\" ; OFFSET_LOG,:(cid;err;offsets) ; `commit set { } ]; }\r\n.kfk.consumecb:{ x[`rcvtime]:.z.p ; MSGS,:: enlist x _ `data  ; `MSG set x  }\r\ncfg:(!) . flip(\r\n  (`metadata.broker.list;`$\"localhost:port\");\r\n  (`bootstrap.servers;`$\"localhost:port\");\r\n  (`group.id;`$\"test_consumer_group_1\");\r\n  (`enable.auto.commit;`false);\r\n  (`enable.auto.offset.store;`false);\r\n  (`auto.offset.reset;`latest);\r\n  (`session.timeout.ms;`60000);\r\n  );\r\n.kfk.Consumer cfg\r\n.kfk.Sub[0i;`test1;enlist[.kfk.PARTITION_UA]!enlist[.kfk.OFFSET.END] ]\r\n```\r\n```\r\ncat other_cons.q\r\nOFFSET_LOG:() ; MSGS:()\r\n\\c 5000 5000\r\nsystem\"sleep 2\"\r\ncommit:{ .kfk.CommitOffsets[0i;`test1;;1b] exec partition!offset from MSGS where offset = (max;offset)fby partition ; `COMMITED set .z.p ;  }\r\n.kfk.offsetcb: {[cid;err;offsets] if[not err like \"Success\" ; 0N!\"offsetcb not success\" ; OFFSET_LOG,:(cid;err;offsets) ]; }\r\n.kfk.consumecb:{ x[`rcvtime]:.z.p ; MSGS,:: enlist x _ `data  ; `MSG set x ; }\r\ncfg:(!) . flip(\r\n  (`metadata.broker.list;`$\"localhost:port\");\r\n  (`bootstrap.servers;`$\"localhost:port\");\r\n  (`group.id;`$\"test_consumer_group_1\");\r\n  (`enable.auto.commit;`false);\r\n  (`enable.auto.offset.store;`false);\r\n  (`auto.offset.reset;`latest);\r\n  (`session.timeout.ms;`60000);\r\n  );\r\nclients:{ .kfk.Consumer cfg } each til 10\r\n{ .kfk.Sub[x;`test1;enlist[.kfk.PARTITION_UA]!enlist[.kfk.OFFSET.END] ] } each clients\r\n```\r\n\r\nSteps to reproduce:\r\n1. Have a process producing on the topic ``` `test1 ```\r\n2. start stale_con.q\r\n3. start other_cons.q once the stale one is up and running\r\n4. manually run commit[] on stale_con.q process in quick succession.\r\n5. If “offsetcb not success” is not seen then restart the other_cons.q process and try again\r\n6. After receiving the \"Offset commit failed - Specified group generation id is not valid\" from offsetcb the consumer won't consume any more messages.\r\n\r\n**Expected behavior**\r\nIf offset commit is unsuccessful the consumer should be able to retry commit or configuration should be set to allow this \r\n\r\n**Desktop (please complete the following information):**\r\n```\r\nq).kfk.VersionSym[]\r\n`1.4.2\r\nKdb: 4.0 2020.10.02\r\nKx kafka release:  v1.4.0```\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 356,
    "metadata": {
      "issue_number": 76,
      "state": "open",
      "labels": [],
      "comments_count": 2,
      "created_at": "2021-01-29T08:31:00Z",
      "updated_at": "2021-02-10T16:11:59Z",
      "closed_at": null,
      "author": "cmccarthy1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-2f56e6619ebb",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/61",
    "title": "Deleting a client and recreating it causes segfault",
    "text": "# Deleting a client and recreating it causes segfault\n\nLooks like .kfk.ClientDel is not cleaning up resources properly. Deleting a client and then recreating one with the same config results in a segfault. Please see below for a reproducible case. \r\n\r\n```\r\n\\l kfk.q\r\n\\c 20 200\r\n\r\nkfk_cfg:(!) . flip(\r\n    (`metadata.broker.list;`172.31.1.104:9011);\r\n    (`group.id;`0)\r\n    );\r\n\r\nclient1: .kfk.Consumer kfk_cfg;\r\n.kfk.Sub[client1;`foo; enlist .kfk.PARTITION_UA];\r\n\r\nsystem \"sleep 2\";\r\n.kfk.Unsub client1;\r\n.kfk.ClientDel client1;\r\n\r\nclient2: .kfk.Consumer kfk_cfg;\r\n```\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 69,
    "metadata": {
      "issue_number": 61,
      "state": "closed",
      "labels": [
        "bug"
      ],
      "comments_count": 2,
      "created_at": "2020-09-25T14:09:34Z",
      "updated_at": "2020-09-29T13:26:10Z",
      "closed_at": "2020-09-28T13:19:31Z",
      "author": "rpoply1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "kx-github_issue-1c33b389ae19",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/57",
    "title": "Process hangs on publish if the producer is deleted",
    "text": "# Process hangs on publish if the producer is deleted\n\nTrying to publish a message on a topic whose associated producer is deleted hangs the process \r\n\r\n```\r\nproducer: .kfk.Producer enlist[`metadata.broker.list]!enlist `localhost:9011;\r\nrandom: .kfk.Topic[producer;`test;()!()];\r\n.kfk.ClientDel producer;\r\n.kfk.Pub[random; -1i; -8!\"hello world\" ;\"\"]\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 41,
    "metadata": {
      "issue_number": 57,
      "state": "closed",
      "labels": [
        "bug"
      ],
      "comments_count": 2,
      "created_at": "2020-07-09T14:02:08Z",
      "updated_at": "2020-10-27T20:47:17Z",
      "closed_at": "2020-10-27T20:47:17Z",
      "author": "rpoply1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "kx-github_issue-209ce62454a0",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/52",
    "title": ".kfk.Metadata lists a topic even if it's deleted using .kfk.TopicDel",
    "text": "# .kfk.Metadata lists a topic even if it's deleted using .kfk.TopicDel\n\nDeleting a topic using .kfk.TopicDel doesn't remove that from the list of topics returned by .kfk.Metadata\r\n\r\n  ```q\r\n    client: .kfk.Consumer[`metadata.broker.list`group.id!`localhost:9092`0];\r\n    .kfk.Metadata[client]`topics;\r\n    topic: .kfk.Topic[client; `new_topic; ()!()];\r\n    .kfk.Metadata[client]`topics;\r\n    .kfk.TopicDel topic;\r\n    .kfk.Metadata[client]`topics;\r\n    ```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 40,
    "metadata": {
      "issue_number": 52,
      "state": "open",
      "labels": [
        "enhancement"
      ],
      "comments_count": 2,
      "created_at": "2020-06-23T15:59:03Z",
      "updated_at": "2021-03-18T17:44:39Z",
      "closed_at": null,
      "author": "rpoply1",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-aa48a61dbbd2",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/44",
    "title": "Support for rd_kafka_consume_batch",
    "text": "# Support for rd_kafka_consume_batch\n\n**Internal Feature Request**\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nWithin the librdkafka C api there appears to be functionality to allow the batched send/receive of data. In many use cases this is preferable to the continuous consumption/sending of data.\r\n\r\n**Describe the solution you'd like**\r\nProvide a logical mechanism for q to expose the use of some of the below librdkafka C functions \r\n* rd_kafka_consume_batch\r\n\r\n**Describe alternatives you've considered**\r\nThere isn't currently an alternative mechanism within the structure of the interface to provide this functionality\r\n\r\n**Additional resource**\r\n* librdkafka documentation for [consume_batch](https://docs.confluent.io/current/clients/librdkafka/rdkafka_8h.html#a53511739a2cf498b8d88287fef6873ce)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 99,
    "metadata": {
      "issue_number": 44,
      "state": "closed",
      "labels": [],
      "comments_count": 2,
      "created_at": "2020-06-16T10:52:47Z",
      "updated_at": "2020-06-18T09:12:32Z",
      "closed_at": "2020-06-18T09:12:32Z",
      "author": "sshanks-kx",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "kx-github_issue-54b5ce73f57a",
    "origin": "kx",
    "source_type": "github_issue",
    "url": "https://github.com/KxSystems/kafka/issues/23",
    "title": "Enable travis ci for project",
    "text": "# Enable travis ci for project\n\nRegister repo with travis - create automated builds on travis",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 16,
    "metadata": {
      "issue_number": 23,
      "state": "closed",
      "labels": [],
      "comments_count": 2,
      "created_at": "2020-03-03T14:39:19Z",
      "updated_at": "2020-03-03T15:27:25Z",
      "closed_at": "2020-03-03T15:27:25Z",
      "author": "sshanks-kx",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  }
]