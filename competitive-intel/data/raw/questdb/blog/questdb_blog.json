[
  {
    "id": "questdb-blog-a1f62e47ab94",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/enterprise",
    "title": "QuestDB Enterprise | QuestDB",
    "text": "QuestDB Enterprise\n\n# Run QuestDB in production with confidence\n\nSecurity, HA, and elastic scale with automatic failover. SSO/RBAC/audit, tiered object storage, and SLA-backed creator support.\nBook a demo\nQuestDB Bring Your Own Cloud â†’\nArrays\nLATEST ON\nSAMPLE BY\nVolume Ratios\n\n```\nDECLARE  @prices := asks[1],  @volumes := asks[2],  @best_price := @prices[1],  @multiplier := 1.01,  @target_price := @multiplier *  @best_price,  @rel_vol := @volumes[    1:insertion_point(@prices, @target_price)  ]SELECT timestamp, array_sum(@rel_vol) total_volumeFROM market_dataWHERE timestamp > dateadd('m', -1, now())AND symbol='EURUSD';\n```\n\nHigh Availability, Auto failover99.9% uptimeAccess control (SSO/RBAC) & audit logsSecurity & governanceMulti-tier storage; storage/compute separationElastic scaleSLA-backed creator supportTrusted partnership\n\n## Production grade performance\n\nPremium Enterprise features\nRun QuestDB at scale, with full support from QuestDB's creators\n\n| Highly availableReplication & auto failover, Multi AZ | Robust securityTLS, SSO (OAuth 2.0/OIDC), RBAC, audit logs | Tiered storageAuto-tiering: Hot â†’ Cold (Parquet/object storage) |\n| --- | --- | --- |\n| Unified query across tiersOne SQL over hot & cold; planner spans partitions + Parquet | Expert supportSLA-backed; architecture & performance reviews | Deploy anywhereCloud, on-prem, hybrid, orBYOC |\n\nHighly available\nReplication & auto failover, Multi AZ\nRobust security\nTLS, SSO (OAuth 2.0/OIDC), RBAC, audit logs\nTiered storage\nAuto-tiering: Hot â†’ Cold (Parquet/object storage)\nUnified query across tiers\nOne SQL over hot & cold; planner spans partitions + Parquet\nExpert support\nSLA-backed; architecture & performance reviews\nDeploy anywhere\nCloud, on-prem, hybrid, orBYOC\nCapital Markets\n\n### BTG Pactual powers real-time market data APIs with QuestDB\n\nLatin America's largest investment bank uses QuestDB to deliver low-latency market data APIs for equities and derivatives trading. QuestDB also supports post-trade analysis, helping traders assess execution quality against benchmarks like VWAP and TWAP.\n- High throughput:Ingesting trades and corporate actions for derivatives in real time\n- Dynamic queries:Materialized views to produce candlestick charts and cascading views (1s, 5s, 1min etc.) for historical data\n- Post-trade troubleshooting:Correlates electronic trading and algorithm events for execution quality\n\"QuestDB has become the standard for data aggregation. Besides that, QuestDB has proven itself to be lightning fast, fast enough to help us when providing raw live intraday trading data through our set of APIs within a few milliseconds.\"Renan AvilaÂ·BTG PactualDirector\n\n## One SQL engine, multiple storage tiers.\n\nScale fast.Stay open.\nQuery QuestDB's native format on local storage and Parquet files on object storage simultaneously, all through a single SQL query. Data tiers automatically without manual intervention. Open formats eliminate lock-in while dataframe libraries and AI frameworks connect natively.\nWHERE symbol in ('AAPL', 'NVDA')\nLATEST ON timestamp PARTITION BY symbol\nCREATE MATERIALIZED VIEW 'trades_OHLC'\nmin(price) AS low\ntimestamp IN today()\nSELECT spread_bps(bids[1][1], asks[1][1])\nFROM read_parquet('trades.parquet')\nSAMPLE BY 15m\nTier One:\nHot ingest (WAL), durable by default\nIncoming data is appended to the write-ahead log (WAL) with ultra-low latency. Writes are made durable before any processing, preserving order and surviving failures without data loss. The WAL is asynchronously shipped to object storage, so new replicas can bootstrap quickly and read the same history.\nTier Two:\nReal-time SQL on live data\nData is time-ordered and de-duplicated into QuestDB's native, time-partitioned columnar format and becomes immediately queryable. Power real-time analysis with vectorized, multi-core execution, streaming materialized views, and time-series SQL (e.g., ASOF JOIN, SAMPLE BY). The query planner spans tiers seamlessly.\nTier Three:\nCold storage, open and queryable\nOlder data is automatically tiered to object storage in Apache Parquet. Query it in-place through QuestDB or use any tool that reads Parquet. This delivers predictable costs, interoperability with AI/ML tooling, and zero lock-in.\nOpen formatsLeverages existing open formats. No vendor lock-in.Apache ParquetEnhanced compression and encoding, for ingress or egress.Super read/writeFast ingest and low latency SQL queriesFull streamStream market data in from feeds or sensors, apply Parquet on readDirect to Parquet?Bypass QuestDB ingest, queryParquetdirectly from the object storeVersatile ecosystemDiverse clients connect to your data, app, AI and ML frameworks\nQuestDB Open Source\nQuestDB Enterprise\nPrimary Use\nEvaluation, prototyping, pilots\nProduction deployments\nEngine & Performance\nHigh-performance core engine\nRead/write isolation with replicas\nSame high-performance core engine\nSecurity\nBasic\nBasic auth over unencrypted transmission\nEnterprise-grade\nRBAC incl. column-level; TLS encryption (all protocols)\nUser Management\nLimited\nBuilt-in Admin user with a configurable read-only user\nAdvanced\nUnlimited native users, groups, and service accounts. SSO supported for OAuth/OIDC/Azure Entra ID\nScalability\nNone\nSingle instance\nHorizontally scalable\nScale to N-replicas\nHigh Availability\nNone\nSingle instance; no replication/failover\nMulti-instance\nCloud-native, high availability, read replicas, primary failover\nBackups & Snapshots\nManual\nFull backups into local storage\nFully automated\nIncremental snapshots into object stores\nDisaster Recovery\nFrom backup\nRestore from a previous full backup\nFlexible\nRestore from a previous full backup or do automated point-in-time (PITR) recovery\nStorage & Data Lake\nNo integrated tiering\nData stored on local storage\nTiered storage\nTiered storage with cloud object stores (Azure Blob, Amazon S3, GCS, Oracle Cloud Storage)\nLifecycle Policies\nExpire\nAutomatic Downsampling with Materialized Views. Data can be deleted via TTL.\nArchive\nAutomatic Downsampling with Materialized Views. Data can be deleted or archived to tiered storage via TTL. On-demand recall of historical data\nDeployment\nSelf-managed\nBinaries, containers, and source code available\nCustom\nSelf-managed or Bring Your Own Cloud (BYOC), with QuestDB ops team taking care of your deployment, on-premises or cloud-native\nSupport & SLAs\nLimited\nCommunity support only\nUnlimited access to dedicated support\nDirect access to QuestDB engineers. Guaranteed response-time SLAs. Guidance beyond break-fix.\nEmergency Patches & Bug Fixes\nCommunity releases only\nBest effort\nYes\nCustomer-priority hotfixes and security patches\nRoadmap & Feature Requests\nLimited\nPublic/community via GitHub\nPriority requests\nEarly access to new features, enterprise-specific enhancements\nDownload now\n\n### Ready to deploy QuestDB Enterprise?\n\nTalk to us to learn more\n\n### Primary Use\n\nQuestDB Open Source\nEvaluation, prototyping, pilots\nQuestDB Enterprise\nProduction deployments\n\n### Primary Use\n\nQuestDB Open Source\nEvaluation, prototyping, pilots\nQuestDB Enterprise\nProduction deployments\n\n### Primary Use\n\nQuestDB Open Source\nEvaluation, prototyping, pilots\nQuestDB Enterprise\nProduction deployments\n\n### Primary Use\n\nQuestDB Open Source\nEvaluation, prototyping, pilots\nQuestDB Enterprise\nProduction deployments\n\n### Engine & Performance\n\nQuestDB Open Source\nHigh-performance core engine\nQuestDB Enterprise\nRead/write isolation with replicas\nSame high-performance core engine\n\n### Engine & Performance\n\nQuestDB Open Source\nHigh-performance core engine\nQuestDB Enterprise\nRead/write isolation with replicas\nSame high-performance core engine\n\n### Engine & Performance\n\nQuestDB Open Source\nHigh-performance core engine\nQuestDB Enterprise\nRead/write isolation with replicas\nSame high-performance core engine\n\n### Engine & Performance\n\nQuestDB Open Source\nHigh-performance core engine\nQuestDB Enterprise\nRead/write isolation with replicas\nSame high-performance core engine\n\n### Security\n\nQuestDB Open Source\nBasic\nBasic auth over unencrypted transmission\nQuestDB Enterprise\nEnterprise-grade\nRBAC incl. column-level; TLS encryption (all protocols)\n\n### Security\n\nQuestDB Open Source\nBasic\nBasic auth over unencrypted transmission\nQuestDB Enterprise\nEnterprise-grade\nRBAC incl. column-level; TLS encryption (all protocols)\n\n### Security\n\nQuestDB Open Source\nBasic\nBasic auth over unencrypted transmission\nQuestDB Enterprise\nEnterprise-grade\nRBAC incl. column-level; TLS encryption (all protocols)\n\n### Security\n\nQuestDB Open Source\nBasic\nBasic auth over unencrypted transmission\nQuestDB Enterprise\nEnterprise-grade\nRBAC incl. column-level; TLS encryption (all protocols)\n\n### User Management\n\nQuestDB Open Source\nLimited\nBuilt-in Admin user with a configurable read-only user\nQuestDB Enterprise\nAdvanced\nUnlimited native users, groups, and service accounts. SSO supported for OAuth/OIDC/Azure Entra ID\n\n### User Management\n\nQuestDB Open Source\nLimited\nBuilt-in Admin user with a configurable read-only user\nQuestDB Enterprise\nAdvanced\nUnlimited native users, groups, and service accounts. SSO supported for OAuth/OIDC/Azure Entra ID\n\n### User Management\n\nQuestDB Open Source\nLimited\nBuilt-in Admin user with a configurable read-only user\nQuestDB Enterprise\nAdvanced\nUnlimited native users, groups, and service accounts. SSO supported for OAuth/OIDC/Azure Entra ID\n\n### User Management\n\nQuestDB Open Source\nLimited\nBuilt-in Admin user with a configurable read-only user\nQuestDB Enterprise\nAdvanced\nUnlimited native users, groups, and service accounts. SSO supported for OAuth/OIDC/Azure Entra ID\n\n### Scalability\n\nQuestDB Open Source\nNone\nSingle instance\nQuestDB Enterprise\nHorizontally scalable\nScale to N-replicas\n\n### Scalability\n\nQuestDB Open Source\nNone\nSingle instance\nQuestDB Enterprise\nHorizontally scalable\nScale to N-replicas\n\n### Scalability\n\nQuestDB Open Source\nNone\nSingle instance\nQuestDB Enterprise\nHorizontally scalable\nScale to N-replicas\n\n### Scalability\n\nQuestDB Open Source\nNone\nSingle instance\nQuestDB Enterprise\nHorizontally scalable\nScale to N-replicas\n\n### High Availability\n\nQuestDB Open Source\nNone\nSingle instance; no replication/failover\nQuestDB Enterprise\nMulti-instance\nCloud-native, high availability, read replicas, primary failover\n\n### High Availability\n\nQuestDB Open Source\nNone\nSingle instance; no replication/failover\nQuestDB Enterprise\nMulti-instance\nCloud-native, high availability, read replicas, primary failover\n\n### High Availability\n\nQuestDB Open Source\nNone\nSingle instance; no replication/failover\nQuestDB Enterprise\nMulti-instance\nCloud-native, high availability, read replicas, primary failover\n\n### High Availability\n\nQuestDB Open Source\nNone\nSingle instance; no replication/failover\nQuestDB Enterprise\nMulti-instance\nCloud-native, high availability, read replicas, primary failover\n\n### Backups & Snapshots\n\nQuestDB Open Source\nManual\nFull backups into local storage\nQuestDB Enterprise\nFully automated\nIncremental snapshots into object stores\n\n### Backups & Snapshots\n\nQuestDB Open Source\nManual\nFull backups into local storage\nQuestDB Enterprise\nFully automated\nIncremental snapshots into object stores\n\n### Backups & Snapshots\n\nQuestDB Open Source\nManual\nFull backups into local storage\nQuestDB Enterprise\nFully automated\nIncremental snapshots into object stores\n\n### Backups & Snapshots\n\nQuestDB Open Source\nManual\nFull backups into local storage\nQuestDB Enterprise\nFully automated\nIncremental snapshots into object stores\n\n### Disaster Recovery\n\nQuestDB Open Source\nFrom backup\nRestore from a previous full backup\nQuestDB Enterprise\nFlexible\nRestore from a previous full backup or do automated point-in-time (PITR) recovery\n\n### Disaster Recovery\n\nQuestDB Open Source\nFrom backup\nRestore from a previous full backup\nQuestDB Enterprise\nFlexible\nRestore from a previous full backup or do automated point-in-time (PITR) recovery\n\n### Disaster Recovery\n\nQuestDB Open Source\nFrom backup\nRestore from a previous full backup\nQuestDB Enterprise\nFlexible\nRestore from a previous full backup or do automated point-in-time (PITR) recovery\n\n### Disaster Recovery\n\nQuestDB Open Source\nFrom backup\nRestore from a previous full backup\nQuestDB Enterprise\nFlexible\nRestore from a previous full backup or do automated point-in-time (PITR) recovery\n\n### Storage & Data Lake\n\nQuestDB Open Source\nNo integrated tiering\nData stored on local storage\nQuestDB Enterprise\nTiered storage\nTiered storage with cloud object stores (Azure Blob, Amazon S3, GCS, Oracle Cloud Storage)\n\n### Storage & Data Lake\n\nQuestDB Open Source\nNo integrated tiering\nData stored on local storage\nQuestDB Enterprise\nTiered storage\nTiered storage with cloud object stores (Azure Blob, Amazon S3, GCS, Oracle Cloud Storage)\n\n### Storage & Data Lake\n\nQuestDB Open Source\nNo integrated tiering\nData stored on local storage\nQuestDB Enterprise\nTiered storage\nTiered storage with cloud object stores (Azure Blob, Amazon S3, GCS, Oracle Cloud Storage)\n\n### Storage & Data Lake\n\nQuestDB Open Source\nNo integrated tiering\nData stored on local storage\nQuestDB Enterprise\nTiered storage\nTiered storage with cloud object stores (Azure Blob, Amazon S3, GCS, Oracle Cloud Storage)\n\n### Lifecycle Policies\n\nQuestDB Open Source\nExpire\nAutomatic Downsampling with Materialized Views. Data can be deleted via TTL.\nQuestDB Enterprise\nArchive\nAutomatic Downsampling with Materialized Views. Data can be deleted or archived to tiered storage via TTL. On-demand recall of historical data\n\n### Lifecycle Policies\n\nQuestDB Open Source\nExpire\nAutomatic Downsampling with Materialized Views. Data can be deleted via TTL.\nQuestDB Enterprise\nArchive\nAutomatic Downsampling with Materialized Views. Data can be deleted or archived to tiered storage via TTL. On-demand recall of historical data\n\n### Lifecycle Policies\n\nQuestDB Open Source\nExpire\nAutomatic Downsampling with Materialized Views. Data can be deleted via TTL.\nQuestDB Enterprise\nArchive\nAutomatic Downsampling with Materialized Views. Data can be deleted or archived to tiered storage via TTL. On-demand recall of historical data\n\n### Lifecycle Policies\n\nQuestDB Open Source\nExpire\nAutomatic Downsampling with Materialized Views. Data can be deleted via TTL.\nQuestDB Enterprise\nArchive\nAutomatic Downsampling with Materialized Views. Data can be deleted or archived to tiered storage via TTL. On-demand recall of historical data\n\n### Deployment\n\nQuestDB Open Source\nSelf-managed\nBinaries, containers, and source code available\nQuestDB Enterprise\nCustom\nSelf-managed or Bring Your Own Cloud (BYOC), with QuestDB ops team taking care of your deployment, on-premises or cloud-native\n\n### Deployment\n\nQuestDB Open Source\nSelf-managed\nBinaries, containers, and source code available\nQuestDB Enterprise\nCustom\nSelf-managed or Bring Your Own Cloud (BYOC), with QuestDB ops team taking care of your deployment, on-premises or cloud-native\n\n### Deployment\n\nQuestDB Open Source\nSelf-managed\nBinaries, containers, and source code available\nQuestDB Enterprise\nCustom\nSelf-managed or Bring Your Own Cloud (BYOC), with QuestDB ops team taking care of your deployment, on-premises or cloud-native\n\n### Deployment\n\nQuestDB Open Source\nSelf-managed\nBinaries, containers, and source code available\nQuestDB Enterprise\nCustom\nSelf-managed or Bring Your Own Cloud (BYOC), with QuestDB ops team taking care of your deployment, on-premises or cloud-native\n\n### Support & SLAs\n\nQuestDB Open Source\nLimited\nCommunity support only\nQuestDB Enterprise\nUnlimited access to dedicated support\nDirect access to QuestDB engineers. Guaranteed response-time SLAs. Guidance beyond break-fix.\n\n### Support & SLAs\n\nQuestDB Open Source\nLimited\nCommunity support only\nQuestDB Enterprise\nUnlimited access to dedicated support\nDirect access to QuestDB engineers. Guaranteed response-time SLAs. Guidance beyond break-fix.\n\n### Support & SLAs\n\nQuestDB Open Source\nLimited\nCommunity support only\nQuestDB Enterprise\nUnlimited access to dedicated support\nDirect access to QuestDB engineers. Guaranteed response-time SLAs. Guidance beyond break-fix.\n\n### Support & SLAs\n\nQuestDB Open Source\nLimited\nCommunity support only\nQuestDB Enterprise\nUnlimited access to dedicated support\nDirect access to QuestDB engineers. Guaranteed response-time SLAs. Guidance beyond break-fix.\n\n### Emergency Patches & Bug Fixes\n\nQuestDB Open Source\nCommunity releases only\nBest effort\nQuestDB Enterprise\nYes\nCustomer-priority hotfixes and security patches\n\n### Emergency Patches & Bug Fixes\n\nQuestDB Open Source\nCommunity releases only\nBest effort\nQuestDB Enterprise\nYes\nCustomer-priority hotfixes and security patches\n\n### Emergency Patches & Bug Fixes\n\nQuestDB Open Source\nCommunity releases only\nBest effort\nQuestDB Enterprise\nYes\nCustomer-priority hotfixes and security patches\n\n### Emergency Patches & Bug Fixes\n\nQuestDB Open Source\nCommunity releases only\nBest effort\nQuestDB Enterprise\nYes\nCustomer-priority hotfixes and security patches\n\n### Roadmap & Feature Requests\n\nQuestDB Open Source\nLimited\nPublic/community via GitHub\nQuestDB Enterprise\nPriority requests\nEarly access to new features, enterprise-specific enhancements\n\n### Roadmap & Feature Requests\n\nQuestDB Open Source\nLimited\nPublic/community via GitHub\nQuestDB Enterprise\nPriority requests\nEarly access to new features, enterprise-specific enhancements\n\n### Roadmap & Feature Requests\n\nQuestDB Open Source\nLimited\nPublic/community via GitHub\nQuestDB Enterprise\nPriority requests\nEarly access to new features, enterprise-specific enhancements\n\n### Roadmap & Feature Requests\n\nQuestDB Open Source\nLimited\nPublic/community via GitHub\nQuestDB Enterprise\nPriority requests\nEarly access to new features, enterprise-specific enhancements\n\n### Ready to deploy QuestDB Enterprise?\n\nTalk to us to learn more\n\n## Keep it secret, keep it safe\n\nEnterprise-grade security\nSecurity is essential. QuestDB offers TLS and Role-Based Access Control (RBAC). Ensure data is safe over-the-wire, and apply fine-grained control over who can access which databases, tables or columns, and whether they can read or write. For inter-machine communication, leverage safe service accounts.\nTrusted by leading companies to power\ncritical production workflows\nCapital Markets\n\n### Exchange surveillance and monitoring\n\n- High cardinality:Store new orders, acks, fills, matching engine outputs, ticks and pcap\n- Scale:Process market and network data at previously unattainable volumes\n- AI-powered real-time analytics:Instant analysis of market data powered by AI models via MindsDB's native QuestDB integration\n\"QuestDB is the first database that scales to the volume and cardinality of data that Beeks Analytics is capable of generating.\"Steve RodgersÂ·Beeks GroupCTO\nCapital Markets\n\n### B3 powers Brazil's stock market\n\nLatin America's largest stock exchange processes millions of trades daily with exceptional performance and robust security for critical market infrastructure.\n- Mission critical:Robust security and resilience for critical market infrastructure\n\"The Central Securities Depository platform demands exceptional performance, robust security, and resilience. We selected QuestDB due to its high performance and straightforward implementation.\"Kleber AlmeidaÂ·B3 ExchangeManager, Exchange Technology\nDigital Assets\n\n### First MiFID II exchange for crypto\n\nOne Trading leverages QuestDB's low-latency architecture and multi-tiered storage for regulated crypto trading. The platform handles billions of trade records while enabling real-time insights for market surveillance.\n- Ultra-high throughput:Ingests market data bursts at up to 4 million rows per second\n\"QuestDB is an essential part of our trading platformâ€”giving us a high-speed, scalable store for billions of trades that we can query in real time to power both customer-facing features and internal systems.\"Steven HarperÂ·One TradingChief Security Officer\nCapital Markets\n\n### Exchange surveillance and monitoring\n\n- High cardinality:Store new orders, acks, fills, matching engine outputs, ticks and pcap\n- Scale:Process market and network data at previously unattainable volumes\n- AI-powered real-time analytics:Instant analysis of market data powered by AI models via MindsDB's native QuestDB integration\n\"QuestDB is the first database that scales to the volume and cardinality of data that Beeks Analytics is capable of generating.\"Steve RodgersÂ·Beeks GroupCTO\nCapital Markets\n\n### B3 powers Brazil's stock market\n\nLatin America's largest stock exchange processes millions of trades daily with exceptional performance and robust security for critical market infrastructure.\n- Mission critical:Robust security and resilience for critical market infrastructure\n\"The Central Securities Depository platform demands exceptional performance, robust security, and resilience. We selected QuestDB due to its high performance and straightforward implementation.\"Kleber AlmeidaÂ·B3 ExchangeManager, Exchange Technology\nDigital Assets\n\n### First MiFID II exchange for crypto\n\nOne Trading leverages QuestDB's low-latency architecture and multi-tiered storage for regulated crypto trading. The platform handles billions of trade records while enabling real-time insights for market surveillance.\n- Ultra-high throughput:Ingests market data bursts at up to 4 million rows per second\n\"QuestDB is an essential part of our trading platformâ€”giving us a high-speed, scalable store for billions of trades that we can query in real time to power both customer-facing features and internal systems.\"Steven HarperÂ·One TradingChief Security Officer\nCustomer storiesLearn how leading firms in capital markets use QuestDB to underpin their trading infrastructure.\n\n## Peace of mind at any scale\n\nDistributed writes\nand reads\nToday: Automatic replication & failover; sub-second read replicas across regions via object storage (S3/Azure/GCS/NFS/HDFS). Clients seamlessly redirect to new primary after replica promotion.\nNext (coming soon): Multi-primary writes for continuous availability and zero-downtime upgrades.\nRead more about high availability\nBring Your Own Cloud\nComing soon...\n\n## You bring the cloud,we'll do the rest.\n\nRetain full control over your data and environment, and trust QuestDB's expert operations team to manage your database operations. With our Bring Your Own Cloud (BYOC) offering, your team can deploy within AWS or Azure, and ensure seamless integration with your existing workloads. No kubernetes required!\nLearn more about BYOC\nQuestDB BYOC\nFull controlDeploy and manage QuestDB within your own cloud infrastructureProactive supportMonitoring systems send logs and metrics back to QuestDBSecure accessStreamlined method for authorized personnel to access instances\nThe next generation has arrivedContact us for a demo\nHyper ingestion, millisecond queries, and powerful SQL.\nLower bills through peak efficiency.\nContact Us\n$\n/$",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3044,
    "metadata": {
      "relevance_score": 0.8571428571428571,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "capital markets",
        "trading",
        "release",
        "enterprise"
      ]
    }
  },
  {
    "id": "questdb-blog-95b8218d9dfc",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/download",
    "title": "Download | QuestDB",
    "text": "\n# Download | QuestDB\n\nSelect your platform and download instantly.\nUltra-fast ingestion\nIngest millions of rows per second with low-latency writes.\nOpen source\nLicensed for usage under the Apache License 2.0.\nMillisecond queries\nSIMD-powered millisecond queries for fast analytics.\nRun anywhere\nHyper-scalers, Docker, Kubernetes, your own servers.\n\n## Why QuestDB?\n\nBuilt for those who love top performance and a superior developer experience.\n\n#### Performance & Speed\n\nIngestion Speed\n4M+ rows/sec\nUltra-fast ingestion, with a robust library of clients and protocols.\nQuery Performance\nSub-millisecond\nLightning-fast SQL queries with vectorized execution engine\nUnlimited Cardinality\nNo artificial limits on unique values in your time-series data\nUncapped Excellence\nNo query limits, no time limits, no row limits.\nMemory Efficiency\nLow baseline\nMemory scales with working set only, not total data size\nMaterialized Views\nMaterialized views for pre-computed results, reducing query latency\n\n#### Ease of Use\n\nSQL Support\nStandard SQL with time-series extensions like SAMPLE BY & LATEST ON\nN-Dimensional Arrays\nStore order books and more in N-dimensional arrays.\nSimple Deployment\nSingle binary, Docker, Kubernetes, or package managers\nWeb Console\nBuilt-in web interface for data exploration and query execution\nComprehensive Docs\nExtensive\ndocumentation\nwith examples and best practices\n\n#### Data Formats & Integration\n\nParquet Storage\nParquet format for data portability and cost-effective storage\nPostgreSQL Wire\nSupports PostgreSQL wire with existing tools and drivers\nInfluxDB Line Protocol\nCompatible with existing InfluxDB clients and ecosystem\n\n### Trusted by industry leaders\n\n\n## Enterprise-ready features\n\nAdvanced capabilities for mission-critical workloads\nContact Sales\nLearn More\nMulti-Tiered Storage\nEfficient and fast storage options featuring native Parquet.\nTLS Encryption\nBuilt-in TLS support for secure data transmission\nRole-Based Access\nEnterprise\nFine-grained permissions and access control (Enterprise feature)\nHigh Availability\nEnterprise\nMulti-primary writes and read replicas (Enterprise feature)\nBYOC\nEnterprise\nBring your own compute, storage, and network (Enterprise feature)\nExpert Support\nEnterprise\nDirect support from QuestDB creators with SLAs (Enterprise feature)\nAerospace\n\n### Airbus monitors fleet data\n\nProcesses billions of data points daily. With QuestDB's ingestion performance, Airbus provides real-time monitoring and predictive maintenance for mission critical components.\n- Massive scale:Billions of daily data points processed efficiently\n\"QuestDB is used at Airbus for real-time applications involving billions of data points per day. For us, QuestDB is an outstanding solution that meets (and exceeds) our performance requirements.\"Oliver PfeifferÂ·AirbusSoftware Architect\nCapital Markets\n\n### BTG Pactual powers market data APIs\n\nLatin America's largest investment bank delivers low-latency market data APIs for equities and derivatives trading with millisecond response times.\n- Lightning fast:Market data APIs respond within milliseconds\n\"QuestDB has become the standard for data aggregation. QuestDB has proven itself to be lightning fast, fast enough to help us when providing raw live intraday trading data through our set of APIs within a few milliseconds.\"Renan AvilaÂ·BTG PactualDirector\nFinancial TechnologyAI for trading insightsAward winning Financial AI analytics platform helps investors make better trading decisions. Achieved 10x faster queries with 75% less hardware after migrating from InfluxDB.Resource optimization:4x reduction in hardware requirements with better performance\"We switched from InfluxDB to QuestDB to get queries that are on average 30x faster utilizing 1/4 of the hardware, without ever overtaxing our servers.\"Armenak MayalianÂ·ReflexivityCTORead Reflexivity's case studyâ†’\nAerospace\n\n### Airbus monitors fleet data\n\nProcesses billions of data points daily. With QuestDB's ingestion performance, Airbus provides real-time monitoring and predictive maintenance for mission critical components.\n- Massive scale:Billions of daily data points processed efficiently\n\"QuestDB is used at Airbus for real-time applications involving billions of data points per day. For us, QuestDB is an outstanding solution that meets (and exceeds) our performance requirements.\"Oliver PfeifferÂ·AirbusSoftware Architect\nCapital Markets\n\n### BTG Pactual powers market data APIs\n\nLatin America's largest investment bank delivers low-latency market data APIs for equities and derivatives trading with millisecond response times.\n- Lightning fast:Market data APIs respond within milliseconds\n\"QuestDB has become the standard for data aggregation. QuestDB has proven itself to be lightning fast, fast enough to help us when providing raw live intraday trading data through our set of APIs within a few milliseconds.\"Renan AvilaÂ·BTG PactualDirector\nFinancial TechnologyAI for trading insightsAward winning Financial AI analytics platform helps investors make better trading decisions. Achieved 10x faster queries with 75% less hardware after migrating from InfluxDB.Resource optimization:4x reduction in hardware requirements with better performance\"We switched from InfluxDB to QuestDB to get queries that are on average 30x faster utilizing 1/4 of the hardware, without ever overtaxing our servers.\"Armenak MayalianÂ·ReflexivityCTORead Reflexivity's case studyâ†’\nThe next generation has arrivedContact us for a demo\nHyper ingestion, millisecond queries, and powerful SQL.\nLower bills through peak efficiency.\nContact Us\n$\n/$",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 744,
    "metadata": {
      "relevance_score": 0.7142857142857143,
      "priority_keywords_matched": [
        "performance",
        "capital markets",
        "trading",
        "financial",
        "enterprise"
      ]
    }
  },
  {
    "id": "questdb-blog-93349a1404ed",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/questdb-9-2-release",
    "title": "QuestDB 9.2: Exact arithmetic and smarter temporal joins | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nWith QuestDB 9.2, you get native\ndecimals\nfor exact arithmetic and a faster engine for temporal joins.\nThis release introduces a new\nDECIMAL\ntype for precise numeric work, a\nDense ASOF JOINalgorithm\nfor workloads with long distance matches, and makes\nsymbol capacity auto scaling\nthe default so\nhigh cardinality datasets work smoothly.\nIf 9.0 armed QuestDB with arrays and 9.1 added nanosecond timestamps and continuous profiling, 9.2\nis about getting your numbers and temporal joins exactly right.\nLet us dive in.\n\n## Exact arithmetic with DECIMAL\n\nFixed-point arithmetic is great for physics, not so great for ledgers.\nIf you have ever seen\n0.30000000000000004\nin a financial report, you know the problem. Floating\npoint types like\nDOUBLE\ntrade a bit of numerical accuracy for speed and range. That is usually\nfine, but not when you are reconciling accounts, PnL, or crypto balances.\nQuestDB 9.2 introduces a native\nDECIMAL(precision, scale)\ntype\nthat stores fixed point numbers\nwith\nexact arithmetic\n.\nFloating point vs DECIMAL\n\n```\n-- With DOUBLE:SELECT 0.1 + 0.2;\n-- With DECIMAL:SELECT 0.1m + 0.2m;\n```\n\n- The first query usesDOUBLEand can produce0.30000000000000004.\n- The second uses DECIMAL literals (themsuffix) and returns0.3exactly.\nNo hidden rounding, no surprises.\n\n### Precision and scale, without thinking about storage\n\nDECIMAL\nfollows the classic SQL model:\n- precisionis the total number of significant digits.\n- scaleis how many digits live after the decimal point.\nFor example,\nDECIMAL(14, 2)\ngives you up to 999,999,999,999.99, which is usually enough for\nmonetary amounts in a single currency.\nDefining financial columns with DECIMAL\n\n```\nCREATE TABLE transactions (  id         LONG,  account_id SYMBOL,  amount     DECIMAL(14, 2),  -- up to 999,999,999,999.99  quantity   DECIMAL(10, 3),  -- up to 9,999,999.999  ts         TIMESTAMP) timestamp(ts);\n```\n\nUnder the hood, QuestDB chooses the smallest suitable storage size for you, from compact one byte\ndecimals up to 32 byte decimal256 values. You do not have to think about which internal type to use.\nYou get:\n- Precision up to76 digits.\n- Storage that scales with precision instead of a one size fits all type.\n- Strict error handling. Operations that would silently lose precision with floating point will\nfail instead of rounding behind your back.\nTIP\nIf you omit precision and scale, QuestDB uses sensible defaults, but for financial workloads it is\nusually better to be explicit and document intent.\n\n### Decimal literals in queries\n\nDecimal literals use the\nm\nsuffix to distinguish them from floating point:\nDecimal literal syntax\n\n```\nSELECT  123.45m      AS price,  0.0875m      AS rate,  1000000.00m  AS notional;\n```\n\nWithout the suffix, the same numbers are treated as\nDOUBLE\n.\nThis makes it clear which parts of your query must be exact, for example:\n- Regulatory ratios.\n- PnL ladders and VaR inputs.\n- Fees and pricing tiers.\nINFO\nExact arithmetic has a cost. DECIMAL is typically around 2x slower than DOUBLE for equivalent\noperations. Use it for columns where precision matters, and keep purely analytical or approximate\nmetrics on DOUBLE.\n\n### Real world use cases\n\nA few examples where DECIMAL simplifies life:\n- Portfolio PnL and reportingStore positions, prices, and PnL at the exact precision you need for portfolio level and\naccount level reports.\n- Ledgers and payoutsRepresent balances and payouts directly in their natural unit without integer scaling tricks\nor helper columns.\n- Crypto balancesMany tokens use high precision with many fractional digits. With DECIMAL you can store them\ndirectly with the correct number of fractional digits.\n- Scientific and industrial measurementsSensor values that need exact rounding behavior, such as lab measurements or billing based on\nmetered usage.\nCombined with\nTIMESTAMP_NS\nfrom 9.1, you can now have both\nexact amounts\nand\nnanosecond\naccurate timestamps\nin the same database.\n\n## Dense ASOF JOIN for long distance matches\n\nASOF JOIN\nis very common in time series analytics. It lets you match each row from one table\nwith the most recent row from another table at or before a given timestamp.\nIn less formal terms: give me the latest known value when this event happened.\nQuestDB already provided\nseveral ASOF JOIN strategies\nthat the optimizer can pick from. In 9.2\nwe add a new\nDense\nalgorithm for workloads where the matching rows are often far apart in time.\n\n### The problem: distant matches and rescans\n\nConsider a classic market data example:\n- Anorderstable with individual orders.\n- A market data table with quotes or book snapshots.\nFor liquid symbols, orders and market data are interleaved tightly. For illiquid products, the\nnext matching quote for an order may be thousands or millions of rows away.\nPrevious strategies were heavily optimized for nearby matches. In datasets with sparse matches,\nthe engine sometimes had to rescan portions of the right hand table many times, which wastes work.\n\n### The Dense algorithm in plain English\n\nThe new Dense strategy behaves like a hybrid:\n- It uses a search to jump close to the correct region in the right hand side.\n- It then scans forward like the Light algorithm, taking advantage of temporal ordering.\n- It reuses previous matches to avoid redoing work when the left hand side walks through similar\ntimestamps.\nYou do not have to think about the details most of the time. QuestDB chooses an\nASOF JOIN\nstrategy based on your query and the data distribution.\nWhen your workload has:\n- Very frequent rows on the left hand side, and\n- Sparse, distant matches on the right hand side,\nthe Dense algorithm can be significantly faster than previous strategies.\n\n### Example: dense ASOF JOIN hint\n\nHere is a minimal example that uses the dense\nASOF JOIN\nstrategy explicitly:\nDense ASOF JOIN on orders and market data\n\n```\nSELECT /*+ asof_dense(orders md) */    orders.timestamp,    orders.symbol,    orders.priceFROM ordersASOF JOIN (md)ON (symbol);\n```\n\nHints like this give you a way to lock in a known good strategy for specific queries.\n\n## Symbol auto scaling is now the default\n\nIn 9.1 we introduced\nsymbolcapacity auto scaling\nas an optional setting. This allowed symbol\ntables to grow dynamically as you ingested more distinct values, instead of being limited by the\ncapacity chosen at table creation time.\nIn 9.2, auto scaling is now enabled by default.\nThe practical effect is simple:\n- High cardinality workloads ingest reliably without hitting capacity limits.\n- You do not need to guess symbol capacity ahead of time for most tables.\n- Existing configuration options remain available if you want to override the behavior.\nIf you are indexing large numbers of unique device identifiers, order ids, or session tokens, you\nget the more robust behavior out of the box.\n\n## Engine robustness and under the hood improvements\n\nBeyond the headline features, QuestDB 9.2 contains a series of incremental improvements across\nthe core engine and SQL layer.\nHighlights include:\n- More resilient async jobs:\nEdge cases where background jobs collided with table drops or extreme system load are handled\nmore gracefully, so background processing remains stable even under stress.\n- Safer deduplication:\nA subtle corner case in deduplicate writes that could lead to corruption has been fixed. This\nimproves safety for high rate ingest workloads that rely on WAL and deduplication.\n- ASOF JOIN performance tweaks:\nDenseASOF JOINis not just an extra algorithm. Existing strategies also see refinements for\ndense interleaving of left and right hand rows.\n- Decimal plumbing and type system work:\nDecimal types are now fully wired into the engine, type tags, and numeric function behavior.\nYou can mix DECIMAL with other numeric types according to clear rules.\n- Graal CE JDK:\nThe runtime has been updated to use Graal Community Edition as the bundled JDK, preparing the\nground for future performance work.\nNone of these changes require configuration on your part, but they reduce surprises in long running\nsystems or under heavy workloads.\n\n## Roundup\n\nQuestDB 9.2 is a focused release that makes the database a better fit for precision heavy workloads\nand complex temporal analytics.\n- DECIMALgives you exact arithmetic with up to 76 digits of precision.\n- Dense ASOF JOINkeeps temporal joins fast even when matches are far apart.\n- Symbol auto scaling by defaultmakes high cardinality ingest safer without manual capacity\ntuning.\n- A host of smaller fixes and improvements make the engine more robust under stress.\nCombined with nanosecond timestamps, arrays, and materialized views from recent releases, QuestDB\ncontinues to evolve as a powerful platform for capital markets, industrial telemetry, and any\nworkload where time and precision matter.\nRead the full 9.2.0 release notes â†’\nReady to try it?\nðŸ‘‰\nDownload QuestDB 9.2\nJoin our\nSlack\nor\nDiscourse\ncommunities to share feedback and\nresults. We are excited to see what you build with native decimals and smarter\nASOF joins.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1468,
    "metadata": {
      "relevance_score": 0.7142857142857143,
      "priority_keywords_matched": [
        "performance",
        "capital markets",
        "trading",
        "financial",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-acee867ac33f",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2022/01/27/release-sql-jit-compiler",
    "title": "QuestDB 6.2 January release, SQL JIT compiler | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nWe've just published 6.2 and it includes a lot of changes, such as SQL JIT\ncompiler, JDK 17 support, SQL and ILP improvements, settings to improve the\nmemory footprint when used with\nGrafana\n,\nautocomplete in the\nWeb Console\n, improved ILP stability,\nand more. Here's a roundup of changes that have just landed in the latest and\ngreatest version!\n\n## JDK 17 support\n\nQuestDB is now compatible with JDK 17, the latest long-term support (LTS) Java\nrelease. We also updated the binary distributions and the Docker image to use\nOpenJDK 17.\n\n## Just-in-Time compiler for SQL engine\n\nRelease 6.2 brings a brand new JIT (Just-in-Time) compiler as a part of the SQL\nengine. The compiler aims to significantly improve execution times for queries\nwith simple arithmetic expressions used to filter the data.\nTo give you an impression on the performance improvements, let's consider the\nfollowing query on the\ntrips\ntable that we use in our\nlive demo\n:\n\n```\nSELECT count(), max(total_amount), avg(total_amount)FROM tripsWHERE total_amount > 150 AND passenger_count = 1;\n```\n\nThe below image shows the execution time for this query with and without enabled\nJIT compiler\n:\nQuery execution times with JIT enabled and disabled\nThe SQL JIT compiler is a beta feature and is disabled by default. To enable it,\nyou should change the\ncairo.sql.jit.mode\nsetting in your\nserver.conf\nfile.\npath/to/server.conf\n\n```\ncairo.sql.jit.mode=on\n```\n\nWhen QuestDB starts with the enabled JIT compiler, the server logs contain\nmessages relating to\nSQL JIT compiler\nlike the following:\n\n```\n2021-12-16T09:25:34.472450Z A server-main SQL JIT compiler mode: on2021-12-16T09:25:34.472475Z A server-main Note: JIT compiler mode is a beta feature.\n```\n\nJIT compilation won't take place for any query you run. To learn when the\ncompilation took place for a query, you should check the server logs to contain\nsomething similar to this message:\n\n```\n2021-12-16T09:35:01.742777Z I i.q.g.SqlCodeGenerator JIT enabled for (sub)query [tableName=trips, fd=62]\n```\n\nFor more information on the JIT compiler, refer to this\nblog post\n.\n\n## New LATEST BY syntax and improvements\n\nThe database now supports a new syntax for LATEST BY clause:\n\n```\nSELECT * FROM tab WHERE x > 0LATEST ON timestamp PARTITION BY y;\n```\n\nThis syntax makes the LATEST BY clause consistent with the query execution order\nsince LATEST BY now must follow the WHERE clause. Release 6.2 also includes a\nnumber of fixes to make sure that the WHERE always gets applied before the\nLATEST BY. For more details on the new syntax, see the\nLATEST BY documentation\n.\n\n## Optimize LIMIT SQL queries\n\nRelease 6.2 includes a number of optimizations for queries with the LIMIT\nclause. The first group of optimizations takes place for queries with ORDER BY\nand LIMIT clause combination. As an example, prior to this release, the below\nquery on the\ntrips\ntable took around 18 seconds. With version 6.2, it takes\naround 0.2 seconds.\n\n```\nSELECT trip_distance FROM tripsORDER BY trip_distance DESC LIMIT 20;\n```\n\nThe second group of optimizations applies to queries that fetch the last N rows\nordered by the designated timestamp. The following query over a table with 100M\nrows took around 105 seconds. With this release, it takes around 1 millisecond.\n\n```\nSELECT * FROM my_tableORDER BY ts DESC LIMIT -100;\n```\n\n\n## Reduced ILP commit timeout\n\nPrior to 6.2, the data ingested through ILP protocol could be committed and thus\navailable to SQL queries after 30 seconds, if the volume of data is small. This\nwas an inconvenient default in a local development environment. From now on, the\ndefault timeout for ILP commit is set to 1 second. For more information on\nsetting this parameter, see the\nserver configuration documentation\n.\n/path/to/server.conf\n\n```\n# Default is 1 secline.tcp.commit.timeout=1000\n```\n\n\n## Lower memory footprint\n\nQuestDB 6.2 includes a number of improvements in query cache handling. The\ndatabase now makes sure to shrink the internal cached data structures upon query\nexecution. This should help with the problem of the overall memory consumption\ngoing up with time due to query caching.\nAnother problem reported by our users is that Grafana does not use prepared\nstatements when sending the queries and the built-in QuestDB's query cache\nbecomes much less efficient. To avoid unnecessary memory usage, we added new\nsettings that allow disabling the SELECT and INSERT query caches.\n/path/to/server.conf\n\n```\n# Default is truepg.select.cache.enabled=false# Default is truepg.insert.cache.enabled=false\n```\n\nWhen the database is used in combination with Grafana, it is recommended to\ndisable SELECT query cache by setting the property\npg.select.cache.enabled=false\n.\n\n## Table autocomplete in Web Console\n\nWeb Console's autocomplete feature now suggests the names of the existing\ntables.\nQuery execution time benchmark\n\n## ILP stability improvements\n\nWe've applied\nfuzz testing\nto our\nInflux Line Protocol implementation. As a result, a number of critical issues\naround various edge cases were found and fixed.\nFor instance, one of the edge cases could be seen in a scenario when the ILP\nrows keep adding new columns to the table. When this was happening, table\nreaders could read a mix of metadata values belonging to two subsequent\ntransactions. Our team did a great job to include a fix that makes sure that\ntable readers read the metadata atomically.\n\n## Simplified network configuration\n\nWe cleaned up all of the\nnetwork configuration\nsettings and made them more intuitive and consistent. For the sake of backward\ncompatibility, all old setting names are also supported. Still, we recommend our\nusers update the configurations to improve the overall developer experience.\n\n## Next up\n\nThe team will be adding\nUPDATE\nsupport in the next release, meanwhile, we're\nworking on replication, further JIT-compiled filter performance improvements,\nand more.\nWe hope you enjoyed the features and functionality in version 6.2. See the\nrelease notes on GitHub\nfor the complete list of additions and fixes. Weâ€™re eagerly awaiting your\nfeedback, so feel free to reach out and let us know how it's running. You can\nlet us know how we're doing or just come by and say hello\nin our community forums\nor browse the\nrepository\non GitHub\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1038,
    "metadata": {
      "relevance_score": 0.5714285714285714,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-b6eb11c652f1",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/olap-vs-time-series-databases-the-sql-perspective",
    "title": "OLAP vs Time-Series Databases: The SQL Perspective | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nWhen I started developing software, the database world was all about\nOLTP\nâ€“\nOnline Transactional Processing. These were the days of databases primarily\nhandling the\nCRUD operations\n: Create, Read,\nUpdate, and Delete.\nSQL\nwas king, universally used despite the subtle\nvariations across platforms.\nBack then, only large corporations could afford Online Analytical Processing\n(\nOLAP\n). They used expensive Data Warehouses to ingest and analyze historical\ndata, which was rarely updated and typically deleted in bulk. Since there was no\nstandard language for OLAP, each system often invented its own.\nBut things changed in the past 10 years. The database landscape exploded; every\nOLAP system adopted SQL as its query language, computation became much cheaper,\nand now\nalmost\nevery database is either open source or offers a generous\nfree-tier. So, today the question isn't whether my\nOLAP database\nsupports SQL or if I can afford it. Instead, it's about whether it supports my\nbusiness use case and how its performance stacks up relative to the money\ninvested in infrastructure.\nIn this article, I'll show you how SQL is used across various\nOLAP databases\nfor\ntime-series analytics. You'll see how some databases have adapted their\nSQL\nextensions\nspecifically for time-series scenarios. We're going to dive into\nquery comparisons in\nQuestDB, TimeScale, DuckDB, and ClickHouse\n, and Iâ€™m\nalso including\nPostgreSQL\nin the mix to offer a perspective on how it\ncompares to these more specialized databases.\n\n## Time-series queries in a nutshell\n\nTime-series analytics\ninvolve handling large data volumes with some typical patterns:\n- Recentindividualrows may provide specific insights, whereas older data is\ntypically more useful when aggregated to reveal broader trends.\n- Filtering bytime intervalsis a common requirement, and it's often\nnecessary to compare the same intervals over different date ranges.\n- Resamplingdata at various time resolutions is a frequent task. It's about\nreadjusting the timeframe of data, aligning points within the same or across\ndifferent tables, and addressing any data gaps.\nLet's see how some\ncommon time-series queries\ncan be executed on the different\ndatabases.\n\n## Latest Record Query\n\nImagine you're ingesting financial\ntick data\n, tracking\nvarious trading symbols across multiple markets and time zones. A typical\nchallenge is retrieving the\nmost recent row\nfor each symbol, a quintessential\ntime-series query.\nClick on the tabs below to see the difference across different database engines.\n- QuestDB\n- DuckDB\n- PostgreSQL, Timescale, and Clickhouse\n\n```\n-- QuestDB implements the LATEST ON ... PARTITION BY extension\nSELECT * FROM tradesLATEST ON timestamp PARTITION BY symbol, side;\n```\n\n\n```\n-- DuckDB does not have an extension for this, but we can use a window-- function and the QUALIFY filter\n  SELECT *,  row_number() OVER(    PARTITION BY symbol, side    ORDER BY timestamp DESC    ) as rownum  FROM trades  QUALIFY rownum=1;\n```\n\n\n```\n-- For this query, there are no extensions in either PostgreSQL, Timescale,-- or Clickhouse. We can use standard SQL with window functions and a CTE\nWITH numbered AS (  SELECT *,  row_number() OVER(    PARTITION BY symbol, side    ORDER BY timestamp DESC    ) as rownum  FROM trades)SELECT * from numbered WHERE rownum=1;\n```\n\nInterestingly, only QuestDB has\na specific SQL extension\nfor\nthis use case. In other databases, we can employ a Window Function to implement\na row counter for each symbol and side, filtering only the first one. An\nalternative might be using a\nGROUP BY\nclause with the\nfirst_value\naggregation function. However, this approach becomes less readable with more\ncolumns. Notably, the standard SQL queries consume more resources than QuestDB's\noptimized implementation, which fetches\nonly the most recent row\nfor each unique combination of provided columns.\n\n## Time-Interval Filtering\n\nA fundamental aspect of exploring\ntime-series data\nis applying\ntime filters\n.\nTypically, analysts start by examining a broad time interval and then\nprogressively focus on more granular timestampsâ€”or sometimes, they do the\nreverse, It's also often crucial to examine consistent time slices across\ndifferent date ranges. Let's see five queries around this topic.\n- QuestDB\n- PostgreSQL and Timescale\n- DuckDB and Clickhouse\n\n```\n-- QuestDB offers the IN extension for time-intervals\nSELECT * FROM trades WHERE timestamp in '2023'; -- whole yearSELECT * FROM trades WHERE timestamp in '2023-12'; -- whole monthSELECT * FROM trades WHERE timestamp in '2023-12-20'; -- whole day\n-- The whole day, extending 15s into the next daySELECT * FROM trades WHERE timestamp in '2023-12-20;15s';\n-- For the past 7 days, 2 seconds before and after midnightSELECT * from trades WHERE timestamp in '2023-09-20T23:59:58;4s;-1d;7'\n```\n\n\n```\n-- PostgreSQL and Timescale can use the EXTRACT function-- and then filter by parts of a date\n-- whole yearSELECT * FROM trades WHERE EXTRACT(YEAR FROM timestamp) = 2023;\n-- whole monthSELECT * FROM trades WHERE EXTRACT(YEAR FROM timestamp) = 2023 AND  EXTRACT(MONTH FROM timestamp) = 12;\n-- whole daySELECT * FROM trades WHERE EXTRACT(YEAR FROM timestamp) = 2023 AND  EXTRACT(MONTH FROM timestamp) = 12 AND  EXTRACT(DAY FROM timestamp) = 20;\n-- The whole day, extending 15s into the next daySELECT * FROM tradesWHERE timestamp between '2023-12-20' AND '2023-12-21T00:00:15';\n-- For the past 7 days, 2 seconds before and after midnightSELECT * FROM tradesWHERE timestamp between '2023-12-14' AND '2023-12-21T00:00:02' AND(  (  EXTRACT(HOUR FROM timestamp)=23 AND  EXTRACT(MINUTE FROM timestamp)=59 AND  EXTRACT(SECOND FROM timestamp)>=58  ) OR  (  EXTRACT(HOUR FROM timestamp)=0 AND  EXTRACT(MINUTE FROM timestamp)=0 AND  EXTRACT(SECOND FROM timestamp)<2  ));\n```\n\n\n```\n\n-- DuckDB and Clickhouse can use datetime functions-- and then filter by parts of a date\n-- whole yearSELECT * FROM trades WHERE year(timestamp)=2023;\n-- whole monthSELECT * FROM trades WHERE year(timestamp)=2023 AND  month(timestamp)=12;\n-- whole daySELECT * FROM trades WHERE year(timestamp)=2023 AND  month(timestamp)=12 AND day(timestamp)=20;\n-- The whole day, extending 15s into the next daySELECT * FROM tradesWHERE timestamp between '2023-12-20' AND '2023-12-21T00:00:15';\n-- For the past 7 days, 2 seconds before and after midnightSELECT * FROM tradesWHERE timestamp between '2023-12-14' AND '2023-12-21T00:00:02' AND(  (  hour(timestamp)=23 AND minute(timestamp)=59 AND second(timestamp)>=58  ) OR  (  hour(timestamp)=0 AND minute(timestamp)=0 AND second(timestamp)<2  ));\n```\n\nYou can see how having native extensions for time-range filters can simplify\nyour SQL. In databases without specific extensions, we\nmight have\nutilized\ntime_diff\nand\ninterval\nfunctions, but the resulting SQL would still be more\ncomplex than just using a specialized operator.\n\n## Joining Tables by Approximate Time\n\nIn analytics, while we often work with\ndenormalized tables\n, joining them with\nother tables can yield insightful results. Joining on common columns is\nstraightforward, but\njoining based on time\ncan be far more insightful.\nHowever, this is complicated by the fact that events seldom occur at precisely\nthe same microsecond. Issues like clock calibration discrepancies or network\nlatencies can lead to slightly different timestamps in different tables.\nMoreover, there might be scenarios where one table logs data every second, and\nanother does so every 15 minutes, yet you still need to join them to construct a\ncoherent snapshot\nof a specific moment in time.\nMost of the analytical databases we are examining support the\nASOF JOIN\n. This join type,\nfor a given row in one table, finds a matching row in another table that\noccurred at the same exact moment or, if not available, the closest preceding\nentry. For databases lacking native\nASOF JOIN\nsupport, a similar result can be\nachieved using a\nlateral join\n.\n- QuestDB\n- PostgreSQL and Timescale\n- DuckDB\n- Clickhouse\n\n```\n-- QuestDB provides ASOF JOIN. Since table definition includes-- the designated timestamp column, no time condition is needed\nSELECT t.*, n.*FROM trades ASOF JOIN news ON (symbol);\n```\n\n\n```\n-- Postgresql and timescale are the only ones with no ASOF JOIN support,-- but we can use a LEFT JOIN LATERAL and filter by time using DISTINCT\nSELECT t.*, n.*FROM trades t LEFT JOIN LATERAL ( SELECT DISTINCT ON (news.symbol) *    FROM news n1    WHERE n1.symbol = t.symbol    AND n1.timestamp <= news.timestamp    ORDER BY n1.symbol, n1.timestamp DESC LIMIT 1) n\n```\n\n\n```\n-- DuckDB provides the ASOF JOIN extension\nSELECT t.*, n.*FROM trades t ASOF JOIN news n  ON t.symbol = n.symbol AND t.timestamp >= n.timestamp;\n```\n\n\n```\n-- Clickhouse provides the ASOF JOIN extension\nSELECT t.*, n.*FROM trades t ASOF JOIN news n  ON t.symbol = n.symbol AND n.timestamp <= t.timestamp;\n```\n\nIn this scenario,\nQuestDB, DuckDB, and Clickhouse\nstand out for their\ndeveloper-friendliness, extending SQL to accommodate this common requirement.\nHowever, there's a nuance with Clickhouse: its\nASOF JOIN\ndoes not allow using\nthe timestamp column as the sole condition. This limitation works fine for our\nexample but\nmight pose challenges\nif you need to match all rows in one table\nwith those in another based solely on\napproximate timestamps\n.\n\n## Time Interval Grouping/Downsampling\n\nRunning aggregations over specific time intervals is another fundamental query\ntype in time-series analytics. For instance, analysts might ask:\n- How many rows are there per second?\n- Are there any trends in trading volume per minute over the last few hours?\n- What's the average order amount per country and category on a monthly basis\nover the last year?\nLet's calculate the total price and volume in our trading table at 15 minutes\nintervals.\n- QuestDB\n- PostgreSQL\n- Timescale\n- DuckDB\n- Clickhouse\n\n```\n-- QuestDB implements the SAMPLE BY extension-- which accepts granularity from years to microseconds.-- GROUP BY and ORDER BY are implicit and not needed\nSELECT   timestamp, symbol,   sum(price) AS price,   sum(amount) AS volumeFROM tradesSAMPLE BY 15m;\n```\n\n\n```\n-- Postgresql supports date_bin, allowing a maximum of 1 month-- time buckets. An alternative should be used for longer intervals-- then we GROUP BY and ORDER BY\nSELECT  date_bin(    '15 minutes', timestamp, date_trunc('day',timestamp)    ) AS time_bucket,  symbol,  sum(price) AS price,  sum(amount) AS volumeFROM tradesGROUP BY timeORDER BY time_bucket\n```\n\n\n```\n-- Timescale supports time_bucket, then GROUP BY and ORDER\nSELECT time_bucket('15 minute', timestamp) AS bucket,    symbol,    sum(price) AS price,    sum(amount) AS volumeFROM tradesGROUP BY bucket, symbolORDER BY bucket ASC;\n```\n\n\n```\n-- DuckDB supports time_bucket, then GROUP BY and ORDER\nSELECT time_bucket(Interval '15 minutes', timestamp) AS bucket,    symbol,    sum(price) AS price,    sum(amount) AS volumeFROM tradesGROUP BY bucket, symbolORDER BY bucket ASC;\n```\n\n\n```\n-- Clickhouse supports toStartOfInterval, then GROUP BY and ORDER\nSELECT    toStartOfInterval(timestamp, INTERVAL 15 MINUTE) AS i,    symbol,    sum(price) AS price,    sum(amount) AS volumeFROM tradesGROUP BY i, symbolORDER BY i ASC;\n```\n\nAll databases include some support for classifying a timestamp into a time\nbucket. Most of them then use the standard\nGROUP BY\nand\nORDER BY\nso you can\nexecute aggregations. One thing I like about the QuestDB\nSAMPLE BYextension\nis\nthat grouping and sorting by timestamp are implicit, making my SQL a bit easier\nto write.\n\n## Time Interval Downsampling with linear interpolation\n\nWouldn't life be great if our data always arrived at regular predictable\nintervals? Reality often presents us with\nirregular time intervals\n, complete\nwith gaps In such cases, it's crucial not only to identify these gaps but also\nto consider interpolating results at these points using\nlinear interpolation\n.\nThis approach ensures a uniform dataset without gaps, which is critical when\ntraining machine learning models or displaying results on a business dashboard.\n- QuestDB\n- PostgreSQL\n- Timescale\n- DuckDB\n- Clickhouse\n\n```\n-- QuestDB SAMPLE BY accepts FILL with different strategies,-- including LINEAR, PREVious row value, NULL, or Literal value\nSELECTtimestamp,sum(price) AS price,sum(amount) AS volumeFROM tradesSAMPLE BY 1s FILL(LINEAR);\n```\n\n\n```\n-- Postgresql. No support for interpolation, so we use window functions.-- We first generate a virtual table with all the intervals we want-- in the result and LEFT JOIN with our table. Now we need to find-- previous and next values, so we can finally interpolate\nWITH time_series AS (    SELECT generate_series(        min(date_trunc('day', timestamp)),        max(timestamp),        interval '1 seconds'    ) AS time_bucket    FROM trades),aggregated_data AS (    SELECT        date_bin(          '1 seconds', timestamp, date_trunc('day', timestamp)          ) AS time_bucket,        symbol,        sum(price) AS price,        sum(amount) AS volume    FROM trades    GROUP BY time_bucket, symbol)SELECT    ts.time_bucket,    ad.symbol,    COALESCE(        ad.price,        last_value(ad.price) OVER w        + (next_value(ad.price) OVER w - last_value(ad.price) OVER w)        * EXTRACT(          epoch FROM ts.time_bucket - last_value(ts.time_bucket) OVER w          )        / NULLIF(          EXTRACT(            epoch FROM next_value(ts.time_bucket) OVER w              - last_value(ts.time_bucket) OVER w            ), 0            )    ) AS interpolated_price,    COALESCE(        ad.volume,        last_value(ad.volume) OVER w        + (next_value(ad.volume) OVER w - last_value(ad.volume) OVER w)        * EXTRACT(          epoch FROM ts.time_bucket - last_value(ts.time_bucket) OVER w          )        / NULLIF(          EXTRACT(            epoch FROM next_value(ts.time_bucket) OVER w              - last_value(ts.time_bucket) OVER w              ), 0              )    ) AS interpolated_volumeFROM time_series tsLEFT JOIN aggregated_data ad ON ts.time_bucket = ad.time_bucketWINDOW w AS (    PARTITION BY ad.symbol    ORDER BY ts.time_bucket    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)ORDER BY ts.time_bucket, ad.symbol;\n```\n\n\n```\n-- Timescale offers the time_bucket_gapfill function which,-- together with interpolate, allows for different gap filling-- strategies\nSELECT time_bucket_gapfill('1 second', timestamp) AS bucket,    interpolate(sum(price)) AS price,    interpolate(sum(amount)) AS volume,FROM tradesGROUP BY bucket, symbolORDER BY bucket ASC;\n```\n\n\n```\n-- Duckdb. No support for interpolation, so we need to use window functions.-- We first generate a virtual table with all the intervals we want-- in the result and LEFT JOIN with our table. Now we need to find-- previous and next values, so we can finally interpolate\nWITH time_series AS (    SELECT unnest(generate_series(        min(date_trunc('second',timestamp)),        max(timestamp),        interval '1 seconds'    )) AS time_bucket    FROM trades),aggregated_data AS (    SELECT time_bucket(Interval '1 seconds', timestamp) AS bucket,    symbol,    sum(price) AS price,    sum(amount) AS volume    FROM trades    GROUP BY bucket, symbol    ORDER BY bucket ASC)SELECT    ts.time_bucket,    ad.symbol,    price,    volume,    COALESCE(        ad.price,        last_value(ad.price) OVER w        + (first_value(ad.price) OVER w - last_value(ad.price) OVER w)        * EXTRACT(          epoch FROM ts.time_bucket - last_value(ts.time_bucket) OVER w          )        / NULLIF(          EXTRACT(            epoch FROM first_value(ts.time_bucket) OVER w              - last_value(ts.time_bucket) OVER w            ), 0            )    ) AS interpolated_price,    COALESCE(        ad.volume,        last_value(ad.volume) OVER w        + (first_value(ad.volume) OVER w - last_value(ad.volume) OVER w)        * EXTRACT(          epoch FROM ts.time_bucket - last_value(ts.time_bucket) OVER w          )        / NULLIF(          EXTRACT(            epoch FROM first_value(ts.time_bucket) OVER w              - last_value(ts.time_bucket) OVER w            ), 0            )    ) AS interpolated_volumeFROM time_series tsLEFT JOIN aggregated_data ad ON ts.time_bucket = ad.bucketWINDOW w AS (    PARTITION BY symbol    ORDER BY time_bucket)ORDER BY ts.time_bucket, ad.symbol;\n```\n\n\n```\n-- Clickhouse. It supports the WITH FILL STEP extension, but-- interpolation is based on last row value only, so no-- easy method for linear interpolation.-- We first fill rows, then use window functions to get results similar-- to linear interpolation. We could make the query more complex to get-- better results, but this is probably enough for this post comparison\nWITH interpolated AS (    SELECT    toStartOfInterval(timestamp, INTERVAL 15 SECOND) AS i,    symbol,    sum(price) AS price,    sum(amount) AS volumeFROM tradesGROUP BY i, symbolORDER BY i ASCWITH FILL STEP Interval 1 SECOND INTERPOLATE(symbol)), prev_and_next AS (  SELECT i, symbol, price, volume,  last_value(volume) OVER prev_w as prev_vol,  first_value(volume) OVER next_w as next_vol,  last_value(price) OVER prev_w as prev_price,  first_value(price) OVER next_w as next_price  FROM interpolated  WINDOW prev_w AS (    PARTITION BY symbol ORDER BY i    ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING    ),    next_w AS (    PARTITION BY symbol ORDER BY i    ROWS BETWEEN 1 FOLLOWING AND UNBOUNDED FOLLOWING    ))SELECT i, symbol,      coalesce(price, (prev_price + next_price)/2) as price,      coalesce(volume,(prev_vol+next_vol)/2) as volumeFROM prev_and_next;\n```\n\nQuestDB and Timescale\nboth offer\nextensions for linear interpolation\n,\nleading to queries that are quite similar to those in the previous sectionâ€”a\nconvenient consistency. On the other hand,\nDuckDB, Clickhouse, and PostgreSQL,\nnot being specifically designed for time-series\n, lack such extensions. For\nPostgreSQL and DuckDB, one must first create a time-series virtual table, then\nperform a join, and use Window Functions for the interpolation calculations.\nClickhouse does provide\nsome native interpolation\ncapabilities, but these are\nlimited to values based on the previous row only, necessitating the use of\nWindow Functions for more complex calculations.\n\n## Conclusion\n\nAs we have explored, SQL is not a one-size-fits-all language; its effectiveness\ncan vary significantly depending on the database and its specific extensions. A\ndatabase like QuestDB, with tailored SQL extensions for particular use cases,\nnotably improves the\ndeveloper experience\nby reducing friction. This ease of\nuse translates into greater productivity, less frustration, and more time for\nanalysts and developers to engage in interactive exploration. Such an\nenvironment fosters deeper insights and, ultimately, can lead to more informed\nbusiness decisions.\nMoreover, QuestDB's SQL extensions are not just about syntax convenience; they\nare backed by\noptimizations\nspecifically designed for the unique\ncharacteristics and volumes of\ntime-series data\n. This focus on efficiency is\na key factor in QuestDB's consistent top performance in\nindustry benchmarks\n.\n\n## Closing Notes\n\nFor those already familiar with SQL, adapting to\nQuestDB's extensions\nshould\nbe straightforward, especially once you grasp the fundamental concepts\nunderpinning QuestDB. If you're new to SQL and eager to dive into time-series\nanalytics, starting with QuestDB offers a smoother learning curve. You'll spend\nless time grappling with complex syntax and more time acquiring practical\nskills.\nYou can try QuestDB's SQL extensions at our\npublic demo\nor downloading\nQuestDB Open Source\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2811,
    "metadata": {
      "relevance_score": 0.5714285714285714,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "trading",
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-52d3ea0f8d90",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/influxdb-vs-questdb-comparison",
    "title": "Benchmark and comparison: QuestDB vs. InfluxDB v1/v2  | QuestDB",
    "text": "This article compares QuestDB and InfluxDB on performance, architecture, and ease of use. Last updated December 2, 2025 with benchmarks for InfluxDB v1.11, v2.7.12, and QuestDB 9.2.2.\nFor InfluxDB 3 Core benchmarks, seeour latest post\n.\nKey results:\nQuestDB ingests data up to\n36x\nfaster than InfluxDB (11.36M rows/sec vs 203K rows/sec at 1M hosts), and runs analytical queries\n23x to 130x\nfaster, with heavy aggregations\n17x\nfaster. InfluxDB is marginally faster (~2x) on two specific simple aggregation queries.\n\n## Introduction to InfluxDB and QuestDB\n\nQuestDB\n(released 2019) is an open-source\ntime-series database\nlicensed under Apache License 2.0. Written in low-latency (Zero-GC) Java and C++, it is designed for low-latency, high-throughput ingestion and fast analytical queries using standard SQL. QuestDB uses a columnar storage model where all time series live in a single table structure, avoiding per-series overhead.\nInfluxDB\n(released 2013) is a time-series database developed by InfluxData. The open-source versions (v1 under MIT, v2 OSS under proprietary license) are written in Go. InfluxDB uses a measurement-based data model where each unique combination of tags creates a separate series with its own storage structure.\n\n| Aspect | QuestDB | InfluxDB |\n| --- | --- | --- |\n| License | Apache 2.0 | v1: MIT, v2 OSS: Proprietary |\n| Implementation | Java, C++ | Go |\n| Query language | Standard SQL | InfluxQL, Flux |\n| Data model | Relational (tables + rows) | Measurement-based (series) |\n| Ingestion protocols | ILP, PostgreSQL wire, HTTP | ILP, HTTP API |\n| High cardinality | No performance impact | Performance degrades |\n\n\n## Performance benchmarks\n\nWe use the open-source, industry-standard\nTime Series Benchmark Suite (TSBS)\nfor all benchmarks, which supports InfluxDB (v1 and v2) and QuestDB out of the box.\nHardware:\nAWS EC2 r8a.8xlarge (32 vCPU, 256 GB RAM, AMD EPYC), GP3 EBS storage (20,000 IOPS, 1 GB/s throughput)\nSoftware:\nUbuntu 22.04, InfluxDB v1.11, InfluxDB v2.7.12, QuestDB 9.2.2 â€” all with default configurations\n\n### Ingestion benchmark\n\nWe test a\ncpu-only\nscenario with two days of CPU data for various numbers of simulated hosts (100, 1K, 4K, 100K, and 1M). This tests how each database handles increasing data volumes and cardinality.\nIn time-series databases,\nhigh cardinality\nrefers to having many unique values in indexed columnsâ€”for example, millions of unique symbols, account IDs, or trading venues. More hosts in this benchmark means higher cardinality.\nExample commands:\n\n```\n$ ./tsbs_generate_data --use-case=\"cpu-only\" --seed=123 --scale=4000 \\    --timestamp-start=\"2016-01-01T00:00:00Z\" \\    --timestamp-end=\"2016-01-03T00:00:00Z\" \\    --log-interval=\"10s\" --format=\"influx\" > /tmp/influx_data\n$ ./tsbs_load_influx --db-name=benchmark --file=/tmp/influx_data \\    --urls=http://localhost:8086 --workers=32\n```\n\nThe results for ingestion with 32 workers:\nâ†‘ Higher is better\n36x\nfaster than InfluxDB v1\n11.36M\nrows/sec peak\n30x\nfaster than InfluxDB v2\n\n| Scale | InfluxDB v1.11 | InfluxDB v2 | QuestDB | QuestDB vs v1 | QuestDB vs v2 |\n| --- | --- | --- | --- | --- | --- |\n| 100 hosts (1.7M rows) | 1.23M rows/sec | 727K rows/sec | 4.02M rows/sec | 3.3x faster | 5.5x faster |\n| 1,000 hosts (17M rows) | 1.17M rows/sec | 667K rows/sec | 7.48M rows/sec | 6.4x faster | 11.2x faster |\n| 4,000 hosts (69M rows) | 787K rows/sec | 514K rows/sec | 8.39M rows/sec | 10.7x faster | 16.3x faster |\n| 100,000 hosts (86M rows) | 491K rows/sec | 402K rows/sec | 11.36M rows/sec | 23x faster | 28x faster |\n| 1,000,000 hosts (432M rows) | ~203K rows/sec | 241K rows/sec | 7.33M rows/sec | 36x faster | 30x faster |\n\nKey observations:\n- QuestDB is3.3x to 36xfaster than InfluxDB, with the gap widening at scale\n- QuestDB peaks at11.36M rows/secat 100K hosts, maintaining high throughput even at 1M hosts\n- InfluxDB throughput degrades significantly as cardinality increases (both v1 and v2)\n\n#### Why does cardinality affect InfluxDB?\n\nThe performance gap widens with scale because of how each database handles cardinality. InfluxDB creates a separate TSM (Time-Structured Merge) tree for each unique series. At 100,000 hosts with 10 metrics each, that's 1,000,000 separate storage structures to maintain, index, and compactâ€”explaining the throughput degradation.\nQuestDB stores all data in a single columnar table regardless of cardinality. Adding more hosts simply adds more rows to the same structure. Throughput starts at ~8M rows/sec for 1K-4K hosts, then peaks at 11.36M rows/sec at 100K hosts as parallelism is fully utilized, before settling at 7.3M rows/sec at 1M hosts due to memory pressure at extreme scaleâ€”still maintaining strong performance throughout.\n\n### Query performance\n\nWhile QuestDB outperforms InfluxDB for ingestion, query performance is equally\nessential for\ntime-series data\nanalysis.\nAs part of the standard TSBS benchmark, we test several types of popular time\nseries queries:\n- single-groupby: Aggregate CPU metrics for random hosts over specified time ranges\n- double-groupby: Aggregate across ALL hosts, grouped by host and time intervals\n- high-cpu-all: Full table scan finding hosts with CPU utilization above threshold\nAll queries target two days of 4000 emulated host data.\nTo run the benchmark:\n\n```\n$ ./tsbs_generate_queries --use-case=\"devops\" --seed=123 --scale=4000 \\    --timestamp-start=\"2016-01-01T00:00:00Z\" \\    --timestamp-end=\"2016-01-03T00:00:00Z\" \\    --queries=1000 --query-type=\"single-groupby-1-1-1\" \\    --format=\"influx\" > /tmp/influx_query\n$ ./tsbs_run_queries_influx --file=/tmp/influx_query \\    --db-name=benchmark --workers=1\n```\n\n\n#### Single-groupby queries\n\nâ†“ Lower is better\nQuery format: metrics-hosts-hours Â· averaged over 10 runs\nQuestDB\nis faster as complexity grows\n4.2x faster\non 5-1-12\n\n| Query | InfluxDB v1.11 | InfluxDB v2.7.12 | QuestDB | Best |\n| --- | --- | --- | --- | --- |\n| single-groupby-1-1-1 | 0.42 ms | 0.73 ms | 1.06 ms | InfluxDB v1 |\n| single-groupby-1-1-12 | 2.30 ms | 3.37 ms | 1.68 ms | QuestDB |\n| single-groupby-1-8-1 | 1.00 ms | 1.63 ms | 1.39 ms | InfluxDB v1 |\n| single-groupby-5-1-1 | 1.09 ms | 1.68 ms | 0.99 ms | QuestDB |\n| single-groupby-5-1-12 | 8.40 ms | 12.24 ms | 1.98 ms | QuestDB |\n| single-groupby-5-8-1 | 3.23 ms | 4.34 ms | 1.54 ms | QuestDB |\n\n\n#### Double-groupby queries\n\nâ†“ Lower is better\nAggregates across ALL hosts, grouped by host and 1-hour intervals Â· averaged over 100 runs\n21-130x\nfaster than InfluxDB\n40-58ms\nQuestDB latency\n0.9-7.5s\nInfluxDB latency\nThese queries aggregate across ALL hosts, grouped by host and 1-hour intervals.\n\n| Query | InfluxDB v1.11 | InfluxDB v2.7.12 | QuestDB | QuestDB vs v1 | QuestDB vs v2 |\n| --- | --- | --- | --- | --- | --- |\n| double-groupby-1 | 853 ms | 935 ms | 40 ms | 21x faster | 23x faster |\n| double-groupby-5 | 3,595 ms | 3,875 ms | 46 ms | 78x faster | 84x faster |\n| double-groupby-all | 6,967 ms | 7,516 ms | 58 ms | 120x faster | 130x faster |\n\n\n#### Heavy queries\n\nhigh-cpu-all query latency Â· â†“ Lower is better\nFull table scan finding hosts with CPU utilization above threshold Â· averaged over 10 runs\n16-17x\nfaster than InfluxDB\n<1s\nQuestDB response\n~16s\nInfluxDB response\nFull table scan finding hosts with CPU utilization above threshold.\n\n| Query | InfluxDB v1.11 | InfluxDB v2.7.12 | QuestDB | QuestDB vs v1 | QuestDB vs v2 |\n| --- | --- | --- | --- | --- | --- |\n| high-cpu-all | 16,045 ms | 16,655 ms | 994 ms | 16x faster | 17x faster |\n\n\n### Explaining query performance\n\nKey finding: QuestDB outperforms both InfluxDB versions\non analytical queries,\ndelivering\n21x to 130x\nfaster results on aggregations across time and hosts. InfluxDB\nv1 shows a marginal edge on two simple aggregation queries, but\nQuestDB dominates on complex workloads where real analytical value lies.\nLet's examine specific query patterns:\n\n#### Double group by queries\n\nAggregate across both time and host.\nQuestDB is\n21x to 130x\nfaster than InfluxDB v1, and\n23x to 130x\nfaster than InfluxDB v2. This is\nwhere QuestDB truly shines. The engine scans all rows within the interval and\naggregates them using multiple threads, parallel execution, and SIMD instructions.\n\n#### Single group by queries\n\nSimple aggregation on metrics for specific hosts over time ranges.\nFor simple aggregation queries on a single host (1-1-1), InfluxDB v1 is ~2.5x faster.\nHowever, as query complexity increases (more metrics, longer time ranges),\nQuestDB takes the lead:\n- 5-1-12 (5 metrics, 12 hours): QuestDB is4.2xfaster than InfluxDB v1,6.2xfaster than InfluxDB v2\n- 5-8-1 (5 metrics, 8 hosts): QuestDB is2.1xfaster than InfluxDB v1,2.8xfaster than InfluxDB v2\n\n#### Heavy analytical queries (high-cpu-all)\n\nFull table scan finding hosts above CPU threshold.\nQuestDB is\n16x to 17x\nfaster than both InfluxDB versions. This query type\ndemonstrates QuestDB's strength in analytical workloads that scan large amounts\nof data.\n\n#### Why these differences?\n\nQuestDB keeps all time series in a single dense table with columnar storage.\nFor queries accessing a single time series, it must filter rows on access.\nInfluxDB stores each time series separately, giving it an advantage for\nsingle-series lookups.\nHowever, for analytical queries spanning multiple series, QuestDB's columnar\nlayout combined with SIMD instructions and multi-threaded processing provides\ndramatically better performance.\n\n## What are the data models used in InfluxDB and QuestDB?\n\nThe data model is fundamental to understanding why these databases perform differently. InfluxDB uses a measurement-based model optimized for tagged time series, while QuestDB uses a relational model that stores all data in tables.\n\n### InfluxDB: Measurement-based model\n\nInfluxDB organizes data around\nmeasurements\n,\ntags\n, and\nfields\n:\n\n```\nmeasurementName,tagKey=tagValue fieldKey=\"fieldValue\" 1465839830100399000--------------- --------------- --------------------- -------------------       |               |                  |                    |  Measurement         Tags              Fields             Timestamp\n```\n\n- Measurement: Similar to a table name, groups related data points\n- Tags: Indexed key-value pairs (strings only) used for filtering and grouping\n- Fields: Non-indexed values containing the actual metrics (floats, integers, strings, booleans)\n- Timestamp: Nanosecond-precision time\nA\nseries\nin InfluxDB is defined as a unique combination of measurement + tagset. For example:\n\n```\ntrades,symbol=AAPL,exchange=NYSE price=185.50,size=100 1705311000123456000trades,symbol=MSFT,exchange=NASDAQ price=390.25,size=250 1705311000123789000\n```\n\nThese two lines create two separate series because their tagsets differ. Each unique series gets its own TSM storage structure. This is why high-cardinality workloads (many unique tag combinations) degrade InfluxDB performanceâ€”thousands of unique\nsymbol\nvalues means thousands of separate series to maintain.\n\n### QuestDB: Relational model\n\nQuestDB uses a standard relational model where data lives in\ntables with typed columns\n:\n\n```\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,          -- Dictionary string (similar to InfluxDB tags)  exchange SYMBOL,  side SYMBOL,  price DOUBLE,  size DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nMarket data in QuestDB is simply rows in a table:\n\n| timestamp | symbol | exchange | side | price | size |\n| --- | --- | --- | --- | --- | --- |\n| 2024-01-15T09:30:00.123456Z | AAPL | NYSE | buy | 185.50 | 100 |\n| 2024-01-15T09:30:00.123789Z | MSFT | NASDAQ | sell | 390.25 | 250 |\n\nAdding new symbols or exchanges doesn't create new storage structuresâ€”it just adds more rows. This is why QuestDB handles high cardinality without performance degradation.\nData type support:\n\n| Category | QuestDB Types |\n| --- | --- |\n| Integer | BYTE, SHORT, INT, LONG, LONG128, LONG256 |\n| Floating point | FLOAT, DOUBLE, DECIMAL |\n| String | STRING, VARCHAR, CHAR, SYMBOL (indexed) |\n| Temporal | TIMESTAMP (nanosecond precision), DATE, INTERVAL |\n| Geospatial | GEOHASH |\n| Collections | ARRAY |\n| Other | BOOLEAN, UUID, IPv4, BINARY |\n\nQuestDB supports\nInfluxDB line protocol\nfor compatibility, automatically mapping tags to SYMBOL columns and fields to appropriate types. QuestDB also extends the protocol with binary support for advanced types like arrays. For full control over schema and types, use the PostgreSQL wire protocol or REST API.\n\n### Key model differences\n\n\n| Aspect | QuestDB | InfluxDB |\n| --- | --- | --- |\n| Data organization | Tables + rows | Measurements + series |\n| Tag handling | SYMBOL columns (indexed strings) | Creates separate series per tagset |\n| High cardinality | No impact (just more rows) | Performance degrades (more series = overhead) |\n| Query language | Standard SQL | InfluxQL / Flux |\n| JOINs | Full SQL JOIN support | Not supported |\n| Schema | Schema-on-write or predefined | Schema-on-write |\n\n\n## Comparing database storage models\n\n\n### InfluxDB: TSM Trees\n\nFor storage, InfluxDB uses Time-Structured Merge (TSM) Trees, an LSM-tree variant optimized for time-series data. Writes first go to a write-ahead log (WAL) for durability, then into an in-memory cache that serves fast reads of recent data. When the cache fills, data is flushed to immutable TSM files on disk. Background compaction continuously merges smaller TSM files into larger ones to improve read efficiency, though this adds write amplification overhead.\nCritically, each unique series (measurement + tagset combination) creates its own TSM structure. This per-series architecture explains why high-cardinality workloads degrade InfluxDB's performanceâ€”more unique series means more TSM structures to maintain, index, and compact.\nInfluxDB has a shard group concept as a\npartitioning\nstrategy, allowing for grouping\ndata by time. Users can provide a\nshard group duration\nwhich defines how large a shard will be and can enable common operations such as\nretention periods for data (deleting data older than X days, for example):\n\n### QuestDB: Three-tier columnar storage\n\nQuestDB implements a three-tier storage architecture optimized for both high-throughput ingestion and fast analytical queries:\nTier 1 - Write-Ahead Log (WAL):\nIncoming writes first land in a write-ahead log, providing durability guarantees. The WAL handles out-of-order data by buffering and sorting before committing to the main storage layer. This design allows QuestDB to sustain millions of rows per second while maintaining data integrity.\nTier 2 - Columnar partitions:\nData is organized into time-partitioned columnar files optimized for query performance. Each partition is a directory containing separate files per column, enabling efficient compression and allowing queries to read only the columns they need. This columnar layout combined with time partitioning enables parallel scans with SIMD instructions across multiple CPU cores.\nTier 3 - Cold storage (Parquet):\nOlder partitions can be converted to Parquet format and moved to object storage (S3, Azure Blob, GCS) or local cold storage. This tiered approach keeps frequently queried data on fast local storage while reducing costs for historical data. Queries transparently span all tiers.\nWHERE symbol in ('AAPL', 'NVDA')\nLATEST ON timestamp PARTITION BY symbol\nCREATE MATERIALIZED VIEW 'trades_OHLC'\nmin(price) AS low\ntimestamp IN today()\nSELECT spread_bps(bids[1][1], asks[1][1])\nFROM read_parquet('trades.parquet')\nSAMPLE BY 15m\nTier One:\nHot ingest (WAL), durable by default\nIncoming data is appended to the write-ahead log (WAL) with ultra-low latency. Writes are made durable before any processing, preserving order and surviving failures without data loss. The WAL is asynchronously shipped to object storage, so new replicas can bootstrap quickly and read the same history.\nTier Two:\nReal-time SQL on live data\nData is time-ordered and de-duplicated into QuestDB's native, time-partitioned columnar format and becomes immediately queryable. Power real-time analysis with vectorized, multi-core execution, streaming materialized views, and time-series SQL (e.g., ASOF JOIN, SAMPLE BY). The query planner spans tiers seamlessly.\nTier Three:\nCold storage, open and queryable\nOlder data is automatically tiered to object storage in Apache Parquet. Query it in-place through QuestDB or use any tool that reads Parquet. This delivers predictable costs, interoperability with AI/ML tooling, and zero lock-in.\nUnlike InfluxDB's per-series TSM architecture, QuestDB stores all time series in a single table structure. This means adding new series (high cardinality) doesn't create additional storage overheadâ€”the same columnar files simply contain more rows. This architectural difference explains why QuestDB maintains consistent performance as cardinality scales.\n\n## Query languages: SQL vs Flux\n\nInfluxDB has gone through multiple query languages: InfluxQL (SQL-like), then Flux (functional), and now SQL again in InfluxDB 3. This journey validates what QuestDB has maintained from the startâ€”SQL is the right choice for time-series data.\n\n### Why SQL matters\n\n- Universal skill: SQL is consistently among the top 3 languages in developer surveys. Most engineers already know it.\n- Tooling ecosystem: SQL integrates with BI tools, notebooks, ORMs, and drivers without custom adapters.\n- Transferable knowledge: Skills learned querying QuestDB apply to PostgreSQL, analytics platforms, and data warehouses.\n\n### Flux vs SQL comparison\n\nFlux uses a functional pipeline syntax that requires learning new concepts:\n\n```\nfrom(bucket: \"metrics\")  |> range(start: -1h)  |> filter(fn: (r) => r._measurement == \"cpu\" and r.host == \"server1\")  |> aggregateWindow(every: 1m, fn: mean)\n```\n\nThe equivalent in QuestDB SQL:\n\n```\nSELECT timestamp, avg(usage)FROM cpuWHERE host = 'server1' AND timestamp > dateadd('h', -1, now())SAMPLE BY 1m;\n```\n\n\n### QuestDB's time-series SQL extensions\n\nQuestDB extends standard SQL with purpose-built functions for time-series analysis:\n\n| Extension | Purpose | Example |\n| --- | --- | --- |\n| SAMPLE BY | Time-based aggregation | SELECT avg(price) FROM trades SAMPLE BY 1h |\n| LATEST ON | Last value per group | SELECT * FROM trades LATEST ON timestamp PARTITION BY symbol |\n| ASOF JOIN | Time-aligned joins | Join trades with quotes at exact timestamps |\n| WHERE IN (ts, ts) | Time range filtering | Optimized partition pruning |\n\nThese extensions maintain SQL compatibility while providing the expressiveness needed for time-series workloads.\n\n## Ecosystem and integrations\n\nBoth databases offer solid integration options, though with different strengths:\n\n| Integration | QuestDB | InfluxDB |\n| --- | --- | --- |\n| Grafana | Native data source | Native data source |\n| Telegraf | Via ILP | Native |\n| PostgreSQL tools | Full compatibility (psql, any PG driver) | Not supported |\n| Client libraries | Python, Java, Go, Node.js, Rust, C/C++, .NET | Python, Java, Go, Node.js, and more |\n| Kafka | Official Kafka connector | Native Kafka consumer |\n| Pandas/Polars | Native integration | Via client library |\n\nQuestDB's advantage\n: PostgreSQL wire protocol compatibility means PostgreSQL client libraries work with QuestDBâ€”including psql, SQLAlchemy, and any PostgreSQL driver.\nInfluxDB's advantage\n: As the older and more widely deployed database, InfluxDB has broader native integrations with monitoring tools and a larger collection of community Telegraf plugins.\n\n## Conclusion\n\n\n### Performance summary\n\n\n| Workload | QuestDB | InfluxDB v2 | Advantage |\n| --- | --- | --- | --- |\n| Ingestion (1M hosts) | 7.33M rows/sec | 241K rows/sec | 30x faster |\n| Ingestion (100K hosts) | 11.36M rows/sec | 402K rows/sec | 28x faster |\n| Double-groupby | 40-58ms | 935ms-7.5s | 23-130x faster |\n| Heavy aggregations | 994ms | 16.6s | 17x faster |\n\nKey takeaways:\n- QuestDB ingests data3x to 36xfaster than InfluxDB, with the advantage growing at scale\n- QuestDB dominates analytical queries:21x to 130xfaster on double-groupby,16xfaster on heavy scans\n- InfluxDB v1 has a slight edge on two simple aggregation queries\n\n### When to choose QuestDB\n\nMarket data and trading infrastructure:\n- Tick data capture and analytics at millions of events per second\n- Order book reconstruction and market depth analysis\n- Post-trade analytics and markouts (ASOF, CROSS, Window JOIN)\n- Materialized OHLCV candlestick charts automatically maintained\nQuantitative finance:\n- Backtesting strategies across historical tick data\n- Real-time P&L and risk calculations\n- Correlate market data and trades with ASOF JOIN\nHigh cardinality and heavy ingestion workloads:\n- Industrial IoT with millions of unique sensors and devices\n- Physical AI and robotics telemetry at scale\n- Fleet management and vehicle tracking with high device counts\n- Energy grid monitoring with dense sensor networks\nSQL-first teams:\n- Standard SQL with time-series extensions (SAMPLE BY, LATEST ON)\n- PostgreSQL compatibility for existing quant tools and workflows\n- Integration with Python, pandas, and Jupyter notebooks\n\n### When to consider InfluxDB\n\n- Simple monitoring dashboards with low cardinality\n- Single-series lookups where sub-millisecond latency is critical\n- Existing Telegraf-based collection pipelines\n- Teams already invested in the InfluxDB/Flux ecosystem\nReady to try QuestDB?\nGet started with the quickstart guide\nor\njoin our Slack community\nto ask questions.",
    "scraped_date": "2026-02-22",
    "content_date": "2025-12-02",
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3272,
    "metadata": {
      "relevance_score": 0.5714285714285714,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-c6d2dba348f8",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2021/04/12/stream-ethereum-data",
    "title": "Streaming on-chain Ethereum data to QuestDB | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nThis submission comes from one of our community contributors\nYitaek Hwang\nwho has put together another\nexcellent tutorial that shows how to stream Ethereum blockchain data into\nQuestDB for time series data visualization and analysis.\nThanks for another great contribution, Yitaek!\n\n## Introduction\n\nPreviously, I wrote about using\nCoinbase API and Kafka Connect\nto track the price of various cryptocurrencies in real-time. While price is an\nimportant factor for a potential investor, on-chain data like block information\n(gas used, difficulty), transactions, and smart contracts also provide useful\nmetrics for technical analysis. In this tutorial, we will pull on-chain data\nfrom Ethereum and stream it to QuestDB for further analysis and visualization.\nDisclaimer:\nThis tutorial is not investment or financial advice. All views\nexpressed here are my own.\n\n## Prerequisites\n\n- Python 3.6+\n- Docker\n- Infuraaccount\nNote: This tutorial uses ethereum-etl 1.6.x series. Later releases may not be\ncompatible with QuestDB.\n\n## Accessing Ethereum on-chain data\n\nInfura is a development platform powered by Consensys with a generous free tier\n(100k requests/day) to pull data from Ethereum Mainnet and Testnets. Create a\nnew project in your Infura account under Ethereum:\nMake note of the HTTPS endpoint for the Mainnet in the following format:\n\n```\nhttps://mainnet.infura.io/v3/<your-project-id>\n```\n\n\n## Create table for time series data\n\nThe ETL script we will use to stream Ethereum data provides the following\non-chain information:\n- Blocks\n- Contracts\n- Logs\n- Token Transfers\n- Tokens\n- Traces\n- Transactions\nFor simplicity, we will only stream blocks and token transfers in this example,\nbut the schema for all the available on-chain data is located under\nethereum-etl-postgres/schema\n.\nThis data will be indexed in QuestDB for high-performance time series data\nanalysis.\nStart the QuestDB Docker container with the\nWeb Console\nand PostgreSQL wire protocol ports exposed:\n\n```\ndocker run -p 9000:9000 -p 8812:8812 questdb/questdb\n```\n\nNavigate to the\nWeb Console\nat localhost:9000 and create\ntables for\nblocks\n:\n\n```\ncreate table blocks(  timestamp string,  number int,  hash string,  parent_hash string,  nonce string,  sha3_uncles string,  logs_bloom string,  transactions_root string,  state_root string,  receipts_root string,  miner symbol,  difficulty long,  total_difficulty long,  size long,  extra_data string,  gas_limit long,  gas_used long,  transaction_count long);\n```\n\nand for\ntoken_transfers\n:\n\n```\ncreate table token_transfers(  token_address symbol,  from_address symbol,  to_address symbol,  value float,  transaction_hash string,  log_index long,  block_timestamp string,  block_number long,  block_hash string);\n```\n\nWhen creating tables that use repetitive strings in QuestDB, we can achieve\nbetter performance on both storage space and query efficiency by using the\nsymbol\ntype.\nWhen sending data over PostgreSQL wire, we can send regular\nstring\ntypes, but\ntreat them as enum-like values stored as integers in QuestDB. For more\ninformation on using this feature, further details can be found on the\nsymbol type documentation\n.\nAfter refreshing the tables list, you should see both populated and weâ€™re ready\nto stream Ethereum data to our database.\n\n## Stream Ethereum on-chain data\n\nBlockchain ETL provides a Python script to pull data from Infura and stream it\nto Google Pub/Sub or Postgres. To start, we need to first install\nethereumetl[streaming] :\n\n```\npip3 install \"ethereum-etl[streaming]\"\n```\n\nNow we need to specify our QuestDB credentials as well as Infura API details.\nPostgres output is in the following format:\n\n```\npostgresql+pg8000://<user>:<password>@<host>:<port>/<database>\n```\n\nwhere API url is specified via\n--provider-uri\n. We can also specify the\nstart-block\nnumber (or omit to download all of Ethereum data from the\nbeginning), and the schemas weâ€™re interested in:\n\n```\nethereumetl stream --start-block 600000 -e block,token_transfer \\--output postgresql+pg8000://admin:quest@localhost:8812/qdb \\--provider-uri https://mainnet.infura.io/v3/<my-project-id>\n```\n\nAs the stream starts up, data will be visible in QuestDB:\nThe script automatically stores the last processed block in\nlast_synced_block.txt\n. If you would like to run the script later, you can\nremove the\n--start-block\nflag and the script will resume from the value stored\nas a checkpoint in\nlast_synced_block.txt\n.\n\n## Exploring data & next steps\n\nQuestDB provides some built-in visualization capabilities. For example, if you\nwould like to explore how\ngas_used\nby\nminer\nlooks, you can navigate to\nChart\non the QuestDB\nWeb Console\nand configure the\nfollowing settings:\n- Chart type - bar\n- Labels -miner\n- Series -gas_used\nThe Ethereum ETL script currently stores\ntimestamp data in RFC3339 format\n,\nwhereas QuestDB expects signed offset from Unix Epoch. This was why our table\nschema used\nstring\ninstead of\ntimestamp\ntype for our time series fields. To\nsee better performance and to use more language features that QuestDB offers,\nthere are a few options. We can either modify the ETL script directly to send\ntimestamp\ntypes, or stream our data to Cloud Pub/Sub, transform the data prior\nto streaming to QuestDB.\nAlternatively, we can use the\nto_timestamp()\nfunction in QuestDB to parse\nstring fields with a date format as timestamps. For example, pulling data from\nthe\nblocks\ntable using\ntimestamp\ntype can be done on-the-fly using this\nquery:\n\n```\nSELECT to_timestamp(timestamp, 'yyyy-MM-dd HH:mm:ss') AS time,        number,        miner,        difficulty,        size,        gas_limit,        gas_usedFROM (blocks ORDER BY timestamp);\n```\n\nFinally, if you are going to stream contracts or token data, you'll need to\nfirst flatten\nfunction_sighashes\nas QuestDB does not support\narray\ntypes\nnatively yet.\n\n## Summary\n\nWe've learned how to stream Ethereum on-chain data from Infura into QuestDB for\ntime series analysis using Python ETL scripts. With this data, you can now\ncalculate popular signals like Network Value to Transaction Ratio or replicate\nanalysis provided on Coin Metrics or Glassnode. As a bonus, you can deploy the\nstreaming application to Kubernetes following the\ninstructions on this GitHub repository\n.\nIf you like this content, we'd love to know your thoughts! Feel free to share\nyour feedback or come and say hello in the\ncommunity forums\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 978,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "financial",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-935309adcc8b",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/questdb-8-2-2",
    "title": "QuestDB 8.2.2 - New real-time monitoring, Table TTL and more | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\n2025 is a fresh new year. There's a renewed sense of optimism, and we're pleased to ride that wave into our latest release. QuestDB had a massively successful year in 2024, being the only time-series database to show significant growth in global\nDB-Engines ranking\n. And we're just getting started.\nBut enough about that, you're here for the goods!\nFor itemized patch notes, see our\nrelease notes page\n.\n\n## Real-time QuestDB monitoring\n\nHow you monitor QuestDB is your choice. To provide our community with a strong out-of-the-box option, we've added a new monitoring dashboard to the Web Console. To access it, click the new\nAdd metrics\nicon above the table view:\nClick the Add Metrics icon above the table view\nThe dashboard can be tailored to show you what you want to track. All data is shown in real-time from internal telemetric data. Now, no third-party service is required to monitor your QuestDB instance. Customize the timezone, time-frame and refresh rate to your liking:\nClick to zoom\n\n## Parallelized Parquet queries\n\nParquet file reading received a nice boost. The\nread_parquet()\nSQL function now supports parallel execution, leveraging QuestDB's query engine to process Parquet files with the same parallel filtering and aggregation capabilities as native tables:\nParallel Parquet query example\nDemo this query\n\n```\nEXPLAIN SELECT  symbol,  max(price)FROM read_parquet('trades.parquet');\n```\n\nParquet queries will now take full advantage of multiple CPU cores, improving performance for large datasets. The parallelization is particularly effective for:\n- Complex filtering operations\n- Aggregations across large datasets\n- Group by operations\nIf you need to disable parallel execution for any reason, you can do so via the\ncairo.sql.parallel.read.parquet.enabled\nconfiguration property:\nserver.conf\n\n```\ncairo.sql.parallel.read.parquet.enabled=false\n```\n\n\n## New SQL syntax\n\nHeadlining this release is a great new tool to manage historical data, and new SQL syntax to help you write performant and concise queries.\n\n## Time-to-Live (TTL) for tables\n\nManaging historical data can become a pain-point. Table TTL lets you set an expiration time for your data, automatically removing old partitions as new data arrives. Ideal for maintaining rolling windows of recent data without manual cleanup:\nCreate a table with TTL\n\n```\n-- Keep only the last 7 days of dataCREATE TABLE metrics (    ts TIMESTAMP,    sensor_id SYMBOL,    value DOUBLE) timestamp(ts)PARTITION BY DAYSET TTL 7d;\n-- Or add TTL to existing tablesALTER TABLE metrics TTL 7d;\n```\n\nQuestDB handles the cleanup automatically as part of regular operations, dropping entire partitions when they fall outside your TTL window. This makes it incredibly efficient for managing large-scale time-series data.\nRead the docs\n\n## DECLARE\n\nEver find yourself repeating the same values across a complex query? The new\nDECLARE\nkeyword lets you define variables once and use them throughout your SQL. It's handy for date ranges, sampling periods, and frequently used values:\nDECLARE example\nDemo this query\n\n```\nDECLARE    @period := 1h,    @symbol := 'BTC-USDT',    @window := today()SELECT    timestamp,    symbol,    avg(price) as avg_priceFROM tradesWHERE symbol = @symbol  AND timestamp IN @windowSAMPLE BY @period;\n```\n\nRead the docs\n\n## New window functions\n\nQuestDB 8.2.2 introduces several new window functions. Whether you're new to them or a seasoned pro, our documentation will help you get started.\n\n### Lead\n\nThe\nlead()\nfunction accesses data from subsequent rows in your result set. This is particularly useful for comparing current values with future values.\nLead example\nDemo this query\n\n```\nSELECT    timestamp,    price,    future_price,    future_price - price AS momentumFROM (    SELECT        timestamp,        price,        LEAD(price, 10) OVER (ORDER BY timestamp) AS future_price    FROM (        SELECT            timestamp,            avg(price) AS price        FROM trades        WHERE timestamp BETWEEN now() AND dateadd('d', -1, now())            AND symbol = 'BTC-USDT'        SAMPLE BY 10m    ))\n```\n\nRead the docs\n\n### Lag\n\nThe\nlag()\nfunction accesses data from previous rows, perfect for comparing current values with historical ones.\nLag example\nDemo this query\n\n```\nSELECT    timestamp,    price,    lag(price) OVER (        PARTITION BY symbol        ORDER BY timestamp    ) as previous_priceFROM trades;LIMIT 20;\n```\n\nRead the docs\n\n### Last value (not null?)\n\nReturns the last value in a window frame, with support for NULL handling through\nIGNORE NULLS\n.\nLast value example\nDemo this query\n\n```\nSELECT    timestamp,    symbol,    price,    last_value(price) OVER (        PARTITION BY symbol        ORDER BY timestamp        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW    ) as last_priceFROM trades;LIMIT 20;\n```\n\nRead the docs\n\n### First value (not null?)\n\nReturns the first value in a window frame, with support for NULL handling through\nIGNORE NULLS\n.\nFirst value example\nDemo this query\n\n```\nSELECT    timestamp,    symbol,    price,    first_value(price) OVER (        PARTITION BY symbol        ORDER BY timestamp        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW    ) as first_priceFROM trades;LIMIT 20;\n```\n\nRead the docs\n\n## New functions\n\nLike most releases, we've added new sugar to help you write performant SQL.\n\n### Timezone awaredateadd()\n\nThe\ndateadd()\nfunction now supports timezone awareness, making it easier to perform date arithmetic across different timezones. This is particularly useful when working with global time-series data.\nTimezone aware dateadd example\nDemo this query\n\n```\nSELECT    timestamp,    dateadd('h', 1, timestamp, 'PST') as one_hour_later_pst,    dateadd('h', 1, timestamp, 'EST') as one_hour_later_estFROM trades;LIMIT 20;\n```\n\nRead the docs\n\n## Reloadable settings\n\nDowntime is (usually) never helpful. To keep QuestDB up and humming, we've added reloadable configurations. You can now reload certain settings without restarting QuestDB. To do so, update your\nserver.conf\nfile and use the new\nreload_config()\nfunction:\n\n```\nSELECT reload_config();\n```\n\nThe server will confirm successful changes in the logs, and you can check which settings are reloadable:\n\n```\n(SHOW PARAMETERS) WHERE reloadable = true;\n```\n\nThe configuration documentation also has a new column to indicate whether or not a setting can be reloaded on the fly.\nRead the docs\n\n## Query tracing\n\nEver wonder which queries are slowing down your system? Query tracing lets you diagnose performance issues by recording execution times in a system table. Enable it with a simple configuration:\nserver.conf\n\n```\nquery.tracing.enabled=true\n```\n\nOnce enabled, QuestDB records query execution data in the\n_query_trace\ntable. You can analyze this data using regular SQL to identify slow queries:\nFind queries taking over 100ms\n\n```\nSELECT    ts,    query_text,    execution_micros / 1000.0 as execution_msFROM _query_traceWHERE execution_micros > 100_000ORDER BY execution_micros DESC;\n```\n\nThe system automatically maintains this table, dropping data older than 24 hours to keep storage usage in check. And like other configuration changes, you can enable tracing without a restart using\nreload_config()\n.\nRead the docs\n\n## We're now questdb.com\n\nWhile it doesn't impact the core database or this version, we'd like to let you know that we've updated our primary domain to\nquestdb.com\n. This is a permanent change, and we've updated all our links and references to the new domain.\nWhy? First, because we've heard\nsome uncertainty\naround the .io domain. And second, the new\n.com\nis more familiar and professional, with that little extra panache that reminds us of the good ol' days.\n\n## Summary\n\nQuestDB 8.2.2 is a solid release - upgrade to get the latest. The Web Console's new built-in monitoring dashboard gives you instant visibility into your system's performance, while Table TTL simplifies data lifecycle management. Parallelized Parquet reads give a solid performance boost, and set the table for deeper Parquet improvements.\nWe've also expanded our analytics capabilities with new window functions, made query writing more elegant with\nDECLARE\n, and improved timezone handling with enhanced date functions. With the new reloadable settings function, you can even tweak your server configuration without restarting your instances. The new query tracing feature helps you identify performance bottlenecks by tracking query execution times.\nAs usual, along with a run of new features, QuestDB 8.2.2 brings a battery of bug fixes and performance improvements.\nDownload QuestDB 8.2.2\nand let us know what you think within our\nCommunity Forum\nor our public\nSlack\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1318,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-3d01199c99e1",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/questdb-and-llm-interaction",
    "title": "Leveraging LLMs to Interact with QuestDB Data | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nThanks to Large Language Models (LLMs), interacting with your data is easier than ever.\nPreviously, you needed to know SQL, a coding language like Python, or some Business\nIntelligence (BI) tool to query and analyze your data stored away in some database. But now,\nyou can simply ask questions in plain English, and let the LLM contextualize that data for you.\nEvery day, new advancements in AI are lowering this barrier further. At first, one would need\nto ask an LLM to write out a SQL query to translate prompts like \"show me the average price of\nBitcoin in the last five days\" and copy-paste that into a dashboard or BI tool. Then came Model\nContext Protocols (MCPs), which standardized how LLMs can retrieve information from external\nsources that the model was not trained on. Once the MCP server is connected, you can now\ninteract with the database directly from your LLM interface (whether that be your terminal,\nIDE, or LLM application) instead of having to switch context. And now, we are seeing semi-autonomous\nagents automating more and more of this away.\nIn this article, we'll explore how to leverage LLM tools like Claude to interact\nwith QuestDB. We'll first look at utilizing QuestDB's built-in REST API as well as using an OSS\nPostgreSQL MCP server. Let's explore how these two approaches differ in philosophy and how we\ncan leverage either path based on the use case.\nTIP\nYou can follow along by using the example prompts to any of the LLM tool of your choice.\nIf you get stuck, you can reference the code snippets for a working demo.\n\n## REST API: The Direct HTTP Approach\n\nOne of the interesting aspects of QuestDB is that it exposes a\nREST HTTP API endpoint\non port\n9000 for querying data. Specifically, it has two endpoints with\n?query\nparameter:\n- /exp: export SQL query as CSV\n- /exec: runs SQL query and returns results as JSON\nSince REST APIs are well understood and widely used, this makes it easy for LLM tools to use\nestablished libraries in whatever coding language to query data. Let's use Claude to\ndemonstrate how easy this is. My prompt was:\n\n```\nQuestDB has a REST API endpoint to query data(https://questdb.com/docs/query/overview/#rest-http-api),write up a quick demo to ingest some crypto dataand show how you can interact with it.\n```\n\nIt then gave me the following instructions:\n- Run QuestDB via Docker:\n\n```\ndocker run -p 9000:9000 -p 8812:8812 questdb/questdb\n```\n\n- Install Python dependencies:\n\n```\npip install requests pandas matplotlib aiohttp\n```\n\nNOTE\nThe code examples for this post are available at\nhttps://github.com/questdb/questdb-llm-interaction-post-examples/tree/main\n.\n- Run the following code:crypto_ingestion_demo.py. At first, it gave me a demo of fetching prices from Binance and\nCoinbase APIs. Binance was returning451 HTTP error, so I asked it to update and gave me the following:\nOnce you run that demo code above, we get some records into our database.\nIngestion script output (Click to Zoom)\nI ran the code a few\ntimes and queried the data via web console at\nlocalhost:9000\n.\nIngested data in the QuestDB Web Console (Click to zoom)\nClaude also generated a script to mimic some natural language interaction and some sample\nanalysis queries in\ncrypto_analyst.py\nfile as shown below:\nWeâ€™ll need bit more data for some more advanced queries like arbitrage opportunities one, but we can at least see\nhardcoded mappings of user intent to analysis as well as csv exports and graph generation:\nFirst block of output from the Crypto Analyst script (Click to zoom)\nAs with all LLMs, not everything is perfect but itâ€™s easy to ask Claude to fix itself. For example, hardcoded query of\ncomparing Binance and Coinbase fails because we didnâ€™t actually get any data from Binance and the query just uses Krakenâ€™s data.\nSecond block of output from the Crypto Analyst script (Click to zoom)\n\n## Pros and Cons of REST API Approach\n\nAs shown by the demo code above, it's extremely easy to generate code samples using the REST\nAPI to ingest and query data in QuestDB. The biggest advantage here is that there is no\nadditional infrastructure needed as communication happens directly with QuestDB. Common\nproduction-level concerns like authentication, connection pooling, etc., are already solved\nproblems that we need to ask the LLM to implement. Since there's great support for HTTP\ninteraction in almost any programming language, this can be deployed almost anywhere (e.g.,\nlocally, web environments, or serverless functions).\nHowever, there are some downsides to this approach. First, we had to map the user intent to\nqueries ourselves in this demo. Otherwise the LLM would have to dynamically do this and\ngenerate the REST API call for us with the query. Also, due to the stateless nature of HTTP,\nwe were creating new connections per query and context management (aka \"memory\") was left for\nus to implement. This may not be important, but if you want to \"chain\" queries together or go\nback to your analysis, this can become cumbersome to implement yourself.\n\n## PostgreSQL MCP Server: The Protocol-Native Approach\n\nNow, let's see what the experience is like leveraging a PostgreSQL MCP server. For those\nunfamiliar with MCP, it is a framework in which AI tools can have structured, persistent access\nto external systems. You can think of it like REST for AI-native systems. Because QuestDB has a\nPostgreSQL-compatible interface running on port 8812, we can simply leverage the OSS PostgreSQL\nMCP server to query data.\nIf you're using Claude Desktop as an MCP client, you can go to\nSettings > Developer\nor\nsimply find the\nclaude_desktop_config.json\nfile and add the following:\n\n```\n{  \"mcpServers\": {    \"postgres\": {      \"command\": \"npx\",      \"args\": [        \"@modelcontextprotocol/server-postgres\",        \"postgresql://admin:quest@localhost:8812/qdb\"      ]    }  }}\n```\n\nNote: we are using an archived version of the PostgreSQL MCP for demo purposes. You can elect\nto use Google's GenAI Toolbox integration instead for production purposes. Once you save the\nconfig, we need to restart our agent. Then let's ask a question: \"Show me all tables in my\nQuestDB database\". It will then prompt you to allow the Postgres interaction.\nClaude's prompt to use the archived PostgreSQL MCP (Click to zoom)\nThen it will return with an answer.\nClaude showing all tables (Click to zoo)\nWe can now ask the same questions from the REST API demo.\nClaude showing the latest prices (Click to zoom)\nNote that the LLM knew to use our\ncrypto_prices\ntable instead of fetching live prices from\nthe Internet for this query.\n\n## Pros and Cons of PostgreSQL MCP\n\nThe two biggest advantages of using PostgreSQL MCP are that\n- it can maintain a persistent connection context to build upon previous queries\n- it has native schema awareness without having to provide that information ourselves.\nWith the REST API approach, we needed to know what our data looked like to build the query, but\nthe MCP server exposes that information already to the LLM so we have less work to do on that\nfront.\nHowever, using MCP servers is not without limitations. First off, the technology is still\nrelatively new and changing all the time, as seen by the \"archival\" status of the PostgreSQL MCP\nserver by the Anthropic team. There are legitimate security and governance questions as we don't\nhave secure MCP registries or gateway solutions yet. Finally, we do need additional\ninfrastructure (i.e., MCP server and client) to run this out of the box. With our local demo,\nthis was simple, but imagine having to productionize this with tight scrutiny from security and\nlegal teams for a financial use case, for example.\n\n## Final Thoughts\n\nIn this article, we explored two different ways to interact with data on QuestDB. First, we\nlooked at leveraging\nQuestDB's REST API\nto ask an LLM tool to generate code for us to both\ningest and query that data. Then, we used a PostgreSQL MCP to query data directly from an AI\nagent and build upon that context.\nAt the time of writing, the MCP ecosystem is still evolving with lots of promise but not yet to\nthe point of production-level support. There are security concerns like prompt poisoning or even\nsupply chain risks that have yet to be solved widely. Still, MCP servers provide context\nawareness and other useful features that work well with QuestDB's PostgreSQL wire endpoint out\nof the box.\nThe good news is that since QuestDB supports both protocols, we can pick and choose what to use\ndepending on the use case. And as AI tools continue to evolve, the ability to query data using\nnatural language will be increasingly easier. Perhaps the interface of choice will be left to a\nsemi-autonomous agent to decide depending on the performance and risk guidelines we provide in\nthe future.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1490,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "trading",
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-76e29ed351e9",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/navigating-access-control-design-clarity-simplicity",
    "title": "Navigating Access Control Design: Pursuing Clarity and Simplicity | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nWhen delving into access control system design, one might assume that it is a\nsolved problem and that all the important decisions have already been made.\nAfter all, many successful applications already exist, and each of them has\nsolved its own access control challenge. While there is no need to re-invent the\nwheel, there are many places where innovation can still occur.\nTo start, let's put access control in simple terms. When a user authenticates\nthemselves, the system reads their permissions and then determines the\noperations that they are allowed to perform. To help with this, users are often\ncategorized into groups or assigned specific roles, and thus synchronize their\npermissions with a wider bucket.\nWhile this appears straightforward, nuanced details can significantly impact the\noverall functionality of an otherwise solid RBAC pattern. In crafting QuestDB's\naccess control solution, we encountered pivotal decisions that shaped our\napproach.\n\n## Groups or Roles?\n\nOur first dilemma lies in naming the entity that we use to organize users into\ncollectives. Role seems like a good choice. A role implies that functionality is\nbundled and then granted to relevant users who fit the role's description. Roles\nare created, assigned to users, and their tasks are defined. In a perfect world,\nit's neat and tidy.\nRoles, in Utopia\nHowever, here's the catch â€” unfortunately, reality falls short of perfection!\nThe initial simplicity soon crumbles as applications evolve, giving rise to a\ntangled and overlapping assortment of roles. These roles may then stray from\ntheir original purpose and lose their authentic alignment with their intended\nfunctionalities. Instead, they morph into a jumble of permissions for diverse\nusers.\nAn analyst might be part developer. One developer may need a single permission\nthat would be destructive in the hands of other another developer. And then you\nneed another admin, but they're only half an admin. It starts clean, but\ncomplexity soon catches up. The challenge lies in maintaining clean roles amid\nthis complexity.\nRoles, in reality\nWhile roles appear attractive at first, the term groups tends to offer more\nflexibility and thus, in theory, greater clarity. Consequently, the integration\nof both groups and roles often leads to even bigger confusion when inheritance\nis involved.\n\n## Exploring Inheritance\n\nUndoubtedly, groups are essential for efficient access control. Organized groups\nsimplify the process of permission distribution. Access can be granted to a\ngroup of users in a single statement. Adding users into that group to then\ninherit those permissions makes intuitive sense.\nHowever, the necessity of nested group hierarchies warrants a second look. The\nquestion is whether to construct an intricate group hierarchy or be content with\na singular level of inheritance.\nThe risk of an intricate hierarchy is that it has the potential to escalate into\nan unmanageable tree-like structure, or even a complex graph, if the system\nallows it. These configurations will soon perplex even the most skilled\nadministrators.\nJust take a quick look at the example below:\nInheritance Hell caused by group hierarchy\nLearning from experience, it became evident that multi-level inheritance results\nin confusion and mismanagement, thus making it a challenge to trace the origins\nand implications of permissions. Opting for a single-level inheritance approach\ngroups users well, but without nesting. This promotes transparency and\npredictability.\n\n## Considering Service Accounts\n\nBeyond an administrator's organizational preferences, the application using the\ndatabase will present its own critical questions that one must consider. For\nexample, should a clear distinction exist between individual and application\naccounts?\nThe database logic alone might not necessitate any distinction. But the\napplication accounts may benefit from strict limitations to prevent inadvertent\nmisuse. A common trap is when an application inherits an overly-permissive\ncapability by accident. Depending on how your groups or roles are arranged, it\ncan be easy to do. We need something to help ensure that we do not make this\nmistake\nThe creation of service accounts serves this purpose. Service accounts mirror\nstandard users, differing only in their exclusion from group affiliations. Their\naccess is explicitly defined, which prevents unintentional overreach and\nmis-assignment.\nHere is a sample arrangement that shows groups, users, and service accounts:\nService Accounts\nAnd the diagram below illustrates how the above users and service accounts could\nbe used to establish connections with the database:\nUsers and Service Accounts connecting to the DB\n\n## Enabling Real-time Changes\n\nEven with service accounts in place, there are still scenarios where\napplications may inadvertently impact the database. At times, there is a need\nfor prompt user access revocation. In these difficult scenarios, real-time\nenforcement of access control changes becomes crucial.\nTrue security must be instantaneous. From a development perspective, it would be\nconvenient to delay or wait until a user reconnects to enforce new permissions.\nInstead, as a preferred alternative, we will apply a copy-on-write solution.\nThis approach avoids synchronization during permission checks, while changes are\nsafely applied in real time without hindering performance.\nBy creating a new copy of the access control list when changes are needed, you\ncan ensure that ongoing permission checks proceed without blocking, even during\nthe modification process. Once the changes are ready, the new access control\nlist can be swapped in using an atomic operation, maintaining data consistency\nand minimizing the impact on performance.\nThe following code snippets illustrate how copy-on-write can be used to update\naccess lists. First, let's look at the AccessListStore class which holds the\naccess lists, and provides an API for modifications:\n\n```\nclass AccessListStore {    // the map contains users with their access lists.    // an access list is a set of permissions granted to a user.    // access lists are versioned: 0, 1, 2, 3...    // they also have a flag which indicates if they are still valid or not,    // only the access list with the latest version number can be valid    ConcurrentMap<User, AccessList> accessLists;\n    // returns the latest version of the user's access list    AccessList getAccessList(User user) {        return accessLists.get(user);    }\n    // granting a new permission to a user    void grant(User user, Permission permission) {        boolean successful = false;        while (!successful) {            // lookup the user's access list            AccessList current = getAccessList(user);            // current: { version = 0, valid = true, permissions=[] }\n            // create a copy of the current access list, only the version is bumped            AccessList next = current.rollVersion();            // next: { version = 1, valid = true, permissions=[] }\n            // add the new permission to the cloned access list            next.add(permission);            // next: { version = 1, valid = true, permissions=[permission] }\n            // publish the updated access list.            // replace is an atomic operation,            // it is successful only if current is still in the map.            // if current is not in the map anymore, another thread was faster,            // and next is based on a stale, invalid access list.            // we have to re-try using the latest version as our new current            // until we are successful (see the condition of the while loop above).            successful = accessLists.replace(user, current, next);\n            // if the access list is successfully updated,            // we should invalidate the old version.            // anyone using current will see that their version is stale now,            // and they should get the updated access list from the map.            if (successful) {                current.invalidate();            }        }    }}\n```\n\nThen we can look at the SecurityContext class, which belongs to a specific user\nand can be used to authorize the operations to be executed by the user:\n\n```\nclass SecurityContext {    User user;    AccessListStore accessListStore;\n    // access list holding the user's permissions    AccessList accessList;\n    void authorize(Permission permission) {        // refresh access list if it is stale,        // check should be executed always on the latest version        if (!accessList.isValid())            accessList = accessListStore.getAccessList(user);        }\n        // check for the permission,        // throw an exception if access is not granted        if (!accessList.hasPermission(permission)) {            throw new SecurityException(\"Access denied\");        }    }}\n```\n\n\n## Conclusion\n\nInnovation consistently finds its space, adapting on top of established\nsolutions, even in thoroughly explored domains such as access control.\nThese nuanced adjustments can be the differentiating factor that determines\nwhether users of one application either struggle with frustration or feel\nseamless satisfaction â€” or, at the very least, experience no added\ndatabase-related woes!\nWant to an easy-to-follow tutorial for QuestDB access control? Checkout our\narticle:\nQuestDB Enterprise: Role-based Access Control Walkthrough\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1412,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "trading",
        "enterprise"
      ]
    }
  },
  {
    "id": "questdb-blog-9289437c47e6",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2021/06/16/high-cardinality-time-series-data-performance",
    "title": "How databases handle 10 million devices in high-cardinality benchmarks | QuestDB",
    "text": "If you're working with large amounts of data, you've likely heard about\nhigh-cardinality\nor ran into issues relating to\nit. It might sound like an intimidating topic if you're unfamiliar with it, but\nthis article explains what cardinality is and why it crops up often with\ndatabases of all types. IoT and monitoring are use cases where high-cardinality\nis more likely to be a concern. Still, a solid understanding of this concept\nhelps when planning general-purpose database schemas and understanding common\nfactors that can influence database performance.\n\n## What is high-cardinality data?\n\nCardinality typically refers to the number of elements in a set's size. In the\ncontext of a time series database (TSDB), rows will usually have columns that\ncategorize the data and act like tags. Assume you have 1000 IoT devices in 20\nlocations, they're running one of 5 firmware versions, and report input from 5\ntypes of sensor per device. The cardinality of this set is 500,000 (\n1000 x 20\nx 5 x 5\n). This can quickly get unmanageable in some cases, as even adding and\ntracking a new firmware version for the devices would increase the set to\n600,000 (\n1000 x 20 x 6 x 5\n).\nIn these scenarios, experience shows that we will want to eventually get\ninsights on more kinds of information about the devices, such as application\nerrors, device state, metadata, configuration and so on. With each new tag or\ncategory we add to our data set, cardinality grows exponentially. In a database,\nhigh-cardinality boils down to the following two conditions:\n- a table has many indexed columns\n- each indexed column contains many unique values\n\n## How can I measure database performance using high-cardinality data?\n\nA popular way of measuring the throughput of time series databases is to use the\nTime Series Benchmark Suite, a collection of Go programs that generate metrics\nfrom multiple simulated systems. For measuring the performance of QuestDB, we\ncreate data in InfluxDB line protocol format, which consists of ten 'tags' and\nten 'measurements' per row.\n\n```\ntsbs_generate_data --scale=100 \\  --timestamp-start=\"2016-01-01T00:00:00Z\" --timestamp-end=\"2016-01-15T00:00:00Z\" \\  --use-case=\"cpu-only\" --seed=123 --log-interval=\"10s\" --format=\"influx\"\n```\n\nIf we want to influence cardinality, we can use the\nscale\nflag, which provides\na value for the number of unique devices we want the test data set to contain.\nAs the number of devices increases, so does the number of unique identifiers\nvalues per data set, and we can control cardinality directly. Here's some\nexample output from the Time Series Benchmark Suite test data with three\ndifferent devices:\n\n```\n\"hostname\",\"region\",\"datacenter\",\"rack\",\"os\",\"arch\",\"team\",\"service\",\"service_version\",\"service_environment\",\"usage_user\",\"usage_system\",\"usage_idle\",\"usage_nice\",\"usage_iowait\",\"usage_irq\",\"usage_softirq\",\"usage_steal\",\"usage_guest\",\"usage_guest_nice\",\"timestamp\"\"host_0\",\"eu-central-1\",\"eu-central-1a\",\"6\",\"Ubuntu15.10\",\"x86\",\"SF\",\"19\",\"1\",\"test\",58,2,24,61,22,63,6,44,80,38,\"2016-01-01T00:00:00.000000Z\"\"host_1\",\"us-west-1\",\"us-west-1a\",\"41\",\"Ubuntu15.10\",\"x64\",\"NYC\",\"9\",\"1\",\"staging\",84,11,53,87,29,20,54,77,53,74,\"2016-01-01T00:00:00.000000Z\"\"host_2\",\"sa-east-1\",\"sa-east-1a\",\"89\",\"Ubuntu16.04LTS\",\"x86\",\"LON\",\"13\",\"0\",\"staging\",29,48,5,63,17,52,60,49,93,1,\"2016-01-01T00:00:00.000000Z\"\n```\n\nThe table that we create on ingestion then stores\ntags\nas\nsymbol\ntypes.\nThis\nsymbol\ntype is used to efficiently store repeating string values so that\nsimilar records may be grouped together. Columns of this type are indexed so\nthat queries across tables by symbol are faster and more efficient to execute.\nThe\nunique\nvalues per\nsymbol\ncolumn in the benchmark test data are:\n- hostname =scale_val\n- region = 3\n- datacenter = 3\n- rack = 3\n- os = 2\n- arch = 2\n- team = 3\n- service = 3\n- service_version = 2\n- service_environment = 2\nThis means we can calculate the cardinality of our test data as:\n\n```\nscale_val x 3 x 3 x 3 x 2 x 2 x 3 x 3 x 2 x 2# orscale_val x 3888\n```\n\n\n## Exploring high-cardinality in a time series database benchmark\n\nWhen we released QuestDB version 6.0,\nwe included benchmark results (see our\nInfluxDB\nand\nTimescaleDB\ncomparisons)\nthat tested the performance of our new ingestion subsystem, but we didn't touch\non the subject of cardinality at all. We wanted to explore this topic in more\ndetail to see how QuestDB can handle different degrees of cardinality. We also\nthought this would be interesting to share with readers as high-cardinality is a\nwell-known topic for developers and users of databases.\nThe tests we ran for our previous benchmarks all used a scale of 4000, meaning\nwe had a cardinality of 15,552,000 for all systems. For this benchmark, we\ncreated multiple data sets with the following scale and the resulting\ncardinality:\n\n| scale | cardinality |\n| --- | --- |\n| 100 | 388,800 |\n| 4000 | 15,552,000 |\n| 100000 | 388,800,000 |\n| 1000000 | 3,888,000,000 |\n| 10000000 | 38,880,000,000 |\n\nWe also wanted to see what happens when we rerun the tests and provide more\nthreads (workers) to each system to observe how a database scales with\ncardinality based on how much work it can perform in parallel. For that reason,\nwe tested each database with the scale values the table above using 4, 6, and 16\nthreads on two different hosts which have the following specifications:\n- AWS EC2 m5.8xlarge instance, Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n- AMD Ryzen 3970X 32-Core, GIGABYTE NVME HD\nThe following chart compares ingestion performance from lowest to highest\ncardinality running on the AWS EC2 instance with four threads:\nTSBS results using 4 threads on AWS EC2 m5.8xlarge\nUsing a dataset with low cardinality of 100 devices, we hit maximum ingestion\nthroughput of 904k rows/sec, with ClickHouse performing closest at 548k\nrows/sec. However, when increasing cardinality to 10 million devices, QuestDB\nsustains 640k rows/sec, and ClickHouse ingestion decreases at a similar rate\nrelative to the device count with 345k rows/sec.\nThe other systems under test struggled with higher unique device count, with\nInfluxDB ingestion dropping to 38k rows/sec and TimescaleDB at 50k rows/sec with\n10M devices. We reran the benchmark suite on the same AWS EC2 instance and\nincreased the worker count (16 threads) to the systems under test:\nTSBS results using 16 threads on AWS EC2 m5.8xlarge\nQuestDB showed a mostly constant ingestion rate of 815k rows/sec with all\ndegrees of cardinality. ClickHouse could ingest 900k rows/sec but requires four\ntimes as many workers as QuestDB to achieve this rate. ClickHouse ingestion\ndrops to 409k rows/sec on the largest data set. There was no significant change\nin performance between four and sixteen workers for TimescaleDB. InfluxDB\nstruggled the most, failing to finish successfully on the largest data set.\nWe ran the same benchmarks on a separate system using the AMD Ryzen 3970X, using\n4, 6, and 16 threads to see if we could observe any changes in ingestion rates:\nTSBS results using 6 threads on AMD Ryzen 3970X\nQuestDB hits maximum throughput with 1M devices during this run, with other\nsystems performing better than on the AWS instance. We can assume that\nTimescaleDB is disk-bound as results change dramatically based on the difference\nbetween the tests run on the EC2 instance. QuestDB performance peaks when using\nfour workers and slows down at 16 workers.\nOne key observation is that QuestDB handles high-cardinality better with more\nthreads. Conversely, when cardinality is low, fewer workers lead to an overall\nhigher maximum throughput and a steeper drop in ingestion rates when going from\n1M devices to 10M. The reason for lower maximum throughput when adding more\nworkers is due to increased thread contention:\nTSBS results for QuestDB using 4 and 16 threads on AWS EC2 m5.8xlarge\n\n## Why QuestDB can easily ingest time series data with high-cardinality\n\nThere are several reasons why QuestDB can quickly ingest data of this type; one\nfactor is the data model that we use to store and index data. High-cardinality\ndata has not been a pain point for our users due to our choices when designing\nthe system architecture from day one. This storage model was chosen for\narchitectural simplicity and quick read and write operations.\nThe main reason why QuestDB can handle high-cardinality data is that we\nmassively parallelize hashmap operations on indexed columns. In addition, we use\nSIMD to do a lot of heavy lifting across the entire SQL engine, which means that\nwe can execute procedures relating to indexes and hashmap lookup in parallel\nwhere possible.\nUsers who have migrated from other\ntime-series databases\ntold us that degraded\nperformance from high-cardinality data manifests with most systems early, but\ntheir threshold for usability is about 300K. After running the benchmark with\nhigh-cardinality, we were pleased with our system stability with up to 10\nmillion devices without a significant performance drop.\n\n## Configuring parameters to optimize ingestion on high-cardinality data\n\nThe ingestion subsystem that\nwe shipped in version 6.0\nintroduces parameters that users may configure server-wide or specific to a\ntable. These parameters specify how long to keep incoming data in memory and how\noften to merge and commit incoming data to disk. The two parameters that are\nrelevant for high-cardinality data ingestion are commit lag and the maximum\nuncommitted rows.\nLag refers to the expected maximum lateness of incoming data relative to the\nnewest timestamp value. When records arrive at the database with timestamp\nvalues out-of-order by the value specified in the commit lag, sort and merge\ncommits are executed. Additionally, the maximum uncommitted rows can be set on a\ntable which is a threshold for the maximum number of rows to keep in memory\nbefore sorting and committing data. The benefit of these parameters is we can\nminimize the frequency of commits depending on the characteristics of the\nincoming data:\n\n```\nalter table cpu set param commitLag=50us;alter table cpu set param maxUncommittedRows=10000000;\n```\n\nIf we take a look at the type of data that we are generating in the Time Series\nBenchmark Suite, we can see that for 10M devices, the duration of the data set\nis relatively short (defined by the timestamp\n--timestamp-start\nand\n--timestamp-end\nflags):\n\n```\ntsbs_generate_data --scale=10000000 \\  --timestamp-start=\"2016-01-01T00:00:00Z\" --timestamp-end=\"2016-01-01T0:00:36Z\"  --log-interval=\"10s\" --format=\"influx\" \\  --use-case=\"cpu-only\" --seed=123 > /tmp/cpu-10000000\n```\n\nThis command generates a data set of 36M rows and spans only 36 seconds of\nsimulated activity. With throughput at this degree, the commit lag can be a much\nsmaller value, such as 50 or 100 microseconds, and the maximum uncommitted rows\ncan be around 10M. The explanation behind these values is that we expect a much\nnarrower band of the lateness of records in terms of out-of-order data, and we\nhave an upper-bounds of 10M records in memory before a commit occurs.\nPlanning the schema of a table for high-cardinality data can also have a\nsignificant performance impact. For example, when creating a table, we can\ndesignate resources for indexed columns to know how many unique values the\nsymbol column will contain, and done via capacity as follows:\n\n```\ncreate table cpu (  hostname symbol capacity 20000000,  region symbol,  ...) timestamp(timestamp) partition by DAY;\n```\n\nIn this case, we're setting a capacity of 20M for the\nhostname\ncolumn, which\nwe know will contain 10M values. It's generally a good idea to specify the\ncapacity of an indexed column at about twice the expected unique values if the\ndata are out-of-order. High RAM usage is associated with using a large capacity\non indexed symbols with high-cardinality data as these values sit on the memory\nheap.\n\n## Next up\n\nThis article shows how high-cardinality can quickly emerge in time series data\nin industrial IoT, monitoring, application data and many other scenarios. If you\nhave have feedback or questions about this article, feel free ask in our\nCommunity Forum\nor browse the\nproject on GitHub\nwhere we welcome\ncontributions of all kinds.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1871,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-2b4c9d788a83",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/exploring-high-resolution-fx-data",
    "title": "Exploring high resolution foreign exchange (FX) data | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nIn a\nprevious post\n, we looked at FX rates from the European Central Bank (ECB) over a period of multiple years. Looking at these rates was a useful way to explore some important parts of European history through a financial lens. While extensive, this dataset was relatively sparse as it only included daily fixings for the currency pairs in scope.\nIn this post, we'll increase the resolution and look at intraday FX data using some of QuestDB's oldest and newest finance functions. High-frequency FX data provides a granular view of market movements, capturing every tick and allowing for detailed analysis of trading patterns and market behavior.\nBefore we dive in, here's a realtime replay to give a sense of the dataset resolution. The width of the chart window is 10 seconds:\n\n### About the dataset\n\nGetting our hands on high-resolution quality market data can be challenging. Some platforms, such as crypto exchanges, offer it for free via an API, while other data is often behind a paywall. In this instance,\nTrueFx\noffers historical CSV downloads on their website after creating an account. This is a nice opportunity to play with high-resolution FX data.\nAs currencies trade over the counter, there is no centralized exchange reporting market data. Instead, the dataset is composed of contributions from FX dealing banks, brokers, and asset managers. What's nice about this dataset is that it provides tick-by-tick prices, with a resolution up to a millisecond.\nHigh-resolution data is valuable because it allows traders and analysts to observe market dynamics in real-time, identify trends, and make informed decisions based on the most current information.\nThe CSV files look like the following:\n\n```\n~/D/october> head EURUSD-2024-10.csvEUR/USD,20241009 14:40:21.818,1.09476,1.09478EUR/USD,20241009 14:40:21.912,1.09473,1.09482EUR/USD,20241009 14:40:22.131,1.09476,1.09478EUR/USD,20241009 14:40:22.600,1.09473,1.09482EUR/USD,20241009 14:40:22.615,1.09471,1.09483EUR/USD,20241009 14:40:22.631,1.09476,1.09478EUR/USD,20241009 14:40:22.725,1.09471,1.09484EUR/USD,20241009 14:40:22.740,1.09472,1.09482\n```\n\n\n### Ingesting the dataset\n\nAfter creating an account with TrueFX, we can browse the available files, tick the pairs and intervals we want, and trigger a download. For our cases, each file covers a month.\nWe import all the files using the\nWeb Console's CSV upload functionality\n, which creates tables based on the filename such as \"USDTRY-2024-10.csv\".\nSince the files don't contain headers, QuestDB needs to be supplied with a\nschema\n. However, it is also possible to import as-is. In this case, the columns would be named f0, f1, etc.\nTo facilitate downstream analysis, we will join the different files together into one table. To do this, we'll use\nCREATE TABLE AS\ncoupled with\nUNION ALL\nto combine the different CSVs.\nWe'll also apply\nORDER BY\ntimestamp and nominate a\ndesignated timestamp\ncolumn.\nLastly, we'll parse the timestamp as it is displayed with a particular format using the\nto_timestamp\nfunction.\nThe final query looks like the following:\n\n```\nCREATE TABLE fxrates AS (    SELECT f0 pair, to_timestamp(f1, 'yyyyMMdd HH:mm:ss.SSS') timestamp, f2 bid, f3 ask    FROM    (\"EURGBP-2024-10.csv\" UNION ALL \"EURUSD-2024-10.csv\" UNION ALL \"GBPUSD-2024-10.csv\")    ORDER BY timestamp asc) timestamp(timestamp)\n```\n\n\n### Mid price smoothing\n\nThe\nmid\nfunction is relatively simple as it's just\n(bid + ask)/2\n. Its function is to act as syntactic sugar whenever dealing with price data that shows both sides of the order book.\nThis way, we can summarize both prices into a single series. Mid-price smoothing is useful because it provides a more stable view of price movements, reducing noise and allowing for clearer trend analysis.\nDeriving the mid is as simple as running the following query:\n\n```\nSELECT timestamp, mid(bid,ask)FROM fxrates where pair = 'EUR/USD' AND $__timeFilter(timestamp)\n```\n\nThe result is the smoother red line which is rendered alongside bid and ask prices on the below chart. The time interval below is only 20 seconds which gives an idea of how frequently the data is updating:\nClick to zoom\nAnother nice feature of deriving the mid price is the smoothing effect. At higher resolution, you may often see the top of the book widen on either one side, or both. What can happen, for example, is that someone trades against the dealer on one side, and then they widen their quotes immediately before tightening again.\nWe can see this effect in the above chart whereby the order book tightenings and widenings seem to always happen in symmetry, which indicates the changes in spread are likely one dealer adjusting their quotes.\nWe can use a combination of\nWITH\nand\nLT JOIN\nto join tables based on timestamp. However, LT JOIN performs on timestamp inequality which means that for a given timestamp in the first table, it will join on the first timestamp which is inferior or equal in the second table.\nIn practice, we can apply LT JOIN on a single table to join it to itself, and this will return both the current value for the timestamp in question, and the immediately preceding value. We can then apply a function to derive values such as the change since the last event:\n\n```\nWITHa AS (SELECT timestamp, bid FROM fxrates WHERE pair = 'EUR/USD'),b AS (SELECT timestamp, bid FROM fxrates WHERE pair = 'EUR/USD')SELECT a.timestamp, b.bid - a.bid bid_delta FROM b LT JOIN a WHERE $__timeFilter(a.timestamp)\n```\n\nIf we repeat this query in three Grafana panels, we can see the smoothing effect of the\nmid\nfunction over the individual bid and offer prices which ends up being much more stable:\nClick to zoom\n\n### Using ASOF JOIN to calculate cross pairs\n\nIn currency markets, crypto markets, and elsewhere, there is often a triangle arbitrage relationship between different instruments. For example, one may see that EURUSD is at 1.04, EURGBP at 0.831, and GBPUSD at 1.2536. To exchange EUR for USD, one can either:\n- Exchange 1 EUR for USD directly at 1.04. This yields 1.04 USD\n- Exchange 1 EUR for GBP at 0.831, and then exchange 0.831 GBP for USD at 1.2536. This yields0.831 x 1.2536 USDwhich yields 1.0417 USD\nIn the above example, the difference between the 2 effective rates (1.04 and 1.0417) means there is an arbitrage opportunity. One can simultaneously:\n- Sell 1.04 USD for 1 EUR\n- Buy 1.0417 USD for 1 EUR\nThis yields a 0.0017 USD riskless profit provided one can conduct both transactions simultaneously. This relationship exists between all currency pairs. The structure of currency dealing is such that these discrepancies are constantly monitored and arbitraged away.\nIn practice, this means that we should not see meaningful differences between two routes of trading the same pair, and when such differences materialize, they should either be tiny or transient.\nSince we're looking at transactional values, we should make sure we use the correct order book sides. For example:\n- The ask side on EURUSD means we're buying EUR and selling USD\n- To compare with an indirect rate, we should use the ask side of EURGBP (buy EUR, sell GBP), and the ask side of GBPUSD (buy GBP, sell USD). The buy GBP and sell GBP sides cancel each other out, and we end up net buying EUR, selling USD\nThe indirect rate can be calculated using QuestDB's\nASOF JOIN\n:\n\n```\nWITHeurgbp AS (SELECT timestamp, ask FROM fxrates WHERE pair = 'EUR/GBP'),gbpusd AS (SELECT timestamp, ask FROM fxrates WHERE pair = 'GBP/USD')SELECT eurgbp.timestamp, eurgbp.ask * gbpusd.ask FROM eurgbp ASOF JOIN gbpusdWHERE $__timeFilter(eurgbp.timestamp)\n```\n\nWhen plotting the direct and indirect rates, we can see that the indirect cost is, unsurprisingly, highly correlated to the direct rate. However, we can also see that the indirect rate always has an offset.\nThis is easy to understand intuitively. By buying EUR directly against the USD, we only conduct one transaction and cross the spread on EURUSD only.\nWhen doing this via GBP, we need to cross the spread twice:\n- on EURGBP\n- on GBPUSD\nThis results in higher trading costs and means arbitrage opportunities are even sparser.\nClick to zoom\nWe can see this offset increase sharply after 23:00. This likely corresponds to the overnight period, between US close and Asian open. All currencies continue trading between dealers, which is why we continue seeing quotes, but some dealers probably turn off quoting or increase spreads in this less liquid period.\nBut below, we can see in the first chart how the spreads widen at 23:00 exactly. In the second chart, we see how this greatly increases the cost of trading indirectly (i.e., crossing the book twice) compared to a direct trade on the pair.\nClick to zoom\n\n### Spread analysis\n\nSince we're talking about spread, we can use the new\nspread_bps\nto get an idea of how tight the pairs are trading, and how this changes over time.\nThe\nspread_bps\nfunction is more syntactic sugar which allows for simple queries. Without\nspread_bps()\nand\nmid()\n, a user would need to write something like\n10000 * (ask - bid) / ((bid + ask) / 2)\nto yield the same result. With them, we can greatly simplify the query:\n\n```\nSELECT timestamp, pair, avg(spread_bps(bid, ask)) spread_bpsFROM fxratesSAMPLE BY 10s\n```\n\nIn this instance, we can see a few things. The first is that EURUSD is very liquid (around 0.6 bps spread) whereas EURGBP and GBPUSD are closer to 0.9 or 1 bps.\nClick to zoom\nWhile these spreads are mostly static during the day, we can see the 'overnight' increase again when zooming out to a full day.\nClick to zoom\nThis increase in spread seems to correlate with the daily one-hour close of the\nCME Currency futures\n.\nQuite possibly, fewer dealers in the OTC markets at these hours, combined with the daily interruption of what is likely one of the only sources of price discovery at this time causes this increase in spread, not the best time to exchange currencies!\nAnother thing we can note on the above chart is the spike at 8 am which correlates with the European market open. However, when we look at other dates, we don't see this spike happening consistently.\nWe can run the following query to see if any trend exists (e.g., spread increase at US or European open):\n\n```\nSELECT hour, avg FROM (SELECT day(timestamp), hour(timestamp), avg(spread_bps(bid,ask))FROM fxrates WHERE pair = 'EUR/GBP'ORDER BY day asc, hour asc)\n```\n\nClick to zoom\nHowever, besides the overnight widening, there does not seem to be a persistent widening of the FX pairs in scope at other times of the day with the exception of the overnight period.\n\n### Looking at regressions and correlations\n\nThe new\nregr_slope\nfunction allows us to calculate the slope of the linear regression for two variables. In the below query, we can look at the relationship of EURUSD (the dependent variable) with GBPUSD (the independent variable).\nThe query makes use of\nSAMPLE BY\nto calculate aggregates, in this case on one-minute intervals, of the regression slope:\n\n```\nWITHeurusd AS (select timestamp, mid(bid, ask) eurusd FROM fxrates WHERE pair = 'EUR/USD'),gbpusd AS (select timestamp, mid(bid, ask) gbpusd FROM fxrates WHERE pair = 'GBP/USD')SELECT eurusd.timestamp, regr_slope(eurusd, gbpusd) FROM eurusd ASOF JOIN gbpusdWHERE $__timeFilter(eurusd.timestamp)SAMPLE BY 1m\n```\n\nBy eyeballing it, the slope over the selected day seems to be around +0.5 which means for any 1% increase of GBPUSD, EURUSD increases by 0.5%. However, one limitation of this is that the slope is only the best fit.\nIt does not give us an estimate of how well this fits. One way to get an intuitive understanding of how significant the correlation is would be to calculate the coefficient of determination.\nOne thing we can do is get a feel for it is to plot the two performances on a\nGrafana\nXY chart. We can again resort to\nASOF JOIN\nto stitch both currencies performance on the same time axis:\n\n```\nWITHeurusd AS (SELECT timestamp, last(mid(bid,ask))/first(mid(bid,ask)) -1 p1 FROM fxrates WHERE pair = 'EUR/USD' SAMPLE BY 1m),gbpusd AS (SELECT timestamp tp, last(mid(bid,ask))/first(mid(bid,ask)) -1 p2 FROM fxrates WHERE pair = 'GBP/USD' SAMPLE BY 1m)SELECT timestamp, p1 eur_usd, p2 gbp_usd FROM eurusd ASOF JOIN gbpusdWHERE $__timeFilter(timestamp)\n```\n\nThis yields the following plot:\nClick to zoom\nGiven the granularity of the dataset, we can look at the correlation for even shorter time frames, for example by changing the\nSAMPLE BY\nparameter from 1m to 1 second. This yields the following scatter. It seems much less concentrated than the previous one because it incorporates much more noise due to the higher sampling frequency:\nClick to zoom\n\n### Conclusion\n\nThis high-resolution dataset is pretty neat because it contains quite a lot of updates and gives a solid feel for what high-frequency data can look like. Such resolution enables us to run quite a range of analyses on diverse time frames, from minutes/hours to sub-second. While we only touched the surface to showcase some of the new functions, we hope it will inspire some to dig into these datasets as they are fascinating to look at.\nFor more financial blogs, checkout:\n- Scaling trading bot with time-series database\n- Making a trading gameboy\n- Stories in French real estate data\n- Ingesting financial tick data using a time-series database\n- Ingesting live market data from DataBento\n- Analyzing Bitcoin options data",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2199,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "trading",
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-dd267fc22d2a",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2020/08/06/my-journey-writing-questdb",
    "title": "My journey making QuestDB | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nA few weeks ago, I posted\nthe story of how I started QuestDB on Hacker News\n.\nSeveral people found the story interesting, so I thought I would post it here\nand describe the passage from working at a large energy trading company,\ndiscovering memory-mapping approaches in Java, the beginnings of building the\nsystem as a side-project, and how we got to where we are today with companies\nrelying on production instances of our\ntime-series database\n.\n\n## How I started building an open source time series database\n\nIt started in 2012 when an energy trading company hired me to rebuild their\nreal-time vessel tracking system. Management wanted me to use a well-known XML\ndatabase that they had just bought a license for. This option would have\nrequired to take down production for about a week just to ingest the data. And a\nweek downtime was not an option. With no more money to spend on software, I\nturned to alternatives such as OpenTSDB but they were not a fit for our data\nmodel. There was no solution in sight to deliver the project.\nThen, I stumbled upon\nPeter Lawreyâ€™s Java Chronicle library\n.\nIt loaded the same data in 2 minutes instead of a week using memory-mapped\nfiles. Besides the performance aspect, I found it fascinating that such a simple\nmethod was solving multiple issues simultaneously: fast write, read can happen\neven before data is committed to disk, code interacts with memory rather than IO\nfunctions, no buffers to copy. Incidentally, this was my first exposure to\nzero-GC Java.\nBut there were several issues. First, at the time It didnâ€™t look like the\nlibrary was going to be maintained. Second, it used Java NIO instead of using\nthe OS API directly. This adds overhead since it creates individual objects with\nsole purpose to hold a memory address for each memory page. Third, although the\nNIO allocation API was well documented, the release API was not. It was really\neasy to run out of memory and hard to manage memory page release. I decided to\nditch the XML DB and then started to write a custom storage engine in Java,\nsimilar to what Java Chronicle did. This engine used memory mapped files,\noff-heap memory and a custom query system for geospatial time series.\nImplementing this was a refreshing experience. I learned more in a few weeks\nthan in years on the job.\nThroughout my career, I mostly worked at large companies where developers are\nâ€œmanagedâ€ via itemized tasks sent as tickets. There was no room for creativity\nor initiative. In fact, it was in oneâ€™s best interest to follow the ticket's\nexact instructions, even if it was complete nonsense. I had just been promoted\nto a managerial role and regretted it after a week. After so much time hoping\nfor a promotion, I immediately wanted to go back to the technical side. I became\nobsessed with learning new stuff again, particularly in the high performance\nspace.\n\n## Turning an open source idea into a full-time project\n\nWith some money aside, I left my job and started to work on QuestDB solo. I used\nJava and a small C layer to interact directly with the OS API without passing\nthrough a selector API. Although existing OS API wrappers would have been easier\nto get started with, the overhead increases complexity and hurts performance. I\nalso wanted the system to be completely GC-free. To do this, I had to build\noff-heap memory management myself and I could not use off-the-shelf libraries. I\nhad to rewrite many of the standard ones over the years to avoid producing any\ngarbage.\nAs I had my first kid, I had to take contracting gigs to make ends meet over the\nfollowing 6 years. All the stuff I had been learning boosted my confidence and I\nstarted performing well at interviews. This allowed me to get better paying\ncontracts, I could take fewer jobs and free up more time to work on QuestDB\nwhile looking after my family. I would do research during the day and implement\nthis into QuestDB at night. I was constantly looking for the next thing, which\nwould take performance closer to the limits of the hardware.\n\n## Learning important lessons from mistakes\n\nA year in, I realised that my initial design was actually flawed and that it had\nto be thrown away. It had no concept of separation between readers and writers\nand would thus allow dirty reads. Storage was not guaranteed to be contiguous,\nand pages could be of various non-64-bit-divisible sizes. It was also very much\ncache-unfriendly, forcing the use of slow row-based reads instead of fast\ncolumnar and vectorized ones. Commits were slow, and as individual column files\ncould be committed independently, they left the data open to corruption.\nAlthough this was a setback, I got back to work. I wrote the new engine to allow\natomic and durable multi-column commits, provide repeatable read isolation, and\nfor commits to be instantaneous. To do this, I separated transaction files from\nthe data files. This made it possible to commit multiple columns simultaneously\nas a simple update of the last committed row id. I also made storage dense by\nremoving overlapping memory pages and writing data byte by byte over page edges.\n\n## Turning goals into a reality\n\nThis new approach improved query performance. It made it easy to split data\nacross worker threads and to optimise the CPU pipeline with prefetch. It\nunlocked column-based execution and additional virtual parallelism with\nSIMD instruction sets\nthanks to\nAgner Fogâ€™s Vector Class Library\n.\nIt made it possible to implement more recent innovations like our\nown version of Google SwissTable\n.\nI published more details when we released a demo server a few weeks ago on\nShowHN\n. This\ndemo\nis still available to try online with a pre-loaded dataset\nof 1.6 billion rows. Although it was hard and discouraging at first, this\nrewrite turned out to be the second best thing that happened to QuestDB.\nThe best thing was that people started to contribute to the project. I am really\nhumbled that Tanc and Nic left our previous employer to build QuestDB. A few\nmonths later, former colleagues of mine left their stable low-latency jobs at\nbanks to join us. I take this as a huge responsibility and I donâ€™t want to let\nthese guys down. The amount of work ahead gives me headaches and goosebumps at\nthe same time.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1117,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "performance",
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-9a95a5f1df56",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/jvm-current-thread-user-time",
    "title": "How a 40-Line Fix Eliminated a 400x Performance Gap | QuestDB",
    "text": "I have a habit of skimming the OpenJDK commit log every few weeks. Many commits are too complex for me to grasp in the limited time I have reserved for this ...\nspecial hobby\n. But occasionally something catches my eye.\nLast week,\nthis commit\nstopped me mid-scroll:\n\n```\n858d2e434dd 8372584: [Linux]: Replace reading proc to get thread CPUtime with clock_gettime\n```\n\nThe diffstat was interesting:\n+96 insertions, -54 deletions\n. The changeset adds a 55-line JMH benchmark, which means the production code itself is actually reduced.\n\n## The Deleted Code\n\nHere's what got removed from\nos_linux.cpp\n:\n\n```\nstatic jlong user_thread_cpu_time(Thread *thread) {  pid_t  tid = thread->osthread()->thread_id();  char *s;  char stat[2048];  size_t statlen;  char proc_name[64];  int count;  long sys_time, user_time;  char cdummy;  int idummy;  long ldummy;  FILE *fp;\n  os::snprintf_checked(proc_name, 64, \"/proc/self/task/%d/stat\", tid);  fp = os::fopen(proc_name, \"r\");  if (fp == nullptr) return -1;  statlen = fread(stat, 1, 2047, fp);  stat[statlen] = '\\0';  fclose(fp);\n  // Skip pid and the command string. Note that we could be dealing with  // weird command names, e.g. user could decide to rename java launcher  // to \"java 1.4.2 :)\", then the stat file would look like  //                1234 (java 1.4.2 :)) R ... ...  // We don't really need to know the command string, just find the last  // occurrence of \")\" and then start parsing from there. See bug 4726580.  s = strrchr(stat, ')');  if (s == nullptr) return -1;\n  // Skip blank chars  do { s++; } while (s && isspace((unsigned char) *s));\n  count = sscanf(s,\"%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu\",                 &cdummy, &idummy, &idummy, &idummy, &idummy, &idummy,                 &ldummy, &ldummy, &ldummy, &ldummy, &ldummy,                 &user_time, &sys_time);  if (count != 13) return -1;\n  return (jlong)user_time * (1000000000 / os::Posix::clock_tics_per_second());}\n```\n\nThis was the implementation behind\nThreadMXBean.getCurrentThreadUserTime()\n. To get the current thread's user CPU time, the old code was:\n- Formatting a path to/proc/self/task/<tid>/stat\n- Opening that file\n- Reading into a stack buffer\n- Parsing through a hostile format where the command name can contain parentheses (hence thestrrchrfor the last))\n- Runningsscanfto extract fields 13 and 14\n- Converting clock ticks to nanoseconds\nFor comparison, here's what\ngetCurrentThreadCpuTime()\ndoes and has always done:\n\n```\njlong os::current_thread_cpu_time() {  return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);}\njlong os::Linux::thread_cpu_time(clockid_t clockid) {  struct timespec tp;  clock_gettime(clockid, &tp);  return (jlong)(tp.tv_sec * NANOSECS_PER_SEC + tp.tv_nsec);}\n```\n\nJust a single\nclock_gettime()\ncall. There is no file I/O, no complex parsing and no buffer to manage.\n\n## The Performance Gap\n\nThe\noriginal bug report\n, filed back in 2018, quantified the difference:\n\"getCurrentThreadUserTime is 30x-400x slower than getCurrentThreadCpuTime\"\nThe gap widens under concurrency. Why is\nclock_gettime()\nso much faster? Both approaches require kernel entry, but the difference is in what happens next.\nThe/procpath:\n- open()syscall\n- VFS dispatch + dentry lookup\n- procfs synthesizes file content at read time\n- kernel formats string into buffer\n- read()syscall, copy to userspace\n- userspacesscanf()parsing\n- close()syscall\nTheclock_gettime(CLOCK_THREAD_CPUTIME_ID)path:\n- single syscall â†’posix_cpu_clock_get()â†’cpu_clock_sample()â†’task_sched_runtime()â†’ reads directly fromsched_entity\nThe\n/proc\npath involves multiple syscalls, VFS machinery, string formatting kernel-side, and parsing userspace-side. The\nclock_gettime()\npath is one syscall with a direct function call chain.\nUnder concurrent load, the\n/proc\napproach also suffers from kernel lock contention. The\nbug report\nnotes:\n\"Reading proc is slow (hence why this procedure is put under the method slow_thread_cpu_time(...)) and may lead to noticeable spikes in case of contention for kernel resources.\"\n\n## Why Two Implementations?\n\nSo why didn't\ngetCurrentThreadUserTime()\njust use\nclock_gettime()\nfrom the start?\nThe answer is (probably) POSIX. The standard mandates that\nCLOCK_THREAD_CPUTIME_ID\nreturns total CPU time (user + system). There's no portable way to request user time only. Hence the\n/proc\n-based implementation.\nThe Linux port of OpenJDK isn't limited to what POSIX defines, it can use Linux-specific features. Let's see how.\n\n## The Clockid Bit Hack\n\nLinux kernels since 2.6.12 (released in 2005)\nencode clock type information directly\ninto the\nclockid_t\nvalue. When you call\npthread_getcpuclockid()\n, you get back a clockid with a specific bit pattern:\n\n```\nBit 2:    Thread vs process clockBits 1-0: Clock type  00 = PROF  01 = VIRT  (user time only)  10 = SCHED (user + system, POSIX-compliant)  11 = FD\n```\n\nThe remaining bits encode the target PID/TID. Weâ€™ll come back to that in the bonus section.\nThe POSIX-compliant\npthread_getcpuclockid()\nreturns a clockid with bits\n10\n(SCHED). But if you flip those low bits to\n01\n(VIRT),\nclock_gettime()\nwill return user time only.\nThe new implementation:\n\n```\nstatic bool get_thread_clockid(Thread* thread, clockid_t* clockid, bool total) {  constexpr clockid_t CLOCK_TYPE_MASK = 3;  constexpr clockid_t CPUCLOCK_VIRT = 1;\n  int rc = pthread_getcpuclockid(thread->osthread()->pthread_id(), clockid);  if (rc != 0) {    // Thread may have terminated    assert_status(rc == ESRCH, rc, \"pthread_getcpuclockid failed\");    return false;  }\n  if (!total) {    // Flip to CPUCLOCK_VIRT for user-time-only    *clockid = (*clockid & ~CLOCK_TYPE_MASK) | CPUCLOCK_VIRT;  }\n  return true;}\nstatic jlong user_thread_cpu_time(Thread *thread) {  clockid_t clockid;  bool success = get_thread_clockid(thread, &clockid, false);  return success ? os::Linux::thread_cpu_time(clockid) : -1;}\n```\n\nAnd that's it. The new version has no file I/O, no buffer and certainly no\nsscanf()\nwith thirteen format specifiers.\n\n## Profiling time!\n\nLet's have a look at how it performs in practice. For this exercise, I am taking the\nJMH test included in the fix\n, the only change is that I increased the number of threads from 1 to 16\nand added a\nmain()\nmethod for simple execution from an IDE:\n\n```\n@State(Scope.Benchmark)@Warmup(iterations = 2, time = 5)@Measurement(iterations = 5, time = 5)@BenchmarkMode(Mode.SampleTime)@OutputTimeUnit(TimeUnit.MICROSECONDS)@Threads(16)@Fork(value = 1)public class ThreadMXBeanBench {    static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();    static long user; // To avoid dead-code elimination\n    @Benchmark    public void getCurrentThreadUserTime() throws Throwable {        user = mxThreadBean.getCurrentThreadUserTime();    }\n    public static void main(String[] args) throws RunnerException {        Options opt = new OptionsBuilder()                .include(ThreadMXBeanBench.class.getSimpleName())                .build();        new Runner(opt).run();    }}\n```\n\nAside: This is a rather unscientific benchmark, I have other processes running on my desktop etc. Anyway, here is the setup: Ryzen 9950X, JDK main branch at commit\n8ab7d3b89f656e5c\n. For the \"before\" case, I reverted the fix rather than checking out an older revision.\nHere is the result:\n\n```\nBenchmark                                             Mode      Cnt     Score   Error  UnitsThreadMXBeanBench.getCurrentThreadUserTime          sample  8912714    11.186 Â± 0.006  us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample              2.000          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample             10.272          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample             17.984          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample             20.832          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample             27.552          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample             56.768          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             79.709          us/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           1179.648          us/op\n```\n\nWe can see that a single invocation took 11 microseconds on average and the median was about 10 microseconds per invocation.\nThe CPU profile looks like this:\nClick to zoom, open in a new tab for interactivity\nThe CPU profile confirms that each invocation of\ngetCurrentThreadUserTime()\ndoes multiple syscalls. In fact, most of the CPU time\nis spent in syscalls. We can see files being opened and closed. Closing alone results in multiple syscalls, including futex locks.\nLet's see the benchmark result with the fix applied:\n\n```\nBenchmark                                             Mode       Cnt     Score   Error  UnitsThreadMXBeanBench.getCurrentThreadUserTime          sample  11037102     0.279 Â± 0.001  us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               0.070          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               0.310          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               0.440          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               0.530          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               0.610          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample               1.030          us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample               3.088          us/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample            1230.848          us/op\n```\n\nThe average went down from 11 microseconds to 279 nanos. This means the latency of the fixed version is 40x lower\nthan the old version. While this is not a 400x improvement, it's within the 30x - 400x range from the original report. Chances are\nthe delta would be higher with a different setup.\nLet's have a look at the new profile:\nClick to zoom, open in a new tab for interactivity\nThe profile is much cleaner. There is just a single syscall. If the profile is to be trusted then most of the time is spent in JVM, outside of the kernel.\n\n## How Documented Is This?\n\nBarely. The bit encoding is stable. It hasn't changed in 20 years, but you won't find it in the\nclock_gettime(2)man page\n.\nThe closest thing to official documentation is the kernel source itself, in\nkernel/time/posix-cpu-timers.c\nand the\nCPUCLOCK_*macros\n.\nThe kernel's policy is clear:\ndon't break userspace\n.\nLinus's position on ABI stability is... unambiguous.\nMy take: If\nglibc depends on it\n, it's not going away.\n\n## Pushing Further\n\nWhen looking at profiler data from the 'after' run, I spotted a further optimization opportunity: A good portion of the remaining syscall is spent inside a radix tree lookup. Have a look:\nClick to zoom\nWhen the JVM calls\npthread_getcpuclockid()\n, it receives a\nclockid\nthat encodes the thread's ID. When this\nclockid\nis passed to\nclock_gettime()\n,\nthe kernel extracts the thread ID and performs a radix tree lookup to find the\npidstructure\nassociated with that ID.\nHowever, the Linux kernel has a fast-path. If the encoded PID in the\nclockid\nis 0, the kernel interprets this as \"the current thread\" and skips the radix tree lookup entirely, jumping to the current task's structure directly.\nThe OpenJDK fix currently obtains the specific TID, flips the bits, and passes it to\nclock_gettime()\n. This forces the kernel to take the \"generalized path\" (the radix tree lookup).\nThe\nsource code\nlooks like this:\n\n```\n/* * Functions for validating access to tasks. */static struct pid *pid_for_clock(const clockid_t clock, bool gettime){[...]\n  /*  * If the encoded PID is 0, then the timer is targeted at current  * or the process to which current belongs.  */  if (upid == 0)      // the fast path: current task lookup, cheap      return thread ? task_pid(current) : task_tgid(current);\n  // the generalized path: radix tree lookup, more expensive  pid = find_vpid(upid);  [...]\n```\n\nIf the JVM constructed the entire\nclockid\nmanually with PID=0 encoded (rather than obtaining the\nclockid\nvia\npthread_getcpuclockid()\n), the kernel could take the fast-path and avoid the radix tree lookup altogether.\nThe JVM already pokes bits in the\nclockid\n, so constructing it entirely from scratch wouldn't be a bigger leap compatibility-wise.\nLet's try it!\nFirst, a refresher on the\nclockid\nencoding. The\nclockid\nis constructed like this:\n\n```\nclockid for TID=42, user-time-only:\n  1111_1111_1111_1111_1111_1110_1010_1101  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€~42â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â””â”˜                                      â”‚ â””â”€ 01 = VIRT (user time only)                                      â””â”€â”€â”€ 1 = per-thread\n```\n\nFor the current thread, we want PID=0 encoded, which gives\n~0\nin the upper bits:\n\n```\n  1111_1111_1111_1111_1111_1111_1111_1101  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ~0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â””â”˜                                      â”‚ â””â”€ 01 = VIRT (user time only)                                      â””â”€â”€â”€ 1 = per-thread\n```\n\nWe can translate this into C++ as follows:\n\n```\n// Linux Kernel internal bit encoding for dynamic CPU clocks:// [31:3] : Bitwise NOT of the PID or TID (~0 for current thread)// [2]    : 1 = Per-thread clock, 0 = Per-process clock// [1:0]  : Clock type (0 = PROF, 1 = VIRT/User-only, 2 = SCHED)static_assert(sizeof(clockid_t) == 4, \"Linux clockid_t must be 32-bit\");constexpr clockid_t CLOCK_CURRENT_THREAD_USERTIME = static_cast<clockid_t>(~0u << 3 | 4 | 1);\n```\n\nAnd then make a tiny teensy change to\nuser_thread_cpu_time()\n:\n\n```\njlong os::current_thread_cpu_time(bool user_sys_cpu_time) {  if (user_sys_cpu_time) {    return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);  } else {   - return user_thread_cpu_time(Thread::current());   + return os::Linux::thread_cpu_time(CLOCK_CURRENT_THREAD_USERTIME);  }\n```\n\nThe\nchange above\nis sufficient to make\ngetCurrentThreadUserTime()\nuse the fast-path in the kernel.\nGiven that we are in nanoseconds territory already, we tweak the test a bit:\n- Increase the iteration and fork count\n- Use just a single thread to minimize noise\n- Switch to nanos\nThe benchmark changes are meant to eliminate noise from the rest of my system and get a more precise measurement of the small delta we expect:\n\n```\n@State(Scope.Benchmark)@Warmup(iterations = 4, time = 5)@Measurement(iterations = 10, time = 5)@BenchmarkMode(Mode.SampleTime)@OutputTimeUnit(TimeUnit.NANOSECONDS)@Threads(1)@Fork(value = 3)public class ThreadMXBeanBench {    static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();    static long user; // To avoid dead-code elimination\n    @Benchmark    public void getCurrentThreadUserTime() throws Throwable {        user = mxThreadBean.getCurrentThreadUserTime();    }\n    public static void main(String[] args) throws RunnerException {        Options opt = new OptionsBuilder()                .include(ThreadMXBeanBench.class.getSimpleName())                .build();        new Runner(opt).run();    }}\n```\n\nThe version currently in JDK main branch gives:\n\n```\nBenchmark                                             Mode      Cnt       Score   Error  UnitsThreadMXBeanBench.getCurrentThreadUserTime          sample  4347067      81.746 Â± 0.510  ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               69.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               80.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               90.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               90.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               90.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample              230.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             1980.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           653312.000          ns/op\n```\n\nWith the manual\nclockid\nconstruction, which uses the kernel fast-path, we get:\n\n```\nBenchmark                                             Mode      Cnt       Score   Error  UnitsThreadMXBeanBench.getCurrentThreadUserTime          sample  5081223      70.813 Â± 0.325  ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               59.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               70.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               70.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               70.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               80.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample              170.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             1830.000          ns/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           425472.000          ns/op\n```\n\nThe average went down from 81.7 ns to 70.8 ns, so about a 13% improvement. The improvements are visible across all percentiles as well.\nIs it worth the loss of clarity from constructing the\nclockid\nmanually rather than using\npthread_getcpuclockid()\n?\nI am not entirely sure. The absolute gain is small and makes additional assumptions about kernel internals, including the size of\nclockid_t\n. On the other hand, it's still a gain without any downside in practice.\n(famous last words...)\n\n## Browsing for Gems\n\nThis is why I like browsing commits of large open source projects. A 40-line deletion eliminated a 400x performance gap. The fix required no new kernel features, just knowledge of a stable-but-obscure Linux ABI detail.\nThe lessons:\nRead the kernel source.\nPOSIX tells you what's portable. The kernel source code tells you what's possible. Sometimes there's a 400x difference between the two. Whether it is worth exploiting is a different question.\nCheck the old assumptions.\nThe\n/proc\nparsing approach made sense when it was written, before anyone realized it could be exploited this way. Assumptions get baked into code. Revisiting them occasionally pays off.\nThe change landed on December 3, 2025. Just one day before the\nJDK 26 feature freeze\n. If you're using\nThreadMXBean.getCurrentThreadUserTime()\n, JDK 26 (releasing March 2026) brings you a free 30-400x speedup!\nUpdate:\nJonas Norlinder (the patch author) shared\nhis own deep-dive\nin the Hacker News discussion - written independently around the same time. Great minds! His is more rigorous on the memory overhead side; mine digs deeper into the bit encoding and the PID=0 fast-path.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2288,
    "metadata": {
      "relevance_score": 0.42857142857142855,
      "priority_keywords_matched": [
        "benchmark",
        "performance",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-095974a82f01",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/how-to-benchmark-raspberry-pi",
    "title": "How to upgrade and benchmark a Raspberry Pi5 | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nThis article will demonstrate how to upgrade your Raspberry Pi with an SSD and\noperating system setup which can take advantage of the new hardware. It'll then\nshow how to setup your OS, the\ntime-series benchmarking suite\n,\nand install QuestDB.\nWith this preparation, you can follow along with\nour own benchmarking attempt\n. It's helpful\nfor those who want to install an M.2 NVMe on their Raspberry Pi, or those who\nwant to do that\nand\nbenchmark QuestDB.\n\n## Make a Pi\n\nStep one. Supe up the Pi.\nAnd for that, we'll use a very solid M.2 NVMe SSD.\nOur chosen NVMe drive will exceed 2000mb/s in both read and write. A\ntop-of-the-line SD card will tickle 300mb/s at best. With a very fast NVMe\ndrive, we'll move data as fast as the CPU can handle. Our bottleneck won't be\nstorage. And an NVMe drive is also much more durable.\nTo prevent this article from stretching too long, we'll summarize the build\nsteps and link to high quality resources.\n\n### Attach the M.2 drive\n\nOur next step is to attach the NVMe drive to the Pi.\nThe manufacturer of the M.2 extension kit, Pimoroni, offers\nclear and easy-to-follow\ndocumentation.\nIt involves fastening the Pi to the board:\nThen assembling it all together, with the delicate-yet-responsible bridge strip\ncable snapped-in:\nAfter that, we secure it in the case. We recommend using a case with a fan. The\nextra air movement will ensure your Pi stays cool and comfortable, even during\nintense benchmarks.\nAll of the remaining steps occur on the software-side.\n\n### Set the Pi to M.2\n\nFor this step, we followed a\nsolid guide from Tom's Hardware\n.\nThe M.2 is not automatically detected. To enable it, we edit the Raspberry Pi\nfirmware settings. To do so, boot into the Pi and alter a couple configs, then\nreboot. If setup correctly, the system will reboot into the M.2 drive - which,\nat the moment, does not have an operating system.\nSo before we do that, we'll boot into Raspberry Pi OS one last time to pre-load\nour M.2 with an OS.\n\n### Install Ubuntu on the M.2\n\nThe Raspberry Pi often comes with an SD card, pre-loaded with Debian-based\nRaspberry Pi OS. However, to standardize our approach somewhat weâ€™ll use the\nRaspberry Pi Imager â€” a fantastic utility â€” to put Ubuntu Server on our NVMe\ndrive.\nFinest flasher ye ever did see\nItâ€™s helpful to pre-configure users, WiFi (if applicable) and SSH. You donâ€™t\nhave to, but it is time saving. That way, the Pi is setup somewhere comfortable\nand we can SSH into it from our usual workstation.\nWeâ€™ll restart into our Ubuntu Server, ready to benchmark.\n\n## Benchmark QuestDB on the Pi\n\nOur installation steps will run in a few parts.\nTo help you run this on your own, theyâ€™re written as bash scripts. Copy them,\nset them to executable, and run them in order. This will get you going. Theyâ€™re\ncommented so that youâ€™re aware of what theyâ€™re doing, as thereâ€™s some nifty\nbits.\nIn total, weâ€™ll:\n- Setup our directory structure\n- Download & install Go, the Time Series Benchmarking Suite (TSBS), the latest\ncut of QuestDB, fresh from GitHub, and utilities and programs to facilitate\nsuch asmake,mavenand Java\n- Generate a boat load of sample data\n- Throw that data into QuestDB, with great haste\n- Query that data out of QuestDB, with similar hastiness\n\n### Prepare dependencies\n\nThe following sets up the main directories and installs dependencies.\nprepare_deps.sh\n\n```\n#!/bin/bash\n# Ensure running as root to avoid permission issues, especially with apt and snapif [ \"$(id -u)\" -ne 0 ]; then    echo \"This script must be run as root\" >&2    exit 1fi\n# Create the main data directory if it doesn't existif [ ! -d \"/data/tsbs\" ]; then  echo \"/data/tsbs does not exist, creating...\"  mkdir -p /data/tsbs/fi\necho \"Navigating to /data directory...\"cd /data\n# Install Go using snapecho \"Installing Go...\"snap install go --classic\n# Clone the TSBS repository if it doesn't existif [ ! -d \"tsbs\" ]; then    echo \"Cloning the TSBS repository...\"    git clone https://github.com/questdb/tsbs || { echo \"Failed to clone TSBS repository\"; exit 1; }ficd tsbs\n# Install make and other potentially missing build essentialsecho \"Updating package lists...\"apt updateecho \"Installing build-essential tools...\"apt install -y make build-essential || { echo \"Failed to install build-essential tools\"; exit 1; }\n# Install Java, for ARM of courseecho \"Installing Java for ARM...\"apt install -y openjdk-11-jdk || { echo \"Failed to install Java\"; exit 1; }export JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-arm64\"echo \"Java home is set to $JAVA_HOME\"\n# Install Mavenecho \"Installing Maven...\"apt install -y maven || { echo \"Failed to install Maven\"; exit 1; }\n# Compile the TSBSecho \"Compiling TSBS...\"make || { echo \"TSBS compilation failed\"; exit 1; }\n# Create data and results directories within the TSBS directorymkdir -p data/quest results\necho \"System & TSBS setup complete. Run questdb.sh!\"\n```\n\nWith this, we've setup the system and TSBS.\nIts directory is where weâ€™ll perform the bulk of our operations.\n\n### Set up QuestDB\n\nNote that weâ€™re using the\nno-jre-bin\nversion of QuestDB. Itâ€™s very\nlightweight, and requires that we bring-our-own Java. It is less than 10MB!\nThe script will also initialize, configure, and start QuestDB:\nstart_questdb.sh\n\n```\n#!/bin/bash\n# Enter data dircd /data/ || { echo \"Failed to change directory to /data/\"; exit 1; }\n# Clone the QuestDB repository if it doesn't existif [ ! -d \"questdb\" ]; then    git clone https://github.com/questdb/questdb.git || { echo \"Failed to clone repository\"; exit 1; }ficd questdb || { echo \"Failed to change directory to questdb\"; exit 1; }\n# Build QuestDB without running testsmvn clean package -DskipTests -P build-web-console,build-binaries || { echo \"Build failed\"; exit 1; }\n# Go back to the parent directorycd .. || { echo \"Failed to navigate back to parent directory\"; exit 1; }\n# Copy the QuestDB binary packagecp questdb/core/target/questdb-*-no-jre-bin.tar.gz questdb-no-jre-bin.tar.gz || { echo \"Failed to copy QuestDB package\"; exit 1; }\n# Extract the QuestDB packagetar -xvf questdb-no-jre-bin.tar.gz || { echo \"Extraction failed\"; exit 1; }\n# Rename the QuestDB directory for easier accessmv questdb*-no-jre-bin questdb || { echo \"Rename failed\"; exit 1; }\n# Start, then stop QuestDB to initialize configuration./questdb/questdb.sh start || { echo \"QuestDB start failed\"; exit 1; }./questdb/questdb.sh stop || { echo \"QuestDB stop failed\"; exit 1; }\n# Create a directory for QuestDB root, adjust as necessarymkdir -p /data/tsbs/QDB_ROOT || { echo \"Failed to create QDB_ROOT directory\"; exit 1; }\n# Update the server configuration to set the database root directorysed -i 's|#cairo.root=db|cairo.root=/data/tsbs/QDB_ROOT|g' /data/questdb-no-jre-bin/conf/server.conf || { echo \"sed operation failed\"; exit 1; }\n# Start QuestDB with the new configuration./questdb/questdb.sh start || { echo \"QuestDB start with new config failed\"; exit 1; }\necho \"QuestDB is alive! Setup and initial configuration complete.\"\n```\n\nQuestDB is alive and well.\nYou can access it at\nhttp://localhost:3000\n.\nThe endpoint we'll target is at port 9000.\nTime to generate data, then ingest the data.\nBut that's a story for another article!\n\n## Summary\n\nThis article guided through the installation and setup of a Raspberry Pi 5 with\nan M.2 NVMe SSD. We also setup QuestDB. It's ready, waiting. Purring, even!\nTo continue the story, head to our\nRaspberry Pi benchmark\narticle.\n\n### Want more Pi?\n\nIf this sort of thing is up your alley, we've got more fun Pi projects:\n- Create an IoT server with QuestDB and a Raspberry Pi\n- Build a temperature IoT sensor with Raspberry Pi Pico & QuestDB\n- Create an ADS-B flight radar with QuestDB and a Raspberry Pi\n- Benchmark QuestDB on a Raspberry Pi 5\nWe'd also love to see your benchmarks.\nCan you replicate this scenario? Improve upon it?\nIs there a DB that can do this better, or faster?\nLet us know on social media or in our engaging\nCommunity Forum\nor our public\nSlack\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1344,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "benchmark",
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-36cdca1dd8c2",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/moving-average-signals-questdb-grafana-coinbase",
    "title": "Moving average signals with QuestDB, Grafana and Coinbase | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nThe idea of quant trading is to try and find new predictive signals to derive\nprofitable trading strategies. As a result there are a plethora of market\nsignals, from simple widespread ones used by retail outlets to complicated\nproprietary strategies employed by hedge funds.\nIn this article, we'll look at using QuestDB queries in a\nGrafana\ndashboard to derive and test simple\ntrading signals. We will use the Coinbase crypto market data as the underlying\nsource of data. If you wish to follow along, please refer to\nour tutorial\non\nsetting up QuestDB and Grafana via Docker or visit the\nGrafana docs\n.\nThis data in particular was pulled using the\ncryptofeed\nlibrary. See\nthis tutorial\nto set it up quickly and start ingesting real trades from the largest crypto\nexchanges directly into QuestDB.\n\n## Calculating a moving average\n\nA moving average computes the average price of a stock over a rolling window of\ntime. Conveniently, this is a strong use of QuestDB's newly released\nwindow functions\n.\nWe can define a moving average in a query as follows:\n\n```\nSELECT timestamp time,       symbol,       price,       avg(price)       OVER (PARTITION BY symbol             ORDER BY timestamp             RANGE 1 HOUR PRECEDING ) moving_average_1h,FROM tradesWHERE $__timeFilter(timestamp)AND   symbol = $Pairs\n```\n\nThe moving average would look like the followingâ€¦\nClick to zoom\n\n## Moving average as a technical analysis indicator\n\nTechnical analysis\nrefers to analysis based solely on chart data. This\ncontrasts with other types of analysis, such as\nfundamental analysis\nwhich\nlooks at the underlying value of the assets to determine its market price. For\nexample, for a stock price, this would mean analyzing the company's business\noperations and cash flows to derive a value for the stock price.\nMoving averages are part of the\ntechnical analysis\nspectrum. The typical way\nmoving averages are looked at from a signal point of view is that the crossing\nof two moving averages over different time intervals indicates the reversal of a\ntrend, and therefore an opportunity to trade.\nFor example, if the shorter term moving average crosses the longer term from the\nbottom, then this would indicate a buy signal, and a crossing from the opposite\ndirection would indicate a sell signal. There are, of course, many limits to\nthis.\nFirst - and by definition - the moving average looks at past price data. The\nunderlying assumption is that the chart data for a given asset is, in itself, a\npredictor of upcoming chart data. Another way to frame that is as a projected\nprice path.\nIn addition, this data is lagging by definition and therefore the indicator\nwould lag as well. The counterpoint to these arguments on\ntechnical analysis\nis that it can sometimes explain price action on assets that don't have much\nunderlying value other than that created through speculation. Whether this is a\ngood indicator or not is outside of our scope, but we thought it would be good\nto mention some context.\nFrom here, we can build an indicator by adding another moving average to our\ninitial query with a different period. We will make these two periods variable\nby using Grafana variables:\n\n```\nSELECT timestamp AS time,       symbol,       price,       AVG(price) OVER (            ORDER BY timestamp            RANGE '$DurationOne' $FrameOne PRECEDING       ) AS mAvgOne,       AVG(price) OVER (            ORDER BY timestamp            RANGE '$DurationTwo' $FrameTwo PRECEDING       ) AS mAvgTwoFROM tradesWHERE $__timeFilter(timestamp)AND   symbol = $Pairs\n```\n\nClick to zoom\n\n### Window functions in QuestDB\n\nBefore we dig deeper, a quick explanation of window functions for those who may\nbe unfamiliar. In QuestDB, a window function is defined by an\nOVER\nclause that\nfollows the window function. This clause includes three components:\npartitioning\n, ordering and frame\nspecification.\nThe\nPARTITION BY\nclause divides the result set into partitions (groups of\nrows) upon which the window function is applied. The\nORDER BY\nclause within\nthe\nOVER\nclause determines the order of the rows in each partition. The frame\nspecification defines the set of rows included in the window, relative to the\ncurrent row.\nWindow functions in QuestDB are often used in analytics for tasks such as\ncalculating running totals or averages, finding the maximum or minimum value in\na sequence or partition, ranking items within a specific category or partition,\nand calculating or cumulative sums. And, of course, to calculate moving\naverages.\n\n### Extracting a signal\n\nHaving built this query, we can now use a subquery to extract the signal of when\nthe values are crossing and then overlay it on our charts.\n\n```\nWITH data AS (    SELECT timestamp AS time,           symbol,           price,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationOne' $FrameOne PRECEDING           ) AS mAvgOne,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationTwo' $FrameTwo PRECEDING           ) AS mAvgTwo    FROM trades    WHERE $__timeFilter(timestamp)    AND   symbol = $Pairs)SELECT time,       symbol,       (mAvgOne - mAvgTwo) AS differenceFROM data\n```\n\nClick to zoom\nThe line shows the difference between the two moving averages. When this\ndifference crosses zero, then could a strong signal. We therefore need to\ntransform the analog line above into two pieces of binary information:\n- A trade action signal (do nothing or trade)\n- A direction signal (buy or sell)\nThe direction is simple: if the difference between the short term and the long\nterm moving averages is positive, then the signal should be 'buy'. Otherwise, it\nshould be sell. Shown with a\nCASE\nfunction to output\n-1\nif the strategy\noutput is a short position and\n1\nif long.\n\n```\nWITH data AS (    SELECT timestamp AS time,           symbol,           price,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationOne' $FrameOne PRECEDING           ) AS mAvgOne,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationTwo' $FrameTwo PRECEDING           ) AS mAvgTwo    FROM trades    WHERE $__timeFilter(timestamp)    AND   symbol = $Pairs)SELECT time,       symbol,       CASE WHEN (mAvgOne - mAvgTwo) > 0 THEN 1            ELSE -1       END AS directionFROM data\n```\n\nClick to zoom\nWe can also vary the frequency, for example with shorter moving average\nintervals. Unsurprisingly, for the same data, it results in a higher frequency\nof trades:\nClick to zoom\nLastly, we need to extract the moment we should trade. To do this, we can check\nfor the moment that the direction signal changes by running an average query\nover two rows. If the average is zero, then the two surrounding values are of\nopposite signs (\n1\nand\n-1\n). We can then conclude there was a change in the\ndirection signal and should therefore make a trade.\n\n```\nWITH data AS (    SELECT timestamp,           symbol,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationOne' $FrameOne PRECEDING           ) AS mAvgOne,           AVG(price) OVER (               ORDER BY timestamp               RANGE '$DurationTwo' $FrameTwo PRECEDING           ) AS mAvgTwo    FROM trades    WHERE symbol = $Pairs    AND   $__timeFilter(timestamp))SELECT  ts,        CASE            WHEN abs(averageCheck) <= 0 THEN 1            ELSE 0        ENDFROM (    SELECT ts,           AVG(Xdirection) OVER (               ORDER BY ts               ROWS 1 PRECEDING           ) AS averageCheck    FROM (        SELECT timestamp AS ts,               symbol,               CASE                   WHEN (mAvgOne - mAvgTwo) > 0 THEN 1                   ELSE -1               END AS Xdirection        FROM data    ))\n```\n\nClick to zoom\nHaving done this, we can vary the symbols, time frames and the window sizes for\nboth moving averages to try different combinations.\n\n### Strategic profits and losses\n\nDoes the strategy make money? Or perhaps more importantly, does it make good\nrisk-adjusted returns? Probably not, but we can check by adjusting the previous\nquery to simulate a trade at the last price whenever the signal is triggered.\nWhile this is not an efficient way of doing this, it's fun!\nAnd hey, in this very scenario, we could have made 60 bucksâ€¦ If we traded over\n50 times, or over 110K USD notional. Guess we'll need to find another strategy!\nClick to zoom\n\n## Summary\n\nIn this article, we explored the concept of moving averages as a trading signal\nusing QuestDB queries in a Grafana dashboard, with Coinbase crypto market data\nas our source. We discussed how moving averages, a form of technical analysis,\ncan indicate potential trading opportunities when two averages over different\ntime intervals cross. We then demonstrated how to build an indicator by adding\nanother moving average to our initial query with a different period, and how to\nextract a signal when these values cross.\nWe also discussed how to determine the direction of the trade (buy or sell)\nbased on the difference between the short term and the long term moving\naverages, and how to identify the moment to trade by checking for the moment\nthat the direction signal changes by running an average query over two rows.\nFinally, we touched on the strategy's profitability and risk-adjusted returns,\nand demonstrated how to simulate a trade at the last price whenever the signal\nis triggered. Have fun and good luck!\nCheckout our real time (crypto dashboard)[/dashboards/crypto] for further\ninspiration.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1492,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-79d869b245cb",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/processing-time-series-with-questdb-apache-kafka",
    "title": "Processing Time-Series Data with QuestDB and Apache Kafka | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nApache Kafka\nis a battle-tested\ndistributed stream-processing platform popular in the financial industry to\nhandle mission-critical transactional workloads. Kafka's ability to handle large\nvolumes of real-time market data makes it a core infrastructure component for\ntrading, risk management, and fraud detection. Financial institutions use Kafka\nto stream data from market data feeds, transaction data, and other external\nsources to drive decisions.\nA common data pipeline to ingest and store financial data involves publishing\nreal-time data to Kafka and utilizing Kafka Connect to stream that to databases.\nFor example, the market data team may continuously update real-time quotes for a\nsecurity to Kafka, and the trading team may consume that data to make buy/sell\norders. Processed market data and orders may then be saved to a time series\ndatabase for further analysis.\nIn this article, we'll create a sample data pipeline to illustrate how this\ncould work in practice. We will poll an external data source (FinnHub) for\nreal-time quotes of stocks and ETFs, and publish that information to Kafka.\nKafka Connect will then grab those records and publish it to a time series\ndatabase (QuestDB) for analysis.\n\n## Prerequisites\n\n- Git\n- Docker Engine: 20.10+\n- Golang 1.19+\n- FinnHub API Token\n\n## Setup\n\nTo run the example locally, first clone the\nrepo\nThe codebase is organized into three parts:\n- Golang code is located at the root of the repo\n- Dockerfile for the Kafka Connect QuestDB image and the Docker Compose YAML\nfile is under docker\n- JSON files for Kafka Connect sinks are underkafka-connect-sinks\n\n## Building the Kafka Connect QuestDBÂ Image\n\nWe first need to build the Kafka Connect docker image with QuestDB Sink\nconnector. Navigate to the\ndocker\ndirectory and run\ndocker-compose build\n.\nThe Dockerfile is simply installing the\nKafka QuestDB Connector via Confluent Hub\non top of the Confluent Kafka Connect base image:\n\n```\nFROM confluentinc/cp-kafka-connect-base:7.3.2\\RUN confluent-hub install --no-prompt questdb/kafka-questdb-connector:0.6\n```\n\n\n## Start Kafka, Kafka Connect,Â QuestDB\n\nNext, we will set up the infrastructure via Docker Compose. From the same\ndocker\ndirectory, run Docker Compose in the background:\n\n```\ndocker-compose up -d\n```\n\nThis will start Kafka + Zookeeper, our custom Kafka Connect image with the\nQuestDB Connector installed, as well as QuestDB. The full content of the Docker\nCompose file is as follows:\n\n```\n---version: \"2\"services:  zookeeper:    image: confluentinc/cp-zookeeper:7.3.2    hostname: zookeeper    container_name: zookeeper    ports:      - \"2181:2181\"    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000\n  broker:    image: confluentinc/cp-kafka:7.3.2    hostname: broker    container_name: broker    depends_on:      - zookeeper    ports:      - \"9092:9092\"      - \"9101:9101\"    environment:      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1      KAFKA_JMX_PORT: 9101      KAFKA_JMX_HOSTNAME: localhost\n  kafka-connect:    image: cp-kafka-connect-questdb    build:      context: .    hostname: connect    container_name: connect    depends_on:      - broker      - zookeeper    ports:      - \"8083:8083\"    environment:      CONNECT_BOOTSTRAP_SERVERS: \"broker:29092\"      CONNECT_REST_ADVERTISED_HOST_NAME: connect      CONNECT_REST_PORT: 8083      CONNECT_GROUP_ID: compose-connect-group      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n  questdb:    image: questdb/questdb    hostname: questdb    container_name: questdb    ports:      - \"9000:9000\"      - \"9009:9009\"\n```\n\n\n## Start the QuestDB Kafka ConnectÂ Sink\n\nWait for the Docker containers to be healthy (the kafka-connect image will log\n\"Finished starting connectors and tasks\" message), and we can create our Kafka\nConnect sinks. We will create two sinks: one for Tesla and one for SPY (SPDR S&P\n500 ETF) to compare price trends of a volatile stock and the overall market.\nIssue the following curl command to create the Tesla sink within the\nkafka-connect-sinks\ndirectory:\n\n```\ncurl -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" --data @questdb-sink-TSLA.json http://localhost:8083/connectors\n```\n\nThe JSON file it posts contains the following configurations.\n\n```\n{  \"name\": \"questdb-sink-SPY\",  \"config\": {    \"connector.class\": \"io.questdb.kafka.QuestDBSinkConnector\",    \"tasks.max\": \"1\",    \"topics\": \"topic_SPY\",    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",    \"key.converter.schemas.enable\": \"false\",    \"value.converter.schemas.enable\": \"false\",    \"host\": \"questdb\",    \"timestamp.field.name\": \"timestamp\"  }}\n```\n\nCreate the sink for SPY as well:\n\n```\ncurl -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" --data @questdb-sink-SPY.json http://localhost:8083/connectors\n```\n\n\n## Streaming real-time stock quotes with Apache Kafka and QuestDB\n\nNow that we have our data pipeline set up, we are ready to stream real time\nstock quotes to\nKafka\nand store them\nin QuestDB.\nFirst, we need to get a free API token from Finnhub Stock API. Create a\nfree account online\nand copy the API key.\nExport that key to our shell under\nFINNHUB_TOKEN\n:\n\n```\nexport FINNHUB_TOKEN=<my-token-here>\n```\n\nThe\nrealtime quote endpoint\nreturns various\nattributes such as the current price, high/low/open quotes, as well as previous\nclose price. Since we are just interested in the current price, we only grab the\nprice and add the ticket symbol and timestamp to the Kafka JSON message.\nThe code below will grab the quote every 30 seconds and publish to the Kafka\ntopic:\ntopic_TSLA\n.\n\n```\npackage main\nimport (\t\"encoding/json\"\t\"fmt\"\t\"io/ioutil\"\t\"net/http\"\t\"os\"\t\"time\"\n\t\"github.com/confluentinc/confluent-kafka-go/v2/kafka\")\ntype StockData struct {\tPrice float64 `json:\"c\"`}\ntype StockDataWithTime struct {\tSymbol    string  `json:\"symbol\"`\tPrice     float64 `json:\"price\"`\tTimestamp int64   `json:\"timestamp\"`}\nfunc main() {\t// Create a new Kafka producer instance\tp, err := kafka.NewProducer(&kafka.ConfigMap{\"bootstrap.servers\": \"localhost:9092\"})\tif err != nil {\t\tpanic(fmt.Sprintf(\"Failed to create producer: %s\\n\", err))\t}\tdefer p.Close()\n\tfor {\t\ttoken, found := os.LookupEnv(\"FINNHUB_TOKEN\")\t\tif !found {\t\t\tpanic(\"FINNHUB_TOKEN is undefined\")\t\t}\t\tsymbol := \"TSLA\"\n\t\turl := fmt.Sprintf(\"https://finnhub.io/api/v1/quote?symbol=%s&token=%s\", symbol, token)\n\t\t// Retrieve the stock data\t\tresp, err := http.Get(url)\t\tif err != nil {\t\t\tfmt.Println(err)\t\t\treturn\t\t}\t\tdefer resp.Body.Close()\n\t\t// Read the response body\t\tbody, err := ioutil.ReadAll(resp.Body)\t\tif err != nil {\t\t\tfmt.Println(err)\t\t\treturn\t\t}\n\t\t// Unmarshal the JSON data into a struct\t\tvar data StockData\t\terr = json.Unmarshal(body, &data)\t\tif err != nil {\t\t\tfmt.Println(err)\t\t\treturn\t\t}\n\t\t// Format data with timestamp\t\ttsData := StockDataWithTime{\t\t\tSymbol:    symbol,\t\t\tPrice:     data.Price,\t\t\tTimestamp: time.Now().UnixNano() / 1000000,\t\t}\n\t\tjsonData, err := json.Marshal(tsData)\t\tif err != nil {\t\t\tfmt.Println(err)\t\t\treturn\t\t}\n\t\ttopic := fmt.Sprintf(\"topic_%s\", symbol)\t\terr = p.Produce(&kafka.Message{\t\t\tTopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\t\t\tValue:          jsonData,\t\t}, nil)\n\t\tif err != nil {\t\t\tfmt.Printf(\"Failed to produce message: %s\\n\", err)\t\t}\n\t\tfmt.Printf(\"Message published to Kafka: %s\", string(jsonData))\n\t\ttime.Sleep(30 * time.Second)\t}}\n```\n\nTo start streaming the data, run the code:\n\n```\n$ go run main.go\n```\n\nTo also get data for SPY, open up another terminal window, modify the code for\nthe symbol to SPY and run the code as well with the token value set.\n\n### Result\n\nAfter running the producer code, it will print out messages that it sends to\nKafka like:\nMessage published to Kafka: {\"symbol\":\"TSLA\",\"price\":174.48,\"timestamp\":1678743220215}\n.\nThis data is sent to the Kafka topic topic_TSLA and sent to QuestDB via the\nKafka Connect sink.\nWe can then navigate to\nhttp://localhost:9000\nto access\nthe QuestDB console. Searching for all records in the topic_TSLA table, we can\nsee our real-time market quotes:\n\n```\nSELECT * FROM 'topic_TSLA'\n```\n\nWe can also look at SPY data from\ntopic_SPY\n:\n\n```\nSELECT * FROM 'topic_SPY'\n```\n\nWith the data now in QuestDB, we can query for aggregate information by getting\nthe average price over 2m window:\n\n```\nSELECT avg(price), timestamp FROM topic_SPY SAMPLE BY 2m;\n```\n\n\n## Conclusion\n\nKafka\nis a trusted component of data\npipelines handling large amounts of time series data such as financial data.\nKafka can be used to stream mission-critical source data to multiple\ndestinations, including time series databases suited for real-time analytics.\nIn this article, we created a reference implementation of how to poll real-time\nmarket data and use Kafka to stream that to QuestDB via Kafka Connect. For more\ninformation on the QuestDB Kafka connector, check out the overview page on the\nQuestDB website\n. It lists more\ninformation on the configuration details and FAQs on setting it up. The\nGitHub repo\nfor the connector also has sample projects including a Node.js and a Java\nexample for those looking to extend this reference architecture.\n\n## Additional resources\n\nApache Kafka Connector for QuestDB\nFinnhub API documentation\nRealtime crypto tracker with QuestDB Kafka Connector\nReal-time analytics and anomaly detection with Apache Kafka, Apache Flink, Grafana & QuestDB",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1341,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "trading",
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-4176e5c0dd94",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2021/11/09/miguel-arregui-working-at-questdb",
    "title": "Why I joined QuestDB as a core database engineer | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nThis post was written by Miguel Arregui, who describes how he developed a\npassion for computing early on, his experience in research at CERN and the ESA,\nand eventually working at QuestDB. Miguel works as a software engineer in the\ncore database team, improving upon the internals of the fastest open source time\nseries database.\n\n## My introduction to computing\n\nMy parents are the kind that pursues crafty hobbies after work and involves\ntheir children, so my childhood was great. I came into existence at\n1978-02-28T08:00:00.000000Z\n. We helped our mechanical engineer dad in the\ngarage and our tailoress mom in her studio. It became ingrained in us to never\nwaste time. When we finished school, I would either do crafty stuff at home or\ndo something else, most competitive sports like sailing and tae-kwon-do.\nOne day on\n1989-05-11T09:00:00.000000Z\n, I received a gift of an\nAmstrad CPC 464\nwith a matching\ngreen phosphorous screen. This was my first encounter with computing. I played\nmany games, learned to copy them (tapes) for sharing, invited friends to code\nsome basic, or transcribe code examples from coding books. Back then, owning a\ncomputer was the exception and a sure flag for nerdiness that I happily wore.\nMy second encounter with computing took place in the form of an Intel Pentium my\ndad bought \"for work.\" The whole family shared an email account and would warn\neach other not to pick up the phone. The internet connection was set up over the\nsame twisted copper wire that also served the phone. I learned DOS and Pascal,\nlater some Linux booted from a floppy, and I was hooked. A little later, I\nstarted my university years, entirely devoted to learning the discipline,\ntechniques, methodology, possibilities, the craft of software engineering had to\noffer.\n\n## Starting to code professionally\n\nUniversity years were fantastic. My sole purpose was to get a degree as fast as\npossible because I also had to pay for student fees and my own upkeep. I took\nside jobs to earn some cash along the way to pay for these, doing what I love\ndoing, which is to code.\nCoding back then meant helping other students learn to code. We designed a lorry\ntracking system to organize the harvesting of oranges in my region, in rural\nSpain, and things of this nature. I was always busy, optimizing time management\nto fit in my hobbies and time with my girlfriend.\nI was on a mission that took me to CERN, culminating in achieving my master's\ndegree. However, when the university was over, I lost my direction; I did not\nknow what to do next. Everything was so exciting up until then, and now all of a\nsudden, the mission was over.\n\n## Finding the right career path\n\nOf course, there were a few potential paths to follow. My model was an engineer\nwho worked for the same company his whole career, so this felt like a big\ndecision to make, and stick with it. These are the years before Facebook,\nLinkedIn, YouTube, social media, even Gmail was not widely adopted. I lived in\nrural Spain, where there wasn't any faith in online business or remote working,\njust oranges.\nI had the opportunity to see the world and code with great teachers in fantastic\nplaces, a bit like Marco Polo. I had many excellent experiences in\nresearch-oriented organizations, like the European Space Agency and the European\nBioinformatics Institute. I worked for a couple major financial institutions,\nfor an array of various smaller size companies in pharmaceutical research as\nwell as finance, and a few times with startups, both promising and unsuccessful.\n\n## My checklist for happiness\n\nIt took me a long time to figure out what I consider essential for performing at\nmy best level and sustaining a happy existence. Here's my list of essentials in\nno particular order:\n- Mundane chores should be minimized, and 'breakthrough stuff' should be\nmaximized.\n- The breakthrough stuff has to directly benefit people's lives, making them\nhappier, more productive.\n- Interaction with users and customers, championing their cause, seeing the\nimpact of what I do.\n- Achieving sustained flow; minimizing things that disrupt deep work.\n- Excellent colleagues with a great sense of humor.\n- Excellent culture, camaraderie, where everybody is aligned with the mission.\n- Ability to effect changes in the culture, product, team fate, company, world.\n- Energy, it needs to feel like we are about to set foot on Mars.\n- Good work-family balance, so that I do not have to give up hobbies or family\nlife.\n- Work from home, anywhere in the world, because I have become a digital nomad.\nThis takes us to now; I have joined QuestDB, a team of like-minded people\npursuing the same goal, ticking all the boxes, and having so much fun doing what\nI love to do.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 850,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "trading",
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-5b378c73679b",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2022/12/13/using-prometheus-loki-grafana-monitor-questdb-kubernetes",
    "title": "Using Prometheus, Loki, and Grafana to monitor QuestDB in Kubernetes | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nOne of our Cloud engineers,\nSteve Sklar\n, shares\nwith us how to use some of the most popular tools in the Kubernetes ecosystem to\nbuild monitoring infrastructure for your QuestDB instances.\n\n## Monitoring QuestDB in Kubernetes\n\nAs any experienced infrastructure operator will tell you, monitoring and\nobservability tools are critical for supporting production cloud services.\nReal-time analytics and logs help to detect anomalies and aid in debugging,\nultimately improving the ability of a team to recover from (and even prevent)\nincidents. Since container technologies are drastically changing the\ninfrastructure world, new tools are constantly emerging to help solve these\nproblems. Kubernetes and its ecosystem have addressed the need for\ninfrastructure monitoring with a variety of newly emerging solutions. Thanks to\nthe orchestration benefits that Kubernetes provides, these tools are easy to\ninstall, maintain, and use.\nLuckily, QuestDB is built with these concerns in mind. From the presence of core\ndatabase features to the support for orchestration tooling, QuestDB is easy to\ndeploy on containerized infrastructure. This tutorial will describe how to use\ntoday's most popular open source tooling to monitor your QuestDB instance\nrunning in a Kubernetes cluster.\n\n## Components\n\nOur goal is to deploy a QuestDB instance on a Kubernetes cluster while also\nconnecting it to centralized metrics and logging systems. We will be installing\nthe following components in our cluster:\n- AQuestDBdatabase server\n- Prometheusto collect and store QuestDB metrics\n- Lokito store logs from QuestDB\n- Promtailto ship\nlogs to Loki\n- Grafanato build dashboards with data from\nPrometheus and Loki\nThese components work together as illustrated in the diagram below:\nOverview of the architecture\n\n## Prerequisites\n\nTo follow this tutorial, we will need the following tools. For our Kubernetes\ncluster, we will be using\nkind\n(Kubernetes In\nDocker) to test the installation and components in an isolated sandbox, although\nyou are free to use any Kubernetes flavor to follow along.\n- dockerorpodman\n- kind\n- kubectl\n- jq\n- curl\n\n### Getting started\n\nOnce you've\ninstalled kind\n,\nyou can create a Kubernetes cluster with the following command:\n\n```\nkind create cluster\n```\n\nThis will spin up a single-node Kubernetes cluster inside a Docker container and\nalso modify your current\nkubeconfig\ncontext to point\nkubectl\nto the\ncluster's API server.\n\n## QuestDB\n\n\n### QuestDB endpoint\n\nQuestDB exposes an HTTP metrics endpoint that can be scraped by Prometheus. This\nendpoint, on port\n9003\n, will return a wide variety of QuestDB-specific metrics\nincluding query, memory usage, and performance statistics. A full list of\nmetrics can be found\nin the QuestDB docs\n.\n\n### Helm installation\n\nQuestDB\ncan be installed using\nHelm\n. You\ncan add the official Helm repo to your registry by running the following\ncommands:\n\n```\nhelm repo add questdb https://helm.questdb.io/helm repo update\n```\n\nThis is only compatible with the Helm chart version 0.25.0 and higher. To\nconfirm your QuestDB chart version, run the following command:\n\n```\nhelm search repo questdb\n```\n\nBefore installing QuestDB, we need to enable the metrics endpoint. To do this,\nwe can override the QuestDB server configuration in a\nvalues.yaml\nfile:\n\n```\n<<EOF > questdb-values.yaml---metrics:  enabled: trueEOF\n```\n\nOnce you've added the repo, you can install QuestDB in the default namespace:\n\n```\nhelm install -f questdb-values.yaml questdb questdb/questdb\n```\n\nTo test the installation, you can make an HTTP request to the metrics endpoint.\nFirst, you need to create a Kubernetes port forward from the QuestDB pod to your\nlocalhost:\n\n```\nexport QUESTDB_POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=questdb,app.kubernetes.io/instance=questdb\" -o jsonpath=\"{.items[0].metadata.name}\")kubectl --namespace default port-forward $QUESTDB_POD_NAME 9003:9003\n```\n\nNext, make a request to the metrics endpoint:\n\n```\ncurl http://localhost:9003/metrics\n```\n\nYou should see a variety of Prometheus metrics in the response:\n\n```\n# TYPE questdb_json_queries_total counterquestdb_json_queries_total 0\n# TYPE questdb_json_queries_completed_total counterquestdb_json_queries_completed_total 0\n...\n```\n\n\n## Prometheus\n\nNow that we've exposed our metrics HTTP endpoint, we can deploy a\nPrometheus\ninstance to scrape the endpoint and store\nhistorical data for querying.\n\n### Helm installation\n\nCurrently, the recommended way of installing Prometheus is using the\nofficial Helm chart\n. You\ncan add the Prometheus chart to your local registry in the same way that we\nadded the QuestDB registry above:\n\n```\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-chartshelm repo update\n```\n\nAs of this writing, we are using the Prometheus chart version\n19.0.1\nand app\nversion\nv2.40.5\n\n### Configuration\n\nBefore installing the chart, we need to configure Prometheus to scrape the\nQuestDB metrics endpoint. To do this, we will need to add our additional scrape\nconfigs to a\nprom-values.yaml\nfile:\n\n```\n<<EOF > prom-values.yaml---extraScrapeConfigs: |  - job_name: questdb    metrics_path: /metrics    scrape_interval: 15s    scrape_timeout: 5s    static_configs:      - targets:        - questdb.default.svc.cluster.local:9003EOF\n```\n\nThis config will make Prometheus scrape our QuestDB metrics endpoint every 15\nseconds. Note that we are using the internal\nservice URL\nprovided to us by Kubernetes, which is only available to resources inside the\ncluster.\nWe're now ready to install the Prometheus chart. To do so, you can run the\nfollowing command:\n\n```\nhelm install -f prom-values.yaml prometheus prometheus-community/prometheus\n```\n\nIt may take around a minute for the application to become responsive as it sets\nitself up inside the cluster. To validate that the server is scraping the\nQuestDB metrics, we can query the Prometheus server for a metric. First, we need\nto open up another port forward:\n\n```\nexport PROM_POD_NAME=$(kubectl get pods --namespace default -l \"app=prometheus,component=server\" -o jsonpath=\"{.items[0].metadata.name}\")kubectl --namespace default port-forward $PROM_POD_NAME 9090\n```\n\nNow we can run a query for available metrics after waiting for a minute or so.\nWe are using\njq\nto filter the output to only\nthe QuestDB metrics:\n\n```\ncurl -s http://localhost:9090/api/v1/label/__name__/values | jq -r '.data[] | select( . | contains(\"questdb_\"))'\n```\n\nYou should see a list of QuestDB metrics returned:\n\n```\nquestdb_commits_totalquestdb_committed_rows_total...\n```\n\n\n## Loki\n\nMetrics are only part of the application support story. We still need a way to\naggregate and access application logs for better insight into QuestDB's\nperformance and behavior. While\nkubectl logs\nis fine for local development and\ndebugging, we will eventually need a production-ready solution that does not\nrequire the use of admin tooling. We will use Grafana's\nLoki\n, a scalable open-source solution that has\ntight Kubernetes integration.\n\n### Helm installation\n\nLike the other components we worked with, we will also be installing Loki using\nan official Helm chart,\nloki-stack\n.\nThe loki-stack helm chart includes Loki, used as the log database, and\nPromtail\n, a log\nshipper that is used to populate the Loki database.\nFirst, lets add the chart to our registry:\n\n```\nhelm repo add grafana https://grafana.github.io/helm-chartshelm repo update\n```\n\nLoki and Promtail are both enabled out of the box, so all we have to do is\ninstall the Helm chart without even supplying our own\nvalues.yaml\n.\n\n```\nhelm install loki grafana/loki-stack\n```\n\nAfter around a minute or two, the application should be ready to go. To test\nthat Promtail is shipping QuestDB logs to Loki, we first need to generate a few\nlogs on our QuestDB instance. We can do this by\ncurl\ning the QuestDB HTTP\nfrontend to generate a few\nINFO\n-level logs. This is exposed on a different\nport than the metrics endpoint, so we need to open up another port forward\nfirst.\n\n```\n# Open up the port forwardexport QUESTDB_POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=questdb,app.kubernetes.io/instance=questdb\" -o jsonpath=\"{.items[0].metadata.name}\")kubectl --namespace default port-forward $QUESTDB_POD_NAME 9000:9000\n```\n\nNow navigate to\nhttp://localhost:9000\n, which should\npoint to the QuestDB HTTP frontend. Your browser should make a request that\ncauses QuestDB to emit a few INFO-level logs.\nYou can query Loki to check if Promtail picked up and shipped those logs. Like\nthe other components, we need to set up a port forward to access the Loki REST\nAPI before running the query.\n\n```\nexport LOKI_POD=$(kubectl get pods --namespace default -l \"name=loki,app=loki\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl --namespace default port-forward $LOKI_POD 3100:3100\n```\n\nNow, you can run the following\nLogQL\nquery against the Loki\nserver to return these logs. By default, Loki will look for logs at most an hour\nold. We will also be using\njq\nto filter the response data.\n\n```\ncurl -s -G --data-urlencode 'query={pod=\"questdb-0\"}' http://localhost:3100/loki/api/v1/query_range | jq '.data.result[0].values'\n```\n\nYou should see a list of logs with timestamps that correspond to the logs from\nthe above sample:\n\n```\n[  [    \"1670359425100049380\",    \"2022-12-13T20:43:45.099494Z I http-server disconnected [ip=127.0.0.1, fd=23, src=queue]\"  ],  [    \"1670359425099842047\",    \"2022-12-13T20:43:45.099278Z I http-server scheduling disconnect [fd=23, reason=12]\"  ],  ...\n```\n\n\n## Grafana\n\nNow that we have all of our observability components set up, we need an easy way\nto aggregate our metrics and logs into meaningful and actionable dashboards. We\nwill install and configure\nGrafana\ninside\nyour cluster to visualize your metrics and logs in one easy-to-use place.\n\n### Helm Installation\n\nThe\nloki-stack\nchart makes this very easy for us to do. We just need to enable\nGrafana by customizing the chart's\nvalues.yaml\nand upgrading it.\n\n```\n<<EOF > loki-values.yaml---grafana:  enabled: trueEOF\n```\n\nWith this setting enabled, not only are we installing Grafana, but we are also\nregistering Loki as a data source in Grafana to save us the extra work.\nNow we can upgrade our Loki stack to include Grafana:\n\n```\nhelm upgrade -f loki-values.yaml loki grafana/loki-stack\n```\n\nTo get the admin password for Grafana, you can run the following command:\n\n```\nkubectl get secret --namespace default loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n```\n\nAnd to access the Grafana frontend, you can use a port forward:\n\n```\nkubectl port-forward --namespace default service/loki-grafana 3000:80\n```\n\n\n### Configuration\n\nFirst navigate to\nhttp://localhost:3000\nin your\nbrowser. You can log in using the username\nadmin\nand the password that you\nobtained in the previous step.\nOnce you've logged in, use the sidebar to navigate to the \"data sources\" tab:\nHere, you can see that the Loki data source is already registered for us:\nWe still need to add our Prometheus data source. Luckily, Grafana makes this\neasy for us.\nClick \"Add Data Source\" in the upper right and select \"Prometheus\". From here,\nthe only thing you need to do is enter the internal cluster URL of your\nPrometheus server's Service:\nhttp://prometheus-server.default.svc.cluster.local\n. Scroll down to the bottom,\nclick \"Save & test\", and wait for the green checkmark popup in the right corner.\nNow you're ready to create dashboards with QuestDB metrics and logs!\n\n## Conclusion\n\nI have provided a step-by-step tutorial to install and deploy QuestDB with a\nmonitoring infrastructure in a Kubernetes cluster. While there may be additional\nconsiderations to make if you want to improve the reliability of the monitoring\ncomponents, you can get very far with a setup just like this one. Here are a few\nideas:\n- Add alerting to a number of targets usingAlertmanager\n- Build interactive dashboards that combine metrics and logs usingGrafana variables\n- Configure Loki to usealternative deployment modesto improve reliability and scalability\n- LeverageThanosincorporate high availability into your\nPrometheus deployment\nIf you like this content, we'd love to know your thoughts! Feel free to share\nyour feedback or just come and say hello in the\nCommunity Forum\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1861,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "performance",
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-3f87610faf84",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2024/03/11/sql-select-statement-best-practices",
    "title": "Maximize your SQL efficiency: SELECT best practices | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nSQL is wildly popular. Within it, the most common command is the SELECT\nstatement. After all, what good is a database full of valuable information if\nyou're unable to read it with flexibility and efficiency?\nThis guide walks you through best SELECT practices. Afterwards, you'll write\nmore efficient and powerful SQL statements. This can save both processing time\nand resources, leading to faster queries and cheaper databases.\nWhile many of the optimization tips would make sense universally across any SQL\ndatabase, the statements we show are specific for QuestDB and many include its\nrobust time-series extensions.\nIf you don't have a database of your own running, consider launching these\nqueries into the\nQuestDB demo instance\n. The example\nqueries leverage the built in data sets when possible.\n\n## Apply columnar specificity\n\nRead only the columns you need and avoid doing select *.\nThe first one may seem like common sense, but searching GitHub for\nSELECT * FROM\nreturns 5.7 million results. Surely a great number of these are\nonly looking for a column or two. Specifying a column vs. all columns narrows\ndown the result set. This reduces data transfer and lightens the load on memory.\nSpecify - or \"scope\" - specific columns, if all you need are specific columns:\n\n```\nSELECT fare_amount from trips;\n```\n\nScoping your columns is always a wise idea. But where it shines most is within\ndatabases that support the\ncolumnar database format\n, such\nas QuestDB.\n\n## Always filter by designated timestamp\n\nDesignated timestamps are a very powerful feature. With timestamps, the database\nquickly locates the initial data point that is relevant to the query.\nQuestDB optimizes query execution by physically organizing data in ascending\norder based on a designated timestamp and partitioning this data according to a\npredefined resolution.\nThis minimizes random access operations, instead favouring sequential access.\nSequential access is preferred by both the operating system and the storage\ndrive, leading to faster and more efficient data retrieval.\nIf possible, always filter by the designated timestamp.\nInstead of returning \"all time\", we return only the time we need. Since data is\nstored in chronological order, a database like QuestDB will know when to stop\nreading as soon it sees a value later than the timestamp boundary. As a result,\nit can immediately return results:\n\n```\nSELECT price, timestamp FROM tradesWHERE timestamp BETWEEN '2022-03-08T08:59:59' AND '2022-03-08T18:59:59';\n```\n\n\n## Optimize partitions\n\nPartitions split databases into smaller chunks. Time is one convenient way to\npartition a database. But consider that your partitions are very large: a YEAR,\nfor example.\nLearn more about\ndatabase partitioning\n.\nYour queries may often apply a short scope, a day, or hours. In this common\ncase, you are accessing a large chunk of un-needed data. This is inefficient.\nConsider the following query to against a table partitioned by YEAR:\n\n```\nSELECT prices, timestampFROM tradesWHERE timestamp >= '2022-03-08T00:00:00' AND timestamp < '2022-03-09T00:00:00';\n```\n\nLooks OK. But because of the underlying partitioning, we need to read - and\ndiscard - many rows until we find the start of the time range, which is in\nAugust. If the table was instead partioned by MONTH, then we would read/discard\na much smaller amount of data, even if we end up opening more files as a result\nof spanning two partitions.\nTry to find the sweet spot based on your most typical queries.\n\n## Divide tables to match query patterns\n\nBuilding off of partition precision, consider matching your tables to your query\ntypes. Apart from matching the timestamps up with an appropriately broad\npartition, it is strong practice to match tables up with query types.\nDivide large single tables into multiple tables that match query types.\nConsider dividing a large table into multiple tables if you have different query\npatterns for different columns in the query. Maybe some columns you typically\nneed to query daily and some monthly. Table makeup can reflect this.\n\n## JOINs are expensive\n\nWhile many tables are appropriate in some cases, a singular table may be the\nright choice if you often find yourself applying JOIN to your queries. A\ndatabase query that JOINs together many tables is typically more expensive than\na similar query that reads from a single table. If different columns are\nfrequently queried together, then a single table is preferred.\nLet's use an example query:\n\n```\nSELECT    pickup_datetime,    fare_amount,    tempF,    windDirFROM    tripsASOF JOIN weatherWHERE    trips.pickup_datetime >= '2018-06-01T00:00:00'AND    trips.pickup_datetime < '2018-06-02T00:00:00';\n```\n\nThe above query is a two-table join. Not bad. Efficient and clean.\nHowever, what if we added four additional tables: drivers, vehicles, and\npayment_info. In this query, t, w, d, v, p, tc are table aliases used for\nclarity and brevity.\n\n```\nSELECT    t.pickup_datetime,    t.fare_amount,    w.tempF,    w.windDir,    d.driver_name,    d.driver_rating,    v.vehicle_make,    v.vehicle_model,    p.payment_method,    p.amount_paid,FROM    trips tASOF JOIN weather wJOIN drivers d ON (driver_id)JOIN vehicles v ON(vehicle_id)LEFT OUTER JOIN payment_info p ON (transaction_id)WHERE    t.pickup_datetime >= '2022-03-08T00:00:00' AND t.pickup_datetime < '2022-03-09T00:00:00';\n```\n\nThis query is much more difficult to process.\nIf these were all part of a singular trip_details table, it would be that much\nmore efficient.\n\n## Match tables up with symbols\n\nA symbol is a data structure used to store repetitive strings. Within the\ndatabase itself, symbol types are stored as a table of integers and their\ncorresponding string values.\nUsing symbols can improve query performance by a large margin, as string\noperations compare and write int types instead of string. As an additional\nbenefit, less storage space is consumed. They're also unobtrusive to the query\nwriter (you!) and offer reduced overall complexity.\nIf you often query by timestamp plus a symbol, like type, country, model and so\non, you may be able to find additional performance through the creation of\nmultiple tables. Typically, one table per different symbol value would provide a\nboost.\nCreating a table with a column named symbol\n\n```\nCREATE TABLE symbol_table  (symbol SYMBOL CAPACITY 128 NOCACHE, timestamp TIMESTAMP)timestamp(timestamp);\n```\n\nHowever do note that if you needed to query two different symbol values, you\nwould need to apply a\nUNION\n, so make sure the trade off is right for you.\nApplying\nUNION\nmerges the results of 2 different queries:\nUnion to merge two query results\n\n```\nsymbol1_tableUNIONsymbol2_table;\n```\n\nAlso, if the number of symbols is in the thousands and you create a different\ntable for each, there will be extra memory & disk consumption. In that case, you\nmay find that you need to manually tune configuration, or perhaps just keep\nusing a single table.\n\n## EXPLAIN your queries first\n\nUse EXPLAIN to see how queries will execute.\nDepending on which column types you are filtering by, or which aggregation\nfunctions you are using, queries might parallelise or not.\nTo demonstrate, let's deconstruct the trades table in the\ndemo instance\n.\nIt is created like so:\n\n```\nCREATE TABLE trades (  symbol SYMBOL CAPACITY 256 CACHE,  side SYMBOL CAPACITY 256 CACHE,  price DOUBLE,  amount DOUBLE,  timestamp TIMESTAMP) TIMESTAMP (timestamp) PARTITION BY DAY\n```\n\nLet's run one of our ill-advised open\n*\nqueries:\n\n```\nEXPLAIN SELECT * FROM trades ORDER BY timestamp DESC;\n```\n\nIt returns:\n\n```\nQUERY PLANDataFrame    Row backward scan    Frame backward scan on: trades\n```\n\nNoteable is that the entire DataFrame was processed backward.\nNow let's try another query, with a filtering parameter.\n\n```\nEXPLAIN SELECT * FROM trades WHERE amount > 100.0;\n```\n\nIt returns:\n\n```\nAsync JIT Filter  filter: 100.0<amount  workers: 1    DataFrame        Row forward scan        Frame forward scan on: trades\n```\n\nAh, now it's a forward scan and we can see the filter.\nLet's try one more example:\n\n```\nEXPLAIN SELECT avg(price), var_pop(price)FROM trades WHERE amount > 100.0;\n```\n\nThis returns:\n\n```\nGroupBy vectorized: false  values: [avg(price),var_pop(price)]    Async JIT Filter workers: 24      filter: 100.0<amount        DataFrame            Row forward scan            Frame forward scan on: trades\n```\n\nNote\nGroupBy vectorized: false\n. Vectorization means that QuestDB will\nparallelize some computations and aggregations. It leads to better performance.\nThe\navg\naggregation does support vectoration, but\nvar_pop\ndoes not.\nRead more about\nvectorization and SIMD operations\n.\nWe can remove\nvar_pop\nand try again:\n\n```\nEXPLAIN SELECT avg(price)FROM trades WHERE amount > 100.0;\n```\n\nWe no longer see the notice that the query as a whole is not applying\nvectorization:\n\n```\nAsync JIT Group By workers: 24  values: [avg(price)]  filter: 100.0<amount    DataFrame        Row forward scan        Frame forward scan on: trades\n```\n\nLearning the nitty-gritty of how your queries are\nactually\nbeing processed\nwill pay future dividends. You'll soon get a sense of just how your queries are\nrunning, and become more familiar with the conditions which lead to high query\nperformance.\n\n## Are you up-to-date?\n\nOK, this one is a bit of a soft ball. But database innovation often moves at a\nstaggering pace. Newer versions tend to improve execution, so as a general rule:\nAlways ensure you are on the latest available version.\nFor example,\nQuestDB 7.3.10\n,\nbrought with it a 5x-10x speed-up in ASOF and LT JOIN queries, as well as a\nnumber of optimizations in parallel filters and GROUP BY, among other\nimprovements.\nSimply bumping the version offers strong value.\nYou'll remain current, and - naturally - very hip.\n\n## Know your queries\n\nImagine you found a query you love.\nLike, LATEST ON running on the trades table to find the most recent pair of\nrows:\n\n```\nSELECT price, symbol, timestamp FROM tradesLATEST ON timestamp PARTITION BY side;\n```\n\nIt's neat to see different trades fly into the database.\nBut let's say we apply\nmany\nsymbols:\n\n```\nSELECT price, symbol, timestamp FROM tradesWHERE symbol IN ('SYM1', 'SYM2', 'SYM3', 'SYM4')LATEST ON timestamp PARTITION BY side;\n```\n\nThis will be a fairly taxing query, as LATEST ON performs better when only a\nsingle symbol is used. How would we know? Well, the\ndocs tell us\n.\nIf you're using a database like QuestDB that has native time-series extensions,\nor any database that offers less common extensions, they may have tradeoffs and\ncaveats.\nAs such, it's always wise to read the docs when using novel extensions.\n\n## Apply early inner-filtering\n\nIn SQL, the \"innermost\" query represents the deepest level of nesting. It is\noften the first part of the query to be executed in a sequence of nested\nqueries, also known as a subquery.\nWhen using subqueries or Common Table Expressions (CTEs), apply filters as early\nas possible in your query structure. Filtering early reduces the volume of data\nprocessed in subsequent steps. Irrelevant rows are removed and not passed down\nthe query chain, thus preventing unnecessary computations on data not included\nin the final result.\nLet's look at an example.\nWe want to calculate the average price of ETH-USD trades bewteen '2022-03-15'\nand '2022-03-19', but only for 'buy' side trades.\nAn inefficient example might look as such:\n\n```\nWITH DailyTrades AS (  SELECT    symbol,    side,    AVG(price) AS avg_price  FROM trades  WHERE timestamp BETWEEN '2022-03-15 00:00:00' AND '2022-03-19 23:59:59'  GROUP BY symbol, side)SELECT * FROM DailyTradesWHERE symbol = 'ETH-USD'AND side = 'buy';\n```\n\nOur DailyTrades CTE calculates the average price for all symbols and sides for\nthe entire day first. Only after aggregation does it filter for 'ETH-USD' and\n'buy' trades. This means the aggregation operation processes more data than is\nneeded. As per the demo instance,\n~230ms\nis our total computation time.\nCan we do better?\n\n```\nSELECT  symbol,  side,  AVG(price) AS avg_priceFROM tradesWHERE  timestamp BETWEEN '2022-03-15 00:00:00' AND '2022-03-19 23:59:59'  AND symbol = 'ETH-USD'  AND side = 'buy'GROUP BY symbol, side\n```\n\nIn this second cut, we've applied filters on timestamp, symbol, and side in the\nmain FROM clause. This ensures that only relevant data is considered from the\nstart. And as a result, our second query finishes around\n~190ms\n, a cool 18%\nimprovement.\nHow come? The GROUP BY clause - an already well optimized extension - now has\nless data to process and thus operates more efficiently. Avoiding an aggregation\nacross the entire dataset is, indeed, bound to be helpful.\n\n## ORDER BY at the end\n\nApply ORDER BY at the end of your statements. Or, if you expect a large result\nset, avoid ORDER BY. Also by default the natural order of results will be\nascending designated timestamp. If thatâ€™s good enough for you, then great! No\nneed for ORDER BY. But assuming you do, consider the following example.\nLet's say we want 10 'buy' side trades for 'ETH-USD' between '2022-03-15' and\n'2022-03-19', including the symbol, side, price, and timestamp.\n\n```\nSELECT *FROM (    SELECT symbol, side, price, timestamp    FROM trades    ORDER BY timestamp DESC) AS OrderedTradesWHERE symbol = 'ETH-USD'AND side = 'buy'AND timestamp BETWEEN '2022-03-15 00:00:00' AND '2022-03-19 23:59:59'LIMIT 10;\n```\n\nThis structure is logically correct, however note that ORDER BY is applied\nwithin a subquery. The query completed at around ~\n200ms\n.\nCan we do better? We can!\nLet's apply ORDER BY at the end:\n\n```\nSELECT symbol, side, price, timestampFROM tradesWHERE symbol = 'ETH-USD'AND side = 'buy'AND timestamp BETWEEN '2022-03-15 00:00:00' AND '2022-03-19 23:59:59'ORDER BY timestamp DESCLIMIT 10;\n```\n\nThis re-ordering of our statement averages around\n~180ms\n. The goal, as in the\nprior best practice, is to present a minimized dataset to ORDER BY. The\nopimization becomes more stark the larger your dataset.\nIt should be noted that, in the case of a database like QuestDB, SQL extensions\ncan also help. For example, if we were to apply a LIMIT clause, the whole\nORDER BY timestamp DESC LIMIT 10\ncan be replaced by\n'LIMIT -10'\n. The natural\nsorting of QuestDB once again saves the day (or the microsecond):\n\n```\nSELECT symbol, side, price, timestampFROM tradesWHERE symbol = 'ETH-USD'AND side = 'buy'AND timestamp BETWEEN '2022-03-15 00:00:00' AND '2022-03-19 23:59:59'LIMIT -10;\n```\n\n\n## Summary\n\nHopefully these 10 SELECT tips will help you refine your queries. Great\nimprovements can often be found through fairly minor adjustments. Applying these\ntips will help you write efficient and flexible queries.\nIf you have any other recommendations, we'd love to hear about them. Tweet or\nx-eet or wave or email or smoke signal us (whatever your flavour), or visit us\nin our\nCommunity Forum\nto share.\nThanks for reading!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2371,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "performance",
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-bb395ad7acff",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/rust-fetch-max-compiler-journey",
    "title": "From Rust to Reality: The Hidden Journey of fetch_max | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\n\n## How a Job Interview Sent Me Down a Compiler Rabbit Hole\n\nI occasionally interview candidates for engineering roles. We need people who understand\nconcurrent programming. One of our favorite questions involves keeping track of a\nmaximum value across multiple producer threads - a classic pattern that appears in many\nreal-world systems.\nCandidates can use any language they want.\nIn Java (the language I know best), you might write a\nCAS loop\n,\nor if you're feeling functional, use\nupdateAndGet()\nwith a lambda:\n\n```\nAtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -> Math.max(current, newScore));\n```\n\nBut that lambda is doing work - it's still looping under the hood, retrying if\nanother thread interferes. You can see the loop right in\nAtomicLong's source code\n.\nThen one candidate chose Rust.\nI was following along as he started typing, expecting to see either an explicit\nCAS loop or some functional wrapper around one. But instead, he just wrote:\n\n```\nhigh_score.fetch_max(new_score, Ordering::Relaxed);\n```\n\n\"Rust has fetch_max built in,\"\nhe explained casually, moving on to the next\npart of the problem.\nHold on. This wasn't a wrapper around a loop pattern - this was a first-class\natomic operation, sitting right there next to\nfetch_add\nand\nfetch_or\n. Java\ndoesn't have this. C++ doesn't have this. How could Rust just... have this?\nAfter the interview, curiosity got the better of me. Why would Rust provide\nfetch_max\nas a built-in intrinsic? Intrinsics usually exist to leverage\nspecific hardware instructions. But x86-64 doesn't have an\natomic max\ninstruction. So there had to be a CAS loop somewhere in the pipeline. Unless...\nmaybe some architectures\ndo\nhave this instruction natively? And if so, how\ndoes the same Rust code work on both?\nI had to find out. Was the loop in Rust's standard library? Was it in LLVM?\nWas it generated during code generation for x86-64?\nSo I started digging. What I found was a fascinating journey through five\ndistinct layers of compiler transformations, each one peeling back another level\nof abstraction, until I found exactly where that loop materialized. Let me share\nwhat I discovered.\n\n## Layer 1: The Rust Code\n\nLet's start with what that candidate wrote - a simple high score tracker that can\nbe safely updated from multiple threads:\n\n```\nuse std::sync::atomic::{AtomicU64, Ordering};\nfn main() {    let high_score = AtomicU64::new(100);\n    // [...]\n    // Another thread reports a new score of 200    let _old_score = high_score.fetch_max(200, Ordering::Relaxed);\n    // [...]}// Save this snippet as `main.rs` we are going to use it later.\n```\n\nThis single line does exactly what it promises: atomically fetches the current\nvalue, compares it with the new one, updates it if the new value is greater, and\nreturns the old value. It's safe, concise, and impossible to mess up. No\nexplicit loops, no retry logic visible anywhere. But how does it actually work under\nthe hood?\n\n## Layer 2: The Macro Expansion\n\nBefore our\nfetch_max\ncall even reaches anywhere close to machine code generation,\nthere's another layer of abstraction at work. The\nfetch_max\nmethod isn't hand-written\nfor each atomic type - it's generated by a Rust macro called\natomic_int!\n.\nIf we peek into Rust's standard library source code, we find that\nAtomicU64\nand all its methods are actually created by\nthis macro\n:\n\n```\natomic_int! {    cfg(target_has_atomic = \"64\"),    // ... various configuration attributes ...    atomic_umin, atomic_umax,  // The intrinsics to use    8,                          // Alignment    u64 AtomicU64              // The type to generate}\n```\n\nInside this macro,\nfetch_max\nis defined as a\ntemplate\nthat works for any integer type:\n\n```\npub fn fetch_max(&self, val: $int_type, order: Ordering) -> $int_type {    // SAFETY: data races are prevented by atomic intrinsics.    unsafe { $max_fn(self.v.get(), val, order) }}\n```\n\nThe\n$max_fn\nplaceholder gets replaced with\natomic_umax\nfor unsigned types\nand\natomic_max\nfor signed types. This single macro definition generates\nfetch_max\nmethods for\nAtomicI8\n,\nAtomicU8\n,\nAtomicI16\n,\nAtomicU16\n, and so\non - all the way up to\nAtomicU128\n.\nSo our simple\nfetch_max\ncall is actually invoking generated code. But what\ndoes the\natomic_umax\nfunction actually do? To answer that, we need\nto see what the Rust compiler produces next.\n\n## Layer 3: LLVM IR\n\nNow that we know\nfetch_max\nis macro-generated code calling\natomic_umax\n,\nlet's see what happens when the Rust compiler processes it. The compiler\ndoesn't go straight to assembly. First, it translates the code into an\nintermediate representation. Rust uses the LLVM compiler project, so it\ngenerates\nLLVM Intermediate Representation (IR)\n.\nIf we peek at the LLVM IR for our\nfetch_max\ncall, we see something like this:\n\n```\n; Before the transformationbb7:  %0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8  ...\n```\n\nThis is LLVM's language for saying:\n\"I need an atomic read-modify-write\noperation. The modification I want to perform is an unsigned maximum.\"\nThis is a powerful, high-level instruction within the compiler itself. But it\nposes a critical question: does the CPU actually have a single instruction\ncalled\numax\n? For most architectures, the answer is no. So\nhow\ndoes the\ncompiler bridge this gap?\n\n### How to See This Yourself\n\nMy goal is not to merely describe what is happening, but to give you the tools to\nsee it for yourself. You can trace this transformation step-by-step on your own\nmachine.\nFirst, tell the Rust compiler to stop after generating the LLVM IR:\n\n```\nrustc --emit=llvm-ir main.rs\n```\n\nThis creates a\nmain.ll\nfile. This file contains the LLVM IR\nrepresentation of your Rust code, including our\natomicrmw umax\ninstruction.\nKeep the file around; we'll use it in the next steps.\n\n## Interlude: Compiler Intrinsics\n\nWe're missing something important. How does the Rust function\natomic_umax\nactually become the LLVM instruction\natomicrmw umax\n? This is where compiler\nintrinsics come into play.\nIf you dig into Rust's source code, you'll find that\natomic_umax\nis\ndefined like this\n:\n\n```\n/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax<T: Copy>(dst: *mut T, val: T, order: Ordering) -> T {    // SAFETY: the caller must uphold the safety contract for `atomic_umax`    unsafe {        match order {            Relaxed => intrinsics::atomic_umax::<T, { AO::Relaxed }>(dst, val),            Acquire => intrinsics::atomic_umax::<T, { AO::Acquire }>(dst, val),            Release => intrinsics::atomic_umax::<T, { AO::Release }>(dst, val),            AcqRel => intrinsics::atomic_umax::<T, { AO::AcqRel }>(dst, val),            SeqCst => intrinsics::atomic_umax::<T, { AO::SeqCst }>(dst, val),        }    }}\n```\n\nBut what is this\nintrinsics::atomic_umax\nfunction? If you\nlook at its\ndefinition\n,\nyou find something slightly unusual:\n\n```\n/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax<T: Copy, const ORD: AtomicOrdering>(dst: *mut T, src: T) -> T;\n```\n\nThere is no body. This is a declaration, not a definition. The\n#[rustc_intrinsic]\nattribute tells the Rust compiler that this function\nmaps directly to a low-level operation understood by the compiler\nitself. When the Rust compiler sees a call to\nintrinsics::atomic_umax\n, it\nknows to\nreplace it\nwith the corresponding\nLLVM intrinsic function\n.\nSo our journey actually looks like this:\n- fetch_maxmethod (user-facing API)\n- Macro expands to callatomic_umaxfunction\n- atomic_umaxis a compiler intrinsic\n- Rustc replaces the intrinsic with LLVM'satomicrmw umaxâ†We are here\n- LLVM processes this instruction...\n\n## Layer 4: The Transformation\n\nLLVM runs a series of \"passes\" that analyze and transform the code. The one we're interested in is called the\nAtomicExpandPass\n.\nIts job is to look at high-level atomic operations like\natomicrmw umax\nand ask\nthe target architecture,\n\"Can you do this natively?\"\nWhen the\nx86-64\nbackend says\n\"No, I can't,\"\nthis pass expands the single\ninstruction into a sequence of more fundamental ones that the CPU\ndoes\nunderstand. The result is a\ncompare-and-swap (CAS) loop\n.\nWe can see this transformation in action by asking LLVM to emit the\nintermediate representation before and after this pass. To see the IR\nbefore\nthe\nAtomicExpandPass\n, run:\n\n```\nllc -print-before=atomic-expand main.ll -o /dev/null\n```\n\nTip: If you do not havellcinstalled, you can askrustcto run the pass for you directly.\nrustc -C llvm-args=\"-print-before=atomic-expand -print-after=atomic-expand\" main.rs\nThe code will be printed to your terminal. The function containing our atomic max\nlooks like this:\n\n```\n*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:  %_0 = alloca [8 x i8], align 8  %order = alloca [1 x i8], align 1  store i8 %0, ptr %order, align 1  %1 = load i8, ptr %order, align 1  %_7 = zext i8 %1 to i64  switch i64 %_7, label %bb2 [    i64 0, label %bb7    i64 1, label %bb5    i64 2, label %bb6    i64 3, label %bb4    i64 4, label %bb3  ]\nbb2:                                              ; preds = %start  unreachable\nbb7:                                              ; preds = %start  %2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8  store i64 %2, ptr %_0, align 8  br label %bb1\nbb5:                                              ; preds = %start  %3 = atomicrmw umax ptr %self, i64 %val release, align 8  store i64 %3, ptr %_0, align 8  br label %bb1\nbb6:                                              ; preds = %start  %4 = atomicrmw umax ptr %self, i64 %val acquire, align 8  store i64 %4, ptr %_0, align 8  br label %bb1\nbb4:                                              ; preds = %start  %5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8  store i64 %5, ptr %_0, align 8  br label %bb1\nbb3:                                              ; preds = %start  %6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8  store i64 %6, ptr %_0, align 8  br label %bb1\nbb1:                                              ; preds = %bb3, %bb4, %bb6, %bb5, %bb7  %7 = load i64, ptr %_0, align 8  ret i64 %7}\n```\n\nYou can see the\natomicrmw umax\ninstruction in multiple places, depending on\nthe memory ordering specified. This is the high-level atomic operation that the\ncompiler backend understands, but the CPU does not.\n\n```\nllc -print-after=atomic-expand main.ll -o /dev/null\n```\n\nThis is the relevant part of the output:\n\n```\n*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:  %_0 = alloca [8 x i8], align 8  %order = alloca [1 x i8], align 1  store i8 %0, ptr %order, align 1  %1 = load i8, ptr %order, align 1  %_7 = zext i8 %1 to i64  switch i64 %_7, label %bb2 [    i64 0, label %bb7    i64 1, label %bb5    i64 2, label %bb6    i64 3, label %bb4    i64 4, label %bb3  ]\nbb2:                                              ; preds = %start  unreachable\nbb7:                                              ; preds = %start  %2 = load i64, ptr %self, align 8               ; seed expected value  br label %atomicrmw.start                       ; enter CAS loop\natomicrmw.start:                                  ; preds = %atomicrmw.start, %bb7  %loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg  %3 = icmp ugt i64 %loaded, %val                 ; unsigned compare (umax semantics)  %new = select i1 %3, i64 %loaded, i64 %val      ; desired = max(loaded, val)  %4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new  %success = extractvalue { i64, i1 } %4, 1       ; boolean: whether the swap happened  %newloaded = extractvalue { i64, i1 } %4, 0     ; value seen in memory before the CAS  br i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeeds\natomicrmw.end:                                    ; preds = %atomicrmw.start  store i64 %newloaded, ptr %_0, align 8  br label %bb1\n[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]\nbb1:                                              ; preds = %bb3, %bb4, %bb6, %bb5, %bb7  %7 = load i64, ptr %_0, align 8  ret i64 %7}\n```\n\nWe can see the pass did not change the first part - it still has the code to dispatch based\non the memory ordering. But in the\nbb7\nblock, where we originally had the\natomicrmw umax\nLLVM instruction, we now see a full compare-and-swap loop.\nA compiler engineer would say that the\natomicrmw umax\ninstruction has been\n\"lowered\" into a sequence of more primitive operations, that are closer to what\nthe hardware can actually execute.\nHere's the simplified logic:\n- Read (seed): grab the current value (expected).\n- Compute:desired = umax(expected, val).\n- Attempt:observed, success = cmpxchg(ptr, expected, desired, [...]).\n- If success, returnobserved(the old value). Otherwiseset expected = observedand loop.\nThis CAS loop is a fundamental pattern in lock-free programming. The compiler\njust built it for us automatically.\n\n## Layer 5: The Final Product (x86-64 Assembly)\n\nWe're at the final step. To see the final machine code, you can tell\nrustc\nto\nemit the assembly directly:\n\n```\nrustc --emit=asm main.rs\n```\n\nThis will produce a\nmain.s\nfile containing the final assembly code.\nInside, you'll find the result of the\ncmpxchg\nloop:\n\n```\n.LBB8_2:\tmovq  -32(%rsp), %rax       # rax = &self\tmovq\t(%rax), %rax        # rax = *self (seed 'expected')\tmovq\t%rax, -48(%rsp)     # spill expected to stack.LBB8_3:                    # loop head\tmovq\t-48(%rsp), %rax     # rax = expected\tmovq\t-32(%rsp), %rcx     # rcx = &self\tmovq\t-40(%rsp), %rdx     # rdx = val\tmovq\t%rax, %rsi          # rsi = expected (scratch)\tsubq\t%rdx, %rsi          # set flags for unsigned compare: expected - val\tcmovaq\t%rax, %rdx          # if (expected > val) rdx = expected; else rdx = val (compute max)\tlock cmpxchgq\t%rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax <- old *rcx; ZF=success\tsete\t%cl                 # cl = success\tmovq\t%rax, -56(%rsp)     # spill observed to stack\ttestb\t$1, %cl             # branch on success\tmovq\t%rax, -48(%rsp)     # expected = observed (for retry)\tjne\t.LBB8_4             # success -> exit\tjmp\t.LBB8_3             # failure â†’ retry\n```\n\nThe syntax might look a bit different from what you're used to, that's because it's\nin AT&T syntax, which is the default forrustc. If you prefer Intel syntax, you can\nuserustc --emit=asm main.rs -C \"llvm-args=-x86-asm-syntax=intel\"to get that.\nI'm not an assembly expert, but you can see the key parts of the CAS loop here:\n- Seed read (first iteration): Load*selfonce to initialize the expected value.\n- Compute umax without branching: The pairsub+cmovaimplementsdesired = max_u(expected, val).\n- CAS operation: On x86-64,cmpxchgusesRAXas the expected value and returns the observed value inRAX;ZFencodes success.\n- Retry or finish: IfZFis clear, we failed and need to retry. Otherwise, we are done.\nNote we did not ask\nrustc\nto optimize the code. If we did, the compiler would\ngenerate more efficient assembly: No spills to the stack, fewer jumps, no\ndispatch on memory ordering, etc. But I wanted to keep the output as close\nto the original IR as possible to make it easier to follow.\n\n## The Beauty of Abstraction\n\nAnd there we have it. Our journey is complete. We started with a safe, clear,\nsingle line of Rust and ended with a CAS loop written in assembly language.\nRustfetch_max\nâ†’\nMacro-generatedatomic_umax\nâ†’\nLLVMatomicrmw umax\nâ†’\nLLVMcmpxchgloop\nâ†’\nAssemblylock cmpxchgloop\nThis journey is a perfect example of the power of modern compilers. We get to\nwork at a high level of abstraction, focusing on safety and logic, while the\ncompiler handles the messy, error-prone, and incredibly complex task of\ngenerating correct and efficient code for the hardware.\nSo, next time you use an atomic, take a moment to appreciate the incredible,\nhidden journey your code is about to take.\nPS: After conducting this journey I learned that\nC++26 addsfetch_max\ntoo!\n\n## Bonus: Apple Silicon (AArch64)\n\nOut of curiosity, I also checked how this looks on Apple Silicon (AArch64).\nThis architecture does have a native\natomic max\ninstruction, so the\nAtomicExpandPass\ndoes not need to lower it into a CAS loop. The LLVM code before and after\nthe pass is identical, still containing the\natomicrmw umax\ninstruction.\nThe final assembly contains a variant of the\nLDUMAX\ninstruction. This is the relevant part of the assembly:\n\n```\nldr    x8, [sp, #16]     # x8 = value to compare withldr    x9, [sp, #8]      # x9 = pointer to the atomic variableldumax x8, x8, [x9]      # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr    x8, [sp, #40]     # Store old valueb      LBB8_11\n```\n\nNote that AArch64 uses\nUnified Assembler Language\n,\nwhen reading the snippet above, it's important to remember that the destination register comes first.\nAnd that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed\nat the hardware level, what are the effects of the\nLOCK\nprefix, dive into differences in memory ordering, etc.\nBut we'll leave that for another day.\nAlice:\n\"Would you tell me, please, which way I ought to go from here?\"\nThe Cat:\n\"That depends a good deal on where you want to get to.\"\nAlice:\n\"I don't much care where.\"\nThe Cat:\n\"Then it doesn't much matter which way you go.\"\nAlice:\n\"...So long as I get somewhere.\"\nThe Cat:\n\"Oh, you're sure to do that, if only you walk long enough.\"\n- Lewis Carroll, Alice's Adventures in Wonderland",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2878,
    "metadata": {
      "relevance_score": 0.2857142857142857,
      "priority_keywords_matched": [
        "trading",
        "release"
      ]
    }
  },
  {
    "id": "questdb-blog-73f5ea837faa",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2021/03/31/automating-etl-jobs-on-time-series-data-on-gcp",
    "title": "Automating ETL jobs on time series data with QuestDB on Google Cloud Platform | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nThis submission comes from one of our community contributors\nGÃ¡bor Boros\nwho has put together another\nexcellent tutorial showing how to use cloud functions together with QuestDB to\nbuild a custom ETL job that runs on time series data. The corresponding\nrepository for this tutorial with code examples is available to\nbrowse on GitHub\n.\nThanks for another great contribution, GÃ¡bor!\n\n## Introduction\n\nIn the world of big data, software developers and data analysts often have to\nwrite scripts or complex software collections to process data before sending it\nto a data store for further analysis. This process is commonly called ETL, which\nstands for Extract, Transform and Load.\n\n## What are ETL jobs for?\n\nLet's consider the following example: a medium-sized webshop with a few thousand\norders per day exports order information hourly. After a while, we would like to\nvisualize purchase trends, and we might want to share the results between\ndepartments or even publicly. Since the exported data contains personally\nidentifiable information (PII), we should anonymize it before using or exposing\nit to the public.\nFor the example above, we can use an ETL job to extract the incoming data,\nremove any PII and load the transformed data into a database used as the data\nvisualization backend later.\n\n## Prerequisites\n\nDuring this tutorial, we will use Python to write the cloud functions, so basic\npython knowledge is essential. Aside from these skills, you will need the\nfollowing resources:\n- AGoogle Cloud Platform(GCP) account and a GCP Project.\n- Enable theCloud Build API-\nwhen enabling APIsensure that the correct GCP project is selected.\n\n## Creating an ETL job\n\nAs an intermediate data store where the webshop exports the data, we will use\nGoogle Storage and use Google Cloud Functions to transform it before loading it\ninto QuestDB.\nWe won't be building a webshop or a data exporter for an existing webshop, but\nwe will use a script to simulate the export to Google Storage. In the following\nsections, we will set up the necessary components on GCP. Ensure the required\nAPIs mentioned in the prerequisites are enabled, and that you have selected the\nGCP project in which you would like to create the tutorial resources.\n\n### Create a Compute Engine instance for QuestDB\n\nFirst things first, we start with installing QuestDB on a virtual machine. To\nget started, navigate to the\nCompute Engine console\n.\nVisiting this page for the first time will take a few moments to initialize.\nAfter the loading indicator has gone, start a new virtual machine:\n- Click oncreateand give the instance the namequestdb-vm\n- Select a region that's geographically close to you\n- Select the first generation \"N1\" series\n- Choose thef1-micromachine type - in a production environment you would\nchoose a more performant instance, but this is sufficient for example\npurposes\n- In the \"Container\" section, check \"Deploy a container image to this VM\ninstance\", and enterquestdb/questdb:latest\n- In the \"Firewall\" section, click on \"Management, security, disks, networking,\nsole tenancy\"\n- In the new panel, select \"Networking\" and addquestdbas a \"Network tag\"\n- Leave all other settings with their defaults, and click on \"create\"\nMake sure you note the \"External IP\" of the instance as we will need that later.\nAfter a short time, the new instance will be up and running. As soon as the\ninstance is provisioned, we can initiate a remote session to install QuestDB by\nclicking\nssh\nin the VM panel.\n\n### Allow networking on the instance\n\nIf we try to open the\nWeb Console\nby opening the\nhttp://<EXTERNAL_IP>:9000\n(where\n<EXTERNAL_IP>\nis the external IP of your\nvirtual machine) it won't load and we will face a timeout. The reason behind\nthis is that the firewall is not opened for port\n9000\nyet.\nTo allow port\n9000\nused by QuestDB, we must allow the port by adding a new\nfirewall rule on the\nfirewall console\n:\n- Click on \"create firewall rule\" at the top of the page\n- Give the rule the name \"QuestDBPorts\"\n- In the \"Target tags\" field, write the same tag used for instance creation\n(questdb)\n- For the \"Source IP ranges\" field, set0.0.0.0/0\n- In the \"Protocols and ports\" section, selecttcpand set port to9000,8812\n- Click on \"create\"\nSome seconds later, the rule will be applied on every instance with the matching\nquestdb\ntag and port\n9000\nwill be open. You may ask what port\n8812\nis for;\nthis port will be used by the Cloud Function later to connect to the database.\nIf you try to open the interactive console again, you should see the\nQuestDB Web Console\nand start writing queries. As our\nfirst query, create the table in which the Cloud Function will write the\nanonymized data. To create the table run the following SQL statement:\n\n```\nCREATE TABLE    purchases(buyer STRING,            item_id INT,            quantity INT,            price INT,            purchase_date TIMESTAMP)    timestamp(purchase_date);\n```\n\nThe query above uses\ntimestamp(purchase_date)\nto set a designated timestamp on\nthe table so we can easily perform time series analysis in QuestDB. For more\ninformation on designated timestamps, see the official\nQuestDB documentation for timestamp\n.\n\n### Create a Storage bucket\n\nNow, we create the bucket where we will store the simulated webshop data.\nStorage buckets are in a single global namespace in GCP, which means that the\nbucket's name must be unique across all GCP customers. You can read more about\nStorage and buckets on Google's\ndocumentation\nsite.\nTo create a new bucket:\n- Navigate to thecloud storage console\n- Select your project if not selected yet\n- Click on \"create bucket\" and choose a unique name\n- Select the same region as the instance above\n- Leave other settings on default and click on \"continue\" to create the bucket\nIf you successfully created the bucket, it should show up in the storage browser\nas you can see below.\nAt this point, we don't set any permissions, ACLs, or visibility settings on the\nbucket, but we will come back to that later.\n\n### Create a Cloud Function\n\nWe have the bucket to upload the data, but we have nothing to process the data\nyet, and for this, we will use Cloud Functions to remove the PII.\nCloud Functions are functions as a service (FaaS) solution within GCP, similar\nto AWS Lambda. The functions are triggered by an event that can come from\nvarious sources. Our scenario Cloud Functions are convenient since we don't need\nto pay for a server to run all day, which is mostly idle; the function will be\nexecuted when the trigger event is fired, and we only pay for the execution time\nthe number of function calls.\nTo create a Cloud Function:\n- Navigate tocloud functions console\n- Click on \"create function\" and give it the nameremove-pii\n- Select the region we are using for other resources\n- For \"Trigger\" select \"Cloud Storage\" from the dropdown list\n- Set the event type to \"Finalise/Create\"\n- Choose the bucket created above and click \"variables, networking, and\nadvanced settings\"\n- Select \"environment variables\" on the tabbed panel\n- Click on \"add variable\" right below the \"Runtime environment variables\"\nsection\n- Add a new variable calledDATABASE_URLwith the valuepostgresql://admin:quest@<EXTERNAL_IP>:8812/qdb, where<EXTERNAL_IP>is\nthe external IP of your virtual machine\n- Click \"save\" then \"next\"\nThe next step is to select the runtime our function will use and provide the\ncode. On this page, we can choose between numerous runtimes, including multiple\nversions of Python, Node.js, Go, Ruby, and even Java.\nSince this tutorial uses Python, select\nPython 3.8\nas it is the latest\nnon-beta version at the time of writing. Leave the rest of the settings as\ndefault, and write the function in the next section. Click \"deploy\" at the\nbottom of the page. Some seconds later, you will see that the deployment of the\nfunction is in progress.\n\n## Generating and processing data\n\nBefore moving on, here's a quick recap on what we did so far:\n- Set up a new Google Storage bucket\n- Created the Cloud Function which will process the data later on\n- Connected the bucket with the function to trigger on a new object is created\non the bucket\nNow for the fun part of the tutorial: writing the data processing script and\nloading the data in the database. Let's write the function to remove PII, but\nfirst, talk a bit about the data's structure.\n\n### Inspecting the data\n\nETL jobs, by their nature, heavily depend on the structure of incoming data. A\njob may process multiple data sources, and data structure can vary per source.\nThe data structure we will use is simple, we have a CSV file with the following\ncolumns:\n- customer email address\n- purchase date\n- item ID for the purchased item\n- quantity\n- the price per item\nAs you see, there is no currency column since we will assume every price is in\none currency. To generate random data, you can use the\nprovided script\nwritten for this tutorial.\n\n### Writing cloud functions\n\nBy now, we have everything to write the data transformer function and test how\nwell the PII removal performs. We will work in the \"inline editor\" of the cloud\nfunction, so as a first step, open the edit the cloud function created above by\nnavigating to the\ncloud functions console\nand\nclicking on the function's name. That will open the details of the function. At\nthe top, click on \"edit\", then at the bottom, click on \"next\" to open the\neditor.\nLet's start with the requirements. On the left-hand side, click on the\nrequirements.txt\nand paste the following:\n\n```\ngoogle-cloud-storage==1.36.2psycopg2==2.8.6sqlalchemy==1.4.2\n```\n\nHere we add the required packages to connect to Google Storage and QuestDB.\nNext, click on\nmain.py\n, remove its whole content and start adding the\nfollowing:\n\n```\nimport csvimport hashlibimport jsonimport loggingimport osfrom dataclasses import dataclassfrom datetime import datetimefrom typing import List\nfrom google.cloud import storagefrom sqlalchemy.sql import textfrom sqlalchemy.engine import Connection, create_engine\nlogger = logging.getLogger(__name__)\n# Create a database engineengine = create_engine(os.getenv(\"DATABASE_URL\"))\n```\n\nAs you may expect, we start with the imports, but we added two extra lines: one\nfor the logger and one for configuring the database engine. We will need the\nlogger to log warnings and exceptions during the execution, while we will use\nthe engine later to insert anonymized data into the database.\nTo make our job easier, we are going to add a data class, called\nRecord\n. This\ndata class will be used to store the parsed and anonymized CSV data for a line\nof the uploaded file.\n\n```\n# ...\n@dataclassclass Record:    buyer: str    item_id: int    quantity: int    price: int    purchase_date: datetime\n# ...\n```\n\nAs we discussed, ETL jobs are validating the data that they receive as input. In\nour case, we will trigger the function if an object is created on the storage.\nThis means any object, like a CSV, PDF, TXT, PNG file, or even a directory, is\ncreated, though we only want to execute CSV files' transformation. To validate\nthe incoming data, we write two simple validator functions:\n\n```\n# ...\ndef is_event_valid(event: dict) -> bool:    \"\"\"    Validate that the event has all the necessary attributes required for the    execution.    \"\"\"\n    attributes = event.keys()    required_parameters = [\"bucket\", \"contentType\", \"name\", \"size\"]\n    return all(parameter in attributes for parameter in required_parameters)\n\ndef is_object_valid(event: dict) -> bool:    \"\"\"    Validate that the finalized/created object is a CSV file and its size is    greater than zero.    \"\"\"\n    has_content = int(event[\"size\"]) > 0    is_csv = event[\"contentType\"] == \"text/csv\"\n    return has_content and is_csv\n# ...\n```\n\nThe first function will validate that event has all the necessary parameters,\nwhile the second function checks that the object created and triggered the event\nis a CSV and has any content. The next function we create is used to get an\nobject from the storage which, in our case, the file triggered the event:\n\n```\n# ...\ndef get_content(bucket: storage.Bucket, file_path: str) -> str:    \"\"\"    Get the blob from the bucket and return its content as a string.    \"\"\"\n    blob = bucket.get_blob(file_path)    return blob.download_as_string().decode(\"utf-8\")\n# ...\n```\n\nAnonymizing the data, in this scenario, is relatively easy, though we need to\nensure we can build statistics and visualizations later based on this data, so\nthe anonymized parts should be consistent for a user. To achieve this, we will\nhash the buyer's email address, so nobody may track it back to the person owning\nthe email, but we can use it for visualization:\n\n```\n# ...\ndef anonymize_pii(row: List[str]) -> Record:    \"\"\"    Unpack and anonymize data.    \"\"\"\n    email, item_id, quantity, price, purchase_date = row\n    # Anonymize email address    hashed_email = hashlib.sha1(email.encode()).hexdigest()\n    return Record(        buyer=hashed_email,        item_id=int(item_id),        quantity=int(quantity),        price=int(price),        purchase_date=purchase_date,    )\n# ...\n```\n\nSo far, we have functions to validate the data, get the file's content which\ntriggered the Cloud Function, and anonymize the data. The next thing we need to\nbe able to do is to load the data into our database. Up to this point, every\nfunction we have wrote was simple, and this one is no exception:\n\n```\n# ...\ndef write_to_db(conn: Connection, record: Record):    \"\"\"    Write the records into the database.    \"\"\"\n    query = \"\"\"    INSERT INTO purchases(buyer, item_id, quantity, price, purchase_date)    VALUES(:buyer, :item_id, :quantity, :price, to_timestamp(:purchase_date, 'yyyy-MM-ddTHH:mm:ss'));    \"\"\"\n    try:        conn.execute(text(query), **record.__dict__)    except Exception as exc:        # If an error occures, log the exception and continue        logger.exception(\"cannot write record\", exc_info=exc)\n# ...\n```\n\nAs you see, writing to the database is easy. We get the connection and the\nrecord we need to write into the database, prepare the query and execute it. In\ncase of an exception, we don't want to block the whole processing, so we catch\nthe exception, log it and let the script go on. If an exception occurred, we can\ncheck it later and fix the script or load the data manually.\nThe last bit is the glue code, which brings together these functions. Let's have\na look at that:\n\n```\n# ...\ndef entrypoint(event: dict, context):    \"\"\"    Triggered by a creation on a Cloud Storage bucket.    \"\"\"\n    # Check if the event has all the necessary parameters. In case any of the    # required parameters are missing, return early not to waste execution time.    if not is_event_valid(event):        logger.error(\"invalid event: %s\", json.dumps(event))        return\n    file_path = event[\"name\"]\n    # Check if the created object is valid or not. In case the object is invalid    # return early not to waste execution time.    if not is_object_valid(event):        logger.warning(\"invalid object: %s\", file_path)        return\n    storage_client = storage.Client()    bucket = storage_client.get_bucket(event[\"bucket\"])\n    data = get_content(bucket, file_path)    reader = csv.reader(data.splitlines())\n    # Anonymize PII and filter out invalid records    records: List[Record] = filter(lambda r: r, [anonymize_pii(row) for row in reader])\n    # Write the anonymized data to database    with engine.connect() as conn:        for record in records:            write_to_db(conn, record)\n```\n\nIn the example above, we call the two validators to ensure it worth processing\nthe data, and we get the file path from the event. After that we initialize the\nclient used to connect to Google Storage, then we get the object's content,\nparse the CSV file, and anonymize the content of it.\nLast but not least, we connect to the database - defined by the\nDATABASE_URL\nconfigured for the engine and write all records to the database one by one.\nAs you see, the entrypoint of the function has been changed as well. In the text\nbox called \"Entrypoint\" set the entrypoint as a function name to call. The\nentrypoint is the function that will be called by Cloud Functions when an event\nis triggered.\n\n## Running the full example\n\nWe are close to finishing this tutorial, so it's time to test our Cloud\nFunction.\nTo test the Cloud Function:\n- Download thepre-made scriptand run it to generate random data.\n- Navigate to the bucket you created\n- Above the list of objects in the bucket (which should be empty) click on\n\"upload files\"\n- Select and upload the randomly-generated data\n- After the file is uploaded, visit theCloud Functions console\n- Click on the actions button and select \"view logs\"\n- Navigate tohttp://<EXTERNAL_IP>:9000, where<EXTERNAL_IP>is the\nexternal IP of your virtual machine\nWe can now execute the following SQL query:\n\n```\nSELECT * FROM purchases ORDER BY purchase_date;\n```\n\nAs you can see, the data is loaded and we have no PII there. By creating a\nsimple chart, we can even observe trends in the generated data, how our\nimaginary buyers purchased items on the webshop.\n\n## Summary\n\nWe've installed QuestDB on Google Cloud Platform, set up a Google Storage bucket\nto store the simulated purchase data exports, built an ETL job that anonymized\nour buyers' data, and loaded it into a time series database, QuestDB. Data\nanalysts could write more jobs as Cloud Functions in multiple languages and set\nup multiple sources. It's also possible to set QuestDB's HTTP endpoints to\nread-only which would allow us to share this dashboard without worrying about\ndestructive commands.\nIf you like this content, we'd love to know your thoughts! Feel free to share\nyour feedback, browse the\nGitHub repository for this tutorial\n,\nor come and say hello in the\ncommunity forums\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2883,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-46e1b4c0020e",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2022/11/23/sql-extensions-time-series-data-questdb-part-ii",
    "title": "SQL Extensions for Time Series Data in QuestDB - Part II | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nThis tutorial follows up on our previous one, where we introduced SQL extensions in QuestDB\nthat make\ntime series analysis\neasier. Today,\nyou will learn about the\nSAMPLE BYextension\nin detail, which will enable you to work with\ntime-series data\nefficiently because of its\nsimplicity and flexibility.\nTo get started with this tutorial, you should know that\nSAMPLE BY\nis a SQL\nextension in QuestDB that helps you group or bucket\ntime-series data\nbased on the\ndesignated timestamp\n. This removes the\nneed for lengthy\nCASE WHEN\nstatements and\nGROUP BY\nclauses. Not only that,\nthe\nSAMPLE BY\nextension enables you to quickly deal with many other\ndata-related issues, such as\nmissing data\n,\nincorrect timezones\n, and\noffsets\n.\nThis tutorial assumes you have an up-and-running QuestDB instance ready for use.\nLet's dive straight into it.\n\n## Setup\n\n\n### Import sample data\n\nSimilar to the previous tutorial, we'll use\nthe NYC taxi rides data for February 2018\n.\nYou can use the following script that utilizes the\nHTTP REST API\nto upload data\ninto QuestDB:\n\n```\ncurl https://s3-eu-west-1.amazonaws.com/questdb.io/datasets/grafana_tutorial_dataset.tar.gz > grafana_data.tar.gztar -xvf grafana_data.tar.gzcurl -F data=@taxi_trips_feb_2018.csv http://localhost:9000/impcurl -F data=@weather.csv http://localhost:9000/imp\n```\n\nAlternatively, you can use\nthe import functionality in the QuestDB console\n, as\nshown in the image below:\n\n### Create an ordered timestamp column\n\nThe\nSAMPLE BY\nkeyword mandates the use of the\ndesignated timestamp\ncolumn to enable\nfurther analysis. Therefore, you'll have to elect the\npickup_datetime\ncolumn\nas the designated timestamp in a new table called\ntaxi_trips\nwith the script\nbelow:\n\n```\nCREATE TABLE taxi_trips AS (  SELECT *    FROM \"taxi_trips_feb_2018.csv\"   ORDER BY pickup_datetime) TIMESTAMP(pickup_datetime)PARTITION BY MONTH;\n```\n\nBy converting the\npickup_datetime\ncolumn to\ntimestamp\n, you are allowing\nQuestDB to use it as the table's\ndesignated timestamp\n. Using this\ndesignated timestamp column, QuestDB is able to index the table to run\ntime-based queries more efficiently. If it all goes well, you should see the\nfollowing data after running a\nSELECT *\nquery on the\ntaxi_trips\ntable:\n\n## Understanding the basics ofSAMPLE BY\n\nThe\nSAMPLE BY\nextension allows you to create groups and buckets of data based\non time ranges. This is especially valuable for\ntime-series data\nas you can calculate\nfrequently used aggregates with extreme simplicity.\nSAMPLE BY\noffers you the\nability to summarize or aggregate data from very fine to very coarse\nunits of time\n, i.e., from\nmicroseconds to years and everything in between (milliseconds, seconds, minutes,\nhours, days, and months). You can also derive other units of time, such as a\nweek or fortnight from the ones provided out of the box.\nLet's look at some examples to understand how to use\nSAMPLE BY\nin different\nscenarios.\n\n### Hourly count of trips\n\nYou can use the\nSAMPLE BY\nkeyword with the\nsample unit\nof\nh\nto get an\nhour-by-hour count of trips for the whole duration of the data set. Running the\nfollowing query, you'll get results in the console:\n\n```\nSELECT  pickup_datetime,  COUNT() total_tripsFROM  taxi_tripsSAMPLE BY 1h;\n```\n\nThere are two ways you can read your data in the QuestDB console: using the\ngrid, which has a tabular form factor, or using a chart, where you can draw up a\nline, bar, or an area chart to\nvisualize your data\n. Here's an example\nof a bar chart drawn from the above query:\n\n### Three-hourly holistic summary of trips\n\nThe\nSAMPLE BY\nextension allows you to group data by any arbitrary number of\nsample units. In the following example, you'll see that the query is calculating\na three-hourly summary of trips with multiple aggregate functions:\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsSAMPLE BY 3h;\n```\n\nYou can view the output of the query in the following grid on the QuestDB\nconsole:\n\n### Weekly summary of trips\n\nAs mentioned above, although there's no sample unit for a week, or a fortnight,\nyou can derive them simply by utilizing the built-in sample units. If you want\nto sample the data by a week, use\n7d\nas the sampling time, as shown in the\nquery below:\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsWHERE  pickup_datetime BETWEEN '2018-02-01' AND '2018-02-28'SAMPLE BY 7d;\n```\n\n\n## Dealing with missing data\n\nIf you've worked a fair bit with data, you already know that data isn't always\nin a pristine state. One of the most common issues, especially with time-series\ndata, is discontinuity, i.e., scenarios where data is missing for specific time\nperiods. You can quickly identify and deal with missing data using the advanced\nfunctionality of the\nSAMPLE BY\nextension.\nQuestDB offers an easy way to generate and fill in missing data with the\nSAMPLE BY\nclause. Take the following example: I've deliberately removed data\nfrom 4 am to 5 am for the 1st of February 2018. Notice how the\nFILLkeyword\n, when used in\nconjunction with the\nSAMPLE BY\nextension, can generate a row for the hour\nstarting at 4 am and fill it with data generated from linear interpolation of\nthe 2 surrounding points:\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsWHERE  pickup_datetime NOT BETWEEN '2018-02-01T04:00:00' AND '2018-02-01T04:59:59'SAMPLE BY 1h FILL(LINEAR);\n```\n\nThe\nFILL\nkeyword demands a\nfillOption\nfrom the following:\n\n| fillOption | Usage scenario | Notes |\n| --- | --- | --- |\n| NONE | When you don't want to populate missing data, and leave it as is | This is the defaultfillOption |\n| NULL | When you want to generate rows for missing time periods, but leave all the values as NULLs |  |\n| PREV | When you want to copy the values of the previous row from the summarized data | This is useful when you expect the numbers to be similar to the preceding time period |\n| LINEAR | When you want to normalize the missing values, you can take the average of the immediately preceding and following row |  |\n| CONST or x | When you want to hardcode values where data is missing | FILL (column_1, column_2, column_3, ...) |\n\nHere's another example of hardcoding values using the FILL(x)\nfillOption\n:\nIn the example above, we've used an inline\nWHERE\nclause to emulate missing\ndata with the help of the\nNOT BETWEEN\nkeyword. Alternatively, you can create a\nseparate table with missing trips using the same idea, as shown below:\n\n```\nCREATE TABLE taxi_trips_missing AS (  SELECT *  FROM taxi_trips  WHERE    pickup_datetime NOT BETWEEN '2018-02-01T04:00:00' AND '2018-02-01T04:59:59');\n```\n\n\n## Working with timezones and offsets\n\nThe\nSAMPLE BY\nextension also enables you to change timezones and add or\nsubtract offsets from your timestamp columns to adjust for any issues you might\nencounter when dealing with different source systems, especially in different\ngeographic areas. It is important to note that, by default, QuestDB aligns its\nsample calculation\nbased on\nthe\nFIRST OBSERVATION\n, as shown in the example below:\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsWHERE  pickup_datetime BETWEEN '2018-02-01T13:35:52' AND '2018-02-28'SAMPLE BY 1d;\n```\n\nNote that now the\n1d\nsample calculation starts at\n13:35:52\nand ends at\n13:35:51\nthe next day. Apart from the option demonstrated above, there are two\nother ways to align your sample calculations - to the\ncalendar time zone\n, and to\ncalendar with offset\n.\nLet's take a look at the other two alignment methods.\n\n### Aligning sample calculation to another timezone\n\nWhen moving data across systems, pipelines, and warehouses, you can encounter\nissues with time zones. For the sake of demonstration, let's assume that you're\nworking in New York City, but you've identified that the timestamps of the data\nset you've loaded into the database are in Australian Eastern Time (instead of\nNew York's EST). Traditionally, this could lead to extra conversion work to\nensure that this new data is comparable to the rest of your data in EST.\nQuestDB allows you to easily fix this issue by aligning your data to another\ntimezone using the\nALIGN TO CALENDAR TIME ZONEoption\nwith the\nSAMPLE BY\nextension. In the example shown below, you can see how an\nALIGN TO CALENDAR TIME ZONE ('AEST')\nhas aligned the\npickup_datetime\n, i.e.,\nthe designated timestamp column to the AEST timezone for Melbourne.\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsSAMPLE BY 3h ALIGN TO CALENDAR TIME ZONE ('AEST');\n```\n\n\n### Aligning sample calculation with offsets\n\nSimilar to the previous example, you can also align the sample calculation by\noffsetting the designated timestamp\ncolumn manually by any\nhh:mm\nvalue between -23:59 to 23:59. In the following\nexample, we're offsetting the sample calculation by -5:30, i.e., negative five\nhours and thirty minutes:\n\n```\nSELECT  pickup_datetime,  COUNT() total_trips,  SUM(passenger_count) total_passengers,  ROUND(AVG(trip_distance), 2) avg_trip_distance,  ROUND(SUM(fare_amount)) total_fare_amount,  ROUND(SUM(tip_amount)) total_tip_amount,  ROUND(SUM(fare_amount + tip_amount)) total_earningsFROM  taxi_tripsSAMPLE BY 3h ALIGN TO CALENDAR WITH OFFSET '-05:30';\n```\n\n\n## Conclusion\n\nIn this tutorial, you learned how to exploit the\nSAMPLE BYextension\nin QuestDB to work\nefficiently with\ntime-series data\n, especially\nin aggregated form. In addition, the\nSAMPLE BY\nextension also allows you to\nfix common problems with\ntime-series data\nattributable to complex data pipelines, disparate source systems in different\ngeographical areas, software bugs, etc. All in all, SQL extensions in QuestDB,\nlike\nSAMPLE BY\n, provide a significant advantage when working with\ntime-series data\nby enabling you to achieve\nmore in fewer lines of SQL.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1627,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-67e1f98fce55",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/dashboards/taxi",
    "title": "NYC Taxi Data Real-Time Dashboard | QuestDB",
    "text": "\n# New York City Taxi DataReal-Time Dashboards\n\nPowered by\nQuestDB and\nGrafana\nA real-time replay of 146,393,317 taxi rides, carrying 238,016,495 passengers across New York City in 2015.\n\n## Geodata\n\nGenerate deep business insights by combining locations and timestamps.\n\n### Insights\n\nDiscover trends in your data using modern SQL analytics.\n\n## Build your own on QuestDB\n\nHigh performance ingest & slick visualizations.\nPerfect for analytics.\nDownload QuestDB\n$\n/$",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 69,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "performance"
      ]
    }
  },
  {
    "id": "questdb-blog-76db1b8f0069",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/visualizing-weather-kafka-grafana",
    "title": "Weather data visualization and forecasting with QuestDB, Kafka and Grafana | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nWeather stations, satellites, and sensor arrays generate a ton of\nweather-related events every millisecond. When captured, this data can go on to\nbecome valuable for various applications such as agriculture, transportation,\nenergy, and disaster management. For this, a specialized time-series database\nlike QuestDB can help store and efficiently process large volumes of data.\nIn this tutorial, we will stream and visualize large volumes of data into\nQuestDB. To do so, we'll use\nKafka\n, a\ndistributed streaming platform that can handle high-volume data streams. Then,\nwe will apply\nGrafana\n, a powerful open-source data\nvisualization tool, to create a cool dashboard for weather forecasting.\n\n## Prerequisites\n\nTo follow along with this tutorial, you will require:\n- ADockerenvironment set up on your machine\n- Docker images for Grafana, Kafka, and QuestDB\n- Node.jsandnpm\n- Basic knowledge of Node.js,Express.js, Kafka, and\nGrafana\n\n## Node.js application setup\n\nTo start, we'll create a pair of microservices. One service will be the Kafka\nstream provider, and the other service will consume these data streams for\nstorage in QuestDB.\nFirst, create a new directory for your project and navigate into it:\n\n```\nmkdir weather-data-visualizationcd weather-data-visualization\n```\n\nThen, create a new Node.js project using the npm command:\n\n```\nnpm init -y\n```\n\nNext, install the following dependencies:\n\n```\nnpm install @questdb/nodejs-client dotenv express kafkajs node-cron node-fetch\n```\n\n- @questdb/nodejs-client: The officialQuestDB Node client library\n- dotenv:Zero-dependency module that loads environment variables from an\nenv file into process.env\n- express: Minimal Node web application framework\n- kafkajs: Kafka client for Node\n- node-cron: Simple and lightweight cron-like job scheduler for Node\n- node-fetch: A lightweight module which brings thewindow.fetchAPI to\nNode\nThe next step is to create a new file named\n.env\nand add the following\nenvironment variables:\n\n```\nPORTA=3001PORTB=3002WEATHER_API_KEY=<your-openweather-api-key>\n```\n\nFor guidance towards safe storage of environment variables, checkout\nthis guide\n.\n\n## Kafka streaming service\n\nNow, with node setup, let's create the services for the application.\nWe need two services:\n- one to stream the weather data from the OpenWeatherMap API\n- another to consume the data streams and store them in QuestDB\nTo this end, we will create two new directories named\napp-producer\nand\napp-consumer\nin the root directory and add a new file called\nindex.js\nto\neach.\nFor the\nindex.js\nfile in the app-producer service, we'll add the following\ncode:\n\n```\nimport express from \"express\"import dotenv from \"dotenv\"import { Kafka } from \"kafkajs\"import cron from \"node-cron\"import fetch from \"node-fetch\"\ndotenv.config()\nconst API_KEY = process.env.WEATHER_API_KEY\nconst cities = [  { name: \"New York\", lat: 40.7128, lon: -74.006 },  { name: \"SÃ£o Paulo\", lat: -23.5505, lon: -46.6333 },  { name: \"London\", lat: 51.5074, lon: -0.1278 },  { name: \"Cairo\", lat: 30.0444, lon: 31.2357 },  { name: \"Sydney\", lat: -33.8688, lon: 151.2093 },  { name: \"Tokyo\", lat: 35.6895, lon: 139.6917 },  { name: \"Moscow\", lat: 55.7558, lon: 37.6173 },  { name: \"Mumbai\", lat: 19.076, lon: 72.8777 },  { name: \"Buenos Aires\", lat: -34.6037, lon: -58.3816 },  { name: \"Cape Town\", lat: -33.9249, lon: 18.4241 },]\nconst app = express()app.use(express.json())\nconst kafka = new Kafka({  clientId: \"weather-producer\",  brokers: [\"kafka:29092\"],})\nconst producer = kafka.producer()\n// Add health check routeapp.get(\"/health\", (req, res) => {  res.status(200).json({ status: \"OK\", message: \"Producer service is healthy\" })})\n// Your logic for fetching, processing, and sending weather data to Kafka will go here\napp.listen(process.env.PORTA, () => {  console.log(\"Weather Producer running on port\", process.env.PORTA)})\n```\n\nIn this file, we create an Express app, connect to the Kafka network, and\ninitialize a Kafka producer. We also add a health check route to the app so that\nwe can confirm it's running well. The Express app is then set to listen to the\nport specified in the\n.env\nfile.\n\n### Fetch weather data\n\nAfter setting up the Express app and health check, we need code to fetch the\nweather data. The first addition is the function that fetches the weather data.\nTo the\nindex.js\nfile in the app-consumer service after the health check route,\nwe'll add the following code:\n\n```\nasync function fetchWeatherData(lat, lon) {  const currentTime = Math.floor(Date.now() / 1000)  try {    const response = await fetch(      `https://api.openweathermap.org/data/3.0/onecall/timemachine?lat=${lat}&lon=${lon}&dt=${currentTime}&units=metric&appid=${API_KEY}`,    )    if (!response.ok) {      throw new Error(`HTTP error! status: ${response.status}`)    }    return await response.json()  } catch (error) {    console.error(\"Error fetching weather data:\", error)    return null  }}\n```\n\nThe\nfetchWeatherData()\nfunction fetches the weather data from the\nOpenWeatherMap API. It's free to use, and very useful for demo purposes.\n\n### Process weather data\n\nNow it is time to process the weather data. An additional function will check if\nthe\nfetchWeatherData()\nfunction returns data. If there is none, it returns an\nempty array. When there is data, it returns an object that matches the QuestDB\nschema.\nAgain in the\nindex.js\nfile in the app-producer service, below the\nfetchWeatherData()\nfunction, we'll add the following code:\n\n```\nfunction processWeatherData(data, city) {  if (!data || !data.data || data.data.length === 0) {    console.error(\"Invalid weather data received\")    return []  }\n  const current = data.data[0]  // Convert Unix timestamps to microseconds  const toDBTime = (unixTimestamp) => {    const toISO = new Date(unixTimestamp * 1000)    const dateObj = new Date(toISO)    const toMicroseconds = BigInt(dateObj.getTime()) * 1000n    console.log(toMicroseconds)\n    return toMicroseconds  }\n  return {    city: city.name,    timezone: data.timezone,    timestamp: toDBTime(current.dt),    temperature: current.temp,    sunrise: toDBTime(current.sunrise),    sunset: toDBTime(current.sunset),    feels_like: current.feels_like,    pressure: current.pressure,    humidity: current.humidity,    clouds: current.clouds,    weather_main: current.weather[0].main,    weather_description: current.weather[0].description,    weather_icon: current.weather[0].icon,  }}\n```\n\nThis\nprocessWeatherData()\nfunction processes the weather data and returns a\nlist of objects to stream to Kafka and save to the database.\n\n### Send data to Kafka\n\nTime to involve Kafka. For this, we'll create yet another function to send the\nprocessed data to the Kafka topic. Our new function will take an array of data,\ncheck for fields in each object that is of type\nBigInt\n, then convert it to\nstring so that we can stream it via Kafka.\nWe do the conversion due to Kafka's preference for serialized messages. JSON is\nthe most common vehicle for this. JSON does not natively support BigInt. So, the\nconversion allows safe streaming to Kafka without any serialization errors.\nIn the\nindex.js\nfile in the app-producer service, below the\nprocessWeatherData()\nfunction, add the following code:\n\n```\nasync function sendToKafka(data) {  try {    await producer.connect()    const messages = data.map((item) => ({      value: JSON.stringify(item, (key, value) =>        typeof value === \"bigint\" ? value.toString() : value,      ),    }))    await producer.send({      topic: \"weather-data\",      messages,    })  } catch (error) {    console.error(\"Error sending data to Kafka:\", error)  } finally {    await producer.disconnect()  }}\n```\n\nWith this, the\nsendToKafka()\nfunction sends the processed data to the Kafka\ntopic.\n\n### Schedule task\n\nFinally, we will create one more function to schedule the task to run every 15\nminutes. It will fetch fresh weather data and send it to the Kafka topic:\n\n```\nasync function generateWeatherData() {  const weatherDataPromises = cities.map((city) =>    fetchWeatherData(city.lat, city.lon),  )  const weatherDataResults = await Promise.all(weatherDataPromises)\n  const processedData = weatherDataResults    .map((data, index) => processWeatherData(data, cities[index]))    .filter((data) => data !== null)\n  if (processedData.length > 0) {    await sendToKafka(processedData)  }  return processedData}\n// Initial rungenerateWeatherData().then((data) => {  if (data) {    console.log(\"Initial weather data sent to Kafka:\", data)  }})\n// Schedule the task to run every 15 minutescron.schedule(\"*/15 * * * *\", async () => {  const weatherData = await generateWeatherData()  if (weatherData) {    console.log(\"Weather data sent to Kafka:\", weatherData)  }})\n```\n\nThe\ngenerateWeatherData()\nfunction triggers the initial call for the above\nfunction in the order outlined above â€“ fetch weather information, process it,\nand send it to the Kafka topic. It then schedules the task to run every 15\nminutes.\nThings to note:\n- The OpenWeatherAPI call is to get the current weather report of the cities\ndefined above. Thecronpackage schedules the task to run every 15\nminutes. You can add as many cities as you want. Also, note that the free\ntier of this API is limited to 60 calls per minute.\n- ThetoDBTime()function formats the Unix time to bigint. The QuestDB\nNode.js client only supports theepoch(microseconds)orbigintdata type\nfor timestamps.\n\n## Saving to QuestDB\n\nIt's QuestDB's turn.\nIn this section, we connect QuestDB to the Kafka producer and consume the\nweather data from the Kafka weather topic in the app-consumer service app. We\nalso add another health check route to the app.\nIn the\nindex.js\nfile in the app-consumer service, we'll add the following\nstarter code:\n\n```\nimport express from \"express\"import dotenv from \"dotenv\"import { Kafka } from \"kafkajs\"import { Sender } from \"@questdb/nodejs-client\"\ndotenv.config()\nconst app = express()app.use(express.json())\nconst kafka = new Kafka({  clientId: \"weather-consumer\",  brokers: [\"kafka:29092\"],})\nconst consumer = kafka.consumer({  groupId: \"weather-group\",  retry: {    retries: 8, // Increase the number of retries  },  requestTimeout: 60000, // Increase the request timeout})\n// Add health check routeapp.get(\"/health\", (req, res) => {  res.status(200).json({ status: \"OK\", message: \"Consumer service is healthy\" })})\n// The logic for starting the consumer and processing the data to QuestDB will go here//...//...\napp.listen(process.env.PORTB, () => {  console.log(\"Weather Consumer running on port\", process.env.PORTB)})\n```\n\n\n### Ingesting into QuestDB\n\nA new function will save data to QuestDB. To do so, We initialize a sender with\nQuestDB configuration details, then map the data from the producer service to\nour preferred schema. The sender is then flushed, and the connection is closed\nafter saving the data:\n\n```\nasync function saveToQuestDB(data) {  const sender = Sender.fromConfig(    \"http::addr=questdb:9000;username=admin;password=quest;\",  )  try {    await sender      .table(\"weather_data\")      .symbol(\"city\", data.city)      .symbol(\"timezone\", data.timezone)      .timestampColumn(\"timestamp\", parseInt(data.timestamp))      .floatColumn(\"temperature\", data.temperature)      .timestampColumn(\"sunrise\", parseInt(data.sunrise))      .timestampColumn(\"sunset\", parseInt(data.sunset))      .floatColumn(\"feels_like\", data.feels_like)      .floatColumn(\"pressure\", data.pressure)      .floatColumn(\"humidity\", data.humidity)      .stringColumn(\"weather_main\", data.weather_main)      .stringColumn(\"weather_desc\", data.weather_description)      .stringColumn(\"weather_icon\", data.weather_icon)      .at(Date.now(), \"ms\")    console.log(\"Data saved to QuestDB\")  } catch (error) {    console.error(\"Error saving data to QuestDB:\", error)  } finally {    await sender.flush()    await sender.close()  }}\n```\n\nThe\nsaveToDB()\nfunction defines the table and its schema. Note the fields\nwe've deemed relevant and the types we've specified that make up our schema.\nIt's important to note here that the QuestDB clients leverage the InfluxDB Line\nProtocol (ILP) over HTTP. With ILP over HTTP, table creates are handled\nautomatically. Once QuestDB receives this payload, the table is created as\nneeded. That means no additional upfront work is required.\nThe data is sent, tables are created, and it's now within QuestDB.\n\n### Starting the consumer\n\nNow we need some logic to subscribe to the Kafka topic from the producer\nservice. We also need to process stringified Kafka messages to JSON, and start\nthe consumer:\n\n```\nasync function processMessage(message) {  const data = JSON.parse(message.value.toString())  console.log(\"Received weather data:\", data)  await saveToQuestDB(data)}\nasync function startConsumer() {  await consumer.connect()  await consumer.subscribe({ topic: \"weather-data\", fromBeginning: true })\n  await consumer.run({    eachMessage: async ({ message }) => {      await processMessage(message)    },  })}\nprocess.on(\"SIGINT\", async () => {  console.log(\"Shutting down gracefully...\")  await consumer.disconnect()  process.exit(0)})\nstartConsumer().catch(console.error)\n```\n\nThe\nprocessMessage()\nfunction processes and sends data to QuestDB. The\nstartConsumer()\nfunction starts the consumer, subscribes to the topic from the\nprovider, and utilizes the\nprocessMessage()\nfunction to process each stream of\ndata.\nIn cases where the app is terminated, the\nprocess.on()\nfunction handles the\ngraceful shutdown of the consumer.\nThings to note:\n- Thesenderconfiguration usesquestdb:9000instead oflocalhost:9000because the QuestDB server runs inside a Docker container, and the hostname\nisquestdb. Uselocalhostor whatever you set as hostname in yourdocker-compose.ymlfile.\n- ThetimestampColumndata usesparseIntmethod to format the JSON data\nfrom the produce to integer since it only accepts unix time or bigint.\n\n## Dockerizing the app\n\nWe have successfully built a service to get weather data from the OpenWeatherMap\nAPI, and then stream it to the Kafka broker. We also have another service to\nthen consume and stream that data to QuestDB. Now let's dockerize these\nservices.\nIn addition to the two services, we'll include a Grafana docker image, Kafka\ndocker image, and our QuestDB docker image. This will enable our applications to\ninteract with and between these applications. To that end, we will create a\nDockerfile for each service.\nThe\nDockerfile\nfor the producer service will look like this:\n\n```\nFROM node:16.9.0-alpineWORKDIR /appCOPY package.json app-producer/index.js /app/RUN npm installRUN npm i -g nodemonCMD [ \"nodemon\", \"index.js\" ]\n```\n\nThe\nDockerfile\nfor the consumer service will look like this:\n\n```\nFROM node:16.9.0-alpineWORKDIR /appCOPY package.json app-consumer/index.js /app/RUN npm installRUN npm i -g nodemonCMD [ \"nodemon\", \"index.js\" ]\n```\n\nThese Dockerfiles are straightforward. We use the Node.js Alpine image and copy\nthe\npackage.json\nfile from the root folder and the\nindex.js\nfile from the\nrespective services to the image. We also install the dependencies and run the\nservice using nodemon.\nNext, we create a\ndocker-compose.yml\nfile in the root folder. This file will\ndefine the services and the networks to which they will connect:\n\n```\nservices:  app1:    container_name: app-producer    build:      context: ./      dockerfile: ./app-producer/Dockerfile    ports:      - \"8080:3001\"    env_file:      - ./.env    depends_on:      - kafka\n  app2:    container_name: app-consumer    build:      context: ./      dockerfile: ./app-consumer/Dockerfile    ports:      - \"8081:3002\"    env_file:      - ./.env    depends_on:      - kafka\n  questdb:    image: questdb/questdb:8.1.0    container_name: questdb    ports:      - \"9000:9000\"    environment:      - QDB_PG_READONLY_USER_ENABLED=true\n  grafana:    image: grafana/grafana-oss:11.1.3    container_name: grafana    ports:      - \"3000:3000\"    depends_on:      - questdb\n  zookeeper:    image: confluentinc/cp-zookeeper:7.3.0    container_name: zookeeper    ports:      - \"2181:2181\"    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000\n  kafka:    image: confluentinc/cp-kafka:7.3.0    container_name: kafka    ports:      - \"9092:9092\"    environment:      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1    depends_on:      - zookeeper\nnetworks:  default:    name: weather-data-visualization-network\n```\n\nThe\ndocker-compose.yml\nfile defines the following services:\n- app1- Producer service that references theDockerfilein theapp-producerfolder and the root folder as context. The service listens to\nport 8080 and exposes it to the host machine. The service also depends on thekafkaservice\n- app2- Consumer service also references theDockerfilein theapp-consumerfolder and the root folder as context. The service listens to\nport 8081 and exposes it to the host machine. It also depends on thekafkaservice\n- questdb- QuestDB service\n- grafana- Grafana service\n- zookeeper- The Zookeeper service, which the Kafka service uses to manage\ntopics and partitions\n- kafka- Kafka service, which we connect to theweather-data-visualization-networknetwork\nWe've also defined the environmental variables for each service. The\nenv_file\nproperty defines the environment variables from the\n.env\nfile.\nWith this setup, we're ready to build the images and run the services.\n\n## Building and running the app\n\nThe app is ready to run on docker containers.\nBuild the images and run the services using the following docker-compose\ncommands:\n\n```\ndocker-compose up -d\n```\n\nThis command will build the images and run the services in detached mode.\nIf all went well, you'll get a message like this that says the services are\nstarting:\ndocker compose up\n\n## Visualizing the data with Grafana\n\nRaw data is interesting. But we can make it much more dynamic with a proper\nvisualization tool. We've added Grafana, and now we can interact with our data\nin QuestDB. To do so, we will configure the\nofficial QuestDB data source\nin Grafana to visualize the data.\nFirst, point your browser to the port that we set for QuestDB in the\ndocker-compose.yml\nfile, which is the default\nhttp://localhost:9000\n.\nWe should see the QuestDB Web Console:\nQuestDB UI for data queries\nNext, access Grafana via\nhttp://localhost:3000\nand log in with the default\nusername and password, which are\nadmin\nand\nadmin\nrespectively. For security\npurposes, you will be prompted to change the password, and after, you will be\nredirected to the Grafana dashboard:\nGrafana UI for data visualization\nNext, we need to create a new data source in Grafana.\nClick on the\nConnections\nbutton on the left side of the screen.\nThen click on the\nAdd new connection\nbutton.\nNow search for the QuestDB data source plugin and install it.\nAfter installation, you will be prompted to configure it or connect it to a data\nsource:\nQuestDB configuration for as Grafana data source\nThe key variables for this configuration are as follows:\n\n```\nServer Address: host.docker.internalServer Port: 8812Username: adminPassword: questTLS/SSL Settings: disabled\n```\n\nAfter configuring the data source, you will be prompted to save it. Click on the\nSave & Test\nbutton to get feedback on whether the connection is successful.\nFinally, for data visualization, Click on\nDashboards\nfrom the sidebar and\nclick the\nAdd Visualization\nbutton to start. If you are on a Grafana dashboard\npage with some existing dashboard visualization(s), click on the\nNew\ndropdown\nat the top right corner of the screen.\nSelect the\nNew Dashboard\nbutton, and you will be taken to the\nAdd Visualization\nscreen to click on it. After that, select the QuestDB data\nsource you configured earlier, and you will be taken to where you will put your\nqueries to start visualizing the weather data:\nGrafana query panel for data visualization\nFor the data we want to visualize, we will use a specialized query:\n\n```\nSELECT   city,   to_str(date_trunc(\"day\", timestamp), \"E, d MMM yyyy\") AS current_day,   to_str(MIN(date_trunc(\"hour\", timestamp)), \"h:mma\") AS first_hour,   to_str(MAX(date_trunc(\"hour\", timestamp)), \"h:mma\") AS last_hour,   LAST(weather_main) AS last_weather,   CONCAT(\"https://openweathermap.org/images/wn/\", LAST(weather_icon), \"@2x.png\") AS weather_icon,   CONCAT(CAST(ROUND(AVG(temperature), 2) AS FLOAT), \" Â°C\") AS avg_temp,   CONCAT(CAST(ROUND(AVG(feels_like), 2) AS FLOAT), \" Â°C\") AS feels_like,   CONCAT(CAST(ROUND(AVG(pressure), 2) AS FLOAT), \" hPa\") AS avg_pressure,   CONCAT(CAST(ROUND(AVG(humidity), 2) AS FLOAT), \" %\") AS avg_humidityFROM weather_dataGROUP BY city, date_trunc(\"day\", timestamp)ORDER BY city, date_trunc(\"day\", timestamp), first_hour\n```\n\nThe query above will ask QuestDB to group the weather data by city and day. It\nwill also show the first and last hour of the day for which the weather data has\nalready been fetched. The latest weather data is shown alongside the weather\nicon, including average temperature, average \"feels like\" temperature, average\npressure, and average humidity. This is the output result in the below table\nformat.\nTo view this data as shown in the screenshot, use the table panel:\nGrafana dashboard result\nTo get the weather icons as they are in the dashboard above, we can add an\noverride for that column. To do so, click the\nOverride\nsection on the\nright-hand side of the query panel:\nAdding override to the weather icon column\n\n## Wrapping up\n\nIn this tutorial we orchestrated a microservice application together with\nQuestDB, Kafka, and Grafana. We also learned how to use QuestDB to store\ntime-series data\nand visualize the data with\nGrafana. QuestDB works great on low hardware, but can scale up to massive\nthroughput.\nWith this toolkit, you can ingest weather data from thousands of different\nsources and scale up to billions of events. Processing all these data in\nreal-time as they come at scale can be a major challenge, and a specialized\ndatabase like QuestDB can help with a lot of heavy lifting.\nWith respect to architecture, this tutorial is not production ready. But it is a\nhelpful proof of concept to showcase how Kafka, QuestDB and Grafana can store\nand visualize different types of\ntime-series data\n. And you'll also know when\nto bring your umbrella.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3097,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-32235e3b84ae",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/building-real-time-bollinger-bands-charts",
    "title": "Building Real-Time Bollinger Bands Charts with SQL and Grafana | QuestDB",
    "text": "Bollinger Bands are one of the most widely used technical indicators in trading, and for good reason: they turn price volatility into something you can see at a glance. The concept is simple, a moving average with upper and lower bands based on standard deviation. When price touches or breaks through the bands, it's at an extreme relative to recent volatility. That's not a signal on its own, but combined with other indicators, it helps traders spot potential reversals and breakout opportunities.\nIn this post, I'll show you how to calculate Bollinger Bands using SQL, visualize them in Grafana, and then overlay them on candlestick charts alongside other indicators. This is exactly the approach we use in our\nlive FX order book dashboard\n.\n\n## What are Bollinger Bands?\n\nBollinger Bands consist of three lines:\n- Middle Band: A simple moving average (SMA), typically over 20 periods\n- Upper Band: The SMA plus 2 standard deviations\n- Lower Band: The SMA minus 2 standard deviations\nThe bands expand when volatility increases and contract when the market is quiet. When the bands contract tightly (a \"squeeze\"), it often precedes a significant price move, though the direction must be determined using other indicators. Traders use Bollinger Bands to spot potential breakouts, gauge trend strength, and identify mean reversion opportunities.\n\n## Calculating Bollinger Bands in QuestDB\n\nHere's the SQL to calculate Bollinger Bands using 15-minute OHLC candles with a 20-period window. You can\nrun this query on our public demo\nto see live results.\nINFO\nThe QuestDB demo uses simulated FX data anchored to real reference prices. Volatility may differ from live markets. See the\ndemo data schema\nfor details.\n\n```\nWITH stats AS (  SELECT    timestamp,    symbol,    close,    AVG(close) OVER (      PARTITION BY symbol      ORDER BY timestamp      ROWS BETWEEN 19 PRECEDING AND CURRENT ROW    ) AS sma20,    AVG(close * close) OVER (      PARTITION BY symbol      ORDER BY timestamp      ROWS BETWEEN 19 PRECEDING AND CURRENT ROW    ) AS avg_close_sq  FROM market_data_ohlc_15m  WHERE timestamp > dateadd('d', -1, now())    AND symbol = 'EURUSD')SELECT  timestamp,  sma20,  sma20 + 2 * sqrt(avg_close_sq - (sma20 * sma20)) AS upper_band,  sma20 - 2 * sqrt(avg_close_sq - (sma20 * sma20)) AS lower_bandFROM statsORDER BY timestamp;\n```\n\n\n### Example output\n\n\n| timestamp | sma20 | upper_band | lower_band |\n| --- | --- | --- | --- |\n| 2026-01-25T14:30:00.000000Z | 1.1793 | 1.1933 | 1.1653 |\n| 2026-01-25T14:45:00.000000Z | 1.1796 | 1.1937 | 1.1655 |\n| 2026-01-25T15:00:00.000000Z | 1.1791 | 1.1935 | 1.1648 |\n| 2026-01-25T15:15:00.000000Z | 1.1785 | 1.1914 | 1.1656 |\n| 2026-01-25T15:30:00.000000Z | 1.1788 | 1.1913 | 1.1663 |\n| 2026-01-25T15:45:00.000000Z | 1.1781 | 1.1907 | 1.1655 |\n| 2026-01-25T16:00:00.000000Z | 1.1778 | 1.1903 | 1.1653 |\n\nNotice how the bands contract as volatility decreases: at 14:30 the band width is about 280 pips, narrowing to 250 pips by 16:00. Traders watch for this contraction (a \"squeeze\") as it often precedes significant price moves. You can quantify this with\nBollinger BandWidth\n, which expresses the width as a percentage and compares it to historical levels, making it easier to spot when volatility is near its 6-month lows.\nThe query uses the variance formula\nsqrt(avg(xÂ²) - avg(x)Â²)\nto calculate standard deviation. This uses population standard deviation rather than sample standard deviation, which is common in trading platforms.\nWith 20 periods of 15-minute candles, the SMA covers a 5-hour window. You can adjust the period count and candle size to match your trading timeframe.\nFor more variations and detailed explanations, see our\nBollinger Bands cookbook recipe\n.\n\n## Visualizing Bollinger Bands in Grafana\n\nRunning the query above in Grafana gives you a clean visualization of the three bands:\nBollinger Bands (20,2) for EUR/USD\nThis is useful for seeing volatility patterns, but in practice you want to see the bands in context with actual price movement.\n\n## Overlaying Indicators on Candlestick Charts\n\nThe real power comes from overlaying Bollinger Bands on top of OHLC candlesticks, alongside other indicators like VWAP or RSI. This gives you a complete picture of price action, volatility, and momentum in a single chart.\nOHLC candles with Bollinger Bands, VWAP, and RSI (12h)\n\n### The Grafana technique\n\nTo build this multi-indicator chart in Grafana:\n- Use the Candlestick visualizationas your base chart type\n- Add multiple queriesto the same panel: one for OHLC data (your candles), and additional queries for each indicator (Bollinger Bands, VWAP, RSI, etc.)\n- Enable \"Include all fields\"in the candlestick chart options under \"Additional fields\". This tells Grafana to render data from all your queries, not just the OHLC fields.\nCandlestick options with OHLC field mapping and 'Include all fields' enabled\n- Use Overrides for styling: Each indicator needs different visual treatment. Use Grafana's override system to set colors, line styles, and axis configuration per query. For example, RSI operates on a 0-100 scale, so you'll want to configure it with a custom unit and place it on the right axis to avoid distorting your price scale.\nUsing overrides to style each indicator independently\nThis same technique works for any combination of indicators. In the example above, I'm showing VWAP (volume-weighted average price) and a 12-hour RSI alongside the Bollinger Bands, each with their own color scheme and axis configuration.\n\n## See it live\n\nYou can explore this visualization with real-time FX data on our\nlive FX order book dashboard\n. The chart in the second row shows Bollinger Bands overlaid on 15-minute candles, defaulting to EUR/USD with other major currency pairs available via the symbol selector.\nFor the complete SQL and more examples, check out the\nBollinger Bands cookbook recipe\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 930,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-95ead2bd8e5a",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2020/11/16/http-server-contribution",
    "title": "Community contribution from Alex Pelagenko improving our HTTP server | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nI have recently made a sizable contribution to QuestDBâ€™s code and wanted to\nshare my experience and feedback while it is still fresh in my head. I am not a\ncomplete outsider for the project and know Vlad personally but other than that\nit was voluntary to add a few lines of code to a project I like.\n\n## The HTTP server for QuestDB\n\nQuestDB has a custom HTTP stack that uses non-blocking socket IO via a thin\nlayer of JNI OS abstraction. Non-blocking IO is handled via two state machines.\nOne for inbound traffic, which includes a series of parsing state machines. The\nother for outbound traffic. We focus on the outbound traffic state machine,\nwhich has to deal with two types of interruptions: slow socket on one side and\ndata availability on the other. While slow socket interruption was already dealt\nwith, the data availability interruption had been handled in a very trivial\nmanner. When data was unavailable, the HTTP stack would report an immediate\nerror and trigger a send-to-socket state machine.\nData availability interruptions are due to QuestDBâ€™s single writer model. A\ntable will be locked while the HTTP server is dealing with a CSV import request.\nA request to alter the locked table will bounce back with an error. Why is this\ninteresting? It is a difficult problem of coordination amongst threads while at\nthe same time keeping the whole stack non-blocking.\n\n## How I added queuing to QuestDB's HTTP stack\n\nThe first hurdle was to understand the stack, which is hard to follow at first\nglance. Control is passed around via both conditional statements and exception\nmechanisms. The thread messaging stack is also unusual. The API is\nnon-blocking - the thread must find another task if the outbound queue is full\nor the inbound queue is empty.\nInstead of rejecting requests due to data availability errors, I added a queuing\nsystem that catches the state of these requests in a priority queue. This queue\nis then processed by idle threads (idle because of IO interruptions) and retried\nat exponentially increasing intervals. For example the first retry will happen\nin 2ms then in 4ms, 8ms, 16ms â€¦ 512ms, 1s, 1s, 1s. The retry interval is a\nconfiguration parameter. Following this addition, operations are queued on the\nserver, meaning that the user does not have to deal with errors or attempt to do\nthis operation again. This piece of code processes priority queue:\n\n```\nprivate boolean sendToOutQueue() {  boolean useful = false;   final long now = clock.getTicks();   while (nextRerun.size() > 0) {       Retry next = nextRerun.peek();       if (next.getAttemptDetails().nextRunTimestamp <= now) {           useful = true;           Retry retry = nextRerun.poll();           if (!sendToOutQueue(retry)) {               nextRerun.add(retry);               return true;           }       }       else {           // All reruns are in the future.           return useful;       }   }   return useful;}\n```\n\nQuestDBâ€™s team was very patient explaining the different bits of the HTTP stack\nand guiding me to figure out how best to solve the multi writing problem. They\nwere also careful not to dampen my enthusiasm with the pull request feedback.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 545,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-75f93e0b3578",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/ingesting-financial-tick-data-using-time-series-database",
    "title": "Ingesting Financial Tick Data Using a Time-Series Database | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nIn this tutorial, weâ€™ll take a look at three different ways to ingest crypto\nmarket data into QuestDB for further analysis:\n- Using theCryptofeedlibrary\n- Writing a custom data pipeline\n- ViaChange Data Capture (CDC)\n\n## Prerequisites\n\nWe will be using QuestDB to ingest and store crypto market data. Create a new\ndirectory and from the directory, run the following to start a local instance of\nQuestDB:\n\n```\nmkdir cryptofeed-questdbcd cryptofeed-questdbdocker run \\  -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\-v \"$(pwd):/var/lib/questdb\" \\  questdb/questdb\n```\n\n\n## Method 1: ingesting data using the Cryptofeed library\n\nOne of the easiest ways to ingest market data is to use an open-source tool\ncalled Cryptofeed. The Python library establishes websocket connections to\nvarious exchanges including Binance,\nOKX\n, Gemini, and\nKraken and returns trade, market, and book update data in a standardized format.\nCryptofeed also has native integration with QuestDB, making it a great choice to\ningest data rapidly.\nTo get started, create a virtual environment with Python 3.8+. We will use\nvenv\nbut you can use conda,\npoetry, or virtualenv as well. We will create a venv for cryptofeed:\n\n```\n$ python3 -m venv cryptofeed$ source cryptofeed/bin/activate\n```\n\nThen install cryptofeed:\npip install cryptofeed\nNavigate into the\ncryptofeed\ndirectory and create a new file\nquestdb.py\n. We\nwill then paste the following to ingest trade data for BTC-USDT pair from OKX\nand Gemini:\n\n```\nfrom cryptofeed import FeedHandlerfrom cryptofeed.backends.quest import TradeQuestfrom cryptofeed.defines import TRADESfrom cryptofeed.exchanges import OKX, Gemini\nQUEST_HOST = '127.0.0.1'QUEST_PORT = 9009\ndef main():   f = FeedHandler()   f.add_feed(OKX(channels=[TRADES], symbols=['BTC-USDT'], callbacks={TRADES: TradeQuest(host=QUEST_HOST, port=QUEST_PORT)}))   f.add_feed(Gemini(channels=[TRADES], symbols=['BTC-USDT'], callbacks={TRADES: TradeQuest(host=QUEST_HOST, port=QUEST_PORT)}))   f.run()\nif __name__ == '__main__':   main()\n```\n\nWhen you run this code, it will automatically create a socket connection with\nOKX and Gemini API and push data to QuestDB. Note that it may take a while to\nsee data populated (especially from Gemini).\nNavigate to localhost:9000 to access the web console. We can query data from OKX\nvia\nSELECT * FROM 'trades-OKX'\n:\nYou can see all the supported exchanges and supported channels (e.g., L1/L2/L3\nbooks, trades, ticket, candles, open interest, etc) on the\nCryptofeed GitHub\npage.\nIf you want to modify the structure of the data ingested into QuestDB, you can\noverride the callback handler. For example, if you want to change the name of\nthe table it writes to or the columns, you can specify the\nwrite\nfunction. In\nfact, the\nQuestDB demo site\nimplements\ncryptofeed to ingest data to the\ntrades\ntable with a custom\ncallback function.\nYou will first need to install aiohttp:\npip install aiohttp\n, as we are using HTTP to ingest data.\n\n```\nimport asyncioimport aiohttpfrom cryptofeed import FeedHandlerfrom cryptofeed.backends.backend import BackendCallbackfrom cryptofeed.defines import TRADESfrom cryptofeed.exchanges import OKX\nQUEST_HOST = '127.0.0.1'QUEST_PORT = 9000\nclass QuestHTTPCallback(BackendCallback):    def __init__(self, host='127.0.0.1', port=9000, **kwargs):        super().__init__(**kwargs)        self.host = host        self.port = port        self.url = f\"http://{host}:{port}/write\"        self.session = None        self.loop = None        self.numeric_type = float        self.none_to = None\n    def start(self, loop=None, multiprocess=False):        \"\"\"Initialize HTTP session - defer actual creation to async context\"\"\"        # Don't create session here, just store the loop reference        self.loop = loop        # Session will be created in the writer method when we're in async context\n    async def stop(self):        \"\"\"Clean up HTTP session\"\"\"        if self.session:            await self.session.close()            self.session = None\n    async def writer(self):        \"\"\"HTTP writer that sends data to QuestDB\"\"\"        # Initialize session if not already done        if self.session is None:            self.session = aiohttp.ClientSession()\n        while True:            try:                async with self.read_queue() as updates:                    if updates:                        # Join multiple updates with newlines                        data = \"\\n\".join(updates) + \"\\n\"\n                        try:                            async with self.session.post(                                self.url,                                data=data,                                headers={'Content-Type': 'text/plain'},                                timeout=aiohttp.ClientTimeout(total=10)                            ) as response:                                if response.status != 200:                                    print(f\"HTTP Error {response.status}: {await response.text()}\")                        except aiohttp.ClientError as e:                            print(f\"HTTP Client Error: {e}\")                        except asyncio.TimeoutError:                            print(\"HTTP request timeout\")                        except Exception as e:                            print(f\"Unexpected error sending to QuestDB: {e}\")            except Exception as e:                print(f\"Error in writer loop: {e}\")                await asyncio.sleep(1)  # Wait before retrying\nclass TradeQuestHTTP(QuestHTTPCallback):    default_key = 'trades'\n    def __init__(self, **kwargs):        super().__init__(**kwargs)        self.key = self.default_key  # Initialize the key attribute\n    async def write(self, data):        \"\"\"Convert trade data to InfluxDB Line Protocol format\"\"\"        # Remove debug logging now that we know the structure        exchange = data.get('exchange', 'unknown')\n        # Format: measurement,tag1=value1,tag2=value2 field1=value1,field2=value2 timestamp        update = (            f'{self.key},'            f'symbol={data[\"symbol\"]},'            f'side={data[\"side\"]},'            f'exchange={exchange} '            f'price={data[\"price\"]},'            f'amount={data[\"amount\"]} '            f'{int(data[\"timestamp\"] * 1_000_000_000)}'        )\n        # Send directly to QuestDB instead of using queue        if self.session is None:            self.session = aiohttp.ClientSession()\n        try:            async with self.session.post(                self.url,                data=update + \"\\n\",                headers={'Content-Type': 'text/plain'},                timeout=aiohttp.ClientTimeout(total=5)            ) as response:                if response.status == 200 or response.status == 204:                    print(f\"âœ“ Sent: {data['exchange']} {data['symbol']} {data['side']} ${data['price']} x {data['amount']}\")                else:                    print(f\"HTTP Error {response.status}: {await response.text()}\")        except Exception as e:            print(f\"Error sending to QuestDB: {e}\")\ndef main():    print(f\"Starting cryptofeed with QuestDB HTTP ILP on {QUEST_HOST}:{QUEST_PORT}\")    print(\"Subscribing to BTC-USDT and ETH-USDT trades on OKX...\")\n    handler = FeedHandler()\n    # Create the callback instance    trade_callback = TradeQuestHTTP(host=QUEST_HOST, port=QUEST_PORT)\n    handler.add_feed(        OKX(            channels=[TRADES],            symbols=['BTC-USDT', 'ETH-USDT'],            callbacks={TRADES: trade_callback}        )    )\n    try:        handler.run()    except KeyboardInterrupt:        print(\"\\nShutting down...\")    finally:        # Clean up HTTP session        if hasattr(trade_callback, 'session') and trade_callback.session:            asyncio.run(trade_callback.stop())\nif __name__ == '__main__':    main()\n```\n\nThe biggest advantage of using Cryptofeed is the large number of preconfigured\nintegrations with various exchanges. The library does the heavy lifting of\nnormalizing the data so ingesting it into QuestDB is very simple. However, if\nyou need more control over the type or format of the data, you may need to call\nthe exchange API directly.\n\n## Method 2: Build a custom market data pipeline\n\nIf Cryptofeed does not support the exchange you are interested in or if you need\nmore control over the type or format of the data, you can opt to write your own\ndata ingestion function. With QuestDB, you have the option to use PostgreSQL\nwire or ILP. Since the ILP is faster and supports schemaless ingestion, we will\nshow an example of using the InfluxDB Line Protocol via QuestDB Python SDK to\ningest price data from Binance and Gemini:\n\n```\nimport requestsimport timefrom questdb.ingress import Sender, TimestampNanos\nconf = f'http::addr=localhost:9000;'\ndef get_binance_data():  url = 'https://api.binance.us/api/v3/ticker/price'  params = {'symbol': 'BTCUSDT'}  response = requests.get(url, params=params)  data = response.json()  btc_price = data['price']  print(f\"BTC Price on Binance: {btc_price}\")  publish_to_questdb('Binance', btc_price)\ndef get_gemini_data():  url = 'https://api.gemini.com/v1/pubticker/btcusdt'  response = requests.get(url)  data = response.json()  btc_price = data['last']  print(f\"BTC Price on Gemini: {btc_price}\")  publish_to_questdb('Gemini', btc_price)\n\ndef publish_to_questdb(exchange, price):  print(\"Publishing BTC price to QuestDB...\")  with Sender.from_conf(conf) as sender:    sender.row(        'prices',        symbols={'pair': 'BTCUSDT'},        columns={'exchange': exchange, 'bid': price},        at=TimestampNanos.now())    sender.flush()\ndef job():  print(\"Fetching BTC price...\")  get_binance_data()  get_gemini_data()\nwhile True:  job()  time.sleep(5)\n```\n\nThe code above polls the REST endpoints of Binance and Gemini API and writes the\ndata to a table called\nprices\n:\nWhile writing a custom data ingestion function is more work than simply using\nCryptofeed, it can be a great option if you need to customize the fields or run\nsome preprocessing logic prior to sending it to QuestDB.\n\n## Method 3: Ingest market data using Change Data Capture (CDC)\n\nFinally, you can ingest data via Change Data Capture (CDC) if you have an\nexternal data stream or database that you can listen on. For example, an\nexternal data market team might publish price data on Kafka or push updates to a\nrelational database\n. Instead of polling this\ndata directly, you could opt to leverage CDC patterns to\nstream changes\nto QuestDB instead.\nAn example of this architecture is detailed in\nRealtime crypto tracker with QuestDB Kafka Connector\n.\nThis reference architecture has a function that polls Coinbase API for latest\nprice data and publishes it to Kafka topics. QuestDB Kafka Connector in turn\npublishes that data to QuestDB.\n\n## Wrapping up\n\nWrapping up QuestDB offers various ways to ingest crypto market data quickly.\nFor a starting point, utilize the Cryptofeed library to connect to various\nexchanges that are already supported, and optionally modify the ingestion by\nimplementing your own callback. If you need to integrate with a data feed not\nsupported by Cryptofeed, you can write a custom data ingestor and publish data\nover InfluxDB line protocol to QuestDB. Finally, if thereâ€™s an existing data\nfeed that Debezium supports (e.g., Kafka, PostgreSQL) then using CDC can be a\ngreat choice to minimize the infrastructure burden.\n\n## Additional resources\n\n- How to start a time-series dashboard with Grafana\n- Processing time-series with QuestDB and Apache Kafka\n- Demo of live crypto data streamed with QuestDB and Grafana\n\n## FAQs\n\n- What is level 1, Level 2, Level 3 market data?Level 1 market data provides basic market data including current bid and ask\nprices, last traded price, and the size on each buy/sell side. Level 2 market\ndata provides market depth data (e.g., 5-10 bid/ask prices). Level 3 adds\nadditional information on market depth data typically up to 20 best bid/ask\nprices along with custom orders.\n- What is an order book?An order book is a collection of buy and sell orders for a security or\ncommodity being traded. It organizes current bids and asks by price, updating\nit constantly as orders are being executed.\n- What are Candlestick charts?Candlestick charts are a popular form of financial chart uses to show the\nprice movement. The candle comprises of opening, closing, high, and low prices\nfor the financial instrument of note.\n- What is an exchange API?Exchange API is the application programming interface that exposes data (e.g.,\nmarket depth, latest price, etc) provided by exchanges.\n- What is Change Data Capture (CDC)?Change Data Capture (CDC) is a design pattern used with databases to track\nchanges to the underlying dataset. CDC enables near real-time data replication\nand integration.\n- What is Kafka?Kafka is a distributed streaming platform that is used for data pipelines,\nstreaming analytics, and data integration.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1635,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "financial"
      ]
    }
  },
  {
    "id": "questdb-blog-1752426bf14d",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/time-series-data-visualization-apache-superset-and-questdb",
    "title": "Time-Series Data Visualization with Apache Superset and QuestDB | QuestDB",
    "text": "QuestDB is the open-source time-series database for demanding workloadsâ€”from trading floors to mission control.\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-readyâ€”no vendor lock-in.\nOne of the most common uses of a\ntime-series database\nis visualizing data on a\ndashboard to inform business decisions. This tutorial will show you how to\nconnect QuestDB and Apache Superset. QuestDB is an open-source time-series\ndatabase and Apache Superset is an open-source data exploration and\nvisualization platform. By combining these, you can create a dashboard\ndisplaying data from multiple tables.\n\n## Installing and starting QuestDB\n\nIf you are not yet using QuestDB, or if you are using a version lower than\n7.1.2, please refer to\nour documentation\n. In my\ncase, I start a QuestDB instance (with ephemeral storage) using Docker:\n\n```\ndocker run \\  -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\  questdb/questdb:7.1.2\n```\n\n\n## The datasets\n\nWe will be working with two datasets that we need to download and import into\nQuestDB. The first one represents taxi rides in the city of New York. The\noriginal dataset\nis very large. Therefore, we are going to work with a subset that represents a\nfew minutes of taxi rides on January 1st, 2018. You can download and import\nthe dataset\nfrom the command line using curl:\n\n```\ncurl https://raw.githubusercontent.com/javier/questdb-quickstart/main/trips.csv > trips_2018.csvcurl -F data=@trips_2018.csv 'http://localhost:9000/imp?overwrite=false&name=trips_2018&timestamp=pickup_datetime&partitionBy=DAY'\n```\n\nThe second dataset represents energy consumption and next-day forecast at\n15-minute intervals for a few European countries. We have curated a subset of\nthe\noriginal dataset\ncontaining\ndata for 2018\n.\nDownload the data and import it to QuestDB with:\n\n```\ncurl https://raw.githubusercontent.com/javier/questdb-quickstart/main/energy_2018.csv > energy_2018.csvcurl -F schema='[{\"name\":\"timestamp\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd HH:mm:ss\"}]' -F data=@energy_2018.csv 'http://localhost:9000/imp?overwrite=false&name=energy_2018&timestamp=timestamp&partitionBy=MONTH'\n```\n\nIf you visit your QuestDB web interface at\nhttp://localhost:9000\n, you should see the two new\ntables. You should have 999 rows in the\ntrips_2018\ntable and 205189 rows in\nthe\nenergy_2018\ntable.\n\n```\nSELECT COUNT() FROM trips_2018;SELECT COUNT() FROM energy_2018;\n```\n\n\n## Installing and starting Apache Superset\n\nYou will also need a working installation of Apache Superset with the\nQuestDB-Connect module installed. See\nSuperset integration\nfor more details. I\nchoose Docker to run Superset, and these are the steps to install it:\n\n```\ngit clone https://github.com/apache/superset.gitcd supersettouch ./docker/requirements-local.txtecho \"questdb-connect==1.1.3\" >> ./docker/requirements-local.txtexport TAG=4.0.2docker compose -f docker-compose-image-tag.yml pulldocker compose -f docker-compose-image-tag.yml up\n```\n\nNote: you may need to upgrade docker-compose or Docker Desktop to newer than v2.24.0 to\nsupport the syntax that docker-compose file is using above.\nBy default, your Apache Superset runs at\nhttp://localhost:8088\nand login credentials are\nadmin/admin.\nApache Superset must be initialized when started for the first time. This\nincludes setting up default databases and sample datasets. This process may take\nsome time before it is fully functional.\n\n## Connecting QuestDB and Superset\n\nIn Superset you create charts that you add to dashboards. Charts get their data\nfrom different databases, so the first step is to configure the database\nconnections you will use. In our case, we need to configure a single connection\nto QuestDB.\nSuperset supports a wide range of databases by default. QuestDB is not one of\nthem, but since we installed the\nquestdb-connect module\n, we can add\nthe connection by following these steps:\n- From the Superset web interface athttp://localhost:8088, selectSettings>Database Connections\n- Select+ DATABASEto add the following parameters:\nSUPPORTED DATABASES\n:\nOther\n/\nDISPLAY NAME\n:\nQuestDB\nSQLALCHEMY URI\n:\nquestdb://admin:quest@host.docker.internal:8812/qdb\nNote that I use\nhost.docker.internal\ninstead of\nlocalhost\nbecause I started\nSuperset from Docker. If you are running Superset outside of Docker, you would\nneed to use\nlocalhost\ninstead.\n\n## Creating a dataset\n\nBefore you create a chart, Superset requires you to create a dataset. In its\nsimplest form, a dataset represents a table in your database. Still, you can\nalso add calculated columns, or create a dataset from a complex SQL statement\nthat references one or more tables.\nFor the first dataset, let's use the\ntrips_2018\ntable we created earlier.\n- From the Superset web interface athttp://localhost:8088, selectDatasets>+ DATASET\n- Select QuestDB as theDATABASEandtrips_2018as the table. A list of\ncolumns is displayed.\n- ClickCREATE DATASET AND CREATE CHART\nThere is also the optional step to edit the dataset by selecting\nDatasets\n. You\ncan remove columns and add calculated columns or metadata.\n\n## Creating your first chart\n\nAt this point, you should see a dialog to select a visualization type.\nIf not, select the\nCharts\nlink at the top of the Superset web interface and\nclick the name of the dataset we created in the previous step.\nWe're now ready to create a chart.\nSuperset has many types of charts. One of the simplest charts is the\nTable\nchart. If you cannot find the thumbnail, you can always type\nTable\nin the\nsearch box or navigate through the different types of charts using the\ncategories on the left.\nWhen you select the\nTable\ntype and\nCREATE NEW CHART\n, you should see the\nChart\ndialog. This is virtually the same for all the charts, with different\noptions depending on the chart type.\nAt the top of this dialog, there are two tabs: one for defining the data to\ndisplay, and one for customizing the chart appearance.\nFor this first chart, we want to display the total number of entries, the\naverage trip distance, and the average total amount paid, grouped by vendor, cab\ntype, and rate code:\n- Addvendor_id,cab_type, andrate_code_idas dimensions by dragging\ncolumns or by clicking on theDIMENSIONSbox\n- AddCOUNT(*),AVG(trip_distance), andAVG(total_amount)using theMETRICSbox\n- AddAVG(total_amount)to thePERCENTAGE METRICSbox, so we can see how\nmuch of the total each row in the table contributes\n- Uncheck theSORT DESCENDINGoption\n- ClickUPDATE CHART\nYour screen should look very similar to the screenshot below:\nTo save the chart, select\nSAVE\nin the top right. Choose a name (e.g., \"Trips\nStats\"), and the name of a new dashboard (e.g., \"QuestDB Demo\"). Click\nSAVE AND GO TO DASHBOARD\n.\nCongrats. You have just created your first chart and dashboard.\nIf you want to spice up the dashboard by adding headings, links, images, tabs,\ndividers, etc., click\nEDIT DASHBOARD\nand play around with the layout. Check\nthe\nSuperset documentation\nfor more information.\n\n## Creating a dataset from a SQL statement\n\nWe will now add a new dataset, but in this case, we want to use QuestDB-specific\nSQL, such as\nSAMPLE BY\n. We can do this by\ncreating a dataset from a SQL statement. Select\nSQL\n>\nSQL Lab\nand type the\nfollowing query in the SQL box:\n\n```\nSELECT timestamp, country_code, SUM(load_forecast) AS daily_forecast,       SUM(load_actual) AS daily_actualFROM energy_2018SAMPLE BY 1d ALIGN TO CALENDAR;\n```\n\nClick\nRUN\nto see the results. The\nenergy_2018\ntable contains data in\n15-minute intervals, and we use\nSAMPLE BY\nto get the total usage and forecast\nper day and country.\nClick the arrow by the\nSAVE\nbutton, select\nSAVE DATASET\n, type the name\nenergy_sampled_1d\n, and then click\nSAVE AND EXPLORE\n.\nYou should now see the\nChart\ndialog. Ignore any error messages you see on the\nnext screen, as they will disappear as we add some fields and update the chart.\nClick the\nview all charts\nlink and look for the\nMixed Chart\ntype, which\nallows us to represent line, bar, and area charts for different series on a\nsingle chart. We will use it to plot the actual versus projected energy\nconsumption by country.\nThis time, the chart dialog has more options than before.\nLet's select the\ntimestamp\ncolumn for the\nShared query fields\nsection,\nSUM(daily_actual)\nas the\nQuery A\nmetric,\ncountry_code\nas the\nQuery A\ndimension,\nSUM(daily_forecast)\nas the\nQuery B\nmetric, and\ncountry_code\nas\nthe\nQuery B\ndimension. Uncheck\nSORT DESCENDING\nfor\nA\nand\nB\nqueries, and\nclick\nUPDATE CHART\n.\nIf all goes well, you will see a line chart representing the energy data. Click\nSAVE\n, name the chart (e.g., \"Energy Forecast\"), and add it to your existing\ndashboard.\n\n## Wrapping up\n\nApache Superset is a complete tool for creating data visualizations. In this\npost, we have connected QuestDB to Apache Superset and added some basic charts.\nThere are many more charts and options you can use.\nThe next step is to explore the Superset documentation. You can learn how to\ncreate business dashboards for both batch and real-time scenarios.\nWe encourage you to share some of your dashboards with\nour\nCommunity Forum\n. You can also play with\nour\nlive demo\nto execute queries on demo datasets.\nOf course, contributions to our\nopen-source\nprojects on GitHub\nare more than\nwelcome.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1417,
    "metadata": {
      "relevance_score": 0.14285714285714285,
      "priority_keywords_matched": [
        "trading"
      ]
    }
  },
  {
    "id": "questdb-blog-8978773490e6",
    "origin": "questdb",
    "source_type": "blog",
    "url": "https://questdb.io/blog/2021/03/09/realtime-stock-alerts-python-grafana-questdb",
    "title": "Real-time stock price alerts using Python, Grafana and QuestDB | QuestDB",
    "text": "QuestDB is a next-generation\n  database for\nmarket data\n. It offers premium ingestion throughput,\n  enhanced SQL analytics that can power through analysis, and cost-saving hardware efficiency. It's\nopen source\n, applies open formats, and is ideal for\ntick data\n.\nThis submission comes from one of our community contributors\nKovid Rathee\nwho has written a great guide\nfor setting up alerting via\nGrafana\n.\nThanks for your contribution, Kovid!\n\n## Introduction\n\nThere are many reasons why reacting to time series data is useful, and the\nquicker you can respond to changes in this data, the better. The best tool for\nthis job is easily a time series database, a type of database designed to write\nand read large amounts of measurements that change over time.\nIn this tutorial, you will learn how to read data from a REST API and\nstream\nit to QuestDB, an open-source time series\ndatabase. We will use Grafana to visualize the data and notify Slack of changes\nthat interest us. We use Python to fetch data from the API and stream it to\nQuestDB, and you can easily customize the scripts to check different stocks or\neven APIs.\n\n## Prerequisites\n\nBefore getting started with the tutorial, you will need the following:\n- Dockerto run Grafana and\nQuestDB with Docker Compose\n- IexFinance account, which offers a free\ntier for 50,000 API calls per month to poll stock prices\n- Slack workspace(optional)\nThe Python example in this tutorial uses\nreal-time price for a stock\n,\nusing the last trade on IEX. Prices outside of market hours can be retrieved\nfrom the\nextendedPrice\nfield from the Quote endpoint. For more information,\nsee the IexCloud quote endpoint\nThis tutorial uses Slack as an example notification channel to deliver alerts\nvia Grafana, but it's simple to choose another channel you would like alerts\ndelivered to, such as your own REST API via webhook,\nKafka\n, email, Pagerduty and more. For\nmore details on the available notification channels, see the Grafana\ndocumentation for the\nlist of supported notifiers\n.\n\n## Start QuestDB and Grafana\n\nTo clone the GitHub repository and start the example project:\n\n```\ngit clone https://github.com/questdb/questdb-slack-grafana-alerts.gitcd questdb-slack-grafana-alertsdocker-compose up\n```\n\nRunning\ndocker-compose up\nbrings up two networked containers:\n- Grafana onhttp://localhost:3000\n- QuestDB onhttp://localhost:9000as well as a port\nopen on8812, which accepts Postgres wire protocol\nRunning\ndocker-compose up\nwill also\nprovision Grafana\nwith the default connection credentials to QuestDB for Postgres authentication.\nBy provisioning credentials, you can use QuestDB as a default data source in\nGrafana right away without manual configuration steps.\nVerify QuestDB and Grafana are running by visiting the URLs listed above or use\ndocker-compose ps\nfrom the repository root which will show you the running\ncontainers:\ndocker-compose ps\n\n```\n     Name                   Command               State                            Ports------------------------------------------------------------------------------------------------------------------grafana_alerts   /run.sh                          Up      0.0.0.0:3000->3000/tcpquestdb_alerts   /app/bin/java --add-export ...   Up      0.0.0.0:8812->8812/tcp, 0.0.0.0:9000->9000/tcp, 9009/tcp\n```\n\n\n## Send data to QuestDB\n\nThe Python dependencies required for this tutorial are provided in the\nrequirements.txt\nfile and can be installed using pip:\n\n```\npip install -r requirements.txt\n```\n\nWe will use the\nIexFinance API\nto fetch\nreal-time stock prices, but first, we can verify the configuration is correct by\nusing a test script that generates dummy data. The mock script generates random\nprices so that we don't deplete the API call limit during testing.\nTo start sending mock data to QuestDB, run the mock data script in the\npython\ndirectory:\n\n```\ncd pythonpython mock_stock_data_example.py\n```\n\nThe script will create a table\nstock_prices\n, and it will start sending mock\ndata to this table. The columns that we have are:\n\n| column | description |\n| --- | --- |\n| stockListed | name of the stock, e.g.,TSLAfor Tesla |\n| stockPrice | price of the stock in USD asdouble |\n| createdDateTime | timestamp at which stockPrice was ingested in QuestDB |\n\nOne feature of QuestDB that we are using for the stock name is the\nsymbol\ndata\ntype optimized for text columns with repetitive values. More information on this\ntype can be found on the QuestDB\ndocumentation for symbol type\n.\nTo verify if data is arriving in QuestDB:\n- Navigate tohttp://localhost:9000\n- Run the following query:\n\n```\nSELECT * FROM stock_prices;\n```\n\nWe should see all rows from our table returned at the bottom panel:\n\n## Query real-time stock prices\n\nOnce you have tested the ingestion using the mock data script, we can start\nmaking requests to an API for live market data and query real-time stock prices.\nTo configure IexFinance API authentication:\n- On the IexFinance console, create an\nAPI token\n- ClickReveal Secret Tokenand copy the value\nNow we can add this token to our project, so the Python scripts have\nprogrammatic access to the IexFinance API. To store project secrets, we are\nusing the\nPython dotenv\npackage,\nwhich allows passing configuration to Python scripts as environment variables.\nWe can explicitly pass a token to each API call to IexFinance, or we can use the\nIEX_TOKEN\nenvironment variable that the Python library will check implicitly.\nTo add the token:\n- Create a new file./python/.env\n- Paste the token in the.envfile in the formatIEX_TOKEN=Skwf93hD...\n- Run the live market data example:\n\n```\ncd pythonpython stock_data_TSLA_example.py\n```\n\nIf the markets are open, real-time prices will be sent to QuestDB:\n\n```\nInserting rows into table 'stock_prices' - press Ctrl-C to stopInserting into 'stock_prices': TSLA 673.58 2021-03-10 12:37:23.147258...\n```\n\n\n## Slack incoming webhook\n\nThe next step is to create an incoming webhook in Slack to send alerts as\nHTTP(S) requests from Grafana. To set up the webhook:\n- Navigate to thecreate a new Slack apppage and\ncreate an app calledPrice Alertsfor your workspace\n- ClickIncoming Webhooksin the features section\n- Activate incoming webhooks and clickAdd New Webhook to Workspace\n- Select the Slack channel you want to receive alerts in and clickAllow\n- Copy the Webhook URL which is in the following format\n\n```\nhttps://hooks.slack.com/services/T123/B0123/2Fb...\n```\n\n\n## Grafana notification channels\n\nThe final step in connecting Slack and Grafana is to create a notification\nchannel. To configure a Slack notification channel to send alerts to:\n- Log in to Grafana athttp://localhost:3000using the\ndefault credentials:\n\n```\nuser:adminpass:admin\n```\n\n- Navigate tohttp://localhost:3000/alerting/notificationsand clickAdd\nchannel\n- Give it the nameStock Price Alertand choose theSlacktype\n- Paste the webhook URL in theUrlfield\n- In theNotification settingsdropdown, enable theInclude imagecheckbox\n- ClickTestto verify that the configuration is correct and then clickSave\nYour Slack workspace should display a test notification coming from Grafana:\n\n## Grafana alerts\n\nNext up, we can create a new panel on Grafana and configure alerts based on\nqueries we're interested in:\n- Navigate tohttp://localhost:3000/dashboard/newand click+ Add new panel\n- In theQuerypanel, click the pencil icon or clickEdit SQL\n- Paste the following example query\n\n```\nSELECT createdDatetime time,       round(avg(stockPrice),2) avgPriceFROM stock_pricesWHERE stock = 'TSLA'SAMPLE BY 5s;\n```\n\nWe now have a visualization of five-second averages of Tesla's stock price:\nTo make sure we don't lose our visualization panel, click\nSave\nand give the\ndashboard a name of our choice.\n\n### Rules and conditions\n\nTo understand how alerts work, let's take a brief look at the concepts. The two\nmain components for setting up alerts are the alert\nRule\nand\nConditions\n.\nThe\nRule\nhas the following settings:\n- Nameto give the alert a descriptive title\n- Evaluate everyis how often the scheduler will evaluate the alert rule\n- forspecifies how long the query needs to violate the thresholds before\ntriggering alert notifications\nThe\nConditions\nsection has the following settings:\n- WHENsets an aggregate function on a series\n- OFis the query to alert on over a time range in the formatquery(query_name, from, until)\n- IS ...allows specifying a comparison to a value or range\nFor more information on the conditions of alerting, see the Grafana\nalert conditions documentation\n.\n\n### Create an alert\n\nThe example alert we are using in this tutorial will be triggered if the minimum\nvalue of the query named\n5-second Avg. of TSLA\nis below\n762\n, and we will set\nthe time range for alerting to the last thirty seconds. To set up this alert:\n- Edit the query and select theAlerttab\n- ClickCreate Alertand give it the nameTesla Stock Price alert\n- In theRulesection:SetEvaluate everyto10 secondsSetforto30 seconds\n- In theConditionssection:SetWHENtomin()SetOFtoquery(5-second Avg. of TSLA, 30s, now())ClickIS ABOVE, change this toIS BELOW, and set the value to762\n- Use the default values in theNo Data & Error Handlingsection\n- InNotifications -> Send to, choose theStock Price Alertsnotification\nchannel\n- Add an optional message to be delivered with the alert\nWhen the conditions of the alert are met, we will see Slack notifications\narriving in the channel that we have configured:\n\n## Summary\n\nIn this tutorial, we've shown how Grafana and QuestDB can be quite powerful for\ndata visualization and alerting. We learned how to ingest live ticker data from\na REST API into QuestDB and how to visualize this data in a Grafana dashboard\nand set up alerts based on predefined conditions.\nThe next steps for improvements to the Python scripts could be command-line\narguments for multiple stock symbols, and better error handling if markets are\nnot open. Feel free to\nsubmit a PR\nif you\nhave a suggestion or improvements to make!\nIf you like this content, we'd love to know your thoughts! Feel free to share\nyour feedback or just come and say hello in the\ncommunity forums\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1552,
    "metadata": {
      "relevance_score": 0.0,
      "priority_keywords_matched": []
    }
  }
]