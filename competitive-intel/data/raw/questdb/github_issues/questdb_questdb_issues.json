[
  {
    "id": "questdb-github_issue-3223697d8b0b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1631",
    "title": "Add all-contribs ðŸŽ‰ ",
    "text": "# Add all-contribs ðŸŽ‰ \n\nAdding the latest contributors! Thanks for making our community so great! ðŸ™ŒðŸ» \n\n## Top Comments\n\n**bsmth**: @all-contributors bot, please add @bziobrowski for code contributions\r\n\r\nThank you for helping us improve the project! ðŸŽ‰\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1632) to add @bziobrowski! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @Zapfmeister for code and user testing\r\n\r\nThank you for your feedback! \n\n---\n\n**bsmth**: @all-contributors bot, please add @mkaruza for code contributions\r\n\r\nThank you for your fixes and improvements! ðŸŽ‰\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1657) to add @mkaruza! :tada:\n\n---\n\n**bsmth**: \r\n@all-contributors bot, please add @DylanDKnight for user testing and bug reports\r\n\r\nThank you for your feedback! \n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1659) to add @DylanDKnight! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @eschultz for fixes\r\n\r\nThank you for your additions! \n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI couldn't determine any contributions to add, did you specify any contributions?\n          Please make sure to use [valid contribution names](https://allcontributors.org/docs/en/emoji-key).\n\n---\n\n**bsmth**: @all-contributors bot, please add @enolal826 for code additions on the UI\r\n\r\nThank you for your contributions! :canvas:\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 185,
    "metadata": {
      "issue_number": 1631,
      "state": "open",
      "labels": [],
      "comments_count": 65,
      "created_at": "2021-11-30T12:35:27Z",
      "updated_at": "2022-08-01T16:28:56Z",
      "closed_at": null,
      "author": "bsmth",
      "top_comments": [
        "**bsmth**: @all-contributors bot, please add @bziobrowski for code contributions\r\n\r\nThank you for helping us improve the project! ðŸŽ‰",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1632) to add @bziobrowski! :tada:",
        "**bsmth**: @all-contributors bot, please add @Zapfmeister for code and user testing\r\n\r\nThank you for your feedback! ",
        "**bsmth**: @all-contributors bot, please add @mkaruza for code contributions\r\n\r\nThank you for your fixes and improvements! ðŸŽ‰\r\n",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1657) to add @mkaruza! :tada:"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ef8ecd6f5f33",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/252",
    "title": "[ONGOING] Add contributors to the readme",
    "text": "# [ONGOING] Add contributors to the readme\n\nWe now integrate with [all-contributors](https://allcontributors.org/) :tada:.\r\n\r\nWe can start adding those who help us a lot to our readme. This is a way for us to recognize really valuable contributions. Those can be commits and code related but also testing, help on finding bugs, new issues, new ideas, documentation improvements, etc.\r\n\r\nYou can find the list of emojis [here](https://allcontributors.org/docs/en/emoji-key), and pick the right one depending on the type of contribution.\r\n\r\nIf you just started looking at QuestDB and want to contribute, please have a look at our [CONTRIBUTING.md](https://github.com/questdb/questdb/blob/master/CONTRIBUTING.md), there is many ways you can help!\n\n## Top Comments\n\n**mpsq**: @all-contributors bot, please add @sirinath for ideas\n\n---\n\n**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/253) to add @sirinath! :tada:\n\n---\n\n**mpsq**: @all-contributors bot, please add @tonytamwk for code and userTesting.\r\n[This](https://github.com/questdb/questdb/pull/193) was an amazing PR, thanks again!\n\n---\n\n**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/254) to add @tonytamwk! :tada:\n\n---\n\n**mpsq**: @all-contributors bot, please add @ideoma for code, userTesting, test.\n\n---\n\n**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/259) to add @ideoma! :tada:\n\n---\n\n**mpsq**: @all-contributors bot, please add @igor-suhorukov for code and ideas.\r\nThanks for you contributions and feedback, it is really appreciated :pray: \n\n---\n\n**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/319) to add @igor-suhorukov! :tada:\n\n---\n\n**mpsq**: @all-contributors bot, please add @mick2004 for code and platform.\r\nThanks a lot for your work Abhishek, and welcome to our community :tada:\n\n---\n\n**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/507) to add @mick2004! :tada:",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 253,
    "metadata": {
      "issue_number": 252,
      "state": "closed",
      "labels": [
        "Documentation"
      ],
      "comments_count": 38,
      "created_at": "2020-04-30T12:57:27Z",
      "updated_at": "2021-03-11T14:49:06Z",
      "closed_at": "2021-03-11T14:49:06Z",
      "author": "mpsq",
      "top_comments": [
        "**mpsq**: @all-contributors bot, please add @sirinath for ideas",
        "**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/253) to add @sirinath! :tada:",
        "**mpsq**: @all-contributors bot, please add @tonytamwk for code and userTesting.\r\n[This](https://github.com/questdb/questdb/pull/193) was an amazing PR, thanks again!",
        "**allcontributors[bot]**: @mpsq \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/254) to add @tonytamwk! :tada:",
        "**mpsq**: @all-contributors bot, please add @ideoma for code, userTesting, test."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-79db10f56d66",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1391",
    "title": "Adding latest contributors ðŸŽ‰ ",
    "text": "# Adding latest contributors ðŸŽ‰ \n\nAdding some of the most recent members of the community to recognize their great contributions!\n\n## Top Comments\n\n**bsmth**: @all-contributors bot, please add @jjsaunier for bug reports and feedback\r\n\r\nThank you for input which helps us inform the product roadmap!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1392) to add @jjsaunier! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @zanek for feedback and ideas\r\n\r\nThank you for giving feedback on project features!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1393) to add @zanek! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @Geekaylee for usertesting and ideas\r\n\r\nThank you for input which helps us inform the product roadmap!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1394) to add @Geekaylee! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @lg31415 for bug reports and feedback\r\n\r\nThank you for input which helps us improve the stability of the project!\r\n\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1395) to add @lg31415! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @null-dev for bug reports\r\n\r\nThank you for helping us improve the project!\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1396) to add @null-dev! :tada:",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 194,
    "metadata": {
      "issue_number": 1391,
      "state": "closed",
      "labels": [],
      "comments_count": 32,
      "created_at": "2021-10-05T13:30:10Z",
      "updated_at": "2021-10-05T16:56:41Z",
      "closed_at": "2021-10-05T16:56:41Z",
      "author": "bsmth",
      "top_comments": [
        "**bsmth**: @all-contributors bot, please add @jjsaunier for bug reports and feedback\r\n\r\nThank you for input which helps us inform the product roadmap!",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1392) to add @jjsaunier! :tada:",
        "**bsmth**: @all-contributors bot, please add @zanek for feedback and ideas\r\n\r\nThank you for giving feedback on project features!",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1393) to add @zanek! :tada:",
        "**bsmth**: @all-contributors bot, please add @Geekaylee for usertesting and ideas\r\n\r\nThank you for input which helps us inform the product roadmap!"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3113d2974096",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3727",
    "title": "About \"could not get table writer\"",
    "text": "# About \"could not get table writer\"\n\n### Describe the bug\n\nHello, I have a project where I get binance spot data from websocket and print it to questdb database. But after a certain time (like 90 million data) I can't insert with wire protocol. What could be the reason? Thank you in advance for your help.\r\n\r\nThis is the error in the error log:\r\n**2023-09-09T08:00:44.900294Z I i.q.c.p.WalWriterPool could not get, busy [table=`binance_spot~25`, thread=54, retries=5]\r\n2023-09-09T08:00:44.900300Z I i.q.c.l.t.LineTcpMeasurementScheduler could not get table writer [tableName=binance_spot, ex=`table busy [reason=unknown]`]**\n\n### To reproduce\n\n- Create table with **CREATE TABLE IF NOT EXISTS binance_spot(pair SYMBOL CAPACITY 2000 NOCACHE INDEX CAPACITY 512, interval SYMBOL CAPACITY 100 NOCACHE INDEX CAPACITY 512, open double, close double, high double, low double, volume double, time timestamp) timestamp(time) PARTITION BY DAY WAL DEDUPLICATE UPSERT KEYS(time, pair, interval)'**\r\n- Insert more than 90 million data\r\n\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.3.1\r\n- **OS**:Ubuntu 20.04(Containerd Docker)\n```\n\n\n### Additional context\n\n_No response_\n\n## Top Comments\n\n**puzpuzpuz**: Hi,\r\n\r\n> I can't insert with wire protocol\r\n\r\nWhat makes you think that you're not able to insert the data?\r\n\r\n> 2023-09-09T08:00:44.900300Z I i.q.c.l.t.LineTcpMeasurementScheduler could not get table writer [tableName=binance_spot, ex=table busy [reason=unknown]]\r\n\r\nThis is an info-level log message which simply means that one of the background (so-called, WAL apply) threads wasn't able to acquire the table writer. The reason for that is usually another background thread holding the writer. Later on that thread will release the writer making it available for other threads. This should not impact ingestion stability or performance.\r\n\r\n\n\n---\n\n**emustafasahin**: This error was shown to me after 100 million data and the DB started not writing anything. Even though I restarted in Docker\n\n---\n\n**puzpuzpuz**: Would you be able to share a reproducer for this scenario? Could you also check if your table gets suspended or not (see https://questdb.io/docs/reference/function/meta/#wal_tables)?\n\n---\n\n**emustafasahin**: Sorry for the delay. When I run the wal_tables() query, suspended returns false. If the situation repeats, I will check again in detail and tell you.\n\n---\n\n**puzpuzpuz**: Thanks. We'll be waiting for further updates on this issue.\n\n---\n\n**emustafasahin**: I solved the problem by focusing on the unavailability of WAL workers. I reduced the number of workers inserting into the database and this time the number of workers doing inserts decreased. I have achieved both a more performance and a more stable recording method. Thank you for your help. In addition, thanks to your construction of this beautiful database, our instant data recording situation has become more performant. Have a good day!\n\n---\n\n**puzpuzpuz**: Great news! I'm closing the issue for now. Feel free to write here or open another one if you face any problems.\n\n---\n\n**emustafasahin**: The problem started happening again and the **wal_tables()** output is like this. In addition, the Python QuestDb error \"Got error: Could not flush buffer: Broken pipe (os error 32) - See https://py-questdb-client.readthedocs.io/en/v1.1.0/troubleshooting.html#inspecting-and-debugging-errors#flush-failed\". What is the cause of this situation?\r\n\r\n<img width=\"1237\" alt=\"Screenshot 2023-09-19 at 11 29 11\" src=\"https://github.com/questdb/questdb/assets/52174044/d85f3fb6-a737-4a29-a2ff-06990022b573\">\r\n\n\n---\n\n**puzpuzpuz**: The transaction numbers (writerTxn and sequencerTxn) on your screenshot look fine: they're equal which means that everything that was written to WAL is now available for querying. The table is also not suspended which is also fine.\r\n\r\nCould you grep your server logs for any ` E ` (error message) or ` C ` (critical error message) lines and share the results?\n\n---\n\n**emustafasahin**: Sure.\r\n<img width=\"930\" alt=\"Screenshot 2023-09-19 at 11 55 02\" src=\"https://github.com/questdb/questdb/assets/52174044/7a6e77b6-0bcd-41b6-951c-0160a5756212\">\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 578,
    "metadata": {
      "issue_number": 3727,
      "state": "open",
      "labels": [
        "Question"
      ],
      "comments_count": 31,
      "created_at": "2023-09-09T08:07:52Z",
      "updated_at": "2023-09-21T13:17:12Z",
      "closed_at": null,
      "author": "emustafasahin",
      "top_comments": [
        "**puzpuzpuz**: Hi,\r\n\r\n> I can't insert with wire protocol\r\n\r\nWhat makes you think that you're not able to insert the data?\r\n\r\n> 2023-09-09T08:00:44.900300Z I i.q.c.l.t.LineTcpMeasurementScheduler could not get table writer [tableName=binance_spot, ex=table busy [reason=unknown]]\r\n\r\nThis is an info-level log message which simply means that one of the background (so-called, WAL apply) threads wasn't able to acquire the table writer. The reason for that is usually another background thread holding the writer. Later on that thread will release the writer making it available for other threads. This should not impact ingestion stability or performance.\r\n\r\n",
        "**emustafasahin**: This error was shown to me after 100 million data and the DB started not writing anything. Even though I restarted in Docker",
        "**puzpuzpuz**: Would you be able to share a reproducer for this scenario? Could you also check if your table gets suspended or not (see https://questdb.io/docs/reference/function/meta/#wal_tables)?",
        "**emustafasahin**: Sorry for the delay. When I run the wal_tables() query, suspended returns false. If the situation repeats, I will check again in detail and tell you.",
        "**puzpuzpuz**: Thanks. We'll be waiting for further updates on this issue."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-943fa8e97867",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2202",
    "title": "recent unsafe memory access operation in compiled Java code",
    "text": "# recent unsafe memory access operation in compiled Java code\n\n### Describe the bug\n\n\r\njava.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\r\nat io.questdb.cairo.SymbolMapWriter.lookupPutAndCache(SymbolMapWriter.java:267) ~[questdb-6.4-jdk8.jar:6.4-jdk8]\r\nat io.questdb.cairo.SymbolMapWriter.put(SymbolMapWriter.java:194) ~[questdb-6.4-jdk8.jar:6.4-jdk8]\r\nat io.questdb.cairo.SymbolMapWriter.put(SymbolMapWriter.java:179) ~[questdb-6.4-jdk8.jar:6.4-jdk8]\r\nat io.questdb.cairo.TableWriter$RowImpl.putSym(TableWriter.java:5459) ~[questdb-6.4-jdk8.jar:6.4-jdk8]\r\n\r\nWhen I insert with batch insert(2000 rows each commit)with 3 threads ,after insert about 850000 rows,there is an error ,then exit.\r\n\r\nquestdb 6.4\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**:6.4\r\n- **OS**: OS:Bsduname:Darwin 19.6.0 Darwin Kernel Version 19.6.0:\r\n- **Browser**:chrome\n```\n\n\n### Additional context\n\n_No response_\n\n## Top Comments\n\n**bluestreak01**: this is embedded use-case I am assuming ? Could you share the inserting code ?\n\n---\n\n**cumtqiangqiang**: > this is embedded use-case I am assuming ? Could you share the inserting code ?\r\n\r\nThanks for your quick replay. yes ,this is embedded use-case. like this.\r\n\r\npublic synchronized void insertBatch(final List<Data> datas) {\r\n\r\n       \r\n        final Iterator<Data> batchIt = batches.iterator();\r\n        try (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(),\r\n                tableName,\r\n                Thread.currentThread().getName())) {\r\n            while (batchIt.hasNext()) {\r\n                final Data data = batchIt.next();\r\n                TableWriter.Row row = writer.newRow(data.getQuestDbMicros());\r\n               \r\n                row.putSym(0, value1);\r\n                row.putStr(1, value2);\r\n                row.putSym(2, value3);\r\n                row.putLong(3, value4);\r\n                row.append();\r\n               \r\n                \r\n            }\r\n            writer.commit();\r\n        }\r\n\r\n    }\r\n\r\n\n\n---\n\n**bluestreak01**: cool, what is table structure ? Reason I ask is that `row.put*` calls do not do any checked as to value vs column types. It is hugely important not to insert wrong value into columns. \n\n---\n\n**cumtqiangqiang**: > cool, what is table structure ? Reason I ask is that `row.put*` calls do not do any checked as to value vs column types. It is hugely important not to insert wrong value into columns.\r\n\r\nThe table structure type is the same with the value type. If it is different ,I think it will not insert any data,but it insert about 850000 rows. And I changed one of Symbol type to string type, then it can run normal for long time, is there any issue about symbol type ? . I'll keep watching my program running status.\n\n---\n\n**bluestreak01**: which column index is timestamp ?\n\n---\n\n**cumtqiangqiang**: > which column index is timestamp ?\r\n\r\ncreate table if not exists table_test (time timestamp,\r\nhost symbol,\r\ncluster symbol,\r\ntesk symbol,\r\ntimeMs long,\r\nexchTimeMs long,\r\norder string,\r\nidText string,\r\nccy string) timestamp(time) PARTITION BY DAY;    It is the sql.\n\n---\n\n**bluestreak01**: when first column is the timestamp you should populate columns starting with 1, e.g. \r\n\r\n```java\r\nfinal Iterator<Data> batchIt = batches.iterator();\r\n    try (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(),\r\n            tableName,\r\n            Thread.currentThread().getName())) {\r\n        while (batchIt.hasNext()) {\r\n            final Data data = batchIt.next();\r\n            TableWriter.Row row = writer.newRow(data.getQuestDbMicros());\r\n           \r\n            row.putSym(1, value1);\r\n            row.putStr(2, value2);\r\n            row.putSym(3, value3);\r\n            row.putLong(4, value4);\r\n            row.append();\r\n           \r\n            \r\n        }\r\n        writer.commit();\r\n    }\r\n```\r\n\r\ncolumn types have to match `put*()` calls exactly, e.g. you cannot `r.putStr(x, \"abc\")` into `SYMBOL` column\n\n---\n\n**cumtqiangqiang**: > \r\n\r\n\r\n\r\n> \r\nYes , I do, I create column index use ++i,\r\nint i = 0;\r\nrow.putSym(++i, this.hostName);\r\nrow.putSym(++i, this.cluster);\r\nrow.putSym(++i, this.tesk);\n\n---\n\n**cumtqiangqiang**: > which column index is timestamp ?\r\n\r\nYes , I do, I create column index use ++i,\r\nint i = 0;\r\nrow.putSym(++i, this.hostName);\r\nrow.putSym(++i, this.cluster);\r\nrow.putSym(++i, this.tesk);\n\n---\n\n**bluestreak01**: oh, I should have read this more intently:\r\n>When I insert with batch insert(2000 rows each commit)with 3 threads ,after insert about 850000 rows,there is an error ,then exit.\r\n\r\nCould you elaborate on 3 threads ? These threads insert into the same table or several ?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 548,
    "metadata": {
      "issue_number": 2202,
      "state": "open",
      "labels": [
        "Question"
      ],
      "comments_count": 31,
      "created_at": "2022-06-08T13:58:12Z",
      "updated_at": "2022-06-17T14:02:18Z",
      "closed_at": null,
      "author": "cumtqiangqiang",
      "top_comments": [
        "**bluestreak01**: this is embedded use-case I am assuming ? Could you share the inserting code ?",
        "**cumtqiangqiang**: > this is embedded use-case I am assuming ? Could you share the inserting code ?\r\n\r\nThanks for your quick replay. yes ,this is embedded use-case. like this.\r\n\r\npublic synchronized void insertBatch(final List<Data> datas) {\r\n\r\n       \r\n        final Iterator<Data> batchIt = batches.iterator();\r\n        try (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(),\r\n                tableName,\r\n                Thread.currentThread().getName())) {\r\n            while (batchIt.hasNext()) {\r\n                final Data data = batchIt.next();\r\n                TableWriter.Row row = writer.newRow(data.getQuestDbMicros());\r\n               \r\n                row.putSym(0, value1);\r\n                row.putStr(1, value2);\r\n                row.putSym(2, value3);\r\n                row.putLong(3, value4);\r\n                row.append();\r\n               \r\n                \r\n            }\r\n            writer.commit();\r\n        }\r\n\r\n    }\r\n\r\n",
        "**bluestreak01**: cool, what is table structure ? Reason I ask is that `row.put*` calls do not do any checked as to value vs column types. It is hugely important not to insert wrong value into columns. ",
        "**cumtqiangqiang**: > cool, what is table structure ? Reason I ask is that `row.put*` calls do not do any checked as to value vs column types. It is hugely important not to insert wrong value into columns.\r\n\r\nThe table structure type is the same with the value type. If it is different ,I think it will not insert any data,but it insert about 850000 rows. And I changed one of Symbol type to string type, then it can run normal for long time, is there any issue about symbol type ? . I'll keep watching my program running status.",
        "**bluestreak01**: which column index is timestamp ?"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1723eacdf8ee",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1680",
    "title": "BETWEEN statement fills up memory",
    "text": "# BETWEEN statement fills up memory\n\n### Describe the bug\n\nWhen running queries with `WHERE timestamp BETWEEN 'TIMESTAMP A' AND 'TIMESTAMP B'` memory consumption spikes and does not return to normal until a restart was performed.\r\n\r\nI am using grafana with dashboard variables which perform some queries on dashboard load. \r\n\r\ne.g. `SELECT DISTINCT ID FROM 'testTable'  WHERE $__timeFilter(timestamp)`\r\n\r\nGrafana replaces the timeFilter with the `WHERE timestamp BETWEEN X Y` statement and in my case, it uses the current timestamp and the timestamp from one hour ago which results in the query always being different. I guess that some kind of caching leads to this issue. \n\n### To reproduce\n\n1. Create table as below\r\n2. Insert some values\r\n3. Run a few queries with different `BETWEEN` timestamps\r\n4. Watch the memory allocation going up \n\n### Expected Behavior\n\nMemory should not spike that much and should at least be freed again.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.1.2\r\n- **OS**: Ubuntu 20.04\n```\n\n\n### Additional context\n\nMemory consumption of QuestDB \r\n\r\n![Bildschirmfoto 2021-12-14 um 03 18 53](https://user-images.githubusercontent.com/27853515/145921082-b24b52ce-9279-4bcb-9748-b62a0b620f57.png)\r\n\r\nMy table design:\r\n\r\n```\r\nCREATE TABLE test(\r\n    ID SYMBOL capacity 1048576 index,\r\n    valueA DOUBLE,\r\n    valueB DOUBLE,\r\n    valueC DOUBLE,\r\n    ts TIMESTAMP\r\n) timestamp(ts);\r\n```\n\n## Top Comments\n\n**bluestreak01**: We test all of our SQL implementations for memory leaks, so memory leak is unlikely.\r\n\r\nOn other hand grafana plugin doesn't use bind variables, which makes questdb query caching inefficient. To reduce memory footprint I would suggest reducing cache size:\r\n\r\n```\r\ncairo.cache.rows=1\r\ncairo.cache.blocks=1\r\n``` \n\n---\n\n**mariusziemke**: Thanks for responding so quickly! Just tried your settings, but it still fills up the memory the same way.. \r\n\r\nI played a bit more with the queries and it looks like query caching in general is using up my memory. So it is not specific to `BETWEEN`, but rather to unique queries as we are doing some dynamic regex matching too.\r\n\r\nCould there be a relation to symbol caching? I've recreated the tables with disabled symbol caching and now I am waiting for a few million rows to be ingested.\r\n\r\nI am still a bit confused why the memory is not freed up after some time - is this expected?\r\n\n\n---\n\n**mariusziemke**: Disabling Symbol caching does not seem to be related. However rerunning the same query multiple times frees up a bit of resources, but i am still consuming Gigabytes of memory after just a few queries.\n\n---\n\n**ideoma**: This is memory mapped, it can be shown like process memory but in fact it is not physical RAM really. When table is queried it's data is mapped to memory and all gauges jump but and then OS reads into RAM from disk page by page lazily when the data is accessed. It also can map same physical RAM into multiple logical addresses expanding process virtual memory usage but not physical.\r\n\r\nAt the end, when OS will need to free up physical RAM it will discard unchanged mapped pages. You will see that QuestDB uses multiple times you physical RAM on queries (even on systems without swap) but it's virtual mapped memory, not physical.\r\n\r\nIf you want the chart to look better you can reduce worker pool sizes trading off response times\r\nhttps://questdb.io/docs/reference/configuration/#shared-worker but I'd not change it unless you see out of memory problems\r\n\r\n\n\n---\n\n**mariusziemke**: That makes sense.. I am just curious why the machine got unresponsive after the gauges jumped. It looked like the system memory was completely filled up, but this shouldn't be a problem if I understand the memory map process correctly.\r\n\r\nI will try to recreate the situation on a VPS and come back to you :-) Thanks for the great support so far!\n\n---\n\n**ideoma**: Unresponsive is a problem. It would be good if you dump thread stacks if it happens with `jstack` or a similar tool for java \r\n\r\nhttps://www.baeldung.com/java-thread-dump\n\n---\n\n**puzpuzpuz**: What metric do you use to measure memory consumption? Virtual memory size, or RSS (Resident Set Size), or something else? If former, you should be using RSS since it stands for physically resident memory, i.e. occupying space in the machine's RAM.\n\n---\n\n**puzpuzpuz**: > Unresponsive is a problem. It would be good if you dump thread stacks if it happens with `jstack` or a similar tool for java\r\n> \r\n> https://www.baeldung.com/java-thread-dump\r\n\r\nIf possible, I'd also recommend running the reproducer query and profiling QuestDB for several seconds (at least, 5-10). This might help us to understand the hot spot.\r\nhttps://github.com/jvm-profiling-tools/async-profiler\n\n---\n\n**mariusziemke**: Sorry for responding that late, I finally had some time to come back to this issue.. \r\n\r\nSome things that could be important:\r\n- I did a heap dump and its only around ~1GB in total size with 950mb used by `DateLocale`. Don't know if there is another issue because almost 1GB for timezone stuff seems a bit much to me.\r\n- When querying with the HTTP API through /exec the [cairo caching limits](https://github.com/questdb/questdb/issues/1680#issuecomment-993341288) seem to work. At least the RSS only fills up to ~5 GB. Limits do not apply for a postgres client and the corresponding configuration for `pg factory cache` doesn't do anything noticeable regarding the RSS.\r\n- I also created a thread dump but it is not really interesting for my eyes (not a Java developer..)\r\n- The RSS also fills up if I query an empty table. The only important thing is that I use a pg-client and some kind of varying condition (e.g. WHERE timestamp BETWEEN X AND Y)\r\n\r\nI probably forgot some things.. If you need the raw dumps or memory + cpu profile we should maybe switch to slack :-)\r\n\r\nThank you in advance.. I will try to do some more research in the next few days.\n\n---\n\n**timscchao**: Hi,\r\n\r\nI got similar issue on questDB with Grafana.\r\nThe java process's VSZ & RSS keep increasing when I reload the dashboard.\r\n(no any new data be insert when test the query)\r\nAnd finaly it cause the system oom-kill.\r\n\r\nHere is the 'ps' output\r\nVSZ          RSS\r\n7424680   231468 (start up)\r\n9233040   1844888 (1st reflash)\r\n10412944   2894080 (2nd reflash)\r\n11482232   3845540 (3rd reflash)\r\n\r\nMy Grafa panel is a repeat panel with ~20 $beacon.\r\nField 'stype' is symbol, and 'value' is a double column.\r\n```\r\nSELECT stype, value, timestamp FROM readings\r\nLATEST BY tag, stype where\r\n  $__timeFilter(timestamp) and\r\n  tag=$beacon\r\n```\r\n\r\nMy Env:\r\n- Ubuntu 20.04.3\r\n- QuestDB 6.1.3 on Docker\r\n- Grafana 8.3.3 on Docker\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 1055,
    "metadata": {
      "issue_number": 1680,
      "state": "closed",
      "labels": [],
      "comments_count": 28,
      "created_at": "2021-12-14T02:28:28Z",
      "updated_at": "2022-02-04T12:07:08Z",
      "closed_at": "2022-02-04T12:07:08Z",
      "author": "mariusziemke",
      "top_comments": [
        "**bluestreak01**: We test all of our SQL implementations for memory leaks, so memory leak is unlikely.\r\n\r\nOn other hand grafana plugin doesn't use bind variables, which makes questdb query caching inefficient. To reduce memory footprint I would suggest reducing cache size:\r\n\r\n```\r\ncairo.cache.rows=1\r\ncairo.cache.blocks=1\r\n``` ",
        "**mariusziemke**: Thanks for responding so quickly! Just tried your settings, but it still fills up the memory the same way.. \r\n\r\nI played a bit more with the queries and it looks like query caching in general is using up my memory. So it is not specific to `BETWEEN`, but rather to unique queries as we are doing some dynamic regex matching too.\r\n\r\nCould there be a relation to symbol caching? I've recreated the tables with disabled symbol caching and now I am waiting for a few million rows to be ingested.\r\n\r\nI am still a bit confused why the memory is not freed up after some time - is this expected?\r\n",
        "**mariusziemke**: Disabling Symbol caching does not seem to be related. However rerunning the same query multiple times frees up a bit of resources, but i am still consuming Gigabytes of memory after just a few queries.",
        "**ideoma**: This is memory mapped, it can be shown like process memory but in fact it is not physical RAM really. When table is queried it's data is mapped to memory and all gauges jump but and then OS reads into RAM from disk page by page lazily when the data is accessed. It also can map same physical RAM into multiple logical addresses expanding process virtual memory usage but not physical.\r\n\r\nAt the end, when OS will need to free up physical RAM it will discard unchanged mapped pages. You will see that QuestDB uses multiple times you physical RAM on queries (even on systems without swap) but it's virtual mapped memory, not physical.\r\n\r\nIf you want the chart to look better you can reduce worker pool sizes trading off response times\r\nhttps://questdb.io/docs/reference/configuration/#shared-worker but I'd not change it unless you see out of memory problems\r\n\r\n",
        "**mariusziemke**: That makes sense.. I am just curious why the machine got unresponsive after the gauges jumped. It looked like the system memory was completely filled up, but this shouldn't be a problem if I understand the memory map process correctly.\r\n\r\nI will try to recreate the situation on a VPS and come back to you :-) Thanks for the great support so far!"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-690063b884a2",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5812",
    "title": "function.cursorClosed() not called consistently",
    "text": "# function.cursorClosed() not called consistently\n\n### To reproduce\n\nThe `function.cursorClosed()` lifecycle method, introduced in [#4633](https://github.com/questdb/questdb/pull/4633), is intended to notify a function that its associated cursor has been closed.\n\n## The Problem\nCurrently, this notification is only triggered for virtual record cursors. This implementation is incomplete because other cursor types, such as those used in GROUP BY operations, do not call it.\n\nFor example, `JsonExtractFunction.cursorClosed()` is never invoked in the `ParallelGroupByFuzzTest#testParallelJsonKeyGroupBy()` test.\n\n## Impact\nThis inconsistency leads to resource leaks. The `json_extract()` function, for instance, relies on this method to release a 1 MB memory buffer. When a cursor using this function is closed without the notification, the memory is not freed. This issue is amplified in parallel GROUP BY scenarios where the function and its memory buffer are cloned. The buffer is free-d only after the whole cursor factory is closed - that's why it was not caught by memory leak guards. \n\n### QuestDB version:\n8.3.3\n\n## Top Comments\n\n**itsaxat7479**:  Potential Solution Proposal\nThanks for reporting this important issue. A potential solution to ensure cursorClosed() is consistently invoked across all cursor typesâ€”including those used in GROUP BY operationsâ€”could be:\n\nCentralize cursorClosed() Invocation:\n\nIntroduce a unified cursor wrapper or decorator pattern around all cursor types.\n\nThis wrapper should ensure that cursorClosed() is always called in the close() method, regardless of the cursor's origin (virtual or real).\n\nUpdate GroupByRecordCursorFactory:\n\nModify factories like ParallelGroupByRecordCursorFactory to explicitly call cursorClosed() on all associated function instances during shutdown or cleanup phases.\n\nEnhance Test Coverage:\n\nAdd a test case (e.g. testCursorClosedIsCalledForAllCursors()) that uses mock functions with logging in cursorClosed() to assert they are properly notified.\n\nThis can catch regressions early.\n\nConsider Buffer Reference Counting:\n\nIf multiple cloned cursors share a buffer, implement a reference count mechanism to ensure the buffer is freed only after all cursors are closed and cursorClosed() has been triggered.\n\nThis would not only fix memory leaks in json_extract() but also make the lifecycle management more robust and extensible for future function types.\n\n\n\n---\n\n**Ritikdhanotiya07**: //1. Utility Function to Notify All Functions\njava\nCopy\nEdit\npublic class FunctionUtils {\n    public static void notifyFunctionsCursorClosed(List<? extends Function> functions) {\n        for (Function function : functions) {\n            function.cursorClosed();\n        }\n    }\n}\n//2. Modify GroupByRecordCursorFactory.java\njava\nCopy\nEdit\n@Override\npublic void close() {\n    super.close(); // existing close logic\n    FunctionUtils.notifyFunctionsCursorClosed(this.functions); // new line\n}\n3. Modify GroupByRecordCursor.java (if separate)\njava\nCopy\nEdit\n@Override\npublic void close() {\n    super.close();\n    FunctionUtils.notifyFunctionsCursorClosed(this.functions); // call close\n}\n 4. Modify ParallelGroupByCursorFactory.java\n@Override\npublic void close() {\n    super.close(); // existing shutdown logic\n    for (List<Function> fnSet : functionInstancesPerShard) {\n        FunctionUtils.notifyFunctionsCursorClosed(fnSet);\n    }\n}\n5. Update JsonExtractFunction.java\n public class JsonExtractFunction extends AbstractFunction {\n    private long buffer = 0;\n    private static final int BUFFER_SIZE = 1024 * 1024;\n\n    public JsonExtractFunction(...) {\n        this.buffer = Unsafe.malloc(BUFFER_SIZE);\n    }\n\n    @Override\n    public void cursorClosed() {\n        if (buffer != 0) {\n            Unsafe.free(buffer, BUFFER_SIZE);\n            buffer = 0;\n        }\n    }\n}\n6. Add Validation in ParallelGroupByFuzzTest.java\n @Test\npublic void testParallelJsonKeyGroupBy() {\n    // setup test\n    AtomicBoolean cursorClosedCalled = new AtomicBoolean(false);\n\n    JsonExtractFunction testFunc = new JsonExtractFunction() {\n        @Override\n        public void cursorClosed() {\n            cursorClosedCalled.set(true);\n            super.cursorClosed();\n        }\n    };\n\n    // execute group by using testFunc...\n    \n    assertTrue(\"Function.cursorClosed was not called!\", cursorClosedCalled.get());\n}\n\n\n7. \n\n---\n\n**Techo-Minds**: Proposed Solution\n1. Extend Base Cursor Implementations\nUpdate all key RecordCursor implementations that internally use Function objects to call cursorClosed() on them in their close() method.\n\nFor example:\n\nðŸ”§ In GroupByRecordCursor.java:\n@Override\npublic void close() {\n    try {\n        // Close internal cursors as usual\n        if (baseCursor != null) {\n            baseCursor.close();\n        }\n    } finally {\n        // Add this block\n        for (int i = 0; i < recordFunctions.size(); i++) {\n            Function function = recordFunctions.getQuick(i);\n            if (function != null) {\n                function.cursorClosed();\n            }\n        }\n    }\n}\nâš ï¸ Important: This must be done recursively for any cursor that wraps or delegates to other cursors.\n\n2. Add Utility Method for Function Cleanup\nTo avoid duplicating the function-closing logic across cursor types, consider a utility method like:\n\npublic static void closeFunctionsQuietly(ObjList<Function> functions) {\n    for (int i = 0, n = functions.size(); i < n; i++) {\n        Function f = functions.getQuick(i);\n        if (f != null) {\n            try {\n                f.cursorClosed();\n            } catch (Throwable ignore) {\n                // Optionally log this\n            }\n        }\n    }\n}\nUse it in close() methods for all cursor classes that manage Function lists.\n\n3. Ensure Cursor Factories Pass Function Lists\nIn cursor factories (e.g., GroupByCursorFactory), make sure the constructed cursors are given access to the Function objects they are supposed to manage/clean.\n\n4. Add Regression Tests\nCreate a test case that:\n\nUses a Function (e.g., JsonExtractFunction) that allocates memory.\n\nVerifies that cursorClosed() was invoked.\n\nCan simulate partial cursor closure (e.g., early exit from parallel/grouped processing).\n\n@Test\npublic void testCursorClosedCalledInGroupBy() {\n    // Create table and data\n    // Run a GROUP BY query using json_extract()\n    // Track cursorClosed() invocation via logging or counter\n\n    // Assert that cursorClosed() is called for each function\n}\n5. Memory Leak Watchdog (Optional but Useful)\nIntroduce test utility that:\n\nTracks memory allocation before/after query\n\nAsserts that no leak occurred\n\nWorks especially for parallel group-by with cloning\n\nâœ… Outcome\nAll Function implementations, including memory-heavy ones like JsonExtractFunction, will have cursorClosed() invoked at the correct point.\n\nPrevents silent memory leaks during parallel or nested query operations.\n\nImproves overall resource hygiene and avoids surprises in long-running or high-concurrency environments.\n\n---\n\n**loordjay**: \nAs a result, functions used in other cursor typesâ€”most notably those involved in GROUP BY operationsâ€”do not receive this notification, leading to resource leaks. \n\n Steps to Reproduce\nRun the test:\nParallelGroupByFuzzTest#testParallelJsonKeyGroupBy()\n\nObserve that JsonExtractFunction.cursorClosed() is not invoked.\n\nâš ï¸ Actual Behavior\ncursorClosed() is not called for cursors associated with GROUP BY operations.\n\nThe 1MB memory buffer allocated by json_extract() is not released at the function level.\n\nThe buffer is only freed when the entire cursor factory is closed, masking the issue in leak detection tools.\n\nâœ… Expected Behavior\nFunction.cursorClosed() should be invoked consistently across all cursor types, not just virtual record cursors.\n\nResources (e.g., memory buffers) managed by the function should be released as soon as the associated cursor is closed.\n\nðŸ“ˆ Impact\nMemory buffers allocated in json_extract() are not promptly released.\n\nIn parallel GROUP BY scenarios, functions (and their buffers) are cloned, amplifying memory usage.\n\nThis leads to hidden memory leaks in long-running or concurrent workloads.\n\nResource cleanup becomes dependent on full factory shutdown rather than clean function-level lifecycle management.\n\nðŸ› ï¸ Suggested Fix\nUpdate the lifecycle management across all cursor types to ensure cursorClosed() is invoked consistently. This includes:\n\nGROUP BY cursors\n\nParallel and concurrent cursor wrappers\n\nAny internal cursor abstractions not currently supporting this callback\n\nProposed Solution\nEnsure that all cursor implementationsâ€”including those used in:\n\nGROUP BY operations\n\nParallel/grouped cursor wrappers\n\nComposite cursor factories\n\nâ€¦explicitly invoke Function.cursorClosed() as part of their teardown logic.\n\nThis will enforce a uniform lifecycle contract and enable reliable, deterministic resource management. \n\nfor (Function f : functions) {\nif (f instanceof LifecycleAwareFunction) {\n((LifecycleAwareFunction) f).cursorClosed();\n}\n}\n\nImpact\nMemory Leaks:\nThe json_extract() function allocates a 1 MB buffer per instance. Without cursorClosed() being called, this buffer is not released.\n\nParallel Amplification:\nIn parallel GROUP BY queries, the function (and its buffer) is cloned multiple times. As a result, memory usage grows rapidly.\n\nDelayed Cleanup:\nSince cursorClosed() is not invoked, the buffer is only released when the entire cursor factory is closed. This delayed release bypasses memory leak guards and obscures the problem\n\n@Override\npublic void close() {\n    super.close();\n    for (Function f : groupByFunctions) {\n        f.cursorClosed();\n    }\n    for (Function f : aggregationFunctions) {\n        f.cursorClosed();\n    }\n}\n\n\n\n\n\n\n---\n\n**Divya857571**: for (Function f : functions) {\n    if (f instanceof LifecycleAwareFunction) {\n        ((LifecycleAwareFunction) f).cursorClosed();\n    }\n}\n\n\n---\n\n**Kishan0717**: You're describing a situation where the cursorClosed() method (e.g., in JsonExtractFunction) is not being called for certain cursor types, specifically during GROUP BY operations. This indicates a lifecycle management issue in the system handling different types of cursors.\n\nProblem Summary:\nThe notification mechanism (likely a callback like cursorClosed()) is only triggered for virtual record cursors.\n\nOther cursor types, such as those used in GROUP BY, do not trigger this notification.\n\nThis leads to incomplete resource cleanup or state finalization, particularly in tests like ParallelGroupByFuzzTest#testParallelJsonKeyGroupBy().\n\nRoot Cause:\nIn many query engines or database internals (e.g., QuestDB or similar systems), cursors can be of various types (e.g., filter, join, group-by, virtual, etc.). Each type may be implemented differently, and unless they all properly delegate to a common finalization or resource release mechanism, some types may skip essential cleanup methods like cursorClosed().\n\nThe fact that JsonExtractFunction.cursorClosed() is not called during GROUP BY suggests that:\n\nThe cursor used in GROUP BY operations does not call Function.cursorClosed() on all functions used in the query.\n\nThe system might only have that logic in the VirtualRecordCursorFactory or similar components.\n\nSuggested Fix:\nYouâ€™ll likely need to ensure that cursorClosed() is consistently called for all Function instances, regardless of cursor type.\n\nPossible Fix Locations:\nGroupByRecordCursor or GroupByFunctionCursor:\n\nLook for the class that implements the cursor used in group-by operations.\n\nEnsure it overrides and implements the close() method properly.\n\nIn close(), iterate over all GroupByFunction or Function instances and call cursorClosed() if implemented.\n\nCommon Interface or Cursor Framework:\n\nIf you have a base cursor class/interface (RecordCursor, AbstractCursor), make sure that close() or a similar lifecycle method calls cursorClosed() on all registered functions.\n\nTest Enhancements:\n\nEnhance ParallelGroupByFuzzTest to assert that cursorClosed() is called for each function (perhaps with a mock or flag).\n\nThis helps guard against regression.\n\nExample Fix (Pseudo-Code):\njava\nCopy\nEdit\n@Override\npublic void close() {\n    try {\n        for (Function f : functions) {\n            f.cursorClosed();\n        }\n    } finally {\n        super.close();\n    }\n}\nSummary:\nTo fix this:\n\nIdentify all cursor implementations used in queries.\n\nEnsure cursorClosed() is called consistently for all function types, including in GROUP BY.\n\nConsider refactoring common logic to a shared superclass if appropriate.\n\nUpdate tests to verify cursorClosed() is called for all involved Function instances.\n\nIf youâ€™d like, you can share relevant cursor and function class code, and Iâ€™ll help write a concrete patch.\n\n---\n\n**mitchodvadiya**: > ### To reproduce\n> The `function.cursorClosed()` lifecycle method, introduced in [#4633](https://github.com/questdb/questdb/pull/4633), is intended to notify a function that its associated cursor has been closed.\n> \n> ## The Problem\n> Currently, this notification is only triggered for virtual record cursors. This implementation is incomplete because other cursor types, such as those used in GROUP BY operations, do not call it.\n> \n> For example, `JsonExtractFunction.cursorClosed()` is never invoked in the `ParallelGroupByFuzzTest#testParallelJsonKeyGroupBy()` test.\n> \n> ## Impact\n> This inconsistency leads to resource leaks. The `json_extract()` function, for instance, relies on this method to release a 1 MB memory buffer. When a cursor using this function is closed without the notification, the memory is not freed. This issue is amplified in parallel GROUP BY scenarios where the function and its memory buffer are cloned. The buffer is free-d only after the whole cursor factory is closed - that's why it was not caught by memory leak guards.\n> \n> ### QuestDB version:\n> 8.3.3\n\n@Override\npublic void close() {\n    super.close();\n    for (Function f : groupByFunctions) {\n        f.cursorClosed();\n    }\n    for (Function f : aggregationFunctions) {\n        f.cursorClosed();\n    }\n}\n\n\n---\n\n**PrshntP**: The root issue in #5812 is that `Function.cursorClosed()` isnâ€™t consistently invoked across all cursor typesâ€”specifically in `GroupBy` and other non-virtual cursorsâ€”causing resource leaks (e.g., `json_extract()` buffers arenâ€™t freed until much later).\n\n---\n\n##  Proposed Solution\n\nEnsure that every cursor implementation (including those used in GROUP BY, parallel queries, etc.) invokes `cursorClosed()` when itâ€™s closed. A centralized, minimal fix in the cursor factory base could look like this:\n\n```java\n// Example in AbstractRecordCursorFactory.close()\n@Override\npublic void close() {\n    try {\n        if (cursor != null) {\n            Function[] functions = recordCursor.getFunctions();\n            for (Function fn : functions) {\n                fn.cursorClosed(cursor);\n            }\n            cursor.close();\n        }\n    } finally {\n        super.close();\n    }\n}\n```\n\nThis ensures:\n\n* Any resource held by a function is released immediately when the cursor is closed.\n* No need to rely on subclasses remembering to call itâ€”covers both virtual and non-virtual record cursors.\n\n---\n\n##  Checklist for Fix\n\n1. **Identify all cursor types** across the codebase (SQL, GroupBy, ParallelGroupBy, Join, etc.).\n2. **Inject invocation** of `cursorClosed()` during their `close()` lifecycle.\n3. **Add tests**, like enhancing `ParallelGroupByFuzzTest`, to assert no memory leaks post-cursor-close.\n4. **Validate memory cleanup** using existing leak detection guards (these were already in place but weren't catching due to delayed cleanup).\n\n---\n\n###  Why this works:\n\n* Centralizing in the base ensures consistent cleanup across types.\n* Direct invocation makes `json_extract()` and similar functions free memory promptly.\n* Tests will then catch leaks early during cursor finalization, not deferred factory closure.\n\n---\n\n\n---\n\n**mitchodvadiya**: Ok\r\n\r\nOn Sun, 6 Jul 2025, 4:34â€¯pm Prashant Paudel, ***@***.***>\r\nwrote:\r\n\r\n> *PrshntP* left a comment (questdb/questdb#5812)\r\n> <https://github.com/questdb/questdb/issues/5812#issuecomment-3041321584>\r\n>\r\n> The root issue in #5812 <https://github.com/questdb/questdb/issues/5812>\r\n> is that Function.cursorClosed() isnâ€™t consistently invoked across all\r\n> cursor typesâ€”specifically in GroupBy and other non-virtual\r\n> cursorsâ€”causing resource leaks (e.g., json_extract() buffers arenâ€™t freed\r\n> until much later).\r\n> ------------------------------\r\n> Proposed Solution\r\n>\r\n> Ensure that every cursor implementation (including those used in GROUP BY,\r\n> parallel queries, etc.) invokes cursorClosed() when itâ€™s closed. A\r\n> centralized, minimal fix in the cursor factory base could look like this:\r\n>\r\n> // Example in ***@***.*** void close() {\r\n>     try {\r\n>         if (cursor != null) {\r\n>             Function[] functions = recordCursor.getFunctions();\r\n>             for (Function fn : functions) {\r\n>                 fn.cursorClosed(cursor);\r\n>             }\r\n>             cursor.close();\r\n>         }\r\n>     } finally {\r\n>         super.close();\r\n>     }\r\n> }\r\n>\r\n> This ensures:\r\n>\r\n>    - Any resource held by a function is released immediately when the\r\n>    cursor is closed.\r\n>    - No need to rely on subclasses remembering to call itâ€”covers both\r\n>    virtual and non-virtual record cursors.\r\n>\r\n> ------------------------------\r\n> Checklist for Fix\r\n>\r\n>    1. *Identify all cursor types* across the codebase (SQL, GroupBy,\r\n>    ParallelGroupBy, Join, etc.).\r\n>    2. *Inject invocation* of cursorClosed() during their close()\r\n>    lifecycle.\r\n>    3. *Add tests*, like enhancing ParallelGroupByFuzzTest, to assert no\r\n>    memory leaks post-cursor-close.\r\n>    4. *Validate memory cleanup* using existing leak detection guards\r\n>    (these were already in place but weren't catching due to delayed cleanup).\r\n>\r\n> ------------------------------\r\n> Why this works:\r\n>\r\n>    - Centralizing in the base ensures consistent cleanup across types.\r\n>    - Direct invocation makes json_extract() and similar functions free\r\n>    memory promptly.\r\n>    - Tests will then catch leaks early during cursor finalization, not\r\n>    deferred factory closure.\r\n>\r\n> ------------------------------\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/questdb/questdb/issues/5812#issuecomment-3041321584>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BS4JOOPW2RCCZ2RJ3RCBI7D3HD7EHAVCNFSM6AAAAACAZVXQ2CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTANBRGMZDCNJYGQ>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n\n\n---\n\n**PARTHPATEL-1623**: import java.util.List;\nimport java.util.Arrays;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class GroupByCursorDemo {\n\n    // ============ Function interface & JsonExtractFunction =============\n\n    interface Function {\n        void cursorClosed();\n    }\n\n    static class JsonExtractFunction implements Function {\n        private byte[] buffer;\n        private static final AtomicInteger buffersAllocated = new AtomicInteger(0);\n\n        public JsonExtractFunction() {\n            buffer = new byte[1024 * 1024]; // 1 MB buffer\n            buffersAllocated.incrementAndGet();\n            System.out.println(\"Allocated buffer. Total buffers: \" + buffersAllocated.get());\n        }\n\n        @Override\n        public void cursorClosed() {\n            if (buffer != null) {\n                buffer = null;\n                buffersAllocated.decrementAndGet();\n                System.out.println(\"Freed buffer. Total buffers: \" + buffersAllocated.get());\n            }\n        }\n\n        public static int getAllocatedBuffers() {\n            return buffersAllocated.get();\n        }\n    }\n\n    // ============== RecordCursor & Factory ==================\n\n    interface RecordCursor {\n        boolean hasNext();\n        void close();\n    }\n\n    interface RecordCursorFactory {\n        RecordCursor getCursor();\n        void close();\n    }\n\n    static class GroupByRecordCursorFactory implements RecordCursorFactory {\n\n        private final List<Function> functions;\n\n        public GroupByRecordCursorFactory(List<Function> functions) {\n            this.functions = functions;\n        }\n\n        @Override\n        public RecordCursor getCursor() {\n            return new GroupByRecordCursor(functions);\n        }\n\n        @Override\n        public void close() {\n            // optional: cleanup at factory level if needed\n        }\n\n        private static class GroupByRecordCursor implements RecordCursor {\n            private final List<Function> functions;\n            private boolean consumed = false;\n\n            public GroupByRecordCursor(List<Function> functions) {\n                this.functions = functions;\n            }\n\n            @Override\n            public boolean hasNext() {\n                if (!consumed) {\n                    consumed = true;\n                    return true;\n                }\n                return false;\n            }\n\n            @Override\n            public void close() {\n                for (Function f : functions) {\n                    f.cursorClosed();\n                }\n            }\n        }\n    }\n\n    // ==================== Main demo =======================\n\n    public static void main(String[] args) {\n        System.out.println(\"Starting GROUP BY cursor test...\");\n\n        JsonExtractFunction jsonFunc1 = new JsonExtractFunction();\n        JsonExtractFunction jsonFunc2 = new JsonExtractFunction();\n\n        GroupByRecordCursorFactory factory = new GroupByRecordCursorFactory(\n            Arrays.asList(jsonFunc1, jsonFunc2)\n        );\n\n        RecordCursor cursor = factory.getCursor();\n\n        while (cursor.hasNext()) {\n            System.out.println(\"Processing record...\");\n        }\n\n        // This is where memory is properly freed\n        cursor.close();\n\n        System.out.println(\"Final allocated buffers: \" + JsonExtractFunction.getAllocatedBuffers());\n    }\n}\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 2687,
    "metadata": {
      "issue_number": 5812,
      "state": "open",
      "labels": [
        "Bug",
        "Java"
      ],
      "comments_count": 25,
      "created_at": "2025-07-04T15:23:03Z",
      "updated_at": "2025-07-28T05:39:41Z",
      "closed_at": null,
      "author": "jerrinot",
      "top_comments": [
        "**itsaxat7479**:  Potential Solution Proposal\nThanks for reporting this important issue. A potential solution to ensure cursorClosed() is consistently invoked across all cursor typesâ€”including those used in GROUP BY operationsâ€”could be:\n\nCentralize cursorClosed() Invocation:\n\nIntroduce a unified cursor wrapper or decorator pattern around all cursor types.\n\nThis wrapper should ensure that cursorClosed() is always called in the close() method, regardless of the cursor's origin (virtual or real).\n\nUpdate GroupByRecordCursorFactory:\n\nModify factories like ParallelGroupByRecordCursorFactory to explicitly call cursorClosed() on all associated function instances during shutdown or cleanup phases.\n\nEnhance Test Coverage:\n\nAdd a test case (e.g. testCursorClosedIsCalledForAllCursors()) that uses mock functions with logging in cursorClosed() to assert they are properly notified.\n\nThis can catch regressions early.\n\nConsider Buffer Reference Counting:\n\nIf multiple cloned cursors share a buffer, implement a reference count mechanism to ensure the buffer is freed only after all cursors are closed and cursorClosed() has been triggered.\n\nThis would not only fix memory leaks in json_extract() but also make the lifecycle management more robust and extensible for future function types.\n\n",
        "**Ritikdhanotiya07**: //1. Utility Function to Notify All Functions\njava\nCopy\nEdit\npublic class FunctionUtils {\n    public static void notifyFunctionsCursorClosed(List<? extends Function> functions) {\n        for (Function function : functions) {\n            function.cursorClosed();\n        }\n    }\n}\n//2. Modify GroupByRecordCursorFactory.java\njava\nCopy\nEdit\n@Override\npublic void close() {\n    super.close(); // existing close logic\n    FunctionUtils.notifyFunctionsCursorClosed(this.functions); // new line\n}\n3. Modify GroupByRecordCursor.java (if separate)\njava\nCopy\nEdit\n@Override\npublic void close() {\n    super.close();\n    FunctionUtils.notifyFunctionsCursorClosed(this.functions); // call close\n}\n 4. Modify ParallelGroupByCursorFactory.java\n@Override\npublic void close() {\n    super.close(); // existing shutdown logic\n    for (List<Function> fnSet : functionInstancesPerShard) {\n        FunctionUtils.notifyFunctionsCursorClosed(fnSet);\n    }\n}\n5. Update JsonExtractFunction.java\n public class JsonExtractFunction extends AbstractFunction {\n    private long buffer = 0;\n    private static final int BUFFER_SIZE = 1024 * 1024;\n\n    public JsonExtractFunction(...) {\n        this.buffer = Unsafe.malloc(BUFFER_SIZE);\n    }\n\n    @Override\n    public void cursorClosed() {\n        if (buffer != 0) {\n            Unsafe.free(buffer, BUFFER_SIZE);\n            buffer = 0;\n        }\n    }\n}\n6. Add Validation in ParallelGroupByFuzzTest.java\n @Test\npublic void testParallelJsonKeyGroupBy() {\n    // setup test\n    AtomicBoolean cursorClosedCalled = new AtomicBoolean(false);\n\n    JsonExtractFunction testFunc = new JsonExtractFunction() {\n        @Override\n        public void cursorClosed() {\n            cursorClosedCalled.set(true);\n            super.cursorClosed();\n        }\n    };\n\n    // execute group by using testFunc...\n    \n    assertTrue(\"Function.cursorClosed was not called!\", cursorClosedCalled.get());\n}\n\n\n7. ",
        "**Techo-Minds**: Proposed Solution\n1. Extend Base Cursor Implementations\nUpdate all key RecordCursor implementations that internally use Function objects to call cursorClosed() on them in their close() method.\n\nFor example:\n\nðŸ”§ In GroupByRecordCursor.java:\n@Override\npublic void close() {\n    try {\n        // Close internal cursors as usual\n        if (baseCursor != null) {\n            baseCursor.close();\n        }\n    } finally {\n        // Add this block\n        for (int i = 0; i < recordFunctions.size(); i++) {\n            Function function = recordFunctions.getQuick(i);\n            if (function != null) {\n                function.cursorClosed();\n            }\n        }\n    }\n}\nâš ï¸ Important: This must be done recursively for any cursor that wraps or delegates to other cursors.\n\n2. Add Utility Method for Function Cleanup\nTo avoid duplicating the function-closing logic across cursor types, consider a utility method like:\n\npublic static void closeFunctionsQuietly(ObjList<Function> functions) {\n    for (int i = 0, n = functions.size(); i < n; i++) {\n        Function f = functions.getQuick(i);\n        if (f != null) {\n            try {\n                f.cursorClosed();\n            } catch (Throwable ignore) {\n                // Optionally log this\n            }\n        }\n    }\n}\nUse it in close() methods for all cursor classes that manage Function lists.\n\n3. Ensure Cursor Factories Pass Function Lists\nIn cursor factories (e.g., GroupByCursorFactory), make sure the constructed cursors are given access to the Function objects they are supposed to manage/clean.\n\n4. Add Regression Tests\nCreate a test case that:\n\nUses a Function (e.g., JsonExtractFunction) that allocates memory.\n\nVerifies that cursorClosed() was invoked.\n\nCan simulate partial cursor closure (e.g., early exit from parallel/grouped processing).\n\n@Test\npublic void testCursorClosedCalledInGroupBy() {\n    // Create table and data\n    // Run a GROUP BY query using json_extract()\n    // Track cursorClosed() invocation via logging or counter\n\n    // Assert that cursorClosed() is called for each function\n}\n5. Memory Leak Watchdog (Optional but Useful)\nIntroduce test utility that:\n\nTracks memory allocation before/after query\n\nAsserts that no leak occurred\n\nWorks especially for parallel group-by with cloning\n\nâœ… Outcome\nAll Function implementations, including memory-heavy ones like JsonExtractFunction, will have cursorClosed() invoked at the correct point.\n\nPrevents silent memory leaks during parallel or nested query operations.\n\nImproves overall resource hygiene and avoids surprises in long-running or high-concurrency environments.",
        "**loordjay**: \nAs a result, functions used in other cursor typesâ€”most notably those involved in GROUP BY operationsâ€”do not receive this notification, leading to resource leaks. \n\n Steps to Reproduce\nRun the test:\nParallelGroupByFuzzTest#testParallelJsonKeyGroupBy()\n\nObserve that JsonExtractFunction.cursorClosed() is not invoked.\n\nâš ï¸ Actual Behavior\ncursorClosed() is not called for cursors associated with GROUP BY operations.\n\nThe 1MB memory buffer allocated by json_extract() is not released at the function level.\n\nThe buffer is only freed when the entire cursor factory is closed, masking the issue in leak detection tools.\n\nâœ… Expected Behavior\nFunction.cursorClosed() should be invoked consistently across all cursor types, not just virtual record cursors.\n\nResources (e.g., memory buffers) managed by the function should be released as soon as the associated cursor is closed.\n\nðŸ“ˆ Impact\nMemory buffers allocated in json_extract() are not promptly released.\n\nIn parallel GROUP BY scenarios, functions (and their buffers) are cloned, amplifying memory usage.\n\nThis leads to hidden memory leaks in long-running or concurrent workloads.\n\nResource cleanup becomes dependent on full factory shutdown rather than clean function-level lifecycle management.\n\nðŸ› ï¸ Suggested Fix\nUpdate the lifecycle management across all cursor types to ensure cursorClosed() is invoked consistently. This includes:\n\nGROUP BY cursors\n\nParallel and concurrent cursor wrappers\n\nAny internal cursor abstractions not currently supporting this callback\n\nProposed Solution\nEnsure that all cursor implementationsâ€”including those used in:\n\nGROUP BY operations\n\nParallel/grouped cursor wrappers\n\nComposite cursor factories\n\nâ€¦explicitly invoke Function.cursorClosed() as part of their teardown logic.\n\nThis will enforce a uniform lifecycle contract and enable reliable, deterministic resource management. \n\nfor (Function f : functions) {\nif (f instanceof LifecycleAwareFunction) {\n((LifecycleAwareFunction) f).cursorClosed();\n}\n}\n\nImpact\nMemory Leaks:\nThe json_extract() function allocates a 1 MB buffer per instance. Without cursorClosed() being called, this buffer is not released.\n\nParallel Amplification:\nIn parallel GROUP BY queries, the function (and its buffer) is cloned multiple times. As a result, memory usage grows rapidly.\n\nDelayed Cleanup:\nSince cursorClosed() is not invoked, the buffer is only released when the entire cursor factory is closed. This delayed release bypasses memory leak guards and obscures the problem\n\n@Override\npublic void close() {\n    super.close();\n    for (Function f : groupByFunctions) {\n        f.cursorClosed();\n    }\n    for (Function f : aggregationFunctions) {\n        f.cursorClosed();\n    }\n}\n\n\n\n\n",
        "**Divya857571**: for (Function f : functions) {\n    if (f instanceof LifecycleAwareFunction) {\n        ((LifecycleAwareFunction) f).cursorClosed();\n    }\n}\n"
      ],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-5b94929ec099",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/269",
    "title": "High idle CPU usage",
    "text": "# High idle CPU usage\n\n**Describe the bug**\r\nThe **idle** CPU usage is pretty high at ~3%.\r\n\r\n**To Reproduce**\r\nFollow https://www.questdb.io/docs/installFromDockerImage\r\nRun `docker stats` to measure CPU usage.\r\n\r\nSometimes the idle cpu usage reaches even 10%, but most of the time it stays stable at ~3%.\r\n\r\n**Expected behavior**\r\nIdle CPU usage is <0.1%.\r\n\r\n**Screenshots**\r\n<img width=\"1054\" alt=\"Screenshot 2020-05-10 at 00 07 01\" src=\"https://user-images.githubusercontent.com/4052440/81486102-cb0fb800-9252-11ea-9c3b-65626476cc20.png\">\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - Ubuntu 18\r\n - Latest Docker image \r\n\r\n\n\n## Top Comments\n\n**bluestreak01**: CPU should \"calm down\" after a period of inactivity. Otherwise threads are \"busy\" rather than blocking.\n\n---\n\n**PiotrDabkowski**: Thanks! For some reason this does not happen, I have checked after ~8 hours of inactivity and still at ~3%. Not a big deal - 3% is not too bad, but would be great to not waste CPU unnecessarily.\r\n\r\nUnfortunately, I do not know Java so I am unable to help with this.\n\n---\n\n**tandr**: I am getting 30-36% idle (I suspect on one core)  on macbook pro 2.3 GHz 8-Core Intel Core i9 . I used questdb-5.0.3-rt-osx-amd64.tar.gz to get it running (would be nice if questdb.sh would have a `version` option, that prints  troubleshooting-related info.)\r\n\r\n<img width=\"630\" alt=\"Screen Shot 2020-09-03 at 2 49 19 PM\" src=\"https://user-images.githubusercontent.com/5227075/92165721-b6a08e00-edf4-11ea-9cac-ee253d6a5f4c.png\">\r\n\r\n\n\n---\n\n**borderBite**: @tandr on my macbook pro I'm also seeing idle cpu usage at ~25% of a single core.\r\nI'm using questdb-5.0.3 which was installed using brew\n\n---\n\n**tomarus**: Hey i'm seeing the same. Around 50% (out of 1200%) cpu usage. It doesn't really look right and it's not cooling down after a period of idling. I want to run this on a low-power wardrobe server but with this cpu usage it's actually not really that efficient.\n\n---\n\n**bluestreak01**: could you try this configuration?\r\n\r\n```\r\nshared.worker.count=1\r\nshared.worker.yield.threshold=1\r\nshared.worker.sleep.threshold=1\r\nline.tcp.io.worker.yield.threshold=1\r\nline.tcp.io.worker.sleep.threshold=1\r\n```\n\n---\n\n**tomarus**: > could you try this configuration?\r\n\r\nThanks a lot, this certainly helped a lot, except the 2nd and 3rd didn't make really much of a difference.\r\n\r\nAfter playing around with other settings i've added those below and the cpu usage is 50% less than yesterday. Which is perfect for my usecase :-)\r\n\r\n```\r\nline.tcp.io.worker.count=1\r\nline.tcp.writer.worker.count=1\r\nline.tcp.writer.worker.sleep.threshold=1\r\nline.tcp.writer.worker.yield.threshold=1\r\n```\r\n\r\nAlso i've noticed when i completely disable the tcp line protocol almost all of the cpu usage is gone. Disabling udp didn't make any difference.\r\n\n\n---\n\n**bsmth**: I am going to close as the [suggestion above](https://github.com/questdb/questdb/issues/269#issuecomment-847434873) seems to fix this issue!\r\n\r\nThank you!\n\n---\n\n**jfcalvo**: I have similar high CPU usage values. \r\n\r\n~15% CPU idle after run `questdb/questdb:6.1` docker image without configuration changes of any type:\r\n\r\n![Captura de pantalla 2021-10-30 a las 16 25 29](https://user-images.githubusercontent.com/446497/139537088-9c6c4224-8f7a-4d04-860d-7914c3e8875d.png)\r\n\r\nTo be honest it's too high to be the default configuration and totally idle.\r\n\r\nI have tried setting the following environments vars without any noticeable change in CPU usage:\r\n\r\n```\r\nQDB_SHARED_WORKER_COUNT=1\r\nQDB_SHARED_WORKER_YIELD_THRESHOLD=1\r\nQDB_SHARED_WORKER_SLEEP_THRESHOLD=1\r\nQDB_LINE_TCP_IO_WORKER_YIELD_THRESHOLD=1\r\nQDB_LINE_TCP_IO_WORKER_SLEEP_THRESHOLD=1\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n---\n\n**vizovitin**: Same here, and none of the configurations suggested here make situation better for QuestDB server 6.0.9 (via `questdb/questdb:latest` docker image). CPU usage is consistently at 5-6% without any external load.\r\n\r\nTurning off line protocol (both tcp and udp) doesn't help either (and I actually need it).\r\n\r\nAlso same with standalone `questdb-6.0.9-rt-linux-amd64` release.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 521,
    "metadata": {
      "issue_number": 269,
      "state": "closed",
      "labels": [
        "Performance"
      ],
      "comments_count": 25,
      "created_at": "2020-05-09T22:13:29Z",
      "updated_at": "2024-07-23T14:21:46Z",
      "closed_at": "2022-01-17T18:29:01Z",
      "author": "PiotrDabkowski",
      "top_comments": [
        "**bluestreak01**: CPU should \"calm down\" after a period of inactivity. Otherwise threads are \"busy\" rather than blocking.",
        "**PiotrDabkowski**: Thanks! For some reason this does not happen, I have checked after ~8 hours of inactivity and still at ~3%. Not a big deal - 3% is not too bad, but would be great to not waste CPU unnecessarily.\r\n\r\nUnfortunately, I do not know Java so I am unable to help with this.",
        "**tandr**: I am getting 30-36% idle (I suspect on one core)  on macbook pro 2.3 GHz 8-Core Intel Core i9 . I used questdb-5.0.3-rt-osx-amd64.tar.gz to get it running (would be nice if questdb.sh would have a `version` option, that prints  troubleshooting-related info.)\r\n\r\n<img width=\"630\" alt=\"Screen Shot 2020-09-03 at 2 49 19 PM\" src=\"https://user-images.githubusercontent.com/5227075/92165721-b6a08e00-edf4-11ea-9cac-ee253d6a5f4c.png\">\r\n\r\n",
        "**borderBite**: @tandr on my macbook pro I'm also seeing idle cpu usage at ~25% of a single core.\r\nI'm using questdb-5.0.3 which was installed using brew",
        "**tomarus**: Hey i'm seeing the same. Around 50% (out of 1200%) cpu usage. It doesn't really look right and it's not cooling down after a period of idling. I want to run this on a low-power wardrobe server but with this cpu usage it's actually not really that efficient."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3b31a9bbb9bf",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1083",
    "title": "Add all contribs ðŸŽ‰ ",
    "text": "# Add all contribs ðŸŽ‰ \n\nAdding latest contributors ðŸŽ‰\n\n## Top Comments\n\n**bsmth**: @all-contributors bot, please add @rrjanbiah for bug reports\r\n\r\nThanks for your contribution!\r\n\n\n---\n\n**bsmth**: @all-contributors bot, please add @rrjanbiah for bug reports\r\n\r\nThanks from the QuestDB team!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1085) to add @rrjanbiah! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @sarunas-stasaitis for bug reports\r\n\r\nThanks from the QuestDB team!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1086) to add @sarunas-stasaitis! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @RiccardoGiro for bug reports\r\n\r\nThanks for your contribution!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1087) to add @RiccardoGiro! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @duggar for bug reports\r\n\r\nThanks from the QuestDB team!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1088) to add @duggar! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @postol for bug reports\r\n\r\nThanks from the QuestDB team!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 151,
    "metadata": {
      "issue_number": 1083,
      "state": "closed",
      "labels": [],
      "comments_count": 24,
      "created_at": "2021-06-03T14:55:50Z",
      "updated_at": "2021-06-04T14:52:50Z",
      "closed_at": "2021-06-04T14:52:50Z",
      "author": "bsmth",
      "top_comments": [
        "**bsmth**: @all-contributors bot, please add @rrjanbiah for bug reports\r\n\r\nThanks for your contribution!\r\n",
        "**bsmth**: @all-contributors bot, please add @rrjanbiah for bug reports\r\n\r\nThanks from the QuestDB team!",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1085) to add @rrjanbiah! :tada:",
        "**bsmth**: @all-contributors bot, please add @sarunas-stasaitis for bug reports\r\n\r\nThanks from the QuestDB team!",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1086) to add @sarunas-stasaitis! :tada:"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-416efc8eaf2f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2348",
    "title": "questdb.jar is referenced in questdb-6.4.3-rt-linux-amd64 questdb.sh script - questdb.sh script seemingly not working on RHEL 8.4",
    "text": "# questdb.jar is referenced in questdb-6.4.3-rt-linux-amd64 questdb.sh script - questdb.sh script seemingly not working on RHEL 8.4\n\n### Describe the bug\r\n\r\nDownloaded latest release to mistakenly think questdb.jar file is missing:\r\n\r\n/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64\r\n#> find . -name questdb.jar\r\n\r\nThis is because questdb.sh references questdb.jar, and the first step in looking when the script failed to run led me to that point.\r\n\r\n### To reproduce\r\n\r\n1. Download tar file \r\n2. Extract tar file\r\n3. Try run ./questdb.sh\r\n4. Check open ports - Nothing relevant to questdb\r\n5. Looked in questdb.sh that references questdb.jar \r\n6. Looked in bin directory of extracted tar\r\n\r\n### Expected Behavior\r\n\r\nquestdb.jar should... Work out of the box for RHEL 8.4.\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:6.4.3\r\n- **OS**:RHEL8.4\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_\n\n## Top Comments\n\n**CJCShadowsan**: And even if it's meant to be uncompiled in the libs dir... It doesn't actually do anything. It logs:\r\n\r\n```\r\ncat /cache/questdb/log/stdout-2022-07-25T23-24-56.txt\r\n2022-07-25T23:24:56.734678Z A server-main QuestDB server 6.4.3. Copyright (C) 2014-2022, all rights reserved.\r\n2022-07-25T23:24:56.908019Z I server-main web console is up to date\r\n2022-07-25T23:24:56.908628Z A server-main Server config: /cache/questdb/conf/server.conf\r\n2022-07-25T23:24:56.962257Z I i.q.c.t.t.InputFormatConfiguration loading input format config [resource=/text_loader.json]\r\n2022-07-25T23:24:56.980850Z A server-main Config changes applied:\r\n2022-07-25T23:24:56.980887Z A server-main   http.enabled : true\r\n2022-07-25T23:24:56.980916Z A server-main   tcp.enabled  : true\r\n2022-07-25T23:24:56.980940Z A server-main   pg.enabled   : true\r\n2022-07-25T23:24:56.980960Z A server-main open database [id=6073491747499511394.-4686445700594837497]\r\n2022-07-25T23:24:56.981005Z A server-main platform [bit=64]\r\n2022-07-25T23:24:56.981030Z A server-main OS/Arch: linux/amd64 [AVX512,10]\r\n2022-07-25T23:24:56.983734Z A server-main available CPUs: 48\r\n2022-07-25T23:24:56.983839Z A server-main db root: /cache/questdb/db\r\n2022-07-25T23:24:56.983865Z A server-main backup root: null\r\n2022-07-25T23:24:56.983958Z A server-main db file system magic: 0xef53 [ext4] SUPPORTED\r\n2022-07-25T23:24:56.984699Z A server-main SQL JIT compiler mode: on\r\n2022-07-25T23:24:56.986168Z I i.q.g.FunctionFactoryCache loading functions [test=false]\r\n2022-07-25T23:24:57.120244Z I i.q.c.m.EngineMigration table structures are up to date\r\n2022-07-25T23:24:57.225889Z I i.q.c.p.ReaderPool open 'telemetry_config' [at=0:0]\r\n2022-07-25T23:24:57.235471Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'version' already exists]\r\n2022-07-25T23:24:57.235701Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'os' already exists]\r\n2022-07-25T23:24:57.235844Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'package' already exists]\r\n2022-07-25T23:24:57.235949Z I i.q.c.p.WriterPool open [table=`telemetry`, thread=1]\r\n2022-07-25T23:24:57.245166Z I i.q.c.TableWriter open 'telemetry'\r\n2022-07-25T23:24:57.248333Z I i.q.c.TableWriter switched partition [path='/cache/questdb/db/telemetry/default']\r\n2022-07-25T23:24:57.248701Z I i.q.c.p.WriterPool >> [table=`telemetry`, thread=1]\r\n2022-07-25T23:24:57.248760Z I i.q.c.p.WriterPool open [table=`telemetry_config`, thread=1]\r\n2022-07-25T23:24:57.248854Z I i.q.c.TableWriter open 'telemetry_config'\r\n2022-07-25T23:24:57.250710Z I i.q.c.TableWriter switched partition [path='/cache/questdb/db/telemetry_config/default']\r\n2022-07-25T23:24:57.251097Z I i.q.c.p.WriterPool >> [table=`telemetry_config`, thread=1]\r\n2022-07-25T23:24:57.252488Z I i.q.g.SqlCompiler plan [q=`select-choose id, enabled, version, os, package from (select [id, enabled, version, os, package] from telemetry_config) limit -(1)`, fd=-1]\r\n2022-07-25T23:24:57.256685Z I i.q.c.TableReader open partition /cache/questdb/db/telemetry_config/default [rowCount=1, partitionNameTxn=-1, transientRowCount=1, partitionIndex=0, partitionCount=1]\r\n2022-07-25T23:24:57.257004Z A i.q.TelemetryJob instance [id=0x05e4a89928e5820025e01c1a5fc9fa, enabled=true]\r\n2022-07-25T23:24:57.267918Z I i.q.c.p.WriterPool open [table=`sys.column_versions_purge_log`, thread=1]\r\n2022-07-25T23:24:57.268068Z I i.q.c.TableWriter open 'sys.column_versions_purge_log'\r\n2022-07-25T23:24:57.269275Z I i.q.c.TableWriter purging non attached partitions [path=/cache/questdb/db/sys.column_versions_purge_log]\r\n2022-07-25T23:24:57.269548Z I i.q.c.p.WriterPool >> [table=`sys.column_versions_purge_log`, thread=1]\r\n2022-07-25T23:24:57.270428Z I i.q.c.p.ReaderPool open 'sys.column_versions_purge_log' [at=0:0]\r\n2022-07-25T23:24:57.271824Z I i.q.g.SqlCompiler plan [q=`select-choose ts, table_name, column_name, table_id, truncate_version, columnType, table_partition_by, updated_txn, column_version, partition_timestamp, partition_name_txn, completed from (select [ts, table_name, column_name, table_id, truncate_version, columnType, table_partition_by, updated_txn, column_version, partition_timestamp, partition_name_txn, completed] from sys.column_versions_purge_log timestamp (ts) where ts > dateadd('d',-(31),now()) and completed = null)`, fd=-1]\r\n2022-07-25T23:24:57.278019Z I i.q.g.SqlCodeGenerator JIT enabled for (sub)query [tableName=sys.column_versions_purge_log, fd=-1]\r\n2022-07-25T23:24:57.293596Z A http-server listening on 0.0.0.0:9000 [fd=89 backlog=256]\r\n2022-07-25T23:24:57.380040Z A server-main Min health server is starting. Health check endpoint will not consider unhandled errors when metrics are disabled.\r\n2022-07-25T23:24:57.381485Z I server-main started\r\n2022-07-25T23:24:57.381568Z I server-main os scheduled [name=questdb-worker-2]\r\n2022-07-25T23:24:57.383595Z A pg-server listening on 0.0.0.0:8812 [fd=93 backlog=10]\r\n2022-07-25T23:24:57.380360Z A http-min-server listening on 0.0.0.0:9003 [fd=91 backlog=4]\r\n```\r\n\r\nAnd then nothing happens. No process running, nothing.\n\n---\n\n**CJCShadowsan**: So either this is extremely poorly documented, or the binary release flat-out doesn't work.\n\n---\n\n**CJCShadowsan**: Before anyone says \"Why can't you just pull the docker container?\"\r\n\r\nThe node is airgapped from the internet, and if you provide a binary? It should work!\n\n---\n\n**jerrinot**: hi @CJCShadowsan,\r\n\r\nthe RT version of QuestDB distributions does not contain questdb.jar. That's by design. If you inspect the archive, you will see a file `./lib/modules`which has approx. 20MB. This is a Java module image which contains QuestDB classes. \r\n\r\nThe logs you posted indicate QuestDB was started. QuestDB itself writes the logs. \r\nCan you please clarify this step in your reproducer? \r\n> Try run ./questdb.sh - Wonder why nothing works\r\n\r\nHow did you establish that nothing works? I just tried it on Ubuntu@x64 and it works just fine for me. Have you tried to connect to localhost:9000? There should be a web server listening on that port. \n\n---\n\n**bluestreak01**: RT is assembled by jlink [documentation](https://docs.oracle.com/javase/9/tools/jlink.htm#JSWOR-GUID-CECAC52B-CFEE-46CB-8166-F17A8E9280E9).\r\n\r\nif you need Jar, download no-jre version\n\n---\n\n**CJCShadowsan**: > hi @CJCShadowsan,\r\n> \r\n> the RT version of QuestDB distributions does not contain questdb.jar. That's by design. If you inspect the archive, you will see a file `./lib/modules`which has approx. 20MB. This is a Java module image which contains QuestDB classes.\r\n> \r\n> The logs you posted indicate QuestDB was started. QuestDB itself writes the logs. Can you please clarify this step in your reproducer?\r\n> \r\n> > Try run ./questdb.sh - Wonder why nothing works\r\n> \r\n> How did you establish that nothing works? I just tried it on Ubuntu@x64 and it works just fine for me. Have you tried to connect to localhost:9000? There should be a web server listening on that port.\r\n\r\nBecause literally... Nothing works:\r\n\r\n```\r\n#> [as1][ 2022-07-26 11:24:18 ] root@test:/root/questdb-6.4.3\r\n#> md5sum questdb-6.4.3-rt-linux-amd64.tar.gz\r\n1bf6b29da74bbd6e93fbec1132bd4220  questdb-6.4.3-rt-linux-amd64.tar.gz\r\n\r\n#> [as1][ 2022-07-26 11:24:28 ] root@test:/root/questdb-6.4.3\r\n#> tar -xf questdb-6.4.3-rt-linux-amd64.tar.gz\r\n\r\n#> [as1][ 2022-07-26 11:24:35 ] root@test:/root/questdb-6.4.3\r\n#> cd questdb-6.4.3-rt-linux-amd64\r\n\r\n#> [as1][ 2022-07-26 11:24:45 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64\r\n#> ls\r\nbin  conf  legal  lib  release\r\n\r\n#> [as1][ 2022-07-26 11:24:48 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64\r\n#> cd bin\r\n\r\n#> [as1][ 2022-07-26 11:24:51 ] root@as1-insight1:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64/bin\r\n#> ls\r\njava  keytool  questdb.sh\r\n\r\n#> [as1][ 2022-07-26 11:24:52 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64/bin\r\n#> ./questdb.sh start -d /cache/questdb -t testing\r\n\r\n  ___                  _   ____  ____\r\n / _ \\ _   _  ___  ___| |_|  _ \\| __ )\r\n| | | | | | |/ _ \\/ __| __| | | |  _ \\\r\n| |_| | |_| |  __/\\__ \\ |_| |_| | |_) |\r\n \\__\\_\\\\__,_|\\___||___/\\__|____/|____/\r\n                        www.questdb.io\r\n\r\nJAVA: ./java\r\n\r\n#> [as1][ 2022-07-26 11:25:15 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64/bin\r\n#> QuestDB server 6.4.3\r\nCopyright (C) 2014-2022, all rights reserved.\r\n\r\nReading log configuration from /cache/questdb/conf/log.conf\r\nLog configuration loaded from: /cache/questdb/conf/log.conf\r\nCreated log directory: log\r\nLog configuration loaded from default internal file.\r\n\r\n\r\n#> [as1][ 2022-07-26 11:25:23 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64/bin\r\n#> netstat -anp | grep 9000\r\n\r\n#> [as1][ 2022-07-26 11:25:34 ] root@test:/root/questdb-6.4.3/questdb-6.4.3-rt-linux-amd64/bin\r\n#>\r\n```\n\n---\n\n**CJCShadowsan**: > hi @CJCShadowsan,\r\n> \r\n> the RT version of QuestDB distributions does not contain questdb.jar. That's by design. If you inspect the archive, you will see a file `./lib/modules`which has approx. 20MB. This is a Java module image which contains QuestDB classes.\r\n\r\nThis is a fair enough point - I wouldn't have gone looking for the words questdb.jar in the questdb.sh script if it had started and ran - red-herring perhaps and a misleading title. I'll update.\n\n---\n\n**CJCShadowsan**: Unfortunately in those logs is as far as it goes... It stops doing anything at that point. No process running, no ports listening, the status option says not running.\r\n\r\nNo further logs to indicate what's happened either.\n\n---\n\n**jerrinot**: hi @CJCShadowsan, that's weird. I just tried it on\r\n```\r\n$ cat /etc/redhat-release\r\nRed Hat Enterprise Linux release 8.6 (Ootpa)\r\n```\r\nand it's starting fine:\r\n```\r\n$ curl \"http://localhost:9000/exec?query=select%20now();\"\r\n{\"query\":\"select now();\",\"columns\":[{\"name\":\"now\",\"type\":\"TIMESTAMP\"}],\"dataset\":[[\"2022-07-26T12:11:05.601440Z\"]],\"count\":1}\r\n```\r\n\r\nIs there anything in a system log? Perhaps kernel OOM killer kicking in? What are the specs of the machine you are using? Is there any other server running on the same box? Are you using the default configuration?\n\n---\n\n**CJCShadowsan**: > hi @CJCShadowsan, that's weird. I just tried it on\r\n> \r\n> ```\r\n> $ cat /etc/redhat-release\r\n> Red Hat Enterprise Linux release 8.6 (Ootpa)\r\n> ```\r\n> \r\n> and it's starting fine:\r\n> \r\n> ```\r\n> $ curl \"http://localhost:9000/exec?query=select%20now();\"\r\n> {\"query\":\"select now();\",\"columns\":[{\"name\":\"now\",\"type\":\"TIMESTAMP\"}],\"dataset\":[[\"2022-07-26T12:11:05.601440Z\"]],\"count\":1}\r\n> ```\r\n> \r\n> Is there anything in a system log? Perhaps kernel OOM killer kicking in? What are the specs of the machine you are using? Is there any other server running on the same box? Are you using the default configuration?\r\n\r\nNo OOM, There are other servers running on the same box (Influx, KairosDB, Cassandra, Grafana, Prometheus) but none using the ports requested.\r\n\r\nSystem is a 48-core system with 256GB Ram.\r\n\r\nLiterally using the default configuration - nothing changed, just attempting to get it to start up with the exact commands listed.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 1358,
    "metadata": {
      "issue_number": 2348,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 23,
      "created_at": "2022-07-25T23:04:21Z",
      "updated_at": "2022-07-26T18:33:03Z",
      "closed_at": "2022-07-26T18:33:03Z",
      "author": "CJCShadowsan",
      "top_comments": [
        "**CJCShadowsan**: And even if it's meant to be uncompiled in the libs dir... It doesn't actually do anything. It logs:\r\n\r\n```\r\ncat /cache/questdb/log/stdout-2022-07-25T23-24-56.txt\r\n2022-07-25T23:24:56.734678Z A server-main QuestDB server 6.4.3. Copyright (C) 2014-2022, all rights reserved.\r\n2022-07-25T23:24:56.908019Z I server-main web console is up to date\r\n2022-07-25T23:24:56.908628Z A server-main Server config: /cache/questdb/conf/server.conf\r\n2022-07-25T23:24:56.962257Z I i.q.c.t.t.InputFormatConfiguration loading input format config [resource=/text_loader.json]\r\n2022-07-25T23:24:56.980850Z A server-main Config changes applied:\r\n2022-07-25T23:24:56.980887Z A server-main   http.enabled : true\r\n2022-07-25T23:24:56.980916Z A server-main   tcp.enabled  : true\r\n2022-07-25T23:24:56.980940Z A server-main   pg.enabled   : true\r\n2022-07-25T23:24:56.980960Z A server-main open database [id=6073491747499511394.-4686445700594837497]\r\n2022-07-25T23:24:56.981005Z A server-main platform [bit=64]\r\n2022-07-25T23:24:56.981030Z A server-main OS/Arch: linux/amd64 [AVX512,10]\r\n2022-07-25T23:24:56.983734Z A server-main available CPUs: 48\r\n2022-07-25T23:24:56.983839Z A server-main db root: /cache/questdb/db\r\n2022-07-25T23:24:56.983865Z A server-main backup root: null\r\n2022-07-25T23:24:56.983958Z A server-main db file system magic: 0xef53 [ext4] SUPPORTED\r\n2022-07-25T23:24:56.984699Z A server-main SQL JIT compiler mode: on\r\n2022-07-25T23:24:56.986168Z I i.q.g.FunctionFactoryCache loading functions [test=false]\r\n2022-07-25T23:24:57.120244Z I i.q.c.m.EngineMigration table structures are up to date\r\n2022-07-25T23:24:57.225889Z I i.q.c.p.ReaderPool open 'telemetry_config' [at=0:0]\r\n2022-07-25T23:24:57.235471Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'version' already exists]\r\n2022-07-25T23:24:57.235701Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'os' already exists]\r\n2022-07-25T23:24:57.235844Z I i.q.TelemetryJob Failed to alter telemetry table [table=telemetry_config,error=column 'package' already exists]\r\n2022-07-25T23:24:57.235949Z I i.q.c.p.WriterPool open [table=`telemetry`, thread=1]\r\n2022-07-25T23:24:57.245166Z I i.q.c.TableWriter open 'telemetry'\r\n2022-07-25T23:24:57.248333Z I i.q.c.TableWriter switched partition [path='/cache/questdb/db/telemetry/default']\r\n2022-07-25T23:24:57.248701Z I i.q.c.p.WriterPool >> [table=`telemetry`, thread=1]\r\n2022-07-25T23:24:57.248760Z I i.q.c.p.WriterPool open [table=`telemetry_config`, thread=1]\r\n2022-07-25T23:24:57.248854Z I i.q.c.TableWriter open 'telemetry_config'\r\n2022-07-25T23:24:57.250710Z I i.q.c.TableWriter switched partition [path='/cache/questdb/db/telemetry_config/default']\r\n2022-07-25T23:24:57.251097Z I i.q.c.p.WriterPool >> [table=`telemetry_config`, thread=1]\r\n2022-07-25T23:24:57.252488Z I i.q.g.SqlCompiler plan [q=`select-choose id, enabled, version, os, package from (select [id, enabled, version, os, package] from telemetry_config) limit -(1)`, fd=-1]\r\n2022-07-25T23:24:57.256685Z I i.q.c.TableReader open partition /cache/questdb/db/telemetry_config/default [rowCount=1, partitionNameTxn=-1, transientRowCount=1, partitionIndex=0, partitionCount=1]\r\n2022-07-25T23:24:57.257004Z A i.q.TelemetryJob instance [id=0x05e4a89928e5820025e01c1a5fc9fa, enabled=true]\r\n2022-07-25T23:24:57.267918Z I i.q.c.p.WriterPool open [table=`sys.column_versions_purge_log`, thread=1]\r\n2022-07-25T23:24:57.268068Z I i.q.c.TableWriter open 'sys.column_versions_purge_log'\r\n2022-07-25T23:24:57.269275Z I i.q.c.TableWriter purging non attached partitions [path=/cache/questdb/db/sys.column_versions_purge_log]\r\n2022-07-25T23:24:57.269548Z I i.q.c.p.WriterPool >> [table=`sys.column_versions_purge_log`, thread=1]\r\n2022-07-25T23:24:57.270428Z I i.q.c.p.ReaderPool open 'sys.column_versions_purge_log' [at=0:0]\r\n2022-07-25T23:24:57.271824Z I i.q.g.SqlCompiler plan [q=`select-choose ts, table_name, column_name, table_id, truncate_version, columnType, table_partition_by, updated_txn, column_version, partition_timestamp, partition_name_txn, completed from (select [ts, table_name, column_name, table_id, truncate_version, columnType, table_partition_by, updated_txn, column_version, partition_timestamp, partition_name_txn, completed] from sys.column_versions_purge_log timestamp (ts) where ts > dateadd('d',-(31),now()) and completed = null)`, fd=-1]\r\n2022-07-25T23:24:57.278019Z I i.q.g.SqlCodeGenerator JIT enabled for (sub)query [tableName=sys.column_versions_purge_log, fd=-1]\r\n2022-07-25T23:24:57.293596Z A http-server listening on 0.0.0.0:9000 [fd=89 backlog=256]\r\n2022-07-25T23:24:57.380040Z A server-main Min health server is starting. Health check endpoint will not consider unhandled errors when metrics are disabled.\r\n2022-07-25T23:24:57.381485Z I server-main started\r\n2022-07-25T23:24:57.381568Z I server-main os scheduled [name=questdb-worker-2]\r\n2022-07-25T23:24:57.383595Z A pg-server listening on 0.0.0.0:8812 [fd=93 backlog=10]\r\n2022-07-25T23:24:57.380360Z A http-min-server listening on 0.0.0.0:9003 [fd=91 backlog=4]\r\n```\r\n\r\nAnd then nothing happens. No process running, nothing.",
        "**CJCShadowsan**: So either this is extremely poorly documented, or the binary release flat-out doesn't work.",
        "**CJCShadowsan**: Before anyone says \"Why can't you just pull the docker container?\"\r\n\r\nThe node is airgapped from the internet, and if you provide a binary? It should work!",
        "**jerrinot**: hi @CJCShadowsan,\r\n\r\nthe RT version of QuestDB distributions does not contain questdb.jar. That's by design. If you inspect the archive, you will see a file `./lib/modules`which has approx. 20MB. This is a Java module image which contains QuestDB classes. \r\n\r\nThe logs you posted indicate QuestDB was started. QuestDB itself writes the logs. \r\nCan you please clarify this step in your reproducer? \r\n> Try run ./questdb.sh - Wonder why nothing works\r\n\r\nHow did you establish that nothing works? I just tried it on Ubuntu@x64 and it works just fine for me. Have you tried to connect to localhost:9000? There should be a web server listening on that port. ",
        "**bluestreak01**: RT is assembled by jlink [documentation](https://docs.oracle.com/javase/9/tools/jlink.htm#JSWOR-GUID-CECAC52B-CFEE-46CB-8166-F17A8E9280E9).\r\n\r\nif you need Jar, download no-jre version"
      ],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-5477eade2e69",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/868",
    "title": "Add contributors additions!",
    "text": "# Add contributors additions!\n\nAdding more contributors to the repo!\n\n## Top Comments\n\n**bsmth**: @all-contributors bot, please add @jaugsburger for significant code contributions and maintenance \r\n\r\nThanks for being part of the QuestDB journey! ðŸŽ‰ \n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/869) to add @jaugsburger! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @TheTanc for management, content and ideas \r\n\r\nThanks for being part of the QuestDB journey! ðŸŽ‰ \r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/872) to add @TheTanc! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @davidgs for filing bugs and writing great content\r\n\r\nThanks for your contributions and efforts building the community, David! ðŸŽ‰ \r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/873) to add @davidgs! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @kaishin for coding an example application \r\n\r\nThank you for the [awesome SwiftNIO client](https://github.com/swift-glide/questdb-nio) using QuestDB! ðŸŽ‰ \r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/882) to add @kaishin! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @bluestreak01 for everything from code to maintenance and testing!\r\n\r\nThanks for the dedication to keeping QuestDB superpowered! ðŸŽ‰\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/883) to add @bluestreak01! :tada:",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 191,
    "metadata": {
      "issue_number": 868,
      "state": "closed",
      "labels": [],
      "comments_count": 23,
      "created_at": "2021-03-18T16:41:43Z",
      "updated_at": "2021-04-28T09:25:37Z",
      "closed_at": "2021-04-28T09:25:37Z",
      "author": "bsmth",
      "top_comments": [
        "**bsmth**: @all-contributors bot, please add @jaugsburger for significant code contributions and maintenance \r\n\r\nThanks for being part of the QuestDB journey! ðŸŽ‰ ",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/869) to add @jaugsburger! :tada:",
        "**bsmth**: @all-contributors bot, please add @TheTanc for management, content and ideas \r\n\r\nThanks for being part of the QuestDB journey! ðŸŽ‰ \r\n",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/872) to add @TheTanc! :tada:",
        "**bsmth**: @all-contributors bot, please add @davidgs for filing bugs and writing great content\r\n\r\nThanks for your contributions and efforts building the community, David! ðŸŽ‰ \r\n"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-57ffdb6bf40b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4829",
    "title": "Automatic recovery for suspended (WAL) tables on start-up",
    "text": "# Automatic recovery for suspended (WAL) tables on start-up\n\n### Is your feature request related to a problem?\r\n\r\nWhen data is streamed to a table 24/7 and there is a power outage (or similar issue), upon resume of the server many (if not all) WAL tables are suspended due to corrupt data (I guess).\r\n\r\nAt this stage, the user needs to go an manually perform an `ALTER TABLE my_table RESUME WAL from txn 1231` \r\nAny corrupted transaction will not enter into the table, so the table will keep being suspended, unless this transaction is skipped. There is a lot of manual work or a separate script which the user needs to write in order to get the table back up and running.\r\n\r\nIt will be nice to have a feature where the user can say:\r\n- i want each table to be auto resumed\r\n- i want it to try and add any txn from WAL but skip the corrupted ones\r\n\r\n### Describe the solution you'd like.\r\n\r\nManually resume the table \r\nor\r\nmanually resume WAL from the last transaction (essentially skipping all of them and discarding them)\r\nor \r\ngoing one by one in the pending transaction until the table is fully caught up and resumed\r\n\r\n### Describe alternatives you've considered.\r\n\r\nNothing else beyond what I wrote.\r\n\r\nEither way I do not understand why a table will be suspended at all. Surely the user wants all possible good data from WAL to go into the table and the table to be back up and running. This can be automated, so the need for suspension at all is something I do not get why it exists and why it awaits user choice and intervention, since the rational choice (it seems) is always the same. \r\nI may be missing something.\r\n\r\n### Full Name:\r\n\r\nStelian Nenkov\r\n\r\n### Affiliation:\r\n\r\nWA\r\n\r\n### Additional context\r\n\r\n_No response_\n\n## Top Comments\n\n**siddharth0815**: @newskooler @nwoolmer is this issue picked already? I would like to work on it\r\nIt would be great if you could provide me with the following info\r\n- Which classes to explore for starters\r\n- Any easy way to replicate this scenario on my local machine?\r\n\n\n---\n\n**nwoolmer**: Hi @siddharth0815, here's some context.\r\n\r\nThe table can become suspended for several reasons:\r\n\r\n- database needs more RAM to execute the txn\r\n- database needs more disk\r\n- transaction cannot be applied (corrupted txn).\r\n\r\nIn the earlier two cases, the database deployment can be upgraded, and then the tables resumed. This is currently performed via [ALTER TABLE RESUME WAL](https://questdb.io/docs/reference/sql/alter-table-resume-wal/). I think the database may try to resume the tables on restart - if it doesn't, then it should, and you can add that.\r\n\r\nIn the case where the txn cannot be applied due to corruption, then it must be skipped. This is also done via a SQL command. \r\n\r\nIn @newskooler's case, he has a deployment which can be sometimes unstable, and encounters corrupted transactions. Rather than having to manually resume the table, he would like to skip as many transactions as is necessary to resume the table.\r\n\r\nPlaces to start:\r\n\r\nSqlCompilerImpl.alterTableResume\r\nTableSequencerAPI.resumeTable\r\n\r\nThose shows how tables can be resumed. \r\n\r\nThe process to resume tables should be asynchronous and executed at startup. This can be implemented using the Task and Job APIs. Tasks can be found in `io.questdb.tasks` and you can search for the job process.\r\n\r\nFor testing, you can use an undocumented function, `ALTER TABLE table_name SUSPEND WAL`. You can also run this with a message on the end i.e `WITH \"message\"`. This requires you to add the `dev.mode.enabled` property to your `server.conf`.\r\n\r\nFor testing resuming corrupted transactions, you can suspend the table, then truncate or add garbage to the `_txn` file. This should be thoroughly tested.\r\n\r\nHope this helps!\n\n---\n\n**jerrinot**: Some additional thoughts/ideas: When starting QuestDB, during the recovery process, it could check if the next transaction can be applied. If not, it could provide hints on what to do, for example, linking to https://questdb.io/docs/reference/sql/alter-table-resume-wal/.\r\n\r\nAlternatively, there could be syntax like `ALTER TABLE <table> RESUME WAL FROM TRANSACTION AUTO;` or `ALTER TABLE <table> RESUME WAL FORCE`, where QuestDB would skip all transactions that cannot be applied. Obviously, this is a drastic option, but in practice, users would likely need to skip the problematic transactions manually anyway.\n\n---\n\n**matt-kubica**: Is there any progress on that? Lack of this feature stops me from being able to effectivelu use questdb for my appliance. \r\n\r\nI want to deploy questdb in an environment with occassional power outages. I performed some test deployments and it seems that my tables get suspended from time to time because corrupted txn. For my appliance it's not a huge deal that some data get lost, however it's troublesome if I need to constantly monitor if table haven't had suspended and manually perform some actions to resume them.\r\n\r\nI was thinking about performing some automation around that, however, being able to controll that with configuration in questdb would be more welcomed.\r\n\r\nIf nobody is currently working on that, maybe I'd give it a try.\n\n---\n\n**newskooler**: Hi @matt-kubica ,\r\nI have a script running which will perform the task of auto-resuming. It's not perfect, but it does the job every time i have an outage. That's the best solution I could find until QDB team get a more native solution (which will practically be the same).\r\nHope that helps\n\n---\n\n**siddharth0815**: Hi @matt-kubica , I am working on this issue, will try to wrap it ASAP\n\n---\n\n**matt-kubica**: Thanks for confirmation @siddharth0815, looking forward to that!\r\n\r\n@newskooler would you mind sharing your solution? I need to solve this problem before QDB gets it natively, it would save me some time if I use yours :)\n\n---\n\n**siddharth0815**: \r\n@nwoolmer, thanks for clarifications, \r\n  I am following the approach told by you\r\nCouple of questions around it\r\n\r\n1. `_txn` file is decoded in some format, how to corrupt data inside it\r\n2. I json serialised the file and found it contains information about txn offsets, partitions etc, what data should I change in that so that a particular txn can be eligible for **being skipped**\r\n\r\n[sample_txn.json](https://github.com/user-attachments/files/17011041/sample_txn.json)\r\n\n\n---\n\n**siddharth0815**: @newskooler can you share zip of any broken table for testing, I'll import in my local to replicate this case\n\n---\n\n**matt-kubica**: @siddharth0815 I have couple of tables that got suspended because corrupted txn. I've compressed whole `/var/lib/questdb`. Hope it will help you. \r\n[questdb.zip](https://github.com/user-attachments/files/17034244/questdb.zip)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 1062,
    "metadata": {
      "issue_number": 4829,
      "state": "closed",
      "labels": [
        "New feature",
        "Core"
      ],
      "comments_count": 22,
      "created_at": "2024-07-29T21:49:34Z",
      "updated_at": "2024-12-18T16:30:24Z",
      "closed_at": "2024-09-24T08:34:40Z",
      "author": "newskooler",
      "top_comments": [
        "**siddharth0815**: @newskooler @nwoolmer is this issue picked already? I would like to work on it\r\nIt would be great if you could provide me with the following info\r\n- Which classes to explore for starters\r\n- Any easy way to replicate this scenario on my local machine?\r\n",
        "**nwoolmer**: Hi @siddharth0815, here's some context.\r\n\r\nThe table can become suspended for several reasons:\r\n\r\n- database needs more RAM to execute the txn\r\n- database needs more disk\r\n- transaction cannot be applied (corrupted txn).\r\n\r\nIn the earlier two cases, the database deployment can be upgraded, and then the tables resumed. This is currently performed via [ALTER TABLE RESUME WAL](https://questdb.io/docs/reference/sql/alter-table-resume-wal/). I think the database may try to resume the tables on restart - if it doesn't, then it should, and you can add that.\r\n\r\nIn the case where the txn cannot be applied due to corruption, then it must be skipped. This is also done via a SQL command. \r\n\r\nIn @newskooler's case, he has a deployment which can be sometimes unstable, and encounters corrupted transactions. Rather than having to manually resume the table, he would like to skip as many transactions as is necessary to resume the table.\r\n\r\nPlaces to start:\r\n\r\nSqlCompilerImpl.alterTableResume\r\nTableSequencerAPI.resumeTable\r\n\r\nThose shows how tables can be resumed. \r\n\r\nThe process to resume tables should be asynchronous and executed at startup. This can be implemented using the Task and Job APIs. Tasks can be found in `io.questdb.tasks` and you can search for the job process.\r\n\r\nFor testing, you can use an undocumented function, `ALTER TABLE table_name SUSPEND WAL`. You can also run this with a message on the end i.e `WITH \"message\"`. This requires you to add the `dev.mode.enabled` property to your `server.conf`.\r\n\r\nFor testing resuming corrupted transactions, you can suspend the table, then truncate or add garbage to the `_txn` file. This should be thoroughly tested.\r\n\r\nHope this helps!",
        "**jerrinot**: Some additional thoughts/ideas: When starting QuestDB, during the recovery process, it could check if the next transaction can be applied. If not, it could provide hints on what to do, for example, linking to https://questdb.io/docs/reference/sql/alter-table-resume-wal/.\r\n\r\nAlternatively, there could be syntax like `ALTER TABLE <table> RESUME WAL FROM TRANSACTION AUTO;` or `ALTER TABLE <table> RESUME WAL FORCE`, where QuestDB would skip all transactions that cannot be applied. Obviously, this is a drastic option, but in practice, users would likely need to skip the problematic transactions manually anyway.",
        "**matt-kubica**: Is there any progress on that? Lack of this feature stops me from being able to effectivelu use questdb for my appliance. \r\n\r\nI want to deploy questdb in an environment with occassional power outages. I performed some test deployments and it seems that my tables get suspended from time to time because corrupted txn. For my appliance it's not a huge deal that some data get lost, however it's troublesome if I need to constantly monitor if table haven't had suspended and manually perform some actions to resume them.\r\n\r\nI was thinking about performing some automation around that, however, being able to controll that with configuration in questdb would be more welcomed.\r\n\r\nIf nobody is currently working on that, maybe I'd give it a try.",
        "**newskooler**: Hi @matt-kubica ,\r\nI have a script running which will perform the task of auto-resuming. It's not perfect, but it does the job every time i have an outage. That's the best solution I could find until QDB team get a more native solution (which will practically be the same).\r\nHope that helps"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-d35a8de76ef3",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1361",
    "title": "use tcp flow to  write data to questdb database with 800 tables  will out of memory",
    "text": "# use tcp flow to  write data to questdb database with 800 tables  will out of memory\n\nThere are 800 tables in questdb databases,my system collect about 50,000 row per second.\r\nthe questDB can only run about 30 minutes,used  memory of the quest process will soon up to 180GB.\r\nthe server.conf is default\r\nthe OS:\r\nCentos7\r\nthe hardware is:\r\nIntel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz\r\n196GB memory \r\nnvme 3T *2\r\nthe same data will ealy write to influxdb in same server.\r\n\n\n## Top Comments\n\n**bluestreak01**: We have memory telemetry in the latest (6.0.6) release. It would be useful to obtain that if thatâ€™s the version you are using\n\n---\n\n**lg31415**: Before i used 6.0.5 can get this problem,Now the version update to 6.0.6,there also have this problemï¼Œhow to open the the new versionâ€˜s memory telemetry function.\n\n---\n\n**ideoma**: It is very primitive as of now, you can do `select dump_memory_usage()` and it will dump some memory breakdown to log file. \r\nFrom top of my head I'd suggest decreasing `cairo.inactive.writer.ttl` in server.conf to something ridiculously low > 0, like 1.\r\n\r\nHow many columns are in total approximately in those 800 tables? Does QuestDB crash or log errors after the period of time?\n\n---\n\n**lg31415**: > It is very primitive as of now, you can do `select dump_memory_usage()` and it will dump some memory breakdown to log file.\r\n> From top of my head I'd suggest decreasing `cairo.inactive.writer.ttl` in server.conf to something ridiculously low > 0, like 1.\r\n> \r\n> How many columns are in total approximately in those 800 tables? Does QuestDB crash or log errors after the period of time?\r\n\r\nevery table of these 800 tables have 12~14 columns ,so total is about 10,000 columns.\r\nThe memory used by QuestDB  whill increase to  server's max size ,and the os system will  crash by  out of memory .\r\n\r\nwhen set cairo.inactive.writer.ttl=1ï¼Œand runing for 1 hour minites the memory breakdown is:\r\n\r\n``` \r\n2021-09-28T03:57:25.517204Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=613818856782, MMAP_DEFAULT=500766149230, NATIVE_DEFAULT=61178912, MMAP_O3=814237024, NATIVE_O3=56237228128, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=5909632, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:03:59.413081Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=539055661298, MMAP_DEFAULT=434370885110, NATIVE_DEFAULT=62891612, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=44143419392, MMAP_TABLE_READER=667584, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:04:17.585273Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=541281975447, MMAP_DEFAULT=434936179662, NATIVE_DEFAULT=62901037, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=45804429312, MMAP_TABLE_READER=667836, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:04:41.556775Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=542998209418, MMAP_DEFAULT=435728999010, NATIVE_DEFAULT=63527748, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=46727217152, MMAP_TABLE_READER=667908, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:05:15.712846Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=546168380234, MMAP_DEFAULT=437959130796, NATIVE_DEFAULT=63993494, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=47666790400, MMAP_TABLE_READER=667944, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:05:38.639395Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=553918038391, MMAP_DEFAULT=444165105068, NATIVE_DEFAULT=64107935, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=49210359808, MMAP_TABLE_READER=667980, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:26.669517Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=580835555121, MMAP_DEFAULT=464801622474, NATIVE_DEFAULT=65737963, MMAP_O3=1030340400, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:33.165831Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=591147162737, MMAP_DEFAULT=474932120730, NATIVE_DEFAULT=65770731, MMAP_O3=461954016, NATIVE_O3=59134174752, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:39.317937Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=570695864937, MMAP_DEFAULT=455692207154, NATIVE_DEFAULT=65803499, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:08:25.014204Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=556681524729, MMAP_DEFAULT=448886525576, NATIVE_DEFAULT=66963589, MMAP_O3=2373969776, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=44260859904, MMAP_TABLE_READER=539910812, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:10:11.923886Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=560262885408, MMAP_DEFAULT=449828940748, NATIVE_DEFAULT=68574592, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=49260691456, MMAP_TABLE_READER=551383540, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:09.594723Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=583313503004, MMAP_DEFAULT=471189447356, NATIVE_DEFAULT=68580496, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:15.425718Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=582507228860, MMAP_DEFAULT=467802655004, NATIVE_DEFAULT=68580496, MMAP_O3=2580518208, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:23.644664Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=566956429004, MMAP_DEFAULT=453477442548, NATIVE_DEFAULT=68580496, MMAP_O3=518107048, NATIVE_O3=59221535536, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:31:39.849557Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=622431751785, MMAP_DEFAULT=506963198148, NATIVE_DEFAULT=90622449, NATIVE_O3=60330868832, MMAP_TABLE_WRITER=52683390976, MMAP_TABLE_READER=119590612, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:35:44.826304Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=608164243531, MMAP_DEFAULT=486770425028, NATIVE_DEFAULT=95534843, MMAP_O3=5195908804, NATIVE_O3=61874372704, MMAP_TABLE_WRITER=50418368512, MMAP_TABLE_READER=1452306664, NATIVE_FAST_MAP=29360128, NATIVE_LONG_LIST=234881024, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:37:54.247800Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=632862167305, MMAP_DEFAULT=508901752794, NATIVE_DEFAULT=98227691, MMAP_O3=4486195552, NATIVE_O3=61874372704, MMAP_TABLE_WRITER=53690064896, MMAP_TABLE_READER=1454226692, NATIVE_FAST_MAP=29360128, NATIVE_LONG_LIST=234881024, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:56:54.510438Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=611077590409, MMAP_DEFAULT=487294147410, NATIVE_DEFAULT=130691687, MMAP_O3=8162101864, NATIVE_O3=60532195424, MMAP_TABLE_WRITER=51961937920, MMAP_TABLE_READER=525942920, NATIVE_FAST_MAP=41943040, NATIVE_LONG_LIST=335544320, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:58:54.169652Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=573365584514, MMAP_DEFAULT=455113669544, NATIVE_DEFAULT=132664022, MMAP_O3=6559504252, NATIVE_O3=58988691552, MMAP_TABLE_WRITER=46911774720, MMAP_TABLE_READER=3037712296, NATIVE_FAST_MAP=58720256, NATIVE_LONG_LIST=469762048, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:22:22.652336Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=604091101501, MMAP_DEFAULT=460081959292, NATIVE_DEFAULT=145365337, MMAP_O3=3225021232, NATIVE_O3=81352720480, MMAP_TABLE_WRITER=51508928512, MMAP_TABLE_READER=5004543576, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:22:48.882525Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=602017541061, MMAP_DEFAULT=460081959292, NATIVE_DEFAULT=145365337, MMAP_O3=1151460792, NATIVE_O3=81352720480, MMAP_TABLE_WRITER=51508928512, MMAP_TABLE_READER=5004543576, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:23:52.238073Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=572129991719, MMAP_DEFAULT=434400650864, NATIVE_DEFAULT=145373471, MMAP_O3=1347643120, NATIVE_O3=82522162160, MMAP_TABLE_WRITER=47129886720, MMAP_TABLE_READER=3811712312, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:24:07.921045Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=572129991719, MMAP_DEFAULT=434400650864, NATIVE_DEFAULT=145373471, MMAP_O3=1347643120, NATIVE_O3=82522162160, MMAP_TABLE_WRITER=47129886720, MMAP_TABLE_READER=3811712312, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n```\r\nthe used memory is 180GB\r\n```\r\n[root@sh-tsdb8 conf]# ps -aux |grep 37045\r\nroot     37045  723 93.3 760367684 183593620 pts/3 Sl 11:43 733:06 ./java -DQuestDB-Runtime-66535 -ea -Dnoebug -XX:+UnlockExperimentalVMOptions -XX:+AlwaysPreTouch -XX:+UseParallelOldGC -Xms16G -Xmx16G -XX:MaxHeapFreeRatio=40 -p ./questdb.jar -m io.questdb/io.questdb.ServerMain -d /srv/1/questdb\r\n[root@sh-tsdb8 conf]# \r\n```\r\nthis is top command  resultï¼š\r\n![image](https://user-images.githubusercontent.com/3609384/135028313-d707b428-124c-4602-a980-8a131500f233.png)\r\n\r\n\r\n\n\n---\n\n**ideoma**: The default configuration allocates around 32+Mb of RAM per column. In your case of 10k columns it results to 320Gb total RAM reserved for writing only.\r\n\r\nIt is excessive and we can do few thing to cut the memory usage, especially in the case when there are many small tables to write. Unfortunately atm the buffer sizes for writing are hard coded \r\n\r\nI'm opening a feature request to make this configurable\n\n---\n\n**ideoma**: @lg31415 Thanks for submitting the issue, it was very useful to realise that the memory usage is far from optimal. Sorry I couldn't provide you an immediate solution or workaround\n\n---\n\n**lg31415**: for test this 320GB memory  way can improve my problemï¼ŒI change my server with 396GB memory.\r\nafter  running for 1 hour, the questDB seem that lost some data from by tcp input when the memory has used 265GB.\r\n![image](https://user-images.githubusercontent.com/3609384/135391400-8510719a-f99c-4d4b-8f6f-9cfe201c21a4.png)\r\n\r\nat last the memroy used is 337GB,this time the questDB  also input some data.\r\n![image](https://user-images.githubusercontent.com/3609384/135392210-f1392bd6-f021-4857-884e-69180de814eb.png)\r\n\r\n![image](https://user-images.githubusercontent.com/3609384/135391630-a6a210b0-cf98-40e5-8824-9eb2f5940902.png)\r\n\r\nit seem  strange, there no quest by client  at all ,but when write data to disk there also some read data from disk .\r\n![image](https://user-images.githubusercontent.com/3609384/135392045-9308c0be-fbf1-4800-ba7e-363b91aa35ed.png)\r\n\r\nthe dump_memory_usage result is \r\n\r\n```\r\n2021-09-30T04:01:11.898451Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=486548146521, MMAP_DEFAULT=350947699586, NATIVE_DEFAULT=116957794, MMAP_O3=7614161656, NATIVE_O3=69021466752, MMAP_TABLE_WRITER=52951842816, MMAP_TABLE_READER=2192276605, NATIVE_FAST_MAP=96468992, NATIVE_LONG_LIST=771751936, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:01:12.564887Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=478933984865, MMAP_DEFAULT=350947699586, NATIVE_DEFAULT=116957794, NATIVE_O3=69021466752, MMAP_TABLE_WRITER=52951842816, MMAP_TABLE_READER=2192276605, NATIVE_FAST_MAP=96468992, NATIVE_LONG_LIST=771751936, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:01:35.167850Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=489673316374, MMAP_DEFAULT=359274937256, NATIVE_DEFAULT=116966633, MMAP_O3=1304654440, NATIVE_O3=69021466752, MMAP_TABLE_WRITER=54059188224, MMAP_TABLE_READER=2192361757, NATIVE_FAST_MAP=96468992, NATIVE_LONG_LIST=771751936, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:03:25.084428Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=507124682309, MMAP_DEFAULT=368269127936, NATIVE_DEFAULT=117009908, MMAP_O3=9264448264, NATIVE_O3=69021466752, MMAP_TABLE_WRITER=54243745792, MMAP_TABLE_READER=2429644873, NATIVE_FAST_MAP=104857600, NATIVE_LONG_LIST=838860800, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:06:35.154807Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=483339561170, MMAP_DEFAULT=354850136852, NATIVE_DEFAULT=117684450, NATIVE_O3=67947724928, MMAP_TABLE_WRITER=49562689536, MMAP_TABLE_READER=6704599260, NATIVE_FAST_MAP=146800640, NATIVE_LONG_LIST=1174405120, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:07:13.623791Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=489448878320, MMAP_DEFAULT=350501317824, NATIVE_DEFAULT=118472624, MMAP_O3=9449955652, NATIVE_O3=68954357888, MMAP_TABLE_WRITER=49562689536, MMAP_TABLE_READER=6705358652, NATIVE_FAST_MAP=146800640, NATIVE_LONG_LIST=1174405120, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:41:11.921886Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=476463620352, MMAP_DEFAULT=344022043478, NATIVE_DEFAULT=121586286, MMAP_O3=74604720, NATIVE_O3=75564580992, MMAP_TABLE_WRITER=49831141376, MMAP_TABLE_READER=2353198732, NATIVE_FAST_MAP=184549376, NATIVE_LONG_LIST=1476395008, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:41:29.658174Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=478391165089, MMAP_DEFAULT=346024006238, NATIVE_DEFAULT=121587459, NATIVE_O3=75564580992, MMAP_TABLE_WRITER=49831141376, MMAP_TABLE_READER=2353384256, NATIVE_FAST_MAP=184549376, NATIVE_LONG_LIST=1476395008, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T04:41:46.212837Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=479768226945, MMAP_DEFAULT=345856943582, NATIVE_DEFAULT=121587459, MMAP_O3=1544080032, NATIVE_O3=75564580992, MMAP_TABLE_WRITER=49831141376, MMAP_TABLE_READER=2353428736, NATIVE_FAST_MAP=184549376, NATIVE_LONG_LIST=1476395008, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T05:17:02.809930Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=493515139159, MMAP_DEFAULT=353157665216, NATIVE_DEFAULT=121695007, NATIVE_O3=82879447168, MMAP_TABLE_WRITER=49059348480, MMAP_TABLE_READER=3725021048, NATIVE_FAST_MAP=192937984, NATIVE_LONG_LIST=1543503872, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T05:17:04.230731Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=493246532531, MMAP_DEFAULT=352889058588, NATIVE_DEFAULT=121695007, NATIVE_O3=82879447168, MMAP_TABLE_WRITER=49059348480, MMAP_TABLE_READER=3725021048, NATIVE_FAST_MAP=192937984, NATIVE_LONG_LIST=1543503872, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n2021-09-30T05:24:57.521490Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=485200179311, MMAP_DEFAULT=346011046738, NATIVE_DEFAULT=121611769, MMAP_O3=298810016, NATIVE_O3=82476793984, MMAP_TABLE_WRITER=51626369024, MMAP_TABLE_READER=93585540, NATIVE_FAST_MAP=192937984, NATIVE_LONG_LIST=1543503872, NATIVE_HTTP_CONN=822254464, NATIVE_PGW_CONN=2013265920]\r\n```\r\n\r\n\r\n\n\n---\n\n**bluestreak01**: Thanks a lot for this information. Could we drill down into data loss please?\r\n\r\nCould you post errors from questdb log? Grep for â€˜ E â€˜\n\n---\n\n**bluestreak01**: How do you run questdb? Docker or binary?\n\n---\n\n**lg31415**: > Thanks a lot for this information. Could we drill down into data loss please?\r\n> \r\n> Could you post errors from questdb log? Grep for â€˜ E â€˜\r\n\r\nAll Error log is:\r\n\r\n```\r\n2021-09-30T02:09:09.847477Z E i.q.TelemetryJob instance [id=0x05cd2cebd45efa000003150609f2b4, enabled=true]\r\n2021-09-30T02:09:11.818709Z E i.q.c.l.t.LineTcpConnectionContext [8047] could not process line data\r\n2021-09-30T02:09:11.979278Z E i.q.c.l.t.LineTcpConnectionContext [29712] could not process line data\r\n2021-09-30T02:09:13.580145Z E i.q.c.l.t.LineTcpConnectionContext [52234] could not process line data\r\n2021-09-30T02:09:13.777086Z E i.q.c.l.t.LineTcpConnectionContext [52249] could not process line data\r\n2021-09-30T02:09:13.906945Z E i.q.c.l.t.LineTcpConnectionContext [52257] could not process line data\r\n2021-09-30T02:09:17.617787Z E i.q.c.l.t.LineTcpConnectionContext [64007] could not process line data\r\n2021-09-30T02:09:18.697859Z E i.q.c.l.t.LineTcpConnectionContext [64156] could not process line data\r\n2021-09-30T02:09:22.872760Z E i.q.c.l.t.LineTcpConnectionContext [65892] could not process line data\r\n2021-09-30T02:09:22.882196Z E i.q.c.l.t.LineTcpConnectionContext [65786] could not process line data\r\n2021-09-30T02:09:22.922344Z E i.q.c.l.t.LineTcpConnectionContext [65890] could not process line data\r\n2021-09-30T02:09:22.985102Z E i.q.c.l.t.LineTcpConnectionContext [65893] could not process line data\r\n2021-09-30T02:09:23.222783Z E i.q.c.l.t.LineTcpConnectionContext [66625] could not process line data\r\n2021-09-30T02:09:23.360938Z E i.q.c.l.t.LineTcpConnectionContext [67008] could not process line data\r\n2021-09-30T02:09:23.460805Z E i.q.c.l.t.LineTcpConnectionContext [65893] could not process line data\r\n2021-09-30T02:09:23.503431Z E i.q.c.l.t.LineTcpConnectionContext [68021] could not process line data\r\n2021-09-30T02:09:23.609420Z E i.q.c.l.t.LineTcpConnectionContext [67310] could not process line data\r\n2021-09-30T02:09:32.953613Z E i.q.c.l.t.LineTcpConnectionContext [72997] could not process line data\r\n2021-09-30T02:10:04.390222Z E i.q.c.l.t.LineTcpConnectionContext [79459] could not process line data\r\n2021-09-30T02:10:04.621068Z E i.q.c.l.t.LineTcpConnectionContext [79314] could not process line data\r\n2021-09-30T02:10:05.239658Z E i.q.c.l.t.LineTcpConnectionContext [79559] could not process line data\r\n2021-09-30T02:10:05.255187Z E i.q.c.l.t.LineTcpConnectionContext [79800] could not process line data\r\n2021-09-30T02:10:57.007830Z E i.q.c.l.t.LineTcpConnectionContext [71718] could not process line data\r\n2021-09-30T02:11:49.244596Z E i.q.c.l.t.LineTcpConnectionContext [622] could not process line data\r\n2021-09-30T02:12:50.866865Z E i.q.c.l.t.LineTcpConnectionContext [39955] could not process line data\r\n2021-09-30T02:12:55.038550Z E i.q.c.l.t.LineTcpConnectionContext [64336] could not process line data\r\n2021-09-30T02:16:54.634291Z E i.q.c.l.t.LineTcpConnectionContext [57845] could not process line data\r\n2021-09-30T02:19:04.914928Z E i.q.c.l.t.LineTcpConnectionContext [70941] could not process line data\r\n2021-09-30T02:28:08.361601Z E i.q.c.l.t.LineTcpConnectionContext [77291] could not process line data\r\n2021-09-30T02:33:54.335948Z E i.q.c.l.t.LineTcpConnectionContext [2114] could not process line data\r\n2021-09-30T02:43:24.878493Z E i.q.c.l.t.LineTcpConnectionContext [81044] could not process line data\r\n2021-09-30T02:58:36.042916Z E i.q.c.l.t.LineTcpConnectionContext [58839] could not process line data\r\n2021-09-30T03:06:30.116145Z E i.q.c.l.t.LineTcpConnectionContext [77177] could not process line data\r\n2021-09-30T03:20:56.663414Z E i.q.c.l.t.LineTcpConnectionContext [22291] could not process line data\r\n2021-09-30T03:23:15.983967Z E i.q.c.l.t.LineTcpConnectionContext [45159] could not process line data\r\n2021-09-30T03:36:18.411071Z E i.q.c.O3PurgeDiscoveryJob queuing [table=cdn.bus.sink.flow.width, ts=2021-09-30T00:00:00.000000Z, txn=863280, errno=-1]\r\n2021-09-30T03:43:46.700868Z E i.q.c.l.t.LineTcpConnectionContext [7495] could not process line data\r\n2021-09-30T03:59:48.303583Z E i.q.c.l.t.LineTcpConnectionContext [36420] could not parse measurement, code NO_FIELDS at 5 line (may be mangled due to partial parsing) is \u0006\r\n2021-09-30T04:08:48.462165Z E i.q.c.p.PGConnectionContext error [pos=7, msg=`not enough columns in group by`, errno=`0]\r\n2021-09-30T04:09:00.152725Z E i.q.c.p.PGConnectionContext error [pos=7, msg=`not enough columns in group by`, errno=`0]\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 1546,
    "metadata": {
      "issue_number": 1361,
      "state": "closed",
      "labels": [],
      "comments_count": 22,
      "created_at": "2021-09-26T14:17:37Z",
      "updated_at": "2023-03-24T07:06:31Z",
      "closed_at": "2023-03-23T11:26:29Z",
      "author": "lg31415",
      "top_comments": [
        "**bluestreak01**: We have memory telemetry in the latest (6.0.6) release. It would be useful to obtain that if thatâ€™s the version you are using",
        "**lg31415**: Before i used 6.0.5 can get this problem,Now the version update to 6.0.6,there also have this problemï¼Œhow to open the the new versionâ€˜s memory telemetry function.",
        "**ideoma**: It is very primitive as of now, you can do `select dump_memory_usage()` and it will dump some memory breakdown to log file. \r\nFrom top of my head I'd suggest decreasing `cairo.inactive.writer.ttl` in server.conf to something ridiculously low > 0, like 1.\r\n\r\nHow many columns are in total approximately in those 800 tables? Does QuestDB crash or log errors after the period of time?",
        "**lg31415**: > It is very primitive as of now, you can do `select dump_memory_usage()` and it will dump some memory breakdown to log file.\r\n> From top of my head I'd suggest decreasing `cairo.inactive.writer.ttl` in server.conf to something ridiculously low > 0, like 1.\r\n> \r\n> How many columns are in total approximately in those 800 tables? Does QuestDB crash or log errors after the period of time?\r\n\r\nevery table of these 800 tables have 12~14 columns ,so total is about 10,000 columns.\r\nThe memory used by QuestDB  whill increase to  server's max size ,and the os system will  crash by  out of memory .\r\n\r\nwhen set cairo.inactive.writer.ttl=1ï¼Œand runing for 1 hour minites the memory breakdown is:\r\n\r\n``` \r\n2021-09-28T03:57:25.517204Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=613818856782, MMAP_DEFAULT=500766149230, NATIVE_DEFAULT=61178912, MMAP_O3=814237024, NATIVE_O3=56237228128, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=5909632, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:03:59.413081Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=539055661298, MMAP_DEFAULT=434370885110, NATIVE_DEFAULT=62891612, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=44143419392, MMAP_TABLE_READER=667584, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:04:17.585273Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=541281975447, MMAP_DEFAULT=434936179662, NATIVE_DEFAULT=62901037, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=45804429312, MMAP_TABLE_READER=667836, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:04:41.556775Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=542998209418, MMAP_DEFAULT=435728999010, NATIVE_DEFAULT=63527748, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=46727217152, MMAP_TABLE_READER=667908, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:05:15.712846Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=546168380234, MMAP_DEFAULT=437959130796, NATIVE_DEFAULT=63993494, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=47666790400, MMAP_TABLE_READER=667944, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:05:38.639395Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=553918038391, MMAP_DEFAULT=444165105068, NATIVE_DEFAULT=64107935, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=49210359808, MMAP_TABLE_READER=667980, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:26.669517Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=580835555121, MMAP_DEFAULT=464801622474, NATIVE_DEFAULT=65737963, MMAP_O3=1030340400, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:33.165831Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=591147162737, MMAP_DEFAULT=474932120730, NATIVE_DEFAULT=65770731, MMAP_O3=461954016, NATIVE_O3=59134174752, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:07:39.317937Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=570695864937, MMAP_DEFAULT=455692207154, NATIVE_DEFAULT=65803499, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=53841068032, MMAP_TABLE_READER=543491180, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:08:25.014204Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=556681524729, MMAP_DEFAULT=448886525576, NATIVE_DEFAULT=66963589, MMAP_O3=2373969776, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=44260859904, MMAP_TABLE_READER=539910812, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:10:11.923886Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=560262885408, MMAP_DEFAULT=449828940748, NATIVE_DEFAULT=68574592, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=49260691456, MMAP_TABLE_READER=551383540, NATIVE_FAST_MAP=8388608, NATIVE_LONG_LIST=67108864, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:09.594723Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=583313503004, MMAP_DEFAULT=471189447356, NATIVE_DEFAULT=68580496, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:15.425718Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=582507228860, MMAP_DEFAULT=467802655004, NATIVE_DEFAULT=68580496, MMAP_O3=2580518208, NATIVE_O3=58384711776, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:11:23.644664Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=566956429004, MMAP_DEFAULT=453477442548, NATIVE_DEFAULT=68580496, MMAP_O3=518107048, NATIVE_O3=59221535536, MMAP_TABLE_WRITER=50804260864, MMAP_TABLE_READER=622421744, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:31:39.849557Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=622431751785, MMAP_DEFAULT=506963198148, NATIVE_DEFAULT=90622449, NATIVE_O3=60330868832, MMAP_TABLE_WRITER=52683390976, MMAP_TABLE_READER=119590612, NATIVE_FAST_MAP=16777216, NATIVE_LONG_LIST=134217728, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:35:44.826304Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=608164243531, MMAP_DEFAULT=486770425028, NATIVE_DEFAULT=95534843, MMAP_O3=5195908804, NATIVE_O3=61874372704, MMAP_TABLE_WRITER=50418368512, MMAP_TABLE_READER=1452306664, NATIVE_FAST_MAP=29360128, NATIVE_LONG_LIST=234881024, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:37:54.247800Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=632862167305, MMAP_DEFAULT=508901752794, NATIVE_DEFAULT=98227691, MMAP_O3=4486195552, NATIVE_O3=61874372704, MMAP_TABLE_WRITER=53690064896, MMAP_TABLE_READER=1454226692, NATIVE_FAST_MAP=29360128, NATIVE_LONG_LIST=234881024, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:56:54.510438Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=611077590409, MMAP_DEFAULT=487294147410, NATIVE_DEFAULT=130691687, MMAP_O3=8162101864, NATIVE_O3=60532195424, MMAP_TABLE_WRITER=51961937920, MMAP_TABLE_READER=525942920, NATIVE_FAST_MAP=41943040, NATIVE_LONG_LIST=335544320, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T04:58:54.169652Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=573365584514, MMAP_DEFAULT=455113669544, NATIVE_DEFAULT=132664022, MMAP_O3=6559504252, NATIVE_O3=58988691552, MMAP_TABLE_WRITER=46911774720, MMAP_TABLE_READER=3037712296, NATIVE_FAST_MAP=58720256, NATIVE_LONG_LIST=469762048, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:22:22.652336Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=604091101501, MMAP_DEFAULT=460081959292, NATIVE_DEFAULT=145365337, MMAP_O3=3225021232, NATIVE_O3=81352720480, MMAP_TABLE_WRITER=51508928512, MMAP_TABLE_READER=5004543576, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:22:48.882525Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=602017541061, MMAP_DEFAULT=460081959292, NATIVE_DEFAULT=145365337, MMAP_O3=1151460792, NATIVE_O3=81352720480, MMAP_TABLE_WRITER=51508928512, MMAP_TABLE_READER=5004543576, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:23:52.238073Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=572129991719, MMAP_DEFAULT=434400650864, NATIVE_DEFAULT=145373471, MMAP_O3=1347643120, NATIVE_O3=82522162160, MMAP_TABLE_WRITER=47129886720, MMAP_TABLE_READER=3811712312, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n2021-09-28T05:24:07.921045Z I i.q.g.e.f.c.DumpMemoryUsageFunctionFactory$DumpMemoryUsage memory usage [TOTAL=572129991719, MMAP_DEFAULT=434400650864, NATIVE_DEFAULT=145373471, MMAP_O3=1347643120, NATIVE_O3=82522162160, MMAP_TABLE_WRITER=47129886720, MMAP_TABLE_READER=3811712312, NATIVE_FAST_MAP=75497472, NATIVE_LONG_LIST=603979776, NATIVE_HTTP_CONN=616690816, NATIVE_PGW_CONN=1476395008]\r\n```\r\nthe used memory is 180GB\r\n```\r\n[root@sh-tsdb8 conf]# ps -aux |grep 37045\r\nroot     37045  723 93.3 760367684 183593620 pts/3 Sl 11:43 733:06 ./java -DQuestDB-Runtime-66535 -ea -Dnoebug -XX:+UnlockExperimentalVMOptions -XX:+AlwaysPreTouch -XX:+UseParallelOldGC -Xms16G -Xmx16G -XX:MaxHeapFreeRatio=40 -p ./questdb.jar -m io.questdb/io.questdb.ServerMain -d /srv/1/questdb\r\n[root@sh-tsdb8 conf]# \r\n```\r\nthis is top command  resultï¼š\r\n![image](https://user-images.githubusercontent.com/3609384/135028313-d707b428-124c-4602-a980-8a131500f233.png)\r\n\r\n\r\n",
        "**ideoma**: The default configuration allocates around 32+Mb of RAM per column. In your case of 10k columns it results to 320Gb total RAM reserved for writing only.\r\n\r\nIt is excessive and we can do few thing to cut the memory usage, especially in the case when there are many small tables to write. Unfortunately atm the buffer sizes for writing are hard coded \r\n\r\nI'm opening a feature request to make this configurable"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-52afd590e855",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5202",
    "title": "PostgreSQL client interface errors",
    "text": "# PostgreSQL client interface errors\n\n### To reproduce\n\nTo create and init a sample table:\r\n```sql\r\nCREATE TABLE bugtest(\r\n  timestamp TIMESTAMP,\r\n  symbol SYMBOL,\r\n  price DOUBLE,\r\n  amount DOUBLE\r\n  ) TIMESTAMP(timestamp)\r\nPARTITION BY DAY;\r\n\r\nINSERT INTO bugtest\r\nVALUES\r\n    ('2021-10-05T11:31:35.878Z', 'AAPL', 245, 123.4),\r\n    ('2021-10-05T12:31:35.878Z', 'AAPL', 245, 123.3),\r\n    ('2021-10-05T13:31:35.878Z', 'AAPL', 250, 123.1),\r\n    ('2021-10-05T14:31:35.878Z', 'AAPL', 250, 123.0);\r\n```\r\n\r\nThe problem appears sometime, randomly, when query from a client using PostgreSQL driver (exposed at port 8812 in the docker image). Sometimes I get no data or seemingly corrupted data (at least with more elaborate queries), the other times I get the correct answer.\r\n\r\nI tried CLI tool \"psql\" from an ubuntu machine many times:\r\n```bash\r\nwhile :\r\ndo\r\n    psql -d \"$ConnectionString\" -c \"SELECT 1 FROM bugtest\"\r\ndone\r\n```\r\nI also tried python driver from a windows machine (conda installation) with the following script:\r\n```python\r\n# script.py\r\nimport os\r\nimport psycopg2\r\n\r\nquest_connection_string = os.environ.get(\"ConnectionString\")\r\nquery = \"SELECT 1 FROM bugtest\"\r\n\r\nwith psycopg2.connect(quest_connection_string) as conn:\r\n    with conn.cursor() as cursor:\r\n        cursor.execute(query)\r\n        response = cursor.fetchall()\r\n        print(response)\r\n```\r\nagain, the effect is visible only sometimes, therefore I do:\r\n```bash\r\nwhile :\r\ndo\r\n    python script.py\r\ndone\r\n```\r\n\r\n### Additional info\r\n\r\n- The query `SELECT 1 FROM bugtest` seems the simplest I could get to fail. Strangely, if I query `SELECT 'x' FROM bugtest` I never found any weirdness.\r\n- The web interface, the one exposed at port 9000 in the docker image, never seems to give any weirdness.\r\n- The log of the server does not register any error. It even cache the query regularly without anything strange.\r\n- I think I got this error with the latest questDB version, although I am not able to prove it did not existed before.\r\n\r\nCould this be a bug of the questDB server?\r\n\r\n\n\n### QuestDB version:\n\n8.2.0\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 24.04 (docker)\n\n### File System, in case of Docker specify Host File System:\n\nzfs\n\n### Full Name:\n\nMarco Trevisiol\n\n### Affiliation:\n\nDeimos\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_\n\n## Top Comments\n\n**enricobenedos**: @mtopolnik, resuming from Slack, we tried with a cleaned instance without using ZFS and we aren't able to reproduce it. \r\nSystem wasn't in idle but it was with low load. Configuration is the default one. But our production instance have been updated more times since the installation.\n\n---\n\n**mtopolnik**: We solved a number of bugs related to PGWire since the release of 8.2.0. I suggest you upgrade to that new version once it's out, and see if it occurs again. If it does, the fast remedy to try is the configuration option `pg.legacy.mode.enabled=true`. It would be of immense value to know if this config change removed the problem, since it would narrow down its origin to the new PGWire code.\n\n---\n\n**bluestreak01**: i can repro the issue, will keep you posted\n\n---\n\n**enricobenedos**: @mtopolnik did you mean something different from `8.2.0`? Because that's exactly the one we were using with the bug. Downgrading it fixed with no issue. Are there incoming new releases?\n\n@bluestreak01 it would be perfect\n\n---\n\n**bluestreak01**: sorry guys, i jumped ahead of myself here. I used a tool that tampers with network packets, which produced corrupt message. I mistakenly thought that we produced corrupt package, but instead it was a tamper tool.\r\n\r\nMay I ask you to send us questdb logs, around the event this error was occurring? The link with zfs is hard to reconcile looking into network protocol alone. I am hoping for a clue in your log really.\r\n\n\n---\n\n**enricobenedos**: Unfortunately we lost them once removed and restarted with the old version.\n\n---\n\n**enricobenedos**: But this morning we update another time to `8.2.0` and we are able to reproduce it.\r\n\r\nThe log since start of new `8.2.0` container: [questdb_error.log](https://github.com/user-attachments/files/17944215/questdb_error.log)\r\n\r\nBy us there aren't logs that show something wrong but the problem exists.\r\n\n\n---\n\n**mtopolnik**: @enricobenedos Yes, I meant the upcoming release that includes several bugfixes related to PGWire.\r\n\r\n>  this morning we update another time to 8.2.0 and we are able to reproduce it.\r\n\r\nCan you please apply the config option `pg.legacy.mode.enabled=true` to see if it goes away? Thank you for the logs!\n\n---\n\n**MarcoTrevisiol**: This is a screenshot of what I am experiencing today\r\n![image](https://github.com/user-attachments/assets/4be81f3b-e603-443f-85ea-1be33eca2725)\r\n\r\nStrangely, it seems like the frequency of this weird behaviour has decrease since yesterday, so that I took me several attempts to record one incident.\r\n\n\n---\n\n**enricobenedos**: @mtopolnik we applied that. Now we need some testing time.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 762,
    "metadata": {
      "issue_number": 5202,
      "state": "closed",
      "labels": [
        "Bug",
        "Postgres Wire"
      ],
      "comments_count": 21,
      "created_at": "2024-11-27T11:38:17Z",
      "updated_at": "2024-12-16T14:26:32Z",
      "closed_at": "2024-12-16T14:26:32Z",
      "author": "MarcoTrevisiol",
      "top_comments": [
        "**enricobenedos**: @mtopolnik, resuming from Slack, we tried with a cleaned instance without using ZFS and we aren't able to reproduce it. \r\nSystem wasn't in idle but it was with low load. Configuration is the default one. But our production instance have been updated more times since the installation.",
        "**mtopolnik**: We solved a number of bugs related to PGWire since the release of 8.2.0. I suggest you upgrade to that new version once it's out, and see if it occurs again. If it does, the fast remedy to try is the configuration option `pg.legacy.mode.enabled=true`. It would be of immense value to know if this config change removed the problem, since it would narrow down its origin to the new PGWire code.",
        "**bluestreak01**: i can repro the issue, will keep you posted",
        "**enricobenedos**: @mtopolnik did you mean something different from `8.2.0`? Because that's exactly the one we were using with the bug. Downgrading it fixed with no issue. Are there incoming new releases?\n\n@bluestreak01 it would be perfect",
        "**bluestreak01**: sorry guys, i jumped ahead of myself here. I used a tool that tampers with network packets, which produced corrupt message. I mistakenly thought that we produced corrupt package, but instead it was a tamper tool.\r\n\r\nMay I ask you to send us questdb logs, around the event this error was occurring? The link with zfs is hard to reconcile looking into network protocol alone. I am hoping for a clue in your log really.\r\n"
      ],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-2418c64889fd",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2884",
    "title": "Table list query doesn't handle symlinks",
    "text": "# Table list query doesn't handle symlinks\n\n### Describe the bug\n\n_No response_\n\n### To reproduce\n\n1. Create a table\r\n2. Stop the database and move table dir to a different location on the same or a separate disk\r\n3. Create a symlink in `root_dir/db/table_name` pointing to the new table dir location\r\n4. Start the database and run `tables();`\n\n### Expected Behavior\n\nThe table should be shown in the result set while it's not the case.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.6.1\r\n- **OS**: Ubuntu 22.04, ext4\r\n- **Browser**:\n```\n\n\n### Additional context\n\n_No response_\n\n## Top Comments\n\n**marregui**: Hola!, but this would mean that the table is a symlink, what would that mean in terms of partition read only?\r\n\r\nThe rationale behind symlinks was to allow partitions to be \"foreign\". This issue would extend the domain of symlinks to include tables, which could have an impact on performance across all tables if the volume of a table is in very slow storage, across a network?.\r\n\r\n\r\n\r\n\n\n---\n\n**puzpuzpuz**: > what would that mean in terms of partition read only?\r\n\r\nIt's unrelated with partitions. The use case is moving a large table to a separate disk, nothing more. All partitions should remain writable. I guess some users might even want to move the entire `./db` directory to a separate disk.\r\n\r\n> which could have an impact on performance across all tables if the volume of a table is in very slow storage, across a network?\r\n\r\nImagine that you have a table that requires 1TB of disk space while you have two disks 1.5TB each. In this situation you'd want to move the table to the second disk while all other tables remain on the first one. Performance won't be impacted as both disks are local.\n\n---\n\n**marregui**: cool, will do!\n\n---\n\n**marregui**: being addressed in https://github.com/questdb/questdb/pull/2710 there is a test to exemplify the description above `ServerMainForeignTableTest.testServerMainCreateTableMoveItsFolderAwayAndSoftLinkIt`.\n\n---\n\n**viper1**: > It's unrelated with partitions. The use case is moving a large table to a separate disk, nothing more. All partitions should remain writable. I guess some users might even want to move the entire `./db` directory to a separate disk.\r\n\r\nthank you, this would be fantastic to have. \r\n\r\na small extension with a new keyword CREATE ARCHIVE TABLE\r\nthat automatically creates the new table in archive_dir/db/table_name would make this completely seamless.\r\n\r\nkeep up the good work.\n\n---\n\n**marregui**: Hola @viper1 happy new year. Would you be able to elaborate on this last feature. Let's define it and see what can be done. Thank you in advance. \n\n---\n\n**viper1**: HNY @marregui. \r\n\r\nit is for the same use case from @puzpuzpuz above. for example we have two 1.5TB disks. \r\nthe main db is on the first disk in root_dir. we create root_dir2 in the second disk. \r\nthen we shut down the database, create the symlink in root_dir/db/table to point to root_dir2/db/table.\r\n\r\nthe extension is, instead of shutting down the database and manually creating the symlink:\r\n1. define root_dir2 in the config. for example something like cairo.archive_dir=root_dir2/db\r\n2. CREATE ARCHIVE TABLE understands that the table should be created in root_dir2 instead of root_dir\r\n\r\nnothing more than that. the benefit is it allows the same functionality with no database downtime and no manual filesystem changes, especially if additional tables need to be created on the second disk\n\n---\n\n**marregui**: I suppose a more general solution would be to extend the create table statement so that it supports:\r\n\r\n```sql\r\nCREATE TABLE table_name ( ... ) PARTITION BY ... WITH .... IN VOLUME '/path/to/wherever/you/want'\r\n```\r\n\r\nso then questdb will create the table in '/path/to/wherever/you/want/table_name' and an automatic softlink \"table_name\" pointing there from the actual database root. Then no config is required. What are your thoughts?\r\n\n\n---\n\n**viper1**: this seems like an elegant solution - more general and also captures the use case well.\r\n\r\n> CREATE TABLE table_name ( ... ) PARTITION BY ... WITH .... IN VOLUME '/path/to/wherever/you/want'\r\n\r\n\n\n---\n\n**marregui**: hi @viper1 I have pushed a commit https://github.com/questdb/questdb/pull/2710/commits/82e3dbf285774523d7b6cee6632e8d4c1dfed969 with the above requirement, it was very simple to do, that is why I did it. I will add more tests I suppose, and I will get feedback from @puzpuzpuz but it will be merged.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 696,
    "metadata": {
      "issue_number": 2884,
      "state": "closed",
      "labels": [
        "New feature",
        "Core"
      ],
      "comments_count": 21,
      "created_at": "2022-12-22T08:05:49Z",
      "updated_at": "2023-02-16T07:52:07Z",
      "closed_at": "2023-02-15T15:03:22Z",
      "author": "puzpuzpuz",
      "top_comments": [
        "**marregui**: Hola!, but this would mean that the table is a symlink, what would that mean in terms of partition read only?\r\n\r\nThe rationale behind symlinks was to allow partitions to be \"foreign\". This issue would extend the domain of symlinks to include tables, which could have an impact on performance across all tables if the volume of a table is in very slow storage, across a network?.\r\n\r\n\r\n\r\n",
        "**puzpuzpuz**: > what would that mean in terms of partition read only?\r\n\r\nIt's unrelated with partitions. The use case is moving a large table to a separate disk, nothing more. All partitions should remain writable. I guess some users might even want to move the entire `./db` directory to a separate disk.\r\n\r\n> which could have an impact on performance across all tables if the volume of a table is in very slow storage, across a network?\r\n\r\nImagine that you have a table that requires 1TB of disk space while you have two disks 1.5TB each. In this situation you'd want to move the table to the second disk while all other tables remain on the first one. Performance won't be impacted as both disks are local.",
        "**marregui**: cool, will do!",
        "**marregui**: being addressed in https://github.com/questdb/questdb/pull/2710 there is a test to exemplify the description above `ServerMainForeignTableTest.testServerMainCreateTableMoveItsFolderAwayAndSoftLinkIt`.",
        "**viper1**: > It's unrelated with partitions. The use case is moving a large table to a separate disk, nothing more. All partitions should remain writable. I guess some users might even want to move the entire `./db` directory to a separate disk.\r\n\r\nthank you, this would be fantastic to have. \r\n\r\na small extension with a new keyword CREATE ARCHIVE TABLE\r\nthat automatically creates the new table in archive_dir/db/table_name would make this completely seamless.\r\n\r\nkeep up the good work."
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-db867e5518b4",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2830",
    "title": "Add support for median function in SQL",
    "text": "# Add support for median function in SQL\n\n### Is your feature request related to a problem?\n\nIt's very easy to compute some aggregation functions, such as `avg` or `max`, but other are harder, such as `median`. Having native support in QuestDB would help a lot.\n\n### Describe the solution you'd like.\n\nIdeally it would be called `median` and would work in the same way as the other aggregation functions: https://questdb.io/docs/reference/function/aggregation/\n\n### Describe alternatives you've considered.\n\nI can \"manually\" do something like: sort the data in descending order and select the top 50 percent of the data and then select the last value.\n\n### Additional context.\n\n_No response_\n\n## Top Comments\n\n**puzpuzpuz**: As a more Postgres-compatible option, we could add support for the `percentile_cont` [function](https://www.postgresql.org/docs/9.4/functions-aggregate.html). Then calculating the median would be as simple as the following:\r\n```sql\r\nSELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY sort_expression) as median FROM my_tab;\r\n```\n\n---\n\n**newskooler**: Often I'd have to make a bunch of aggregations to data.\r\nFor example:\r\n\r\n`SELECT max(price) price_high, avg(price) price_mean, median(price) price_median FROM table`\r\nWhat you suggest would not let me do that, right? I will have to do another select on top of an existing select? Possibly even more complicated than that. Is that right?\n\n---\n\n**puzpuzpuz**: With `percentile_cont` your query should look like the following:\r\n```sql\r\nSELECT max(price) price_high, avg(price) price_mean, percentile_cont(0.5) WITHIN GROUP (ORDER BY price) price_median FROM table;\r\n```\r\n\r\nAm I missing something?\n\n---\n\n**newskooler**: If it works like this, then yes - that would be fine! : )\n\n---\n\n**newskooler**: Though I would still suggest the following:\r\n1. Change `avg` -> `mean` because `avg` is ambiguous.\r\n2. Have a higher level method of `percentile_cont(0.5) WITHIN GROUP (ORDER BY price)` -> `median`.\r\n\r\nBoth of these should improve easy of use. \n\n---\n\n**puzpuzpuz**: > 1. Change `avg` -> `mean` because `avg` is ambiguous.\r\n\r\nWe favor Postgres SQL dialect which calls it `avg`.\n\n---\n\n**marregui**: low hunging t-shirt\n\n---\n\n**kevinmingtarja**: Hi, I'd like to contribute to this issue!\n\n---\n\n**puzpuzpuz**: @kevinmingtarja that's great! I don't think there is anyone working on it.\n\n---\n\n**marregui**: @kevinmingtarja that is fantastic! thank you. The issue has been assigned to you, now you lead. Good luck! if you run into walls please do not hesitate to interact with us as you need.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 379,
    "metadata": {
      "issue_number": 2830,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "SQL",
        "hacktoberfest"
      ],
      "comments_count": 21,
      "created_at": "2022-11-29T09:54:18Z",
      "updated_at": "2025-03-10T13:18:27Z",
      "closed_at": "2025-03-10T13:18:27Z",
      "author": "newskooler",
      "top_comments": [
        "**puzpuzpuz**: As a more Postgres-compatible option, we could add support for the `percentile_cont` [function](https://www.postgresql.org/docs/9.4/functions-aggregate.html). Then calculating the median would be as simple as the following:\r\n```sql\r\nSELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY sort_expression) as median FROM my_tab;\r\n```",
        "**newskooler**: Often I'd have to make a bunch of aggregations to data.\r\nFor example:\r\n\r\n`SELECT max(price) price_high, avg(price) price_mean, median(price) price_median FROM table`\r\nWhat you suggest would not let me do that, right? I will have to do another select on top of an existing select? Possibly even more complicated than that. Is that right?",
        "**puzpuzpuz**: With `percentile_cont` your query should look like the following:\r\n```sql\r\nSELECT max(price) price_high, avg(price) price_mean, percentile_cont(0.5) WITHIN GROUP (ORDER BY price) price_median FROM table;\r\n```\r\n\r\nAm I missing something?",
        "**newskooler**: If it works like this, then yes - that would be fine! : )",
        "**newskooler**: Though I would still suggest the following:\r\n1. Change `avg` -> `mean` because `avg` is ambiguous.\r\n2. Have a higher level method of `percentile_cont(0.5) WITHIN GROUP (ORDER BY price)` -> `median`.\r\n\r\nBoth of these should improve easy of use. "
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-20dc3ea14d50",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1679",
    "title": "SQL: Join Column Type Mismatch",
    "text": "# SQL: Join Column Type Mismatch\n\n### Describe the bug\n\nWhen 2 tables joined on INT and LONG columns error shown that there is join column mismatch\n\n### To reproduce\n\n```\r\nselect x, y\r\nfrom long_sequence(100) ls\r\njoin (\r\n  select cast(x as int) y from long_sequence(100)\r\n) as ls2\r\non ls.x = ls2.y\r\n```\n\n### Expected Behavior\n\nINT can be be upcast to LONG and equality should be checked as LONG equality so that above query returns 100 rows\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.1.2\r\n- **OS**: any\r\n- **Browser**: any\n```\n\n\n### Additional context\n\nComparing mixed data types works for many numeric types in `WHERE`. Same should be in `JOIN` condition\n\n## Top Comments\n\n**amankothiyal04**: `When 2 tables joined on INT and LONG columns error shown that there is join column mismatch\r\n\r\nselect x, y\r\nfrom long_sequence(100) ls\r\njoin (\r\n  select x ,y from long_sequence(100)\r\n) as ls2\r\non CAST(ls.x as int)= ls2.y`\n\n---\n\n**amankothiyal04**: > `When 2 tables joined on INT and LONG columns error shown that there is join column mismatch\r\n> \r\n> select x, y from long_sequence(100) ls join ( select x ,y from long_sequence(100) ) as ls2 on CAST(ls.x as int)= ls2.y`\r\n\r\nWill this resolve the issue?\n\n---\n\n**kitsiosvas**: I am interested in working on this issue if still available\r\n\r\n\n\n---\n\n**mchirag2002**: I would like to work on this if it's still available\n\n---\n\n**zly7**: What's the status of this issue? I'd like to work on it!\n\n---\n\n**puzpuzpuz**: @zhangliyuSustech I don't think someone is working on this one, so your contribution is highly welcome. ðŸ‘\n\n---\n\n**zly7**: I have scheduled this issue, I will try my best to fix it in three weeks. \n\n---\n\n**zly7**: I think this issue is beyond my ability, but I will continuously pay attention to it and discuss with my classmates for solution.\n\n---\n\n**bziobrowski**: No problem. Thanks for letting us now . \n\n---\n\n**XinyiQiao**: Hi, what's the status of this issue? I'd like to work on it if it's still open.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 334,
    "metadata": {
      "issue_number": 1679,
      "state": "open",
      "labels": [
        "SQL",
        "Good first issue"
      ],
      "comments_count": 21,
      "created_at": "2021-12-13T22:50:43Z",
      "updated_at": "2023-06-19T13:43:47Z",
      "closed_at": null,
      "author": "ideoma",
      "top_comments": [
        "**amankothiyal04**: `When 2 tables joined on INT and LONG columns error shown that there is join column mismatch\r\n\r\nselect x, y\r\nfrom long_sequence(100) ls\r\njoin (\r\n  select x ,y from long_sequence(100)\r\n) as ls2\r\non CAST(ls.x as int)= ls2.y`",
        "**amankothiyal04**: > `When 2 tables joined on INT and LONG columns error shown that there is join column mismatch\r\n> \r\n> select x, y from long_sequence(100) ls join ( select x ,y from long_sequence(100) ) as ls2 on CAST(ls.x as int)= ls2.y`\r\n\r\nWill this resolve the issue?",
        "**kitsiosvas**: I am interested in working on this issue if still available\r\n\r\n",
        "**mchirag2002**: I would like to work on this if it's still available",
        "**zly7**: What's the status of this issue? I'd like to work on it!"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-72328c0e1da5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3816",
    "title": "Table partitions for all tables in the QuestDB",
    "text": "# Table partitions for all tables in the QuestDB\n\n### Is your feature request related to a problem?\n\nI need to get information about partitions for all tables in QuestDB.\n\n### Describe the solution you'd like.\n\nI'd like to have a function like `table_partitions`, for example `all_tables_partitions`, which returns information about partitions for all tables in the database. For example:\r\n```\r\nall_tables_partitions()\r\n```\r\n\r\nAlternative can be a function `tables_partitions` to take array of tables `string[]` as an argument and returning information about partitions for each table in the array. For example:\r\n```\r\ntables_partitions(['table_1', 'table_2', 'table_3'])\r\n```\r\nor\r\n```\r\ntables_partitions(SELECT name FROM tables())\r\n```\n\n### Describe alternatives you've considered.\n\nRight now, I get the list of tables and use `concat` with `union all`  to create a query:\r\n```\r\nselect concat(\r\n  'select ''',name,''' as table_name,''',\r\n  walEnabled,''' as walEnabled,''',\r\n  partitionBy,''' as partitionBy,count(*) partition_count,sum(numrows) row_count,(sum(disksize)) as size_b from table_partitions(''',\r\n  name,''') group by table_name union all')\r\nfrom tables()\r\norder by name;\r\n```\r\nThe final query looks like this:\r\n```\r\nselect 'table_1' as table_name,'false' as walEnabled,'DAY' as partitionBy,count(*) partition_count,sum(numrows) row_count,(sum(disksize)) as size_b from table_partitions('table_1') group by table_name union all\r\nselect 'table_2' as table_name,'false' as walEnabled,'DAY' as partitionBy,count(*) partition_count,sum(numrows) row_count,(sum(disksize)) as size_b from table_partitions('table_2') group by table_name union all\r\nselect 'table_3' as table_name,'false' as walEnabled,'DAY' as partitionBy,count(*) partition_count,sum(numrows) row_count,(sum(disksize)) as size_b from table_partitions('table_3') group by table_name\r\n```\r\nI think this is too tedious and complicated.\n\n### Additional context.\n\n_No response_\n\n## Top Comments\n\n**Anubhav099**: This seems really interesting and maybe something I can handle. I am a first timer and would really be grateful for some guidance. Please let me take this on. Took me some time but I have managed to run the source code locally. I am really looking forward to make some contributions here. Thank you in advance :).\n\n---\n\n**abhishek-ssingh**: Hi @Maros112358 , Can you explain the classes we should be working on to implement this feat. \n\n---\n\n**agsti**: Hey @Maros112358 , wondering, what is the shape that you intend to have of this? a sql function? or just a code function somewhere?\n\n---\n\n**Maros112358**: Hello @agsti, I am not sure if I understand your question. Would you mind to elaborate?\n\n---\n\n**agsti**: @Maros112358  Sure, I mean, do you intend to do something like `SELECT * from tables_partitions(['table_1', 'table_2', 'table_3'])` ? or just call it from java code?\n\n---\n\n**Maros112358**: @agsti Thank you. I want to use as a sql function in query. \n\n---\n\n**agsti**: @Maros112358  Gotcha, then:\r\nDo we want to keep the same schema as `table_partitions(<table>)` ?\r\n\r\nif so\r\n\r\nCan I reuse logic from `ShowPartitionsRecordCursorFactory.java` ?\r\n\r\nI think I would create a new `AbstractRecordCursorFactory` that gives different `ShowPartitionsRecordCursor`s over time. Can I do that?\r\n\r\nOtherwise I guess is about creating a sibling to ShowPartitionsRecordCursorFactory.java with the schema of \r\n``` string table_name,\r\nboolean walEnabled,\r\nstring partitionBy,\r\nint partition_count,\r\nint row_count\n\n---\n\n**Maros112358**: @agsti I would like this new function to return same columns as function `table_partitions`, just for all the tables in the database. I am not familiar with the codebase, therefore it's up to you how you implement. \n\n---\n\n**agsti**: I have decided to modify `table_partitions` function as I think it requires the least amount of code than the other options and there would be a big overlap of functionality between this and another function. I opened a MR That is not tagged in this issue \n\n---\n\n**agsti**: MR is quite ready now, maybe I could get this task assigned",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 569,
    "metadata": {
      "issue_number": 3816,
      "state": "open",
      "labels": [
        "New feature",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 20,
      "created_at": "2023-10-05T13:06:29Z",
      "updated_at": "2024-08-05T05:43:29Z",
      "closed_at": null,
      "author": "Maros112358",
      "top_comments": [
        "**Anubhav099**: This seems really interesting and maybe something I can handle. I am a first timer and would really be grateful for some guidance. Please let me take this on. Took me some time but I have managed to run the source code locally. I am really looking forward to make some contributions here. Thank you in advance :).",
        "**abhishek-ssingh**: Hi @Maros112358 , Can you explain the classes we should be working on to implement this feat. ",
        "**agsti**: Hey @Maros112358 , wondering, what is the shape that you intend to have of this? a sql function? or just a code function somewhere?",
        "**Maros112358**: Hello @agsti, I am not sure if I understand your question. Would you mind to elaborate?",
        "**agsti**: @Maros112358  Sure, I mean, do you intend to do something like `SELECT * from tables_partitions(['table_1', 'table_2', 'table_3'])` ? or just call it from java code?"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1cc328e02d37",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2895",
    "title": "High Disk usage!?",
    "text": "# High Disk usage!?\n\n### Describe the bug\r\n\r\nHi \r\n\r\nthis is an awesome product, very impressive. I have been trying it out and comparing to some other time series databases as well. \r\n\r\nQuick one: it appears that it can use up a lot of disk space. for instance, I have a OHLCV table with 6052159 rows. I have a date, timestamp, ticker(Symbol) and OHLCV columns. the OHLCV db directory seems to take up close to 20GB disk space. \r\n\r\n- this appears quite high, no? is this expected?\r\n- is there a way we can reduce disk usage by tables?\r\n\r\nta! \r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.6.1\r\n- **OS**: M1 Mac Monterey\r\n- **Browser**: Safari\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_\n\n## Top Comments\n\n**ideoma**: The record length would be 8+8+4+8*5=60 bytes. With 6052159 rows it should be 346Mb.\r\n\r\nPlease post create table statement and `du -sh *` inside the table directory to find out why you have unexpected high disk consumption.\r\n\r\n\n\n---\n\n**puzpuzpuz**: You could also try ZFS with enabled compression to save some disk space (compression ratio should be around 2-3x with the default LZ4 algorithm). Just keep in mind that ZFS is not something we have tested and, thus, there is no official support for this file system (however, this may change in future).\n\n---\n\n**Roh-codeur**: @ideoma : sure please see below. please note I removed some dirs from the du -sh output. Hope this helps\r\n\r\n```\r\nDROP TABLE IF EXISTS ohlcv;\r\n\r\nCREATE TABLE ohlcv(\r\n        Ticker SYMBOL CAPACITY 10000,\r\n        Exchange SYMBOL CAPACITY 50,\r\n        DataSource SYMBOL CAPACITY 50,\r\n        Date TIMESTAMP,\r\n        Open double,\r\n        High double,\r\n        Low double,\r\n        Close double,\r\n        AdjustedClose double,\r\n        Volume double,\r\n        timestamp TIMESTAMP)\r\n  TIMESTAMP(date) \r\n  PARTITION BY MONTH;\r\n```\r\n\r\n```\r\ncd /opt/homebrew/var/questdb/db/ohlcv\r\n\r\ndu -sh *\r\n 65M\t2000-01.592\r\n 66M\t2000-02.592\r\n 66M\t2000-03.592\r\n 66M\t2000-04.592\r\n....\r\n 90M\t2022-10.592\r\n 90M\t2022-11.592\r\n 16K\tDataSource.c\r\n 16K\tDataSource.k\r\n 16K\tDataSource.o\r\n 16K\tDataSource.v\r\n 16K\tExchange.c\r\n 16K\tExchange.k\r\n 16K\tExchange.o\r\n 16K\tExchange.v\r\n 48K\tTicker.c\r\n 16K\tTicker.k\r\n 16K\tTicker.o\r\n 32K\tTicker.v\r\n 16K\t_cv\r\n 16K\t_meta\r\n 16K\t_meta.prev\r\n 16K\t_todo_\r\n 32K\t_txn\r\n 36K\t_txn_scoreboard\r\n\r\n```\r\n\n\n---\n\n**Roh-codeur**: @puzpuzpuz : sure, thanks. I will look into this as well.\n\n---\n\n**ideoma**: Something does not add up. Your record size is 68 bytes and there is 65+Mb per monthly partition, it should be around 1M rows per month. In 22 years it should be about 264M rows, not 6M as you said.\r\n\r\nWhat does QuestDB returns in `select count(*) from ohclv`?\n\n---\n\n**Roh-codeur**: > What does QuestDB returns in `select count(*) from ohclv`?\r\n\r\nit lists out 6,052,159. I have rows per ticker. not all tickers have data for 22 years, some only have data for a couple of years, or for a few months. \r\n\r\n```\r\ncd /opt/homebrew/var/questdb/db\r\ndu -sh ohlcv/\r\n 19G\tohlcv/\r\n```\r\n\r\nPlease do let me know if I can provide more information to help with this please\r\n\r\nta! \n\n---\n\n**ideoma**: How many row it is in the first partition which is `2000-01` and takes 66Mb?\r\n\r\n```\r\nselect count(*) from ohclv where timestamp in '2000-01'\r\n```\r\n\r\nIt should be near 1M rows as per row size. If it's not then list the files in the folder with the sizes please. If it is, there must be a partition folder where the size on the disk does not match record count return by the query would be good if you can find and list the files there.\r\n\r\n\n\n---\n\n**Roh-codeur**: > How many row it is in the first partition which is `2000-01` and takes 66Mb?\r\n> \r\n> ```\r\n> select count(*) from ohclv where timestamp in '2000-01'\r\n> ```\r\n> \r\n\r\nit comes out as 9864. Also, the query was:\r\n`select count(*) from ohlcv where date in '2000-01';\r\n`\r\n```\r\ncd /opt/homebrew/var/questdb/db/ohlcv/2000-01.592\r\n\r\ndu -sh *\r\n 80K\tAdjustedClose.d\r\n 80K\tClose.d\r\n 40K\tDataSource.d\r\n 16K\tDataSource.k\r\n 80K\tDataSource.v\r\n 80K\tDate.d\r\n 40K\tExchange.d\r\n 16K\tExchange.k\r\n 80K\tExchange.v\r\n 80K\tHigh.d\r\n 80K\tLow.d\r\n 80K\tOpen.d\r\n 40K\tTicker.d\r\n 64K\tTicker.k\r\n 64M\tTicker.v\r\n 80K\tVolume.d\r\n 80K\ttimestamp.d\r\n```\r\n\r\nPlease note the Ticker.v file taking up 65M. the rest are all small. \r\n\n\n---\n\n**puzpuzpuz**: @Roh-codeur it looks like you have indexes on `DataSource`, `Exchange` and `Ticker` columns. Is that correct? The DDL you've shared doesn't include indexes, so that's why I'm asking. Could you try dropping the indexes and check the disk space after that? https://questdb.io/docs/reference/sql/alter-table-alter-column-drop-index/\n\n---\n\n**Roh-codeur**: @puzpuzpuz @ideoma my sincerest apologies to you both. You are right, I did indeed have indices, I was looking to remove them to check the disk usage impact. I will drop and report back. \r\n\r\nqq: do we expect indices to have a significant impact. \r\nAlso, i will try out without indices - do we expect the query performance to remain the same? ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 780,
    "metadata": {
      "issue_number": 2895,
      "state": "closed",
      "labels": [],
      "comments_count": 20,
      "created_at": "2023-01-01T21:20:35Z",
      "updated_at": "2023-08-30T05:30:35Z",
      "closed_at": "2023-01-12T21:12:31Z",
      "author": "Roh-codeur",
      "top_comments": [
        "**ideoma**: The record length would be 8+8+4+8*5=60 bytes. With 6052159 rows it should be 346Mb.\r\n\r\nPlease post create table statement and `du -sh *` inside the table directory to find out why you have unexpected high disk consumption.\r\n\r\n",
        "**puzpuzpuz**: You could also try ZFS with enabled compression to save some disk space (compression ratio should be around 2-3x with the default LZ4 algorithm). Just keep in mind that ZFS is not something we have tested and, thus, there is no official support for this file system (however, this may change in future).",
        "**Roh-codeur**: @ideoma : sure please see below. please note I removed some dirs from the du -sh output. Hope this helps\r\n\r\n```\r\nDROP TABLE IF EXISTS ohlcv;\r\n\r\nCREATE TABLE ohlcv(\r\n        Ticker SYMBOL CAPACITY 10000,\r\n        Exchange SYMBOL CAPACITY 50,\r\n        DataSource SYMBOL CAPACITY 50,\r\n        Date TIMESTAMP,\r\n        Open double,\r\n        High double,\r\n        Low double,\r\n        Close double,\r\n        AdjustedClose double,\r\n        Volume double,\r\n        timestamp TIMESTAMP)\r\n  TIMESTAMP(date) \r\n  PARTITION BY MONTH;\r\n```\r\n\r\n```\r\ncd /opt/homebrew/var/questdb/db/ohlcv\r\n\r\ndu -sh *\r\n 65M\t2000-01.592\r\n 66M\t2000-02.592\r\n 66M\t2000-03.592\r\n 66M\t2000-04.592\r\n....\r\n 90M\t2022-10.592\r\n 90M\t2022-11.592\r\n 16K\tDataSource.c\r\n 16K\tDataSource.k\r\n 16K\tDataSource.o\r\n 16K\tDataSource.v\r\n 16K\tExchange.c\r\n 16K\tExchange.k\r\n 16K\tExchange.o\r\n 16K\tExchange.v\r\n 48K\tTicker.c\r\n 16K\tTicker.k\r\n 16K\tTicker.o\r\n 32K\tTicker.v\r\n 16K\t_cv\r\n 16K\t_meta\r\n 16K\t_meta.prev\r\n 16K\t_todo_\r\n 32K\t_txn\r\n 36K\t_txn_scoreboard\r\n\r\n```\r\n",
        "**Roh-codeur**: @puzpuzpuz : sure, thanks. I will look into this as well.",
        "**ideoma**: Something does not add up. Your record size is 68 bytes and there is 65+Mb per monthly partition, it should be around 1M rows per month. In 22 years it should be about 264M rows, not 6M as you said.\r\n\r\nWhat does QuestDB returns in `select count(*) from ohclv`?"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1ae080312a74",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1239",
    "title": "Adding the latest contributors ðŸŽ‰ ",
    "text": "# Adding the latest contributors ðŸŽ‰ \n\nAdding some of the most recent members of the community to recognize their great contributions!\n\n## Top Comments\n\n**bsmth**: @all-contributors bot, please add @nexthack for code additions\r\n\r\nThank you for your contribution which helps improve the usability of the system!\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1240) to add @nexthack! :tada:\n\n---\n\n**bsmth**: \r\n@all-contributors bot, please add @g-metan for bug reports\r\n\r\nThank you for reporting issues and helping to improve QuestDB!\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1241) to add @g-metan! :tada:\n\n---\n\n**bsmth**: \r\n@all-contributors bot, please add @tim2skew for bug reports and user testing\r\n\r\nThank you for your continued use and feedback on the features we consider for the roadmap!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1243) to add @tim2skew! :tada:\n\n---\n\n**bsmth**: @all-contributors bot, please add @ospqsp for bug reports\r\n\r\nThank you for your feedback!\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1244) to add @ospqsp! :tada:\n\n---\n\n**bsmth**: \r\n@all-contributors bot, please add @SuperFluffy for bug reports\r\n\r\nThank you for reporting issues and helping to improve QuestDB!\r\n\n\n---\n\n**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1245) to add @SuperFluffy! :tada:",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 195,
    "metadata": {
      "issue_number": 1239,
      "state": "closed",
      "labels": [],
      "comments_count": 20,
      "created_at": "2021-08-11T14:17:09Z",
      "updated_at": "2021-08-12T08:35:49Z",
      "closed_at": "2021-08-12T08:35:49Z",
      "author": "bsmth",
      "top_comments": [
        "**bsmth**: @all-contributors bot, please add @nexthack for code additions\r\n\r\nThank you for your contribution which helps improve the usability of the system!\r\n",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1240) to add @nexthack! :tada:",
        "**bsmth**: \r\n@all-contributors bot, please add @g-metan for bug reports\r\n\r\nThank you for reporting issues and helping to improve QuestDB!\r\n",
        "**allcontributors[bot]**: @bsmth \n\nI've put up [a pull request](https://github.com/questdb/questdb/pull/1241) to add @g-metan! :tada:",
        "**bsmth**: \r\n@all-contributors bot, please add @tim2skew for bug reports and user testing\r\n\r\nThank you for your continued use and feedback on the features we consider for the roadmap!"
      ],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-551aa28dd84a",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/363",
    "title": "first() / last() implementation",
    "text": "# first() / last() implementation\n\n**Is your feature request related to a problem? Please describe.**\r\nthe aggregation function `last()` only supports certain types (timestamp, long, int). However `last(symbol)` and `last(double)` for example are not supported.\r\n`first()` is not implemented.\r\n\r\n**Describe the solution you'd like**\r\nFinish the implementation for `last` and implement `first`\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 52,
    "metadata": {
      "issue_number": 363,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL",
        "Good first issue"
      ],
      "comments_count": 20,
      "created_at": "2020-06-04T16:25:53Z",
      "updated_at": "2020-09-06T19:16:33Z",
      "closed_at": "2020-09-06T19:16:33Z",
      "author": "TheTanc",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-cac2f15c9327",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/15",
    "title": "Unique key support",
    "text": "# Unique key support\n\nHi Vlad,\n\nI've recently came across your NFSDB. It looks impressive. I've been playing with it quite a lot and found one potential issue. There is probably a problem with indexing when a column contains too many different values. For such scenario I've tried to use a standard index with the hitcount set to a high number and it made the db very very slow. It seems the db gets slower exponentially with growing number of different items in the indexed column. To reproduce the issue, just set up the index like this and insert 1M items with different unique ids:\n\n$sym(\"uniqueId\").index().size(15).valueCountHint(1000_000)\n\nIt might be a good idea to have something like UNIQUE KEY index/constraint which creates a special index using a hashtable. This would allow user to find exactly one item based on a unique key.\n\nOr am I missing something? Please let me know your thoughts.\n\nThanks a lot,\nJaromir\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 156,
    "metadata": {
      "issue_number": 15,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature"
      ],
      "comments_count": 20,
      "created_at": "2014-08-29T17:00:59Z",
      "updated_at": "2014-09-01T10:41:44Z",
      "closed_at": "2014-09-01T10:30:28Z",
      "author": "jaromirs",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-f3b2147f1bc2",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1865",
    "title": "Potential memory leak in 6.2",
    "text": "# Potential memory leak in 6.2\n\n### Describe the bug\n\nWe are running QuestDB 6.2 (container) and ingesting data at 14kHz (via ILP, single writer) for a long period. During this period (couple of days), we do not run a single query.\r\n\r\nThere, we observed that the memory usage of QuestDB rises over time, finally allocating all available memory. When limiting the memory using cgroups (technically via docker-compose mem_limit setting), we observe that the process is periodically OOMd and restarted. Further we observed the following:\r\n\r\n- we do not overrun the DB (all data ends up on disk until last commit before OOM, checked after restart)\r\n- when stopping the ingestion, the memory usage does not shrink\r\n- the JVM heap is limited to 1GB (that could be put into the docs)\r\n- Restarts happen very periodically\r\n\r\n**time between OOMs (restarts)**\r\n\r\n- 03:48:02\r\n- 04:03:43\r\n- 03:53:27\r\n- 03:58:54\r\n- 05:00:44\r\n\r\n**docker-compose.yml**\r\n\r\n```\r\nservices:\r\n  questdb:\r\n    image: docker.io/questdb/questdb:6.2\r\n    volumes:\r\n      - /mnt/db-storage:/root/.questdb\r\n      # Note: setting log-level via env var did not work\r\n      - ./questdb-conf/log.conf:/root/.questdb/conf/log.conf:ro\r\n    ports:\r\n      - '8812:8812'\r\n      - '9000:9000'\r\n      - '9009:9009'\r\n    environment:\r\n      - QDB_LINE_TCP_MAINTENANCE_JOB_INTERVAL=10000\r\n      - QDB_LINE_TCP_DEFAULT_PARTITION_BY=HOUR\r\n      - QDB_CAIRO_COMMIT_LAG=10000\r\n      - QDB_HTTP_QUERY_CACHE_ENABLED=false\r\n      - QDB_PG_SELECT_CACHE_ENABLED=false\r\n      - QDB_PG_INSERT_CACHE_ENABLED=false\r\n    cpus: 4\r\n    mem_limit: 2G\r\n    restart: unless-stopped\r\n```\r\n\r\n**QuestDB output after startup**\r\n\r\n```\r\n2022-02-08T06:36:59.104203Z A server-main Server config : /root/.questdb/conf/server.conf\r\n2022-02-08T06:36:59.111826Z A server-main Config changes applied:\r\n2022-02-08T06:36:59.111843Z A server-main   http.enabled : true\r\n2022-02-08T06:36:59.111870Z A server-main   tcp.enabled  : true\r\n2022-02-08T06:36:59.111891Z A server-main   pg.enabled   : true\r\n2022-02-08T06:36:59.111912Z A server-main open database [id=8916382354024914915.-5271762009964388491]\r\n2022-02-08T06:36:59.111948Z A server-main platform [bit=64]\r\n2022-02-08T06:36:59.111969Z A server-main OS/Arch: linux/amd64 [AVX2,8]\r\n2022-02-08T06:36:59.112352Z A server-main available CPUs: 4\r\n2022-02-08T06:36:59.112388Z A server-main db root: /root/.questdb/db\r\n2022-02-08T06:36:59.112410Z A server-main backup root: null\r\n2022-02-08T06:36:59.112482Z A server-main db file system magic: 0x6edc97c2 [BTRFS] SUPPORTED\r\n2022-02-08T06:36:59.112712Z A server-main SQL JIT compiler mode: off\r\n2022-02-08T06:36:59.298217Z A i.q.TelemetryJob instance [id=0x05d6f771771573000001f41cdc0163, enabled=true]\r\n2022-02-08T06:36:59.307648Z A http-server listening on 0.0.0.0:9000 [fd=58 backlog=256]\r\n2022-02-08T06:36:59.347898Z A pg-server listening on 0.0.0.0:8812 [fd=62 backlog=10]\r\n2022-02-08T06:36:59.380170Z A tcp-line-server listening on 0.0.0.0:9009 [fd=64 backlog=256]\r\n2022-02-08T06:36:59.408147Z A server-main enjoy\r\n2022-02-08T06:36:59.345514Z A http-min-server listening on 0.0.0.0:9003 [fd=60 backlog=4]\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\nFixed upper bound of allocated memory. At least for the containerized version, this limit should be read from the cgroup.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.2 (container)\r\n- **OS**: Debian Bullseye\r\n- **container runtime**: podman + crun (rootless)\r\n- **storage**: BTRFS (for DB), fuse-overlayfs for container rootfs\n```\n\n\n### Additional context\n\nWithout memory limitation:\r\n\r\n![questdb-leak](https://user-images.githubusercontent.com/9655545/152939008-8d4ca766-d64e-47a1-b0fa-405eab1e9a70.PNG)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 395,
    "metadata": {
      "issue_number": 1865,
      "state": "closed",
      "labels": [
        "ILP"
      ],
      "comments_count": 19,
      "created_at": "2022-02-08T07:39:15Z",
      "updated_at": "2022-03-02T16:02:12Z",
      "closed_at": "2022-02-17T18:44:37Z",
      "author": "fmoessbauer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-f7954de8b341",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1061",
    "title": "Unsafe memory access",
    "text": "# Unsafe memory access\n\n**Describe the bug**\r\n\r\nWe occasionally see our servers embedding QuestDb core with\r\n\r\n`java.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\\n\\tat io.questdb.cairo.IntervalFwdDataFrameCursor.next(IntervalFwdDataFrameCursor.java:68)\\n\\tat io.questdb.griffin.engine.table.DataFrameRecordCursor.nextFrame(DataFrameRecordCursor.java:102)\\n\\tat io.questdb.griffin.engine.table.DataFrameRecordCursor.nextRow(DataFrameRecordCursor.java:80)\\n\\tat io.questdb.griffin.engine.table.DataFrameRecordCursor.hasNext(DataFrameRecordCursor.java:60)\\n\\tat \r\n`\r\n\r\n**Environment (please complete the following information):**\r\n\r\n - OS: Linux - 4.18.0-240.15.1.el8_3.x86_64\r\n - Jdk: OpenJDK 64-Bit Server VM AdoptOpenJDK (11.0.10+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64)\r\n - QuestDb version: 5.0.6\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 67,
    "metadata": {
      "issue_number": 1061,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 19,
      "created_at": "2021-05-25T10:05:07Z",
      "updated_at": "2021-10-14T14:09:54Z",
      "closed_at": "2021-10-14T14:09:54Z",
      "author": "bratseth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-39dc0c453ae0",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5169",
    "title": "Could not mmap and subsequent crash on simple query, more frequently with CTEs",
    "text": "# Could not mmap and subsequent crash on simple query, more frequently with CTEs\n\n### To reproduce\n\n1. Run a query:\n```sql\nSELECT\n  SUM(column_a) AS alias_a,\n  SUM(column_b) AS alias_b,\n  SUM(column_b)::float / SUM(column_a) AS alias_c,\n  SUM(column_b)::float / 30.2 AS alias_d,\n  1 - (\n    SUM(\n      CASE\n        WHEN column_c <= 29 THEN 1\n        ELSE 0\n      END\n    )::float / DATEDIFF('h', '2024-08-01T07:00:00.000Z', '2024-08-15T07:00:00.000Z')\n  ) AS alias_e,\n  SUM(\n    CASE\n      WHEN column_c <= 29 THEN 1\n      ELSE 0\n    END\n  ) AS alias_f\nFROM table_name\nWHERE\n  timestamp >= '2024-08-01T07:00:00.000Z'\n  AND timestamp < '2024-08-15T07:00:00.000Z'\n  AND type = 'SITE';\n;\n```\nWith the above query, QuestDB will crash some of the time (I had to revise the below query with CTEs because QuestDB seemed to never be able to handle a query with multiple CTEs). However, with the below query with CTEs, we get a guaranteed crash:\n\n```sql\nWITH daily_energy AS (\n  SELECT \n    timestamp,\n    SUM(columnA) AS columnA,\n    SUM(columnB) AS columnB,\n    SUM(columnB)::float / 303261235.2 AS columnC\n  FROM table__name_ems__hourly\n  WHERE\n    id = 'Site'\n    AND category = 'BASE'\n  SAMPLE BY 1d \n    FROM dateadd('y', -1, '2024-08-31T23:59:59.000000') \n    TO '2024-08-31T23:59:59.000000' \n    FILL(NULL)\n  ALIGN TO CALENDAR TIME ZONE 'America/Los_Angeles'\n  ORDER BY timestamp ASC\n),\ndaily_availability AS (\n  SELECT \n    timestamp,\n    COALESCE(COUNT(id), 0) AS columnD\n  FROM table__name_ems__hourly\n  WHERE\n    id = 'Site'\n    AND category = 'BASE'\n    AND columnE <= 29\n  SAMPLE BY 1d \n    FROM dateadd('y', -1, '2024-08-31T23:59:59.000000') \n    TO '2024-08-31T23:59:59.000000' \n    FILL(NULL)\n  ALIGN TO CALENDAR TIME ZONE 'America/Los_Angeles'\n  ORDER BY timestamp ASC\n),\ndate_range_energy AS (\n  SELECT\n    SUM(columnA) AS range__columnA,\n    SUM(columnB) AS range__columnB,\n    SUM(columnB)::float / 303261235.2 AS range__columnC,\n    SUM(columnB) / SUM(columnA)::float AS range__columnD\n  FROM daily_energy\n  WHERE\n    timestamp BETWEEN '2024-08-21T00:00:00.000000' AND '2024-08-25T23:59:59.000000'\n),\ndate_range_availability AS (\n  SELECT\n    SUM(columnD) AS range__columnE,\n    1 - (SUM(columnD) / datediff('h', '2024-08-21T00:00:00.000000', '2024-08-26T00:00:00.000000')::float) AS range__columnF\n  FROM daily_availability\n  WHERE\n    timestamp BETWEEN '2024-08-21T00:00:00.000000' AND '2024-08-25T23:59:59.000000'\n),\nmtd_energy AS (\n  SELECT\n    SUM(columnA) AS mtd__columnA,\n    SUM(columnB) AS mtd__columnB,\n    SUM(columnB)::float / 303261235.2 AS mtd__columnC,\n    SUM(columnB) / SUM(columnA)::float AS mtd__columnD\n  FROM daily_energy\n  WHERE\n    timestamp BETWEEN dateadd('M', -1, '2024-08-31T23:59:59.000000') AND '2024-08-31T23:59:59.000000'\n),\nmtd_availability AS (\n  SELECT\n    SUM(columnD) AS mtd__columnE,\n    1 - (SUM(columnD) / datediff('h', dateadd('M', -1, '2024-08-31T23:59:59.000000'), '2024-08-31T23:59:59.000000')::float) AS mtd__columnF\n  FROM daily_availability\n  WHERE\n    timestamp BETWEEN dateadd('M', -1, '2024-08-31T23:59:59.000000') AND '2024-08-31T23:59:59.000000'\n),\nytd_energy AS (\n  SELECT\n    SUM(columnA) AS ytd__columnA,\n    SUM(columnB) AS ytd__columnB,\n    SUM(columnB)::float / 303261235.2 AS ytd__columnC,\n    SUM(columnB) / SUM(columnA)::float AS ytd__columnD\n  FROM daily_energy\n  WHERE\n    timestamp BETWEEN date_trunc('year', '2024-08-31T23:59:59.000000') AND '2024-08-31T23:59:59.000000'\n),\nytd_availability AS (\n  SELECT\n    SUM(columnD) AS ytd__columnE,\n    1 - (SUM(columnD) / datediff('h', date_trunc('year', '2024-08-31T23:59:59.000000'), '2024-08-31T23:59:59.000000')::float) AS ytd__columnF\n  FROM daily_availability\n  WHERE\n    timestamp BETWEEN date_trunc('year', '2024-08-31T23:59:59.000000') AND '2024-08-31T23:59:59.000000'\n),\nytd_trend_365d AS (\n  SELECT \n    to_timezone(timestamp, 'America/Los_Angeles') AS timestamp,\n    SUM(columnA) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '365' DAY PRECEDING AND CURRENT ROW\n    ) AS ytd_sum__columnA,\n    SUM(columnB) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '365' DAY PRECEDING AND CURRENT ROW\n    ) AS ytd_sum__columnB,\n    AVG(columnA) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '365' DAY PRECEDING AND CURRENT ROW\n    ) AS ytd_avg__columnA,\n    AVG(columnB) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '365' DAY PRECEDING AND CURRENT ROW\n    ) AS ytd_avg__columnB,\n    AVG(columnC) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '365' DAY PRECEDING AND CURRENT ROW\n    ) AS ytd_avg__columnC\n  FROM daily_energy\n  ORDER BY timestamp DESC\n  LIMIT 1\n),\nytd_trend_15d AS (\n  SELECT\n    to_timezone(timestamp, 'America/Los_Angeles') AS timestamp,\n    SUM(columnA) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '15' DAY PRECEDING AND CURRENT ROW\n    ) AS ms_15d__columnA,\n    SUM(columnB) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '15' DAY PRECEDING AND CURRENT ROW\n    ) AS ms_15d__columnB,\n    AVG(columnA) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '15' DAY PRECEDING AND CURRENT ROW\n    ) AS ma_15d__columnA,\n    AVG(columnB) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '15' DAY PRECEDING AND CURRENT ROW\n    ) AS ma_15d__columnB,\n    AVG(columnC) OVER (\n      ORDER BY timestamp ASC\n      RANGE BETWEEN '15' DAY PRECEDING AND CURRENT ROW\n    ) AS ma_15d__columnC\n  FROM daily_energy\n  ORDER BY timestamp DESC\n  LIMIT 1\n)\n\nSELECT\n  ytd_trend_365d.timestamp,\n  date_range_energy.range__columnA,\n  mtd_energy.mtd__columnA,\n  ytd_energy.ytd__columnA\nFROM ytd_trend_365d\nCROSS JOIN ytd_energy\nCROSS JOIN date_range_energy\nCROSS JOIN mtd_energy\n;\n```\n\n2. Observe error `could not mmap  [size=1355328, offset=0, fd=4501795742419913, memUsed=1387729134912, fileLen=1355776]`, followed by the next query causing QuestDB to crash and restart.\n\n### QuestDB version:\n\n8.1.4\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nDebian GNU/Linux 12 (bookworm)\nDefault helm chart (https://artifacthub.io/packages/helm/questdb/questdb)\n\n### File System, in case of Docker specify Host File System:\n\nAmazon EBS disk gp3 \n\n### Full Name:\n\nPeter Klingelhofer\n\n### Affiliation:\n\nEnergy Vault\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\nAll columns being queried are DOUBLE in the schema. \n\nWe also occasionally see this error as well:\n`Error: could not open read-only [file=/var/lib/questdb/db/table__name_ems__hourly~8/2024-10-18.40666/hvac_temperature_status__mixed_air__c__mean.d.393]`\n`Error: [24]: could not open read-write with clean allocation [file=/var/lib/questdb/db/table__name_ems__hourly~8/_txn_scoreboard] undefined`\n\n\n\n```\n2024-11-18T15:31:16.515701245Z # A fatal error has been detected by the Java Runtime Environment:\n2024-11-18T15:31:16.515706286Z #\n2024-11-18T15:31:16.515709013Z #  SIGSEGV (0xb) at pc=0x00007f874fa617e2, pid=1, tid=95\n2024-11-18T15:31:16.515711397Z #\n# JRE version: OpenJDK Runtime Environment Corretto-17.0.11.9.1 (17.0.11+9) (build 17.0.11+9-LTS)\n2024-11-18T15:31:16.515718794Z # Java VM: OpenJDK 64-Bit Server VM Corretto-17.0.11.9.1 (17.0.11+9-LTS, mixed mode, tiered, compressed oops, compressed class ptrs, parallel gc, linux-amd64)\n2024-11-18T15:31:16.515721704Z # Problematic frame:\n2024-11-18T15:31:16.515726190Z # J 8690 c2 io.questdb.cairo.wal.WalTxnDetails.loadTransactionDetailsV1(Lio/questdb/std/str/Path;Lio/questdb/cairo/wal/seq/TransactionLogCursor;IJ)V io.questdb@8.1.4 (428 bytes) @ 0x00007f874fa617e2 [0x00007f874fa61000+0x00000000000007e2]\n2024-11-18T15:31:16.515729743Z #\n2024-11-18T15:31:16.515732511Z # Core dump will be written. Default location: /var/lib/questdb/core.1\n2024-11-18T15:31:16.515735431Z #\n# An error report file with more information is saved as:\n2024-11-18T15:31:16.515752778Z # /var/lib/questdb/db/hs_err_pid+1.log\n```\n\nFull crash log:\n[crash+28.log.zip](https://github.com/user-attachments/files/17803241/crash%2B28.log.zip)\n\nTable DDL:\n[tableDDL.csv.zip](https://github.com/user-attachments/files/17803304/tableDDL.csv.zip)\n\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 874,
    "metadata": {
      "issue_number": 5169,
      "state": "closed",
      "labels": [],
      "comments_count": 18,
      "created_at": "2024-11-15T21:07:29Z",
      "updated_at": "2025-01-24T12:45:25Z",
      "closed_at": "2025-01-21T21:23:28Z",
      "author": "peterklingelhofer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e5c89ddf1c6c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4120",
    "title": "Port remaining parallelizable aggregate functions to off-heap data structures",
    "text": "# Port remaining parallelizable aggregate functions to off-heap data structures\n\n### Is your feature request related to a problem?\n\n#4097 ported `min(str)`, `max(str)`, as well as `count_distinct()` for long, int, and IPv4 types to parallel GROUP BY, but some functions remain unported. Namely:\n- [x] `count_distinct(uuid)`: requires a new long128 hash set, similar to the `GroupByLongHashSet` one\n- [x] `count_distinct(long256)`: requires a new long256 hash set, similar to the `GroupByLongHashSet` one\n- [ ] `approx_percentile(double)`: this one is tricky as we'll have to port HdrHistogram to become off-heap and flyweight\n- [x] all `first`/`last` and `first_not_null`/`last_not_null` functions: to port them, we'll have to access and store row ids in the group by map\n- [ ] `isOrdered(IPv4)`/`isOrdered(long)` functions: again, we need to track row ids\n- [x] `ksum`/`nsum`\n\n\nThere is also `count_distinct(symbol)`, but we have early exit logic in that function (see #3974), so we don't want to port it, at least for now.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nAndrei Pechkurov\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 182,
    "metadata": {
      "issue_number": 4120,
      "state": "open",
      "labels": [
        "Enhancement",
        "Help wanted",
        "SQL",
        "Performance",
        "hacktoberfest"
      ],
      "comments_count": 18,
      "created_at": "2024-01-11T12:35:32Z",
      "updated_at": "2025-12-22T12:13:42Z",
      "closed_at": null,
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-fb7a45f94947",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3084",
    "title": "How to continue to maintain the presence of web console functionality when using the Java embedded API",
    "text": "# How to continue to maintain the presence of web console functionality when using the Java embedded API\n\n### Is your feature request related to a problem?\n\nHi, I am using the Java embedded API, and I want to continue using the functionality provided by the web console. What should I do. I want to use the web console as a database management tool\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 84,
    "metadata": {
      "issue_number": 3084,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 18,
      "created_at": "2023-03-22T06:13:51Z",
      "updated_at": "2023-04-19T05:16:32Z",
      "closed_at": "2023-03-23T20:00:44Z",
      "author": "daodol",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ef17f1f82c77",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/297",
    "title": "CSV double import bug",
    "text": "# CSV double import bug\n\n**Describe the bug**\r\nWhen importing a `csv`, values of type `double` are subject to add trailing digits. So value `1.234` in the csv might end up being stored as `1.2340000000004` for example.\r\n\r\n**Expected behavior**\r\nValues should be imported normally without trailing digits being added.\r\n\r\n**Environment (please complete the following information):**\r\n - AWS linux\r\n - Version: 4.2\r\n\r\n**Screenshots**\r\n<img width=\"528\" alt=\"Screenshot 2020-05-18 at 11 25 19\" src=\"https://user-images.githubusercontent.com/52114895/82206345-f7d16880-98ff-11ea-8197-15ce59527717.png\">\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 71,
    "metadata": {
      "issue_number": 297,
      "state": "closed",
      "labels": [
        "Bug",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 18,
      "created_at": "2020-05-18T11:06:35Z",
      "updated_at": "2022-04-26T13:45:50Z",
      "closed_at": "2022-04-26T13:45:50Z",
      "author": "TheTanc",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-a35e05ceb8a6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5270",
    "title": "Db stuck and unresponsive",
    "text": "# Db stuck and unresponsive\n\n### To reproduce\n\nI haven't specific steps to reproduce that but today we received a lot of errors from ours productions app due to db not reachable.\r\nIt is completely stuck (it's the second time in a more or less a week), I attached the full container log.\r\n\r\nThe only thing we do every day, except for Sunday, is a database restart in order to backup the Hyper-V environment. In our case Hyper-V host the Ubuntu VM that runs QuestDb using Docker on ZFS.\r\nAll the VMs are shutdown gracefully before backup.\n\n### QuestDB version:\n\n8.2.1\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 22.04 (Docker)\n\n### File System, in case of Docker specify Host File System:\n\nZFS\n\n### Full Name:\n\nEnrico Benedos\n\n### Affiliation:\n\nDeimos\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n[xam.zip](https://github.com/user-attachments/files/18226540/xam.zip)\r\n![image](https://github.com/user-attachments/assets/37f8dad8-59fa-45d9-9dca-4d32b3afa6de)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 166,
    "metadata": {
      "issue_number": 5270,
      "state": "closed",
      "labels": [],
      "comments_count": 17,
      "created_at": "2024-12-23T07:43:14Z",
      "updated_at": "2025-07-30T18:10:03Z",
      "closed_at": "2025-01-09T14:18:05Z",
      "author": "enricobenedos",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-8243615c9ae8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4231",
    "title": "First and Last Aggregate Function Optimization",
    "text": "# First and Last Aggregate Function Optimization\n\n### Is your feature request related to a problem?\n\nWhen first or last aggregate functions are used without a keyed group by, there is still a full scan or every row. This isn't necessary and can lead to suboptimal execution time.\n\n### Describe the solution you'd like.\n\nWhen first or last is used without keyed groups, there could be an alternate execution plan that utilizes something along the lines of an order by and limit to improve the overall execution time. \n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nNicholas Guerra\n\n### Affiliation:\n\nKronus Engineering\n\n### Additional context\n\n### SELECT last(timestamp) FROM systemic\r\n```\r\nGroupBy vectorized: false\r\n  values: [last(timestamp)]\r\n    DataFrame\r\n      Row forward scan\r\n      Frame forward scan on: system_io\r\n```\r\n```\r\n1 row in 139ms\r\nExecute: 102.24ms\r\nNetwork: 36.76ms \r\nTotal: 139ms\r\n```\r\n\r\n### SELECT timestamp FROM system_io ORDER BY timestamp DESC LIMIT 1\r\n```\r\nLimit lo: 1\r\n  DataFrame\r\n    Row backward scan\r\n    Frame backward scan on: system_io\r\n```\r\n```\r\n1 row in 39ms\r\nExecute: 257.21Î¼s\r\nNetwork: 38.74ms\r\nTotal: 39ms\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 177,
    "metadata": {
      "issue_number": 4231,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL",
        "Good first issue",
        "Performance"
      ],
      "comments_count": 17,
      "created_at": "2024-02-22T20:08:13Z",
      "updated_at": "2024-06-24T09:58:57Z",
      "closed_at": "2024-06-24T09:58:57Z",
      "author": "nicholas-a-guerra",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-0acea9e57435",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6246",
    "title": "Query performance degradation with high SYMBOL cardinality in QuestDB",
    "text": "# Query performance degradation with high SYMBOL cardinality in QuestDB\n\n### To reproduce\n\nQuestDB recently added **Symbol Capacity auto-scaling** in release 9.1.0 which should allow tables to handle many distinct SYMBOL values without ingestion issues.\n\nHowever, in production we observe **significant query performance degradation** when the number of SYMBOLs in a table grows beyond ~1k, even with auto-scaling enabled.\n\nOur current setup:\n\n- ~300M rows\n- ~10k distinct SYMBOL values (telemetries)\n\nTable schema:\n```\n\nCREATE TABLE 'synthetic_table_test' ( \n\tmetric_name SYMBOL CAPACITY 256 CACHE,\n\tdouble_value DOUBLE,\n\tquality LONG,\n\ttimestamp TIMESTAMP\n) timestamp(timestamp) PARTITION BY MONTH WAL;\n```\n\n**Problem:**\n\n- With ~10k SYMBOLs, queries take **15â€“25 seconds.**\n- Loading a single telemetry (i.e., filtering by one SYMBOL) in a chart is **too slow to be usable**.\n\n**Investigation:**\nWe performed synthetic tests to isolate the problem:\n\n- With only 10 SYMBOL values â†’ queries are very fast.\n- With ~10 k SYMBOL values â†’ queries degrade to several seconds.\n- The performance issue correlates directly with the number of SYMBOLs in the table, not with table row count or returned rows count.\n\n**Reproduction:**\n\nWe created a repo with a synthetic test script:\n\n1. Creates two synthetic table s(syntetic_table_testX) with the same schema as above.\n2. Inserts 300 millions rows.\n3. Runs 30 random queries, each filtering by a single random SYMBOL (e.g., metric_1234) and returning same number of rows (28k).\n4. Collects and plots as scatter plots the two tests as query time vs metric name.\n\n**Result:**\n\nThe first plot shows how performance is in the order of the milliseconds when total SYMBOLs are 10 (so 10 different metrics), the total number of rows if fixed at 300M and returned result 28k rows (28801).\n\n<img width=\"1000\" height=\"600\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2595c205-3e89-4450-8fe4-a86382ce4be9\" />\n\nThe second plot, with the same total row count (3M) and the same returned result size (~28k rows), but with 10k distinct SYMBOLs (metrics), shows that query times increase as SYMBOL cardinality grows. **The outlier (>25 seconds) corresponds to the very first query,** where no caching is yet availableâ€”unfortunately, this is also the typical case when a user loads a chart for a new metric.\n\n<img width=\"1000\" height=\"600\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eca80ab5-3c3e-46f5-9412-42320205b2be\" />\n\nNotice also how unpredictable the execution times are even though the data is synthetic (data points are evenly separated, each SYMBOL/metric has same number of rows).\n\n**Expected behavior:**\nQuery performance when filtering by a single SYMBOL **should remain consistent regardless of the total number of SYMBOLs** present in the table.\nWe tried adding indexes, shelving a few milliseconds but the problem so far in unsolvable if you need to store 1k or more different names indexed with the SYMBOL.\n\n\n### QuestDB version:\n\n9.1.0\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nDocker container inside EC2 running on AWS (amazon/al2023-ami-2023.6.20250317.2-kernel-6.1-x86_64)\n\n### File System, in case of Docker specify Host File System:\n\nt3a.large, 200GB gp3 16k IOPS 250 MB/s throughput (xfs rw,seclabel,relatime,attr2,inode64,logbufs=8,logbsize=32k,sunit=1024,swidth=1024,noquota 0 0)\n\n### Full Name:\n\nRino Pauletto\n\n### Affiliation:\n\nRebernig\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\nAttached below the files to execute the same tests we did:\n\n[questdb_connection.py](https://github.com/user-attachments/files/22793254/questdb_connection.py)\n[requirements.txt](https://github.com/user-attachments/files/22793255/requirements.txt)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 533,
    "metadata": {
      "issue_number": 6246,
      "state": "closed",
      "labels": [],
      "comments_count": 16,
      "created_at": "2025-10-09T08:45:00Z",
      "updated_at": "2025-11-05T14:45:03Z",
      "closed_at": "2025-11-05T14:45:03Z",
      "author": "tolap22",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-00a567ea464b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3168",
    "title": "web console UI grid not showing results",
    "text": "# web console UI grid not showing results\n\n### Describe the bug\n\nsimply as per title, any query on the editor show the number of columns of the result but the grid in the UI is always empty.\r\n\r\nApparently the component has been updated in [3068](https://github.com/questdb/questdb/pull/3068)\n\n### To reproduce\n\nopen web console UI\r\nwrite a \"select * from tableName\"\r\nthe grid shows no results\r\n\r\nthe javascript of the UI fails with:\r\n\r\n```\r\nUncaught TypeError: Cannot read properties of undefined (reading 'digest')\r\n    at qdb.js:2:913132\r\n    at qdb.js:2:913264\r\n    at qdb.js:2:913268\r\n```\r\n\n\n### Expected Behavior\n\nthe table should show result\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.1\r\n- **OS**: Any\r\n- **Browser**: Any\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 115,
    "metadata": {
      "issue_number": 3168,
      "state": "closed",
      "labels": [
        "Bug",
        "UI"
      ],
      "comments_count": 16,
      "created_at": "2023-04-07T12:31:15Z",
      "updated_at": "2023-04-12T21:33:14Z",
      "closed_at": "2023-04-12T21:33:14Z",
      "author": "tolap22",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-2adf761163ad",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2189",
    "title": "could not open read-only [file=/home/tey/.questdb/db/stock_history/default/change_percent.d",
    "text": "# could not open read-only [file=/home/tey/.questdb/db/stock_history/default/change_percent.d\n\n### Describe the bug\n\n_No response_\n\n### To reproduce\n\n1.Open web console\r\n2.Run sql  `SELECT * from 'stock_history'`\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB 6.4**:\r\n- **OS Fedora 36**:\n```\n\n\n### Additional context\n\nDDL:\r\n\r\nCREATE TABLE 'stock_history' (\r\n  ts TIMESTAMP,\r\n  code SYMBOL capacity 256 CACHE index capacity 256,\r\n  symbol STRING,\r\n  market_type INT,\r\n  name STRING,\r\n  change_percent DOUBLE,\r\n  settlement DOUBLE,\r\n  turnover_ratio DOUBLE,\r\n  volume LONG,\r\n  amount LONG,\r\n  market_total_capitalization DOUBLE,\r\n  created_day SYMBOL capacity 256 CACHE index capacity 256,\r\n  created_at  STRING\r\n) timestamp (ts);\r\n\r\nQuestdb data from home.\r\n\r\n[questdb.zip](https://github.com/questdb/questdb/files/8839483/questdb.zip)\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 94,
    "metadata": {
      "issue_number": 2189,
      "state": "open",
      "labels": [
        "Bug",
        "Question",
        "SQL"
      ],
      "comments_count": 16,
      "created_at": "2022-06-05T07:13:13Z",
      "updated_at": "2023-02-27T08:02:39Z",
      "closed_at": null,
      "author": "b0123456789",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-59236f05c2ef",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2118",
    "title": "Support for string_agg(distinct)",
    "text": "# Support for string_agg(distinct)\n\n### Is your feature request related to a problem?\n\nI'd like to be able aggregate distinct values of a column into a string.\n\n### Describe the solution you'd like.\n\nAdd an aggregating function which does the equivalent of what `string_agg` does in PostgreSQL. The `count_distinct` function almost does what I want, but instead of counting the different distinct values, I'd like to get them. Alternative would be adding something like `array_agg`, but if I understood things correctly, array types isn't well supported in questdb.\n\n### Describe alternatives you've considered.\n\nI do the aggregation now in a layer above, which isn't the most scalable solution and brings some overhead.\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 117,
    "metadata": {
      "issue_number": 2118,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "Good first issue"
      ],
      "comments_count": 16,
      "created_at": "2022-05-12T12:27:06Z",
      "updated_at": "2024-10-01T15:11:10Z",
      "closed_at": "2023-11-23T12:01:44Z",
      "author": "kallewesth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2bcc1d27a65d",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1490",
    "title": "An exception occurred while querying data",
    "text": "# An exception occurred while querying data\n\n### Describe the bug\r\n\r\n```\r\n2021-10-27 19:44:43.692 [pool-3-thread-7811] DEBUG c.s.m.q.CaptureResultMapper.getCaptureListByTime - ==>  Preparing: select * from capture_result_4_20211027 where cameraNo = ? and actionTime >= ? and actionTime < ? order by actionTime \r\n2021-10-27 19:44:43.692 [pool-3-thread-7811] DEBUG c.s.m.q.CaptureResultMapper.getCaptureListByTime - ==> Parameters: 16827(Integer), 2021-10-27 04:30:00(String), 2021-10-27 04:35:00(String)\r\n2021-10-27 19:44:43.698 [pool-3-thread-7811] INFO  c.s.service.impl.quest.CaptureResultServiceImpl - èŽ·å–capture_result_4_20211027æ•°æ®å¤±è´¥,error:\r\n### Error querying database.  Cause: org.postgresql.util.PSQLException: ERROR: [0]: max txn-inflight limit reached [txn=201417, min=176665]\r\n  Position: 15\r\n### The error may exist in com/secusoft/mapper/quest/CaptureResultMapper.java (best guess)\r\n### The error may involve com.secusoft.mapper.quest.CaptureResultMapper.getCaptureListByTime-Inline\r\n### The error occurred while setting parameters\r\n### SQL: select * from capture_result_4_20211027 where cameraNo = ? and actionTime >= ? and actionTime < ? order by actionTime\r\n### Cause: org.postgresql.util.PSQLException: ERROR: [0]: max txn-inflight limit reached [txn=201417, min=176665]\r\n  Position: 15\r\n; uncategorized SQLException for SQL []; SQL state [00000]; error code [0]; ERROR: [0]: max txn-inflight limit reached [txn=201417, min=176665]\r\n  Position: 15; nested exception is org.postgresql.util.PSQLException: ERROR: [0]: max txn-inflight limit reached [txn=201417, min=176665]\r\n  Position: 15\r\n```\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.0.5\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 195,
    "metadata": {
      "issue_number": 1490,
      "state": "closed",
      "labels": [],
      "comments_count": 16,
      "created_at": "2021-10-28T09:01:46Z",
      "updated_at": "2024-10-31T09:14:54Z",
      "closed_at": "2021-11-16T09:58:28Z",
      "author": "yjclsx",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-318790b28a61",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/436",
    "title": "The \"sum 1 billion doubles\" result on the front page can be incorrect.",
    "text": "# The \"sum 1 billion doubles\" result on the front page can be incorrect.\n\nI have installed QuestDB on my machine to reproduce the claimed results.\r\n\r\nI have:\r\nAMD Ryzen 9 3950X 16-Core Processor\r\n64 GiB RAM\r\n\r\nQuestDB is installed as the following:\r\n```\r\n$ wget https://github.com/questdb/questdb/releases/download/5.0.0/questdb-5.0.0-bin.tar.gz\r\n$ tar xf questdb-5.0.0-bin.tar.gz\r\n$ sudo apt install openjdk-14-jre\r\n$ export JAVA_HOME=/usr/lib/jvm/java-14-openjdk-amd64\r\n\r\n./questdb.sh start\r\n\r\nException in thread \"main\" io.questdb.network.NetworkError: [98] could not bind socket\r\n        at io.questdb@5.0.0/io.questdb.std.ThreadLocal.initialValue(ThreadLocal.java:36)\r\n        at java.base/java.lang.ThreadLocal.setInitialValue(ThreadLocal.java:195)\r\n        at java.base/java.lang.ThreadLocal.get(ThreadLocal.java:172)\r\n        at io.questdb@5.0.0/io.questdb.network.NetworkError.instance(NetworkError.java:46)\r\n        at io.questdb@5.0.0/io.questdb.network.AbstractIODispatcher.<init>(AbstractIODispatcher.java:96)\r\n        at io.questdb@5.0.0/io.questdb.network.IODispatcherLinux.<init>(IODispatcherLinux.java:36)\r\n        at io.questdb@5.0.0/io.questdb.network.IODispatchers.create(IODispatchers.java:41)\r\n        at io.questdb@5.0.0/io.questdb.cutlass.http.HttpServer.<init>(HttpServer.java:72)\r\n        at io.questdb@5.0.0/io.questdb.cutlass.http.HttpServer.create0(HttpServer.java:148)\r\n        at io.questdb@5.0.0/io.questdb.WorkerPoolAwareConfiguration.create(WorkerPoolAwareConfiguration.java:60)\r\n        at io.questdb@5.0.0/io.questdb.cutlass.http.HttpServer.create(HttpServer.java:110)\r\n        at io.questdb@5.0.0/io.questdb.ServerMain.main(ServerMain.java:145)\r\n        \r\nThe exception message is not quite handy.\r\n\r\n$ questdb status\r\nquestdb: command not found\r\n\r\nThe example in documentation looks wrong.\r\n\r\n$ ./questdb.sh status\r\n\r\n  ___                  _   ____  ____\r\n / _ \\ _   _  ___  ___| |_|  _ \\| __ )\r\n| | | | | | |/ _ \\/ __| __| | | |  _ \\\r\n| |_| | |_| |  __/\\__ \\ |_| |_| | |_) |\r\n \\__\\_\\\\__,_|\\___||___/\\__|____/|____/\r\n                        www.questdb.io\r\n\r\nNot running\r\n\r\nThe link \"with the HTTP API\" on the web page https://questdb.io/docs/guideBinaries goes to psql.\r\n\r\nLooks like it is using the same port as ClickHouse, so it cannot work with default configuration.\r\n\r\nhttp://localhost:9000/\r\n\r\nSELECT 1;\r\n-- does not work\r\n\r\nSELECT 1 FROM dual;\r\n-- does not work\r\n\r\ncreate table zz as (select rnd_double() d from long_sequence(1000000000));\r\nselect sum(d) from zz;\r\n-- when I copy-paste both commands to the web UI, it shows me\r\n\r\n[16:47:58] select sum(d) from zztable does not exist [name=zz]\r\n\r\nIf I execute these commands one by one, it's Ok.\r\n\r\nTable creation is quite slow (about ten seconds), but still Ok.\r\n\r\nselect sum(d) from zz;\r\n\r\nOk, sustained query time after a few runs is about 282 ms.\r\n```\r\n\r\nThe sustained query time is about 282 ms.\r\n\r\nI'm trying to reproduce the same with ClickHouse (master, clang release build):\r\n\r\n```\r\nSET max_memory_usage = '20G'\r\nCREATE TABLE zz (x Float64) ENGINE = Memory;\r\nINSERT INTO zz SELECT number FROM numbers_mt(1000000000)\r\n(3.8 seconds)\r\n\r\nSELECT sum(x) FROM zz\r\n0.241 sec.\r\n```\r\n\r\nThe sustained query time is about 240 ms.\r\n\r\nActually there is no much difference and ClickHouse gives slightly better results.\r\n(Or I did not follow the installation steps of QuestDB correctly?)\r\n\r\nIf I reduce the number of threads to 4 to align with the results from\r\nhttps://questdb.io/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second\r\nClickHouse gives 232 ms (even slightly better).\r\n\r\nThe \"sum int\" is executed in 0.121 (ClickHouse) vs. 0.135 (QuestDB, results on AMD Ryzen 3900X from your blog article).\r\nAgain, no much difference and ClickHouse is slightly better.\r\n\r\nI suspect that the results for ClickHouse in \"sum double\" benchmark were improved after https://github.com/ClickHouse/ClickHouse/pull/10992\r\n\r\nPS. Thank you for the benchmark and for the great product!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 443,
    "metadata": {
      "issue_number": 436,
      "state": "closed",
      "labels": [
        "Question"
      ],
      "comments_count": 16,
      "created_at": "2020-06-24T14:09:12Z",
      "updated_at": "2020-09-15T08:30:00Z",
      "closed_at": "2020-07-21T09:08:43Z",
      "author": "alexey-milovidov",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-f7aa0a22d5c8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5915",
    "title": "Improve limit -1 performance",
    "text": "# Improve limit -1 performance\n\n### Is your feature request related to a problem?\n\nNoticed that limit -1 can be quite slow, specially while ingesting data on a large table.\n\nThis can be tested on demo box with the new dataset.\n\n```\nselect * from market_data limit -1;\nselect * from market_data latest by symbol;\n```\nExplain says latest on does a frame backward scan, while limit -1 needs to skip over billions of frames, then do a row forward scan.\n\n```\nLimit lo: -1 skip-over-rows: 154330575 limit: 1\n    PageFrame\n        Row forward scan\n        Frame forward scan on: core_price\n```\nor for a larger table\n```\nLimit lo: -1 skip-over-rows: 1744670104 limit: 1\n    PageFrame\n        Row forward scan\n        Frame forward scan on: market_data\n```\nIâ€™ve observed during heavy ingestion latest on is way faster (few ms) than limit -1 (several seconds)\n\nEquivalent plans for the latest on\n```\nLatestByDeferredListValuesFiltered\n    Frame backward scan on: core_price\n```\n```\nLatestByDeferredListValuesFiltered\n    Frame backward scan on: market_data\n```\n\nAs hinted by @puzpuzpuz, I tried\n\n```\nselect * from market_data order by timestamp desc limit 1;\n```\n\nAnd this was good, just a few milliseconds. With this plan\n\n```\nLimit lo: 1 skip-over-rows: 0 limit: 1\n    PageFrame\n        Row backward scan\n        Frame backward scan on: market_data\n```\n\n### Describe the solution you'd like.\n\nI would expect `limit -1` to apply the same frame backward strategy that the `order by desc limit 1` is doing\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\njavier\n\n### Affiliation:\n\nquestdb\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 254,
    "metadata": {
      "issue_number": 5915,
      "state": "closed",
      "labels": [
        "Enhancement",
        "SQL",
        "Performance",
        "Friction"
      ],
      "comments_count": 15,
      "created_at": "2025-07-08T12:06:03Z",
      "updated_at": "2025-08-04T13:16:52Z",
      "closed_at": "2025-08-04T13:16:52Z",
      "author": "javier",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ca1559f2610e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5025",
    "title": "Avoid multiple SQL function compilations when preparing a parallel filter/GROUP BY query",
    "text": "# Avoid multiple SQL function compilations when preparing a parallel filter/GROUP BY query\n\n### Is your feature request related to a problem?\n\nCurrently, when compiling a parallel filter/GROUP BY query we're compiling SQL functions from AST multiple times, once per the owner query thread and each shared worker thread. We could speed up the query compilation by compiling the functions only once and then cloning the functions per worker. This needs a new `deepClone()` method on the `Function` interface and proper implementations in all functions.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nAndrei Pechkurov\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 113,
    "metadata": {
      "issue_number": 5025,
      "state": "closed",
      "labels": [
        "Enhancement",
        "SQL",
        "Performance"
      ],
      "comments_count": 15,
      "created_at": "2024-10-07T08:31:35Z",
      "updated_at": "2024-12-02T16:08:27Z",
      "closed_at": "2024-12-02T16:08:26Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-dd6658edc5c6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4746",
    "title": "Support exclusive and half-open intervals in BETWEEN expressions",
    "text": "# Support exclusive and half-open intervals in BETWEEN expressions\n\n### Is your feature request related to a problem?\r\n\r\nQuestDB supports BETWEEN expressions:\r\n\r\n```sql\r\nWHERE timestamp BETWEEN x AND y\r\n```\r\n\r\nThese expressions serve as a convenient syntax for representing inclusive intervals. This is syntax sugar for:\r\n\r\n```sql\r\nWHERE timestamp >= x AND timestamp <= y\r\n```\r\n\r\nFor many applications, an inclusive interval is fine. However, there are other types of intervals that can be useful to represent: inclusive, exclusive, left-open, right-open.\r\n\r\nWe could extend the BETWEEN syntax to support these intervals.\r\n\r\nThis can also serve as a small performance optimisation, since both conditions will be calculated in a single function call.\r\n\r\n\r\n### Describe the solution you'd like.\r\n\r\nNew syntax to cover the other three main cases of intervals:\r\n\r\n- [x] Inclusive: `WHERE timestamp >= x AND timestamp <= y`\r\n    - Current syntax: `WHERE timestamp BETWEEN x AND y`\r\n    - Extension: `WHERE timestamp BETWEEN INCLUSIVE x AND y` \r\n    - (but default remains as-is, not necessary to specify inclusive)\r\n    \r\n- [ ] Exclusive: `WHERE timestamp > x and timestamp < y`\r\n    - Extension: `WHERE timestamp BETWEEN EXCLUSIVE x AND y` \r\n    \r\n- [ ] Left-open: `WHERE timestamp > x AND timestamp <= y`\r\n    - Extension: `WHERE timestamp BETWEEN LEFT OPEN x AND y`\r\n    \r\n- [ ] Right-open: `WHERE timestamp >= x AND timestamp < y`\r\n    - Extension: `WHERE timestamp BETWEEN RIGHT OPEN x AND y`\r\n    \r\n\r\nThe extra terms could also be placed at the end i.e:\r\n\r\n`WHERE timestamp BETWEEN x AND y EXCLUSIVE`\r\n\r\n### Describe alternatives you've considered.\r\n\r\nUsing `<`, `<=`, `>`, `>=` operators.\r\n\r\n### Full Name:\r\n\r\nNick Woolmer\r\n\r\n### Affiliation:\r\n\r\nQuestDB\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 274,
    "metadata": {
      "issue_number": 4746,
      "state": "open",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 15,
      "created_at": "2024-07-04T10:14:02Z",
      "updated_at": "2024-07-16T17:55:16Z",
      "closed_at": null,
      "author": "nwoolmer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-dfa49ed6eeba",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4457",
    "title": "Unsupported machine",
    "text": "# Unsupported machine\n\n### To reproduce\n\nQuestDB 7.4.3 built from sources in FreeBSD jail hosted on Ampere powered VM (AARCH64) in Oracle Cloud.\r\nOpenjdk 11.0.22\r\nApache Maven 3.9.6\r\nTrying to start QuestDB got an error:\r\n\r\n```\r\njava.lang.UnsatisfiedLinkError: /tmp/libquestdb9744066241967681339.so: /tmp/libquestdb9744066241967681339.so: unsupported machine (Possible cause: can't load AMD 64-bit .so on a AARCH64-bit platform)\r\n        at java.base/java.lang.ClassLoader$NativeLibrary.load0(Native Method)\r\n        at java.base/java.lang.ClassLoader$NativeLibrary.load(ClassLoader.java:2450)\r\n        at java.base/java.lang.ClassLoader$NativeLibrary.loadLibrary(ClassLoader.java:2506)\r\n        at java.base/java.lang.ClassLoader.loadLibrary0(ClassLoader.java:2705)\r\n        at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2635)\r\n        at java.base/java.lang.Runtime.load0(Runtime.java:768)\r\n        at java.base/java.lang.System.load(System.java:1854)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.std.Os.loadLib(Os.java:221)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.std.Os.loadLib(Os.java:199)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.std.Os.<clinit>(Os.java:266)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.Bootstrap.<clinit>(Bootstrap.java:696)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.ServerMain.<init>(ServerMain.java:74)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.ServerMain.main(ServerMain.java:168)\r\nException in thread \"main\" java.lang.NoClassDefFoundError: Could not initialize class io.questdb.std.Os\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.log.LogFactory.<clinit>(LogFactory.java:1152)\r\n        at io.questdb@7.4.3-SNAPSHOT/io.questdb.ServerMain.main(ServerMain.java:171)\r\n```\r\n\r\nProbably, something need to be rebuilt on current arch, but I cannot find any document about it.\r\n\n\n### QuestDB version:\n\nQuestDB 7.4.3-SNAPSHOT\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nFreeBSD 14.0-RELEASE-p6 (jail)\n\n### File System, in case of Docker specify Host File System:\n\nUFS\n\n### Full Name:\n\nPeter TKATCHENKO\n\n### Affiliation:\n\nFlytrace\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 181,
    "metadata": {
      "issue_number": 4457,
      "state": "closed",
      "labels": [
        "Friction"
      ],
      "comments_count": 15,
      "created_at": "2024-04-29T20:02:03Z",
      "updated_at": "2024-05-20T21:31:46Z",
      "closed_at": "2024-05-20T21:31:46Z",
      "author": "Peter2121",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-4103b2c39037",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2578",
    "title": "ðŸŒ Hacktoberfest 2022: list of SQL functions for contributors",
    "text": "# ðŸŒ Hacktoberfest 2022: list of SQL functions for contributors\n\nHello everyone, [happy Hacktoberfest](https://hacktoberfest.com/)!\r\n\r\nQuestDB is accepting PRs for Hacktoberfest 2022! \r\nWe create this issue to list some feature ideas you can contribute to our core database! \r\n\r\n\r\n# Ideas\r\n\r\nWe collect a list of ideas you could contribute to QuestDB, focusing on PostgreSQL compatibility. \r\n\r\nðŸ¥ Junior\r\nðŸ” Experienced\r\n\r\n## Numeric ðŸ¥ \r\n- [x] `ceiling` https://github.com/questdb/questdb/pull/2600\r\n- [x] `log10` https://github.com/questdb/questdb/pull/2579\r\n- [x] align the use of `log` and use it as `ln` https://github.com/questdb/questdb/pull/2605\r\n- [ ] `width_bucket` https://github.com/questdb/questdb/pull/2630\r\n\r\n## String ðŸ¥ \r\n- [x] add `lower` and `upper` as aliases to our existing functions to_lower, to_upper to be PostgreSQL compatible https://github.com/questdb/questdb/pull/2582\r\n- [x] add `position` as alias of `strpos` to be PostgreSQL compatible https://github.com/questdb/questdb/pull/2599\r\n- [x] `ltrim`, `rtrim`,`trim`, https://github.com/questdb/questdb/pull/2613\r\n- [x] `lpad`, `rpad`, https://github.com/questdb/questdb/pull/2631\r\n- [x] `starts_with`, https://github.com/questdb/questdb/pull/2608\r\n- [x] `split_part`,  https://github.com/questdb/questdb/pull/2644\r\n\r\nReference: https://www.postgresql.org/docs/15/functions-string.html\r\n\r\n## Window functions ðŸ” \r\n\r\n- [ ] corr\r\n- [ ] stddev\r\n- [ ] mode\r\n- [ ] percentile_count, percentile_disc https://github.com/questdb/questdb/issues/1371\r\n- [x] rank, https://github.com/questdb/questdb/pull/2624 \r\n- [ ] dense_rank\r\n- [ ] percent_rank\r\n- [ ] ntile\r\n- [ ] lag\r\n- [ ] lead\r\n- [ ] first_value\r\n- [ ] last_value\r\n- [ ] nth_value\r\n\r\n# Tips\r\n\r\n- Pay attention to [our contribution guidelines](https://github.com/questdb/questdb/blob/master/CONTRIBUTING.md).\r\n- Start with existing issues (or refer to the above ideas) rather than creating new ones. While new ideas are generally welcomed, they don't always fit the project roadmap.\r\n- Filter issues with `good first issues` or `help wanted` tags if you're new to the projects.\r\n- Avoid commenting on all the available issues but those you really plan to work on, leaving some opportunities to other contributors.\r\n- Our maintainers will review your pull requests, please make sure you address all the comments before asking for another review. \r\n- Join our [Slack](https://slack.questdb.io/) in #contributors channel if you need extra help. \r\n\r\n# Others\r\n\r\nNot familiar with SQL functions or database development? Here are other repositories that accept contributions: \r\n- [Web console](https://github.com/questdb/ui/tree/main/packages/web-console): Monorepo that contains the code of QuestDB GUI. It is a web application written with Typescript and React.\r\n- [Documentation](https://github.com/questdb/questdb.io): QuestDB's website for documentation, suitable for technical writers who have experience with Markdown and Docusaurus. Read [our guidelines for docs contributors](https://github.com/questdb/questdb.io#contributing).",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 371,
    "metadata": {
      "issue_number": 2578,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "SQL",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 15,
      "created_at": "2022-10-04T09:29:26Z",
      "updated_at": "2023-10-03T06:07:08Z",
      "closed_at": "2023-10-03T06:07:08Z",
      "author": "pswu11",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-98d1d8a401d6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2019",
    "title": "Very frequent random crash",
    "text": "# Very frequent random crash\n\n### Describe the bug\n\nWe are seeing the very frequent crash. there is no particular sequence or with certain data. the crash is really random.\r\n\r\n A fatal error has been detected by the Java Runtime Environment:\r\n SIGSEGV (0xb) at pc=0x00007f88f8bfd930, pid=548526, tid=548553\r\nJRE version: OpenJDK Runtime Environment AdoptOpenJDK (11.0.9.1+1) (build 11.0.9.1+1)\r\nJava VM: OpenJDK 64-Bit Server VM AdoptOpenJDK (11.0.9.1+1, mixed mode, tiered, compressed oops, parallel gc, linux-amd64)\r\n Problematic frame:\r\n J 2408 c2 io.questdb.cairo.vm.api.MemoryCR.getLong(J)J io.questdb@6.2 (44 bytes) @ 0x00007f88f8bfd930 [0x00007f88f8bfd8c0+0x0000000000000070]\r\n\r\n No core dump will be written. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n\r\nstdout-2022-04-07T22-34-26.txt](https://github.com/questdb/questdb/files/8446400/stdout-2022-04-07T22-34-26.txt)\r\n[hs_err_pid548526.log](https://github.com/questdb/questdb/files/8446421/hs_err_pid548526.log)\r\n\r\n\n\n### To reproduce\n\nThere is no particular step to reproduce the issue. issues are seen very often with our test case environment\n\n### Expected Behavior\n\nquestDB running continuously with out interruption\n\n### Environment\n\n```markdown\n- **QuestDB version**:6.2\r\n- **OS**: ubuntu 20.04\r\n- **Browser**:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 158,
    "metadata": {
      "issue_number": 2019,
      "state": "closed",
      "labels": [
        "Bug",
        "Question",
        "ILP"
      ],
      "comments_count": 15,
      "created_at": "2022-04-07T21:00:10Z",
      "updated_at": "2022-05-04T11:40:34Z",
      "closed_at": "2022-05-04T11:40:34Z",
      "author": "sadaiyandi-exalens",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-04a9ff0c2533",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1268",
    "title": "Adding a LAG equivalent function",
    "text": "# Adding a LAG equivalent function\n\nHi,\r\n\r\nThis has already been discussed in the community Slack but a function equivalent to the Postgres LAG function.\r\nOther DB's support this function as well Influx have it as DIFFERENCE, Graphite as derivative().\r\n\r\nWhy is this important? Just to quote the Graphite documentation as it's a clear cut example:\r\n> Each time you run ifconfig, the RX and TXPackets are higher (assuming there is network traffic.) By applying the derivative function, you can get an idea of the packets per minute sent or received, even though youâ€™re only recording the total.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 98,
    "metadata": {
      "issue_number": 1268,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "SQL",
        "Good first issue"
      ],
      "comments_count": 15,
      "created_at": "2021-08-24T02:58:46Z",
      "updated_at": "2024-09-20T15:00:43Z",
      "closed_at": "2024-09-20T15:00:43Z",
      "author": "toomuchio",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-0e867344ed9c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3855",
    "title": "send data from my java app with influxdb library using http protocol",
    "text": "# send data from my java app with influxdb library using http protocol\n\n### Describe the bug\r\n\r\nHi\r\nI'm using influxdb need to replace it with questdb.\r\nin my app current use influxdb java library to send data to influx (over http protocol).\r\n\r\nnow question is is there any way to send data to questiondb in that way? i mean over http via influxdb java library?\r\n\r\n\r\nFYI: read this:   https://questdb.io/docs/reference/api/ilp/overview/\r\n\r\nDifference from InfluxDB\r\nQuestDB TCP Receiver uses InfluxDB Line Protocol as both serialization and the transport format. InfluxDB on other hand uses HTTP as the transport and InfluxDB Line Protocol as serialization format. For this reason the existing InfluxDB client libraries will not work with QuestDB.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 116,
    "metadata": {
      "issue_number": 3855,
      "state": "closed",
      "labels": [
        "Question",
        "ILP"
      ],
      "comments_count": 14,
      "created_at": "2023-10-16T12:55:29Z",
      "updated_at": "2023-10-20T08:36:42Z",
      "closed_at": "2023-10-20T08:36:42Z",
      "author": "mehrdad2000",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-9a90eb7d8a81",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2892",
    "title": "Client gets disconnected when rounding a value in SQL (used `pcli`)",
    "text": "# Client gets disconnected when rounding a value in SQL (used `pcli`)\n\n### Describe the bug\n\n```\r\n2022-12-29T17:46:01.841044Z I i.q.g.SqlCompiler batch [text=select trade.ts, trade.sym, first(px) o, max(px) h, min(px) l, last(px) c, log(last(px)) - log(first(px)) logr, round(sum(abs(qty)),2) v, avg(leverage) avglvg, log(last(leverage)) - log(first(leverage)) loglvgchg from trade asof join byb_ltinfo on (sym) where trade.sym = 'BTC3LUSDT' sample by 5m ;]\r\n2022-12-29T17:46:01.841679Z C i.q.c.p.PGWireServer internal error [ex=\r\njava.lang.AssertionError\r\n        at io.questdb.std.ObjList.getQuick(ObjList.java:173)\r\n        at io.questdb.griffin.SqlOptimiser.rewriteSelectClause0(SqlOptimiser.java:3158)\r\n        at io.questdb.griffin.SqlOptimiser.rewriteSelectClause(SqlOptimiser.java:2975)\r\n        at io.questdb.griffin.SqlOptimiser.optimise(SqlOptimiser.java:3546)\r\n        at io.questdb.griffin.SqlCompiler.compileExecutionModel(SqlCompiler.java:1112)\r\n        at io.questdb.griffin.SqlCompiler.compileUsingModel(SqlCompiler.java:1193)\r\n        at io.questdb.griffin.SqlCompiler.compileInner(SqlCompiler.java:1146)\r\n        at io.questdb.griffin.SqlCompiler.compileBatch(SqlCompiler.java:314)\r\n        at io.questdb.cutlass.pgwire.PGConnectionContext.processQuery(PGConnectionContext.java:2237)\r\n        at io.questdb.cutlass.pgwire.PGConnectionContext.parse(PGConnectionContext.java:1557)\r\n        at io.questdb.cutlass.pgwire.PGConnectionContext.handleClientOperation(PGConnectionContext.java:385)\r\n        at io.questdb.cutlass.pgwire.PGJobContext.handleClientOperation(PGJobContext.java:89)\r\n        at io.questdb.cutlass.pgwire.PGWireServer$1.lambda$$0(PGWireServer.java:83)\r\n        at io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODispatcher.java:169)\r\n        at io.questdb.cutlass.pgwire.PGWireServer$1.run(PGWireServer.java:110)\r\n        at io.questdb.mp.Worker.run(Worker.java:118)\r\n]\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.6.1\r\n- **OS**: Linux\r\n- **Browser**:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 128,
    "metadata": {
      "issue_number": 2892,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 14,
      "created_at": "2022-12-29T17:50:39Z",
      "updated_at": "2023-02-01T09:18:32Z",
      "closed_at": "2023-01-26T21:30:41Z",
      "author": "vbmithr",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-39f34b4dddcb",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2831",
    "title": "Slow inserts",
    "text": "# Slow inserts\n\n### Describe the bug\r\n\r\nHi, I absolutely love this product, QuestDB team has done an amazing job! I do face an issue\r\n\r\nI have a table which has 3 Symbol columns(Ticker, Exchange, etc), a Date/Timestamp column and OHLCV columns. the Symbol columns have indices on them as well. When I try to insert rows in this table, the performance is quite bad, I am afraid. it takes more than an hour to insert 1 million rows. (I commit about 100 tickers at a time, could be between 100k - 200k rows). \r\n\r\n1. the bad performance could be because of the Indices - can you confirm? if so, is there a way to disable indices?\r\n2. I tried without indices on Symbol columns, the query performance with indices is far better - is this expected?\r\n3. I note that the new version of QuestDB is better at handling commits. I have some code to close connection using close(true). should I batch the results myself or let QuestDB handle it? \r\n4. I havent tried disabling indices and reinserting. looks like the only I can do that is by dropping indices and then Reindex - is there another way\r\n\r\nI use the python lib to insert data in the database. even with the above, I cant match the million rows per second performance. I must admit my knowledge of the lib is quiet nascent, a piece of documentation on optimizing inserts would help users like me a lot. \r\n\r\nthanks again\r\nRoh\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 277,
    "metadata": {
      "issue_number": 2831,
      "state": "closed",
      "labels": [
        "Question",
        "Performance"
      ],
      "comments_count": 14,
      "created_at": "2022-11-29T11:58:46Z",
      "updated_at": "2023-02-22T07:01:55Z",
      "closed_at": "2023-02-22T06:39:15Z",
      "author": "Roh-codeur",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-829d00a31716",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2623",
    "title": "Escape mechanism doesn't work in like/ilike functions",
    "text": "# Escape mechanism doesn't work in like/ilike functions\n\n### Describe the bug\r\n\r\nLike and ilike functions don't recognize default escape character '\\\\' nor allow using a different one via ESCAPE clause . \r\n\r\n### To reproduce\r\n\r\n```sql\r\nSELECT '\\quest' LIKE '\\_uest'; \r\n--true but should be false\r\n\r\nSELECT 'quest_' LIKE 'quest\\_'\r\n--false but should be true \r\n\r\nselect 'quest' like 'quest' escape 'Z'\r\n--found [tok=''Z'', len=3] ',', 'from' or 'over' expected\r\n```\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:6.5.4/master\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 90,
    "metadata": {
      "issue_number": 2623,
      "state": "open",
      "labels": [
        "Bug",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 14,
      "created_at": "2022-10-11T09:56:08Z",
      "updated_at": "2025-09-30T19:30:19Z",
      "closed_at": null,
      "author": "bziobrowski",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-24780674864d",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2505",
    "title": "Case sensitive DROP table + misleading error message",
    "text": "# Case sensitive DROP table + misleading error message\n\n### Describe the bug\n\nIt seems SQL DROP is case sensitive. This is probably not intentional. Moreover it produces a confusing error message when case is not matching.\n\n### To reproduce\n\nIn a web console:\r\n1. Create a new table: `create table Orders2 (name String);`\r\n2. Try to drop it: `drop table orders2;`\r\n3. It produces the following error: `Could not lock 'orders2' [reason='missing or owned by other process']`\r\n\r\nSometimes it works OK and the table is dropped correctly. If that's the case then go back to #1 and try it again. I'll probably fail to drop the table for 2nd time. This tells me it's related to caching. \n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.5.2\r\n- **OS**: MacOS\r\n- **Browser**: Firefix & webconsole\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 144,
    "metadata": {
      "issue_number": 2505,
      "state": "open",
      "labels": [
        "Bug",
        "SQL",
        "hacktoberfest"
      ],
      "comments_count": 14,
      "created_at": "2022-09-06T13:53:39Z",
      "updated_at": "2024-10-20T09:15:46Z",
      "closed_at": null,
      "author": "jerrinot",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b8bb8743f518",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2294",
    "title": "SQL: `select * trades` should be parse error",
    "text": "# SQL: `select * trades` should be parse error\n\n### Describe the bug\n\n_No response_\n\n### To reproduce\n\n1. go to (demo)[https://demo.questdb.io/]\r\n2. query `select * trades`. Note of missing `FROM` keyword\r\n\r\nOne row with 1 returned\n\n### Expected Behavior\n\nSQL error\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.4.2\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 55,
    "metadata": {
      "issue_number": 2294,
      "state": "closed",
      "labels": [
        "Bug",
        "Good first issue"
      ],
      "comments_count": 14,
      "created_at": "2022-07-04T15:27:21Z",
      "updated_at": "2022-09-22T13:04:26Z",
      "closed_at": "2022-09-22T10:37:42Z",
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-1cc39591b95f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2106",
    "title": "Unhandled Error causing :9003 to report unhealthy status",
    "text": "# Unhandled Error causing :9003 to report unhealthy status\n\n### Describe the bug\r\n\r\nnot entirely sure as not immediately detected\r\n\r\n(sorry!...will upgrade process to more immediately flag unhandled errors/unhealthy status)\r\n\r\ni run at terminal prompt :~/qdb/log$ grep -B 20 -ir 'unhandled error \\[job=' '.'\r\n\r\n\r\n## To reproduce\r\n\r\n### OCCURANCE A:\r\n\r\n```\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.536226Z I i.q.c.TableWriter sorting o3 [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.536535Z I i.q.c.TableWriter sorted [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.539416Z I i.q.c.TableWriter merged partition [table=`EOD`, ts=2007-01-01T00:00:00.000000Z, txn=28]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.539878Z I i.q.c.TableWriter purged [path=/REDACTED_PATH]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.539910Z I i.q.c.p.WriterPool << [table=`EOD`, thread=17]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.583007Z I http-server scheduling disconnect [fd=7, reason=12]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:08.583049Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:11.912711Z I http-server connected [ip=127.0.0.1, fd=7]\r\n./stdout-2022-05-02T05-00-33.txt-oops2: 9\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:12.040412Z I i.q.c.t.TextLoader configured [table=`EOD`, overwrite=false, durable=false, atomicity=2, partitionBy=YEAR, timestamp=TradeDate]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:12.040601Z I i.q.c.t.TextDelimiterScanner not enough lines [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:12.040617Z I i.q.c.h.HttpConnectionContext failed query result cannot be delivered. Kicked out [fd=7, error=not enough lines [table=EOD]]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:12.040684Z I http-server scheduling disconnect [fd=7, reason=14]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:12.045229Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.674614Z I http-server connected [ip=127.0.0.1, fd=7]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.674810Z I i.q.c.t.TextLoader configured [table=`EOD`, overwrite=false, durable=false, atomicity=2, partitionBy=YEAR, timestamp=TradeDate]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.674961Z I i.q.c.t.TextDelimiterScanner not enough lines [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.674973Z I i.q.c.h.HttpConnectionContext failed query result cannot be delivered. Kicked out [fd=7, error=not enough lines [table=EOD]]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.675040Z I http-server scheduling disconnect [fd=7, reason=14]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:55:17.676600Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt:2022-05-02T05:55:17.676711Z E server-main unhandled error [job=io.questdb.cutlass.http.HttpServer$1@3a6f2de3, ex=\r\n```\r\n\r\n\r\n\r\n### OCCURANCE B:\r\n\r\n```\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.649042Z I i.q.c.TableWriter sorting o3 [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.649385Z I i.q.c.TableWriter sorted [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.653030Z I i.q.c.TableWriter merged partition [table=`EOD`, ts=2007-01-01T00:00:00.000000Z, txn=31]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.653576Z I i.q.c.TableWriter purged [path=/REDACTED_PATH/2007.30]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.653621Z I i.q.c.p.WriterPool << [table=`EOD`, thread=16]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.780798Z I http-server scheduling disconnect [fd=7, reason=12]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:50.780812Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt-oops2: 9\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.209580Z I http-server connected [ip=127.0.0.1, fd=7]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.230886Z I i.q.c.t.TextLoader configured [table=`EOD`, overwrite=false, durable=false, atomicity=2, partitionBy=YEAR, timestamp=TradeDate]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.231030Z I i.q.c.t.TextDelimiterScanner not enough lines [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.231044Z I i.q.c.h.HttpConnectionContext failed query result cannot be delivered. Kicked out [fd=7, error=not enough lines [table=EOD]]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.231111Z I http-server scheduling disconnect [fd=7, reason=14]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:53.235641Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.868201Z I http-server connected [ip=127.0.0.1, fd=7]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.995069Z I i.q.c.t.TextLoader configured [table=`EOD`, overwrite=false, durable=false, atomicity=2, partitionBy=YEAR, timestamp=TradeDate]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.995214Z I i.q.c.t.TextDelimiterScanner not enough lines [table=EOD]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.995224Z I i.q.c.h.HttpConnectionContext failed query result cannot be delivered. Kicked out [fd=7, error=not enough lines [table=EOD]]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.995279Z I http-server scheduling disconnect [fd=7, reason=14]\r\n./stdout-2022-05-02T05-00-33.txt-2022-05-02T05:57:59.999182Z I http-server disconnected [ip=127.0.0.1, fd=7, src=queue]\r\n./stdout-2022-05-02T05-00-33.txt:2022-05-02T05:57:59.999259Z E server-main unhandled error [job=io.questdb.cutlass.http.HttpServer$1@3a393455, ex=\r\n```\r\n\r\n### Expected Behavior\r\n\r\nno unhandled error.\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.2.1\r\n- **OS**: ubuntu\r\n- **Browser**: edge / python request lib\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nplease let me know how i can better log these issues--or create a self reporting module for errors...?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 420,
    "metadata": {
      "issue_number": 2106,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 14,
      "created_at": "2022-05-06T17:23:03Z",
      "updated_at": "2022-05-17T11:16:35Z",
      "closed_at": "2022-05-17T11:16:35Z",
      "author": "ospqsp",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-3843019eaa43",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1843",
    "title": "Detect and prevent duplicate timestamps from being inserted into a time-partitioned table",
    "text": "# Detect and prevent duplicate timestamps from being inserted into a time-partitioned table\n\n### Is your feature request related to a problem?\n\nIn a time-partitioned table, it is currently possible to insert two rows with the exact same timestamp.\n\n### Describe the solution you'd like.\n\nWhen inserting a row, is it possible to detect if the timestamp of that row already exists in the time-partitioned table and prevent the row from being inserted?\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\nFor example, when using Kafka to populate QuestDB, there may be a situation where a message is processed more than once, causing the exact same row with the same timestamp to be inserted, which we wish to avoid.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 121,
    "metadata": {
      "issue_number": 1843,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 14,
      "created_at": "2022-01-28T16:40:13Z",
      "updated_at": "2024-07-30T06:43:04Z",
      "closed_at": "2024-07-30T06:43:04Z",
      "author": "AaronCQL",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-18333e4de337",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1763",
    "title": "SIGSEV in find_latest_for_key()",
    "text": "# SIGSEV in find_latest_for_key()\n\n### Describe the bug\r\n\r\nOccasionally, questdb crashes with a SIGSEV, but what causes it is still unclear. The corresponding error log is attached, which includes details of the environment quest was running in.\r\n\r\ntable schema:\r\n```\r\nheight - long\r\nhash - string\r\ntimestamp - timestamp\r\npa - symbol\r\nta - string\r\noa - string\r\noamt - long\r\nra - string\r\nramt - long\r\nexc - string\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.1.3\r\n- **OS**: Ubuntu 20.04\r\n- **Browser**: Not Applicable\r\n```\r\n\r\n\r\n### Additional context\r\nQuest log (11/01/2022): [questdb-debug-info-error_private.log](https://github.com/questdb/questdb/files/7843090/questdb-debug-info-error_private.log)\r\n\r\nError log (11/01/2022): [hs_err_pid31077_private.log](https://github.com/questdb/questdb/files/7843089/hs_err_pid31077_private.log)\r\n\r\nError log: [hs_err_pid1059.log](https://github.com/questdb/questdb/files/7822432/hs_err_pid1059.log)\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 111,
    "metadata": {
      "issue_number": 1763,
      "state": "open",
      "labels": [
        "Bug"
      ],
      "comments_count": 14,
      "created_at": "2022-01-06T14:29:32Z",
      "updated_at": "2022-02-17T10:53:17Z",
      "closed_at": null,
      "author": "KendrickAng",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-10e882253911",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/569",
    "title": "ARM64/apple silicon support",
    "text": "# ARM64/apple silicon support\n\nI tried to build and run ARM64 docker image referenced this doc, but I could'nt run it.\r\nhttps://github.com/questdb/questdb/tree/master/core#switch-docker-desktop-to-linux\r\n\r\nMy approach was below.\r\n\r\nFirst, I couldn't run the command on the document because repo has no file such as `Dockerfile-linux-arm64`\r\n\r\n```\r\ndocker buildx build --platform linux/arm64 -t questdb/questdb:4.0.0-linux-arm64 --file Dockerfile-linux-arm64 . --load\r\n```\r\n\r\nSo I created a new one based on `Dockerfile-linux` which modefied it.\r\n\r\n```\r\ndiff Dockerfile-linux Dockerfile-linux-arm64\r\n\r\n18c18\r\n< RUN tar xvfz questdb-*-rt-linux-amd64.tar.gz\r\n---\r\n> RUN tar xvfz questdb-*-rt-linux-aarch64.tar.gz\r\n24c24\r\n< COPY --from=0 /build/questdb/core/target/questdb-*-rt-linux-amd64 .\r\n---\r\n> COPY --from=0 /build/questdb/core/target/questdb-*-rt-linux-aarch64 .\r\n```\r\n\r\nImage build was succeed, So I tried to run it on [Ubuntu 18.04 for Raspberry Pi ARM64](https://wiki.ubuntu.com/ARM/RaspberryPi) and I had an error as bellow.\r\n\r\n```\r\nsudo docker run -p 9000:9000 -p 8812:8812 -p 9009:9009 bathtimefish/questdb:5.0.3-linux-arm64\r\nQuestDB server [DEVELOPMENT]\r\nCopyright (C) 2014-2020, all rights reserved.\r\n\r\n2020-08-30T11:35:00.568759Z I server-main extracted [path=/root/.questdb/public/573e9313e05d89f4acb5d005476c61bb.woff]\r\n2020-08-30T11:35:02.747060Z I server-main extracted [path=/root/.questdb/public/6437d5f4c14e01aaf4801aef3909151b.woff]\r\n2020-08-30T11:35:02.757650Z I server-main extracted [path=/root/.questdb/public/7dafb1a4bab4938a38b2d68d9c4575a9.woff]\r\n2020-08-30T11:35:02.769892Z I server-main extracted [path=/root/.questdb/public/80b32d346cfff1921b52.worker.js]\r\n2020-08-30T11:35:02.796667Z I server-main extracted [path=/root/.questdb/public/a940d584750708f5435ce2c523498ddb.woff]\r\n2020-08-30T11:35:02.805294Z I server-main extracted [path=/root/.questdb/public/d0b483ce9717b92b4bf815d5d4db597a.woff]\r\n2020-08-30T11:35:02.808302Z I server-main extracted [path=/root/.questdb/public/index.html]\r\n2020-08-30T11:35:02.827243Z I server-main extracted [path=/root/.questdb/public/qdb.css]\r\n2020-08-30T11:35:02.949609Z I server-main extracted [path=/root/.questdb/public/qdb.js]\r\n2020-08-30T11:35:02.952580Z I server-main extracted [path=/root/.questdb/public/assets/console-configuration.json]\r\n2020-08-30T11:35:02.955519Z I server-main extracted [path=/root/.questdb/public/assets/favicon.png]\r\n2020-08-30T11:35:02.997642Z I server-main extracted [path=/root/.questdb/conf/date.formats]\r\n2020-08-30T11:35:03.005191Z I server-main extracted [path=/root/.questdb/conf/mime.types]\r\n2020-08-30T11:35:03.010691Z I server-main extracted [path=/root/.questdb/conf/server.conf]\r\n2020-08-30T11:35:03.891395Z I i.q.c.t.t.InputFormatConfiguration loading [from=/text_loader.json]\r\nException in thread \"main\" 2020-08-30T11:35:04.027201Z I server-main database root [dir=/root/.questdb/db/]\r\njava.lang.UnsatisfiedLinkError: 'int io.questdb.std.Vect.getSupportedInstructionSet()'\r\n\tat io.questdb@5.0.3-SNAPSHOT/io.questdb.std.Vect.getSupportedInstructionSet(Native Method)\r\n\tat io.questdb@5.0.3-SNAPSHOT/io.questdb.std.Vect.getSupportedInstructionSetName(Vect.java:40)\r\n\tat io.questdb@5.0.3-SNAPSHOT/io.questdb.ServerMain.main(ServerMain.java:134)\r\n```\r\n\r\nHow can I do for that?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 243,
    "metadata": {
      "issue_number": 569,
      "state": "closed",
      "labels": [],
      "comments_count": 14,
      "created_at": "2020-08-30T14:04:40Z",
      "updated_at": "2022-05-27T05:56:44Z",
      "closed_at": "2021-03-30T09:42:03Z",
      "author": "bathtimefish",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-38bef3269f7c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/353",
    "title": "SHOW TABLES should list results alphabetically by default",
    "text": "# SHOW TABLES should list results alphabetically by default\n\nIn the current version, `SHOW TABLES` does not return the result in a sorted manner. Adding `ORDER BY tableName` does not help. Most DBs sort the output of `SHOW TABLES` alphabetically by default, I think it would make sense if we did the same.\r\n\r\n__Note:__ This is not a UI-only change regarding results in the Web Console, but requires changes in how the SQL is executed in the backend.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 78,
    "metadata": {
      "issue_number": 353,
      "state": "open",
      "labels": [
        "Enhancement",
        "SQL",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 14,
      "created_at": "2020-06-02T12:57:36Z",
      "updated_at": "2025-08-03T13:42:12Z",
      "closed_at": null,
      "author": "mpsq",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2818ab883f37",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/53",
    "title": "Problems loading 7M real data",
    "text": "# Problems loading 7M real data\n\nHi Vlad, as i told you I'm just in the middle of testing questdb. I want to test it with real data, I mean data we use in our current business.\n\nIn our case we've got a Oracle Database with 7/8 millions of bills. I've got a java program that extracts the data from the Oracle DB and writes the information in two different desatinations; a csv file and a questDB storage.\n\nI'm using a modified version program that i'm beeen using to test other different libraries. The program uses sql2o to gather data from the relational DB and SimpleFlatMapper to generate the CSV file with the data. There is not any problem to generate the CSV file with the 8M of records.\n\nBut the problem comes up as I try to store the data in questDB. I get the error below. I've repeated the execution several times and the error appears,more or less, with the same number of bills loaded.\n\nIn attach, you'll find the java programs i've written.\n\n[f.zip](https://github.com/bluestreak01/questdb/files/335644/f.zip)\n[model.zip](https://github.com/bluestreak01/questdb/files/335647/model.zip)\n\nun 27, 2016 8:24:07 PM jac.f readAndFlushAllFactura\nINFORMACIÃ“N: .. y en el CSV\njun 27, 2016 8:24:07 PM jac.f readAndFlushAllFactura\nINFORMACIÃ“N: .. y en questDB\njun 27, 2016 8:24:19 PM jac.f readAndFlushAllFactura\nINFORMACIÃ“N: Recuperados 850000 registros de Facturas\njun 27, 2016 8:24:20 PM jac.f readAndFlushAllFactura\nINFORMACIÃ“N: .. y en el CSV\n# \n# A fatal error has been detected by the Java Runtime Environment:\n# \n# EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000002c99667, pid=12972, tid=15236\n# \n# JRE version: Java(TM) SE Runtime Environment (8.0_91-b15) (build 1.8.0_91-b15)\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b15 mixed mode windows-amd64 compressed oops)\n# Problematic frame:\n# J 1965 C2 com.questdb.Partition.append(Ljava/lang/Object;)V (402 bytes) @ 0x0000000002c99667 [0x0000000002c99460+0x207]\n# \n# Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\n# \n# An error report file with more information is saved as:\n# D:\\JAC\\Dropbox\\des\\Viesgo\\questdb\\hs_err_pid12972.log\n# \n# If you would like to submit a bug report, please visit:\n# http://bugreport.java.com/bugreport/crash.jsp\n# \n\nProcess finished with exit code 1\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 342,
    "metadata": {
      "issue_number": 53,
      "state": "closed",
      "labels": [],
      "comments_count": 14,
      "created_at": "2016-06-27T19:09:18Z",
      "updated_at": "2016-07-15T09:08:56Z",
      "closed_at": "2016-07-15T09:08:56Z",
      "author": "jacuesta",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-4368da3fe714",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5926",
    "title": "Inconsistent behavior of floating point infinity",
    "text": "# Inconsistent behavior of floating point infinity\n\nWe store infinity to the DOUBLE column, and then it poisons future computation.\n\n### To reproduce\n\n```sql\ncreate table tango (x double, y double);\ninsert into tango values (1, 0), (1, 1);\nselect x/y from tango;\n\nx/y\n----\nnull\n1.0\n\nselect sum(x/y) from tango;\n\nsum(x/y)\n---------\n1.0\n\ninsert into tango select 0, x/y from tango;\nselect y from tango;\n\ny\n----\n0.0\n1.0\nnull\n1.0\n\nselect sum(y) from tango;\n\nsum(y)\n-------\nnull\n```\n\n\n### QuestDB version:\n\n8.3.3\n\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 84,
    "metadata": {
      "issue_number": 5926,
      "state": "open",
      "labels": [],
      "comments_count": 13,
      "created_at": "2025-07-09T11:41:17Z",
      "updated_at": "2025-07-26T17:14:52Z",
      "closed_at": null,
      "author": "mtopolnik",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-70616fe3d1c6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5680",
    "title": "Optimise performance of var-args functions by adding loop-unrolled variants",
    "text": "# Optimise performance of var-args functions by adding loop-unrolled variants\n\n### Is your feature request related to a problem?\n\nThere are a variety of `MultiArgFunction` functions, with variable argument lists. Often, these lists will be short (<5 arguments), but there is no guarantee that the JVM will unroll their loops automatically.\n\nFunctions that perform relatively simple operations may benefit from manually unrolled variants that skip loop and conditional checks, and instead have a straight bytecode path for the calculations.\n\n\n\n### Describe the solution you'd like.\n\nReview var-args functions, particularly those with numeric arguments and basic calculations. If they are a candidate for loop unrolling, implement and benchmark to prove it is faster.\n\nExamples of var-args functions:\n\n- greatest/least\n- IN\n- CASE\n- coalesce\n- ~l2price~\n- concat\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nNick Woolmer\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 148,
    "metadata": {
      "issue_number": 5680,
      "state": "open",
      "labels": [
        "Help wanted",
        "New feature",
        "SQL",
        "Good first issue",
        "hacktoberfest",
        "internal"
      ],
      "comments_count": 13,
      "created_at": "2025-05-15T16:10:17Z",
      "updated_at": "2025-10-02T14:38:42Z",
      "closed_at": null,
      "author": "nwoolmer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3087a47f8ec2",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4898",
    "title": "A random crash occurs after adding a new column to a data table",
    "text": "# A random crash occurs after adding a new column to a data table\n\n### To reproduce\n\nA database that has been running normally for half a year starts to crash randomly after adding new columns,I don't know how to reproduce it.\r\nerror log:\r\n```\r\n2024-08-28T06:43:27.817165Z I i.q.c.p.PGConnectionContext query cache used [fd=2872]\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x00000165b5e8430a, pid=14228, tid=1512\r\n#\r\n# JRE version: OpenJDK Runtime Environment Corretto-11.0.19.7.1 (11.0.19+7) (build 11.0.19+7-LTS)\r\n# Java VM: OpenJDK 64-Bit Server VM Corretto-11.0.19.7.1 (11.0.19+7-LTS, mixed mode, tiered, compressed oops, parallel gc, windows-amd64)\r\n# Problematic frame:\r\n# J 2641 c2 io.questdb.cairo.sql.PageAddressCacheRecord.getDouble(I)D io.questdb@7.3.1 (44 bytes) @ 0x00000165b5e8430a [0x00000165b5e842a0+0x000000000000006a]\r\n#\r\n# Core dump will be written. Default location: C:\\Windows\\system32\\hs_err_pid14228.mdmp\r\n#\r\n# An error report file with more information is saved as:\r\n# D:\\EMS\\qdbroot\\db\\hs_err_pid+14228.log\r\nCompiled method (c2) 508528854 2641       4       io.questdb.cairo.sql.PageAddressCacheRecord::getDouble (44 bytes)\r\n total in heap  [0x00000165b5e84110,0x00000165b5e844e8] = 984\r\n relocation     [0x00000165b5e84288,0x00000165b5e842a0] = 24\r\n main code      [0x00000165b5e842a0,0x00000165b5e843a0] = 256\r\n stub code      [0x00000165b5e843a0,0x00000165b5e843b8] = 24\r\n oops           [0x00000165b5e843b8,0x00000165b5e843c0] = 8\r\n metadata       [0x00000165b5e843c0,0x00000165b5e843d8] = 24\r\n scopes data    [0x00000165b5e843d8,0x00000165b5e84450] = 120\r\n scopes pcs     [0x00000165b5e84450,0x00000165b5e844c0] = 112\r\n dependencies   [0x00000165b5e844c0,0x00000165b5e844c8] = 8\r\n nul chk table  [0x00000165b5e844c8,0x00000165b5e844e8] = 32\r\nCompiled method (c2) 508528923 4366       4       io.questdb.cutlass.pgwire.PGConnectionContext::bindValuesUsingSetters (388 bytes)\r\n total in heap  [0x00000165b5f7fa90,0x00000165b5f837c0] = 15664\r\n relocation     [0x00000165b5f7fc08,0x00000165b5f7fd58] = 336\r\n main code      [0x00000165b5f7fd60,0x00000165b5f81f80] = 8736\r\n stub code      [0x00000165b5f81f80,0x00000165b5f81fe0] = 96\r\n oops           [0x00000165b5f81fe0,0x00000165b5f81ff0] = 16\r\n metadata       [0x00000165b5f81ff0,0x00000165b5f82120] = 304\r\n scopes data    [0x00000165b5f82120,0x00000165b5f82f88] = 3688\r\n scopes pcs     [0x00000165b5f82f88,0x00000165b5f83578] = 1520\r\n dependencies   [0x00000165b5f83578,0x00000165b5f835a0] = 40\r\n handler table  [0x00000165b5f835a0,0x00000165b5f836c0] = 288\r\n nul chk table  [0x00000165b5f836c0,0x00000165b5f837c0] = 256\r\nCompiled method (c2) 508528951 5743       4       io.questdb.cutlass.pgwire.PGConnectionContext::appendRecord (773 bytes)\r\n total in heap  [0x00000165b6335c90,0x00000165b6338fe8] = 13144\r\n relocation     [0x00000165b6335e08,0x00000165b6336078] = 624\r\n main code      [0x00000165b6336080,0x00000165b6337740] = 5824\r\n stub code      [0x00000165b6337740,0x00000165b6337970] = 560\r\n oops           [0x00000165b6337970,0x00000165b6337980] = 16\r\n metadata       [0x00000165b6337980,0x00000165b6337a50] = 208\r\n scopes data    [0x00000165b6337a50,0x00000165b6338348] = 2296\r\n scopes pcs     [0x00000165b6338348,0x00000165b6338a68] = 1824\r\n dependencies   [0x00000165b6338a68,0x00000165b6338a88] = 32\r\n handler table  [0x00000165b6338a88,0x00000165b6338e90] = 1032\r\n nul chk table  [0x00000165b6338e90,0x00000165b6338fe8] = 344\r\nCould not load hsdis-amd64.dll; library not loadable; PrintAssembly is disabled\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/corretto/corretto-11/issues/\r\n#\r\n```\r\n\n\n### QuestDB version:\n\n7.3.1\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nWindows Server 2016\n\n### File System, in case of Docker specify Host File System:\n\nNTFS\n\n### Full Name:\n\nyimeng\n\n### Affiliation:\n\ngivemeakiss.yimeng@gmail.com\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n[service-2024-08-22T17-27-46.zip](https://github.com/user-attachments/files/16778755/service-2024-08-22T17-27-46.zip)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 414,
    "metadata": {
      "issue_number": 4898,
      "state": "open",
      "labels": [],
      "comments_count": 13,
      "created_at": "2024-08-28T08:36:43Z",
      "updated_at": "2024-09-05T09:38:05Z",
      "closed_at": null,
      "author": "y1meng",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-a79a791b7f4f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4806",
    "title": "OOM logging is badly formatted",
    "text": "# OOM logging is badly formatted\n\n### To reproduce\r\n\r\n```\r\n2024-07-22T17:51:34.453589Z I i.q.c.TableWriter o3 commit [table=testWalWriteFullRandom_nonwal, o3RowCount=1769558]\r\nsun.misc.Unsafe.allocateMemory() OutOfMemoryError [RSS_MEM_USED=4368849184, size=1055916032, memoryTag=45], original message: 2024-07-22T17:51:36.654908Z C i.q.c.TableWriter could not sort varsize o3 column [table=testWalWriteFullRandom_nonwal, column=var_top, type=VARCHAR, long0=1717520080960, long1=-1, long2=-1, long3=-1, errno=-1, ex=sun.misc.Unsafe.allocateMemory() OutOfMemoryError [RSS_MEM_USED=4368849184, size=1055916032, memoryTag=45], original message: ]\r\n\r\n2024-07-22T17:51:37.098299Z I i.q.c.p.WriterPool << [table=`testWalWriteFullRandom_nonwal~`, thread=811]\r\n2024-07-22T17:51:37.124256Z E i.q.t.AbstractCairoTest Error in test: \r\nio.questdb.cairo.CairoException: [-1] commit failed, see logs for details [table=testWalWriteFullRandom_nonwal, tableDir=testWalWriteFullRandom_nonwal~]\r\n\tat io.questdb.cairo.CairoException.instance(CairoException.java:323)\r\n\tat io.questdb.cairo.CairoException.critical(CairoException.java:69)\r\n\tat io.questdb.cairo.TableWriter.checkO3Errors(TableWriter.java:3399)\r\n\tat io.questdb.cairo.TableWriter.consumeColumnTasks(TableWriter.java:3706)\r\n\tat io.questdb.cairo.TableWriter.dispatchColumnTasks(TableWriter.java:4631)\r\n\tat io.questdb.cairo.TableWriter.o3Commit(TableWriter.java:5478)\r\n\tat io.questdb.cairo.TableWriter.commit(TableWriter.java:3525)\r\n\tat io.questdb.cairo.TableWriter.commit(TableWriter.java:994)\r\n\tat io.questdb.test.griffin.wal.FuzzRunner.applyNonWal(FuzzRunner.java:233)\r\n\tat io.questdb.test.griffin.wal.FuzzRunner.runFuzz(FuzzRunner.java:800)\r\n\tat io.questdb.test.griffin.wal.AbstractFuzzTest.lambda$fullRandomFuzz$0(AbstractFuzzTest.java:135)\r\n\tat io.questdb.test.AbstractCairoTest.lambda$assertMemoryLeak$8(AbstractCairoTest.java:975)\r\n\tat io.questdb.test.tools.TestUtils.assertMemoryLeak(TestUtils.java:582)\r\n\tat io.questdb.test.AbstractCairoTest.assertMemoryLeak(AbstractCairoTest.java:972)\r\n\tat io.questdb.test.AbstractCairoTest.assertMemoryLeak(AbstractCairoTest.java:955)\r\n\tat io.questdb.test.griffin.wal.AbstractFuzzTest.fullRandomFuzz(AbstractFuzzTest.java:135)\r\n\tat io.questdb.test.griffin.wal.WalWriterFuzzTest.testWalWriteFullRandom(WalWriterFuzzTest.java:209)\r\n```\r\n\r\n- OOM exception is logged using `println`, not using loggers\r\n- The logging is 2 times nested, OOM inside OOM\r\n- RSS limit is not logged, only used memory and (allocation?) size.\r\n- Human readable memory sizes would be preferable\r\n\r\n\r\n\r\n### QuestDB version:\r\n\r\n8.0.3+\r\n\r\n### OS, in case of Docker specify Docker and the Host OS:\r\n\r\nWindows\r\n\r\n### File System, in case of Docker specify Host File System:\r\n\r\nntfs\r\n\r\n### Full Name:\r\n\r\nAlex Pelagenko\r\n\r\n### Affiliation:\r\n\r\nQuestDB\r\n\r\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\r\n\r\n- [X] Yes, I have\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 213,
    "metadata": {
      "issue_number": 4806,
      "state": "open",
      "labels": [
        "Enhancement",
        "Good first issue",
        "Bug 0"
      ],
      "comments_count": 13,
      "created_at": "2024-07-23T08:21:19Z",
      "updated_at": "2025-08-14T00:03:56Z",
      "closed_at": null,
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e72a66400460",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1882",
    "title": "Worker fences questionable",
    "text": "# Worker fences questionable\n\n### Describe the bug\r\n\r\nThe worker contains the following code:\r\n\r\n```\r\nfor (int i = 0; i < n; i++) {\r\n     Unsafe.getUnsafe().loadFence();\r\n      try {\r\n          try {\r\n               useful |= jobs.get(i).run(workerId);\r\n          } catch (Throwable e) {\r\n               onError(i, e);\r\n          }\r\n        } finally {\r\n          Unsafe.getUnsafe().storeFence();\r\n          }\r\n     }\r\n```\r\n\r\nI believe this is not working the way it is intended to be working.\r\n\r\nIf any of the jobs has modified its state and such a job is accessed by another thread to reads its state, then there is no guarantee that the access happens after the storeFence and before the loadFence. So you could end up with a plain load seeing the plain store made by a different thread. Because there is no happens-before edge, there is a data race.\r\n\r\nIf the job.run doesn't modify any unsynchronized state, then I don't see the purpose of the loadFence and the storeFence.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 149,
    "metadata": {
      "issue_number": 1882,
      "state": "closed",
      "labels": [
        "Bug",
        "Question"
      ],
      "comments_count": 13,
      "created_at": "2022-02-13T13:21:21Z",
      "updated_at": "2023-02-16T08:58:19Z",
      "closed_at": "2023-02-16T08:58:19Z",
      "author": "pveentjer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-c881f4e2070f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/262",
    "title": "UPDATE support",
    "text": "# UPDATE support\n\n**Is your feature request related to a problem? Please describe.**\r\nTables with [designated timestamp](https://www.questdb.io/docs/designatedTimestamp) support inserts with a `timestamp` superior or equal to the latest. In some instances, new entries could be unnecessary duplicates, for example if a sensor sends the same reading several times, or if the reading does not change (only the timestamp).\r\n\r\n**Describe the solution you'd like**\r\nSupporting UPSERT to complement INSERT and avoid writing data unnecessarily.\r\n\r\n**EDIT**: This should actually be called `UPDATE`. Renaming the issue.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 83,
    "metadata": {
      "issue_number": 262,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 13,
      "created_at": "2020-05-01T21:06:16Z",
      "updated_at": "2023-09-21T06:19:48Z",
      "closed_at": "2022-06-21T10:15:45Z",
      "author": "TheTanc",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-f5eb5ec0e3ab",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/167",
    "title": "Zero or Minimal Connection, Driver & Communication Overhead - Embeddability | Fast IPC",
    "text": "# Zero or Minimal Connection, Driver & Communication Overhead - Embeddability | Fast IPC\n\nAbility to use the DB as an embedded DB than a separate server running in the same JVM and application process as the application.\r\n\r\nThis will cut the latency and overhead of connecting to the DB and associated overheads. Also, you need not go through a driver.\r\n\r\nE.g.\r\n\r\n- https://www.h2database.com/html/main.html\r\n- https://db.apache.org/derby/\r\n- http://hsqldb.org/\r\n- https://ignite.apache.org/\r\n\r\nIf the database st to be shared with multiple application, a fast IPC scheme can also be provided where the DB is accessible through: \r\n\r\n- [Fast Binary Encoding (FBE)](https://chronoxor.github.io/FastBinaryEncoding/)\r\n- [Simple Binary Encoding (SBE)](https://real-logic.github.io/simple-binary-encoding/)\r\n- [gRPC](https://grpc.io/)\r\n- [MessagePack](https://msgpack.org/)\r\n- [Concise Binary Object Representation (CBOR)](http://cbor.io/)\r\n\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 114,
    "metadata": {
      "issue_number": 167,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 13,
      "created_at": "2020-04-18T08:05:51Z",
      "updated_at": "2021-02-19T12:52:27Z",
      "closed_at": "2021-02-19T12:52:26Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-02996f70ba26",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3377",
    "title": "Cannot insert row in a table",
    "text": "# Cannot insert row in a table\n\n### Describe the bug\n\nI have two identical applications that run on two different computers and insert data on questDB(7.1.1) via ILP. No data has been entered on a PC for two days. I tried to enter some data manually but they are still not entered.\r\n\r\n![image](https://github.com/questdb/questdb/assets/74403751/927ffd41-9d7e-4614-9983-475db68c547f)\r\n\r\n\r\nChecking the logs I found these errors, but nothing related to the Beams table:\r\n\r\n2023-05-22T10:19:03.703194Z E i.q.c.w.WalPurgeJob could not delete directory [path=E:\\QuestDB\\db\\Lots~3\\wal17, errno=5]\r\n2023-05-22T10:19:03.705895Z E i.q.c.w.WalPurgeJob could not delete directory [path=E:\\QuestDB\\db\\AlarmHistory~6\\wal564, errno=5]\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**:7.1.1\r\n- **OS**:Windows 10 Enterprise LTSC\r\n- **Browser**: Chrome\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 116,
    "metadata": {
      "issue_number": 3377,
      "state": "closed",
      "labels": [],
      "comments_count": 12,
      "created_at": "2023-05-22T10:30:54Z",
      "updated_at": "2023-10-10T00:09:15Z",
      "closed_at": "2023-05-23T15:23:32Z",
      "author": "micheleBridi",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7a4ac5c531ce",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3081",
    "title": "ORDER BY TIMESTAMP DESC issues / crashes",
    "text": "# ORDER BY TIMESTAMP DESC issues / crashes\n\n### Describe the bug\r\n\r\nExperiencing some unexpected behavior when using `order by timestamp desc`, including result counts that do not match what I expect and sometimes causes crashes of the QuestDB server\r\n\r\nI have attached the logs for the quest server that include the crashes.\r\n\r\n[LOGS.zip](https://github.com/questdb/questdb/files/11031934/LOGS.zip)\r\n\r\n\r\n### To reproduce\r\n\r\nI have attached a Gradle project that can be run with `./gradlew runSortDescRepro`\r\n\r\nThe source code is currently (but may change as I haven't gotten the same behavior as my actual system just yet):\r\n\r\n[test-questdb.zip](https://github.com/questdb/questdb/files/11031913/test-questdb.zip)\r\n\r\n```\r\npackage test.questdb;\r\n\r\nimport java.sql.Connection;\r\nimport java.sql.DriverManager;\r\nimport java.sql.PreparedStatement;\r\nimport java.sql.ResultSet;\r\nimport java.time.Instant;\r\nimport java.time.ZoneOffset;\r\nimport java.time.format.DateTimeFormatter;\r\nimport java.util.HashSet;\r\nimport java.util.Properties;\r\nimport java.util.Random;\r\nimport java.util.Set;\r\nimport io.questdb.client.Sender;\r\n\r\npublic class QuestDbSortDescRepro {\r\n\r\n  // edit as needed\r\n  static final String QUESTDB_ILP_ADDRESS = \"questdb:9009\";\r\n  static final String QUESTDB_QUERY_JDBC_URL = \"jdbc:postgresql://questdb:8812/qdb\";\r\n  static final Properties QUESTDB_QUERY_JDBC_PROPS = new Properties();\r\n  static {\r\n    QUESTDB_QUERY_JDBC_PROPS.put(\"user\", \"admin\");\r\n    QUESTDB_QUERY_JDBC_PROPS.put(\"password\", \"quest\");\r\n    QUESTDB_QUERY_JDBC_PROPS.put(\"sslmode\", \"disable\");\r\n  }\r\n\r\n  static final String TABLE = \"sort-desc-repro-3\";\r\n  \r\n  static final String COL_ID = \"id\";\r\n  static final String COL_LINKED_ID = \"linkedId\";\r\n  static final String COL_STR_1 = \"stringCol1\";\r\n  static final String COL_STR_2 = \"stringCol2\";\r\n  static final String COL_DESC = \"description\";\r\n  static final String COL_ACTIVE = \"active\";\r\n  static final String COL_WHEN_START = \"whenStartMs\";\r\n  static final String COL_WHEN_END = \"whenEndMs\";\r\n  static final String COL_DESIGNATED_TIMESTAMP = \"timestamp\";\r\n\r\n  static final Random RAND = new Random(System.currentTimeMillis());\r\n\r\n  static final String[] IDS = genStrings(16, \"generated-id-\");\r\n  static final String[] LINKED_IDS = genLinkedIds(IDS);\r\n  static final String[] STR1_VALS = genStrings(8, \"STR1-\");\r\n  static final String[] STR2_VALS = genStrings(4, \"STR1-\");\r\n\r\n  static long minTimeMs = 0L;\r\n\r\n  static String[] genStrings(int n, String base) {\r\n    String[] out = new String[n];\r\n    for (int j = 0; j < n; j++) {\r\n      out[j] = base + j;\r\n    }\r\n    return out;\r\n  }\r\n\r\n  static String[] genLinkedIds(String[] idsBase) {\r\n    String[] out = new String[idsBase.length];\r\n    for (int j = 0; j < idsBase.length; j++) {\r\n      out[j] = \"linkedId-\" + idsBase[j];\r\n    }\r\n    return out;\r\n  }\r\n\r\n  static String randString(int minLen, int maxLen) {\r\n    StringBuilder buf = new StringBuilder();\r\n    int len = RAND.nextInt(minLen, maxLen + 1);\r\n    for (int j = 0; j < len; j++) {\r\n      if (minLen > 10 && RAND.nextDouble() > 0.8) {\r\n        // sometimes append a UTF-8 character\r\n        buf.append(\"Ã¼\");\r\n      } else {\r\n        // A-Z, a-z\r\n        buf.append((char) (RAND.nextBoolean() ? RAND.nextInt(0x41, 0x5B) : RAND.nextInt(0x61, 0x7b)));\r\n      }\r\n    }\r\n    return buf.toString();\r\n  }\r\n\r\n  static <T> T randValue(T[] arr) {\r\n    return arr[RAND.nextInt(0, arr.length)];\r\n  }\r\n\r\n  static final String t() {\r\n    return String.format(\"[%-30s] \", Instant.now().atOffset(ZoneOffset.UTC).toString());\r\n  }\r\n\r\n  static void log(String fmt, Object... args) {\r\n    System.out.printf(t() + fmt + \"%n\", args);\r\n  }\r\n\r\n  static void insertRow(int row, Sender ilp, Set<Long> allTimestamps) throws Exception {\r\n    String id = randValue(IDS);\r\n    String linkedId = randValue(LINKED_IDS);\r\n    String desc = randString(32, 256);\r\n    String str1 = randValue(STR1_VALS);\r\n    String str2 = randValue(STR2_VALS);\r\n    boolean active = RAND.nextBoolean();\r\n    Long whenStartMs = System.currentTimeMillis() + RAND.nextInt(1, 1 + row);\r\n    if (row > 2000) {\r\n      whenStartMs += 300L;\r\n    }\r\n    if (minTimeMs == 0L || whenStartMs < minTimeMs) {\r\n      minTimeMs = whenStartMs;\r\n    }\r\n    // mostly null\r\n    Long whenEndMs = RAND.nextDouble() > 0.9 ? whenStartMs + RAND.nextLong(1000L, 2000L) : null;\r\n    long tsNanos = whenStartMs * 1_000_000L;\r\n    allTimestamps.add(tsNanos);\r\n    ilp = ilp\r\n        .table(TABLE)\r\n        .symbol(COL_ID, id)\r\n        .symbol(COL_LINKED_ID, linkedId)\r\n        .stringColumn(COL_DESC, desc)\r\n        .stringColumn(COL_STR_1, str1)\r\n        .stringColumn(COL_STR_2, str2)\r\n        .boolColumn(COL_ACTIVE, active)\r\n        .longColumn(COL_WHEN_START, whenStartMs);\r\n    if (whenEndMs != null) {\r\n      ilp = ilp.longColumn(COL_WHEN_END, whenEndMs);\r\n    }\r\n    ilp.at(tsNanos);\r\n  }\r\n\r\n  static void insertData(int nRows) throws Exception {\r\n    Set<Long> allTimestamps = new HashSet<>();\r\n    try (Sender ilp = Sender.builder().address(QUESTDB_ILP_ADDRESS).build()) {\r\n      log(\"insert [%d] rows into table [\" + TABLE + \"] ...\", nRows);\r\n      for (int j = 1; j <= nRows; j++) {\r\n        insertRow(j, ilp, allTimestamps);\r\n      }\r\n      ilp.flush();\r\n      log(\"insert [%d] rows - DONE [%d] unique timestamps\", nRows, allTimestamps.size());\r\n    }\r\n    log(\"wait 5s for inserted data to settle ...\");\r\n    Thread.sleep(5000L);\r\n    log(\"exit insertData()\");\r\n  }\r\n\r\n  static int getCountStar(String query) throws Exception {\r\n    int count = -1;\r\n    try(Connection conn = DriverManager.getConnection(QUESTDB_QUERY_JDBC_URL, QUESTDB_QUERY_JDBC_PROPS);\r\n        PreparedStatement stmt = conn.prepareStatement(query)) {\r\n      log(\" run query [%s] ...\", stmt);\r\n      try(ResultSet result = stmt.executeQuery()) {\r\n        if (result.next()) {\r\n          count = result.getInt(1);\r\n        }\r\n      }\r\n      log(\" query [%s] completed: count:[%d]\", stmt, count);\r\n    }\r\n    return count;\r\n  }\r\n\r\n  static int getRowCount(String query) throws Exception {\r\n    int count = 0;\r\n    try(Connection conn = DriverManager.getConnection(QUESTDB_QUERY_JDBC_URL, QUESTDB_QUERY_JDBC_PROPS);\r\n        PreparedStatement stmt = conn.prepareStatement(query)) {\r\n      stmt.setFetchSize(100_000);\r\n      log(\" run query [%s] ...\", stmt);\r\n      try(ResultSet result = stmt.executeQuery()) {\r\n        while(result.next()) {\r\n          count++;\r\n        }\r\n      }\r\n    } catch(Exception e) {\r\n      log(\"  %n  ##### UNEXPECTED ERROR %s #####%n\", e);\r\n      e.printStackTrace();\r\n      return -1;\r\n    }\r\n    log(\" query [%s] completed: count:[%d]\", query, count);\r\n    return count;\r\n  }\r\n\r\n  static void performQueries() throws Exception {\r\n    DateTimeFormatter timeFormat = DateTimeFormatter.ofPattern(\r\n        \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\");\r\n    String queryTimeFilter = timeFormat.format(\r\n        Instant.ofEpochMilli(minTimeMs - (2 * 86_400_000L) )\r\n        .atOffset(ZoneOffset.UTC));\r\n    String queryBase = \"select * from '\" + TABLE\r\n        + \"' WHERE timestamp >= '\" + queryTimeFilter + \"' ORDER BY \" + COL_DESIGNATED_TIMESTAMP + \" \";\r\n\r\n    int countStar = getCountStar(\"SELECT COUNT(*) FROM '\" + TABLE + \"'\");\r\n    int ascCount = getRowCount(queryBase + \"ASC\");\r\n    int ascLimitCount = getRowCount(queryBase + \"ASC LIMIT 100000\");\r\n    int descLimitCount = getRowCount(queryBase + \"DESC LIMIT 100000\");\r\n    int descCount = getRowCount(queryBase + \"DESC\");\r\n\r\n    boolean allEqual = countStar == descCount\r\n        && descCount == descLimitCount && descCount == ascCount && descCount == ascLimitCount;\r\n    log(\"%n -- QUERY RESULTS -- %n\"\r\n        + \"  count(*)       = [%d]%n\"\r\n        + \"  descCount      = [%d]%n\"\r\n        + \"  descLimitCount = [%d]%n\"\r\n        + \"  ascCount       = [%d]%n\"\r\n        + \"  ascLimitCount  = [%d]%n\"\r\n        + \"  allEqual       = [%b]%n\"\r\n        + \" -----------------%n\",\r\n        countStar, descCount, descLimitCount, ascCount, ascLimitCount, allEqual);\r\n  }\r\n\r\n  public static void main(String[] args) {\r\n    int exitCode = 0;\r\n    log(\"enter\");\r\n    try {\r\n      insertData(RAND.nextInt(4096, 8192));\r\n      log(\"%n performing queries ...%n%n\");\r\n      performQueries();\r\n    } catch (Throwable t) {\r\n      t.printStackTrace();\r\n      exitCode = 1;\r\n    }\r\n    log(\"exit\");\r\n    System.exit(exitCode);\r\n  }\r\n}\r\n```\r\n\r\n### Expected Behavior\r\n\r\nAll query counts are equal\r\n\r\nActual shows that at minimum the desc limit count is usually 0 or sometimes causes an EOF error on the JDBC connection and a crash on the QuestDB server.\r\n\r\ne.g.\r\n```\r\n -- QUERY RESULTS --\r\n  count(*)       = [35732]\r\n  descCount      = [35732]\r\n  descLimitCount = [0]\r\n  ascCount       = [35732]\r\n  ascLimitCount  = [35732]\r\n  allEqual       = [false]\r\n -----------------\r\n```\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 7.0.1 - Docker Image\r\n- **OS**: Amazon Linux\r\n- **Browser**: N/A (Java ILP Client, JDBC Postgres Query Client)\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n```\r\n---\r\nversion: '3'\r\n\r\nservices:\r\n  questdb:\r\n    image: questdb/questdb:7.0.1\r\n    container_name: questdb\r\n    restart: unless-stopped\r\n    environment:\r\n      QDB_SHARED_WORKER_COUNT: ${QUESTDB_SHARED_WORKER_COUNT:-44}\r\n      QDB_LINE_TCP_WRITER_WORKER_COUNT: ${QUESTDB_SHARED_WORKER_COUNT:-44}\r\n      QDP_LINE_TCP_CONNECTION_POOL_CAPACITY: 128\r\n      QDB_LINE_TCP_MSG_BUFFER_SIZE: 1048576\r\n      QDB_LINE_TCP_DEFAULT_PARTITION_BY: 'HOUR'\r\n      QDB_LINE_DEFAULT_PARTITION_BY: 'HOUR'\r\n      QDB_CAIRO_WAL_ENABLED_DEFAULT: 'false'\r\n      QDB_CAIRO_O3_LAG_CALCULATION_WINDOW_SIZE: 128\r\n    volumes:\r\n      - questdb-data:/var/lib/questdb\r\n    ports:\r\n      - \"9000:9000\"\r\n      - \"9009:9009\"\r\n      - \"8812:8812\"\r\n\r\nvolumes:\r\n  questdb-data:\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 1035,
    "metadata": {
      "issue_number": 3081,
      "state": "closed",
      "labels": [],
      "comments_count": 12,
      "created_at": "2023-03-21T17:45:36Z",
      "updated_at": "2023-03-24T11:16:51Z",
      "closed_at": "2023-03-24T11:16:51Z",
      "author": "andyb-ev",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-0efcadf873de",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2517",
    "title": "buggy invalid column error when joining two tables ",
    "text": "# buggy invalid column error when joining two tables \n\n### Describe the bug\r\n\r\nI have update questdb to 6.52 version but the  invalid column error is very strange, the code in the bracket runs correctly,the close column no problem\r\n\r\n\r\n\r\n\r\n\r\n```sql\r\nselect distinct a.timestamp,sum((open/close-1)*rweight*rho)/sum(rweight*rho) as r\r\nfrom (\r\n select code,timestamp_floor('m',timestamp)  timestamp,\r\n  open ,close\r\nfrom 'kline_5M'\r\n where timestamp>to_timestamp('2022-09-09T00:15', 'yyyy-MM-ddTHH:mm')\r\n ) as a join 'factor_weight' as b\r\n on a.code=b.code\r\n group by groups ,sub_plate\r\n order by a.timestamp\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.5.2\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 101,
    "metadata": {
      "issue_number": 2517,
      "state": "open",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 12,
      "created_at": "2022-09-11T09:45:44Z",
      "updated_at": "2023-05-15T09:31:19Z",
      "closed_at": null,
      "author": "Luyang2902",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-c612e32c8b51",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2369",
    "title": ".net  Npgsql   Insert too slow",
    "text": "# .net  Npgsql   Insert too slow\n\n### Is your feature request related to a problem?\n\nHello, I am the open source author of. NET ORM. It is too slow to insert Npgsql. Do you have a good solution\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\nHello, I am the open source author of. NET ORM. It is too slow to insert Npgsql. Do you have a good solution\n\n### Additional context.\n\nHello, I am the open source author of. NET ORM. It is too slow to insert Npgsql. Do you have a good solution",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 100,
    "metadata": {
      "issue_number": 2369,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 12,
      "created_at": "2022-07-31T09:11:57Z",
      "updated_at": "2022-08-01T22:44:48Z",
      "closed_at": null,
      "author": "DotNetNext",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-cbef0caac756",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1984",
    "title": "Why can't the sampling function be based on microseconds? Currently, it can only be based on milliseconds",
    "text": "# Why can't the sampling function be based on microseconds? Currently, it can only be based on milliseconds\n\n### Is your feature request related to a problem?\n\n![image](https://user-images.githubusercontent.com/37134815/160073726-1b54a396-2077-4e64-80ea-57ef6a022981.png)\r\nhere is details, in griffin\\engine\\groupby\\TimestampSamplerFactory.java\n\n### Describe the solution you'd like.\n\nThe sampling dimension provided by questDB is at least 'T', which means milliseconds, but the scene on my side is microseconds, so I want questDB to adapt.\r\nThe code in the screenshot is the part where I found the suspected related sampling logic. The case 'U' in the screenshot is added by myself. I don't know if this can solve this microsecond sampling, and I don't know how I should verify my Is the modification logic correct?\r\n\r\nWhat is the outsourcing method of questDB so that I can verify my modification logic\r\nIn addition, I would like to ask the code managers, can I support microsecond sampling with this modification?\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 161,
    "metadata": {
      "issue_number": 1984,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 12,
      "created_at": "2022-03-25T07:23:31Z",
      "updated_at": "2022-06-15T15:25:31Z",
      "closed_at": "2022-06-15T15:25:31Z",
      "author": "VansirCQ",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ad7c3c1d908b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1493",
    "title": "\"Invalid metadata\" error message when running queries after upgrading 6.1",
    "text": "# \"Invalid metadata\" error message when running queries after upgrading 6.1\n\n### Describe the bug\n\nHello,\r\n\r\nAfter upgrading QuestDB to the latest version (`6.1`), my 2 databases are unusable.\r\nI can't even make a `SELECT` inside it: If I go the webpage, and just type the database name (or `SELECT * FROM`) and run the query, I get this error message:\r\n`[0]: Invalid metadata at fd=17. Invalid column type 0 at [7]`\r\n\r\nOr even this message on another database:\r\n`[0]: Invalid metadata at fd=18. NULL column name at [1]`\r\n\r\nI have also one old database I have not modified since few months and I get also:\r\n`[0]: Invalid metadata at fd=17. NULL column name at [1]`\r\n\r\nI have tried to reverse my Docker image to 6.0.9, but I get en error when running a query on databases that the version doesn't match (of course).\r\nI have tried to insert datas inside databases, but I get same error message.\n\n### To reproduce\n\n1. Upgrade from 6.0.9 to 6.1\r\n2. Run a query on database\r\n3. Get the error message `[0]: Invalid metadata at fd=`\n\n### Expected Behavior\n\nQuery run normally.\n\n### Environment\n\n```markdown\n- **QuestDB version**:6.1\r\n- **OS**: Ubuntu 20.04.2 (docker)\r\n- **Browser**: Google Chrome 95.0.4638.54 (64 bits)\n```\n\n\n### Additional context\n\nLogs of a query:\r\n```\r\n2021-10-29T07:54:09.549722Z I i.q.c.h.p.JsonQueryProcessorState [16] exec [q='SELECT * FROM crypto_market']\r\n2021-10-29T07:54:09.549750Z I i.q.c.h.p.QueryCache miss [thread=questdb-worker-3, sql=SELECT * FROM crypto_market]\r\n2021-10-29T07:54:09.549957Z I i.q.c.p.ReaderPool open 'crypto_market' [at=0:0]\r\n2021-10-29T07:54:09.550235Z I i.q.c.h.p.JsonQueryProcessorState [16] syntax-error [q=`SELECT * FROM crypto_market`, at=14, message=`[0]: Invalid metadata at fd=17. Invalid column type 0 at [7]`]\r\n2021-10-29T07:54:09.551893Z I i.q.c.h.p.JsonQueryProcessor all sent [fd=16, lastRequestBytesSent=322, nCompletedRequests=45, totalBytesSent=119043]\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 271,
    "metadata": {
      "issue_number": 1493,
      "state": "closed",
      "labels": [
        "Bug",
        "Upgrading"
      ],
      "comments_count": 12,
      "created_at": "2021-10-29T07:55:22Z",
      "updated_at": "2021-11-22T14:06:26Z",
      "closed_at": "2021-11-22T14:06:26Z",
      "author": "Elpatii",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-237bc129394c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/819",
    "title": "ActiveRecord (Rails) support",
    "text": "# ActiveRecord (Rails) support\n\n**Is your feature request related to a problem? Please describe.**\r\nI would like to be able to use QuestDB in the context of a Rails application. Currently when ActiveRecord starts up, there is some initialization SQL to get table metadata ([source](https://github.com/rails/rails/blob/main/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb#L632-L636)).\r\n\r\n**Steps to start rails app**\r\n\r\n```bash\r\ngem install rails\r\nrails new myapp --database=postgresql\r\nDATABASE_URL=\"postgresql://admin:quest@localhost:8812/qdb\" rails server\r\n```\r\nThen navigate to [http://127.0.0.1:3000/](http://127.0.0.1:3000/) which will run the following query in the backend:\r\n\r\n```sql\r\nSELECT t.oid, t.typname, t.typelem, t.typdelim, t.typinput, r.rngsubtype, t.typtype, t.typbasetype\r\nFROM pg_type as t\r\nLEFT JOIN pg_range as r ON oid = rngtypid\r\nWHERE\r\n  t.typname IN ('int2', 'int4', 'int8', 'oid', 'float4', 'float8', 'text', 'varchar', 'char', 'name', 'bpchar', 'bool', 'bit', 'varbit', 'timestamptz', 'date', 'money', 'bytea', 'point', 'hstore', 'json', 'jsonb', 'cidr', 'inet', 'uuid', 'xml', 'tsvector', 'macaddr', 'citext', 'ltree', 'line', 'lseg', 'box', 'path', 'polygon', 'circle', 'time', 'timestamp', 'numeric', 'interval')\r\n  OR t.typtype IN ('r', 'e', 'd')\r\n  OR t.typinput = 'array_in(cstring,oid,integer)'::regprocedure\r\n  OR t.typelem != 0\r\n```\r\n\r\nThis query will fail with\r\n\r\n```\r\nInvalid column: t.typdelim\r\n```\r\n\r\n**Additional context**\r\nIt's possible to add the following lines to [TypeCatalogueCursor](https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/griffin/engine/functions/catalogue/TypeCatalogueCursor.java#L117-L127) to initially get past this error for troubleshooting:\r\n\r\n```java\r\n        metadata.add(new TableColumnMetadata(\"typdelim\", ColumnType.INT, null));\r\n        metadata.add(new TableColumnMetadata(\"typinput\", ColumnType.INT, null));\r\n```\r\n\r\nbut the query still fails due to missing metadata. Note that there is a case mismatch in the query for `typname` column and returned values:\r\n\r\n![grafik](https://user-images.githubusercontent.com/43580235/108203856-55127880-7123-11eb-9775-eed080e8fb3d.png)\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 221,
    "metadata": {
      "issue_number": 819,
      "state": "open",
      "labels": [
        "New feature",
        "Postgres Wire",
        "Compatibility"
      ],
      "comments_count": 12,
      "created_at": "2021-02-17T12:25:27Z",
      "updated_at": "2024-05-07T11:03:15Z",
      "closed_at": null,
      "author": "bsmth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7aa554c4411f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/166",
    "title": "Feature: add support for replication / distributed nodes",
    "text": "# Feature: add support for replication / distributed nodes\n\nIs it possible to have clustering and horizontal scalability without additional complication? Simply the ability to add and remove nodes without major complication and configuration. Nodes may get added when they are spun up or come online and are removed when they are shut down or go offline.\r\n\r\nMaybe the core DB can be a single node DB with another layer handling the distributed plumbing. So if you just want a fast embedded DB you have that and if you want a DB with scalability you have that also.\r\n\r\nE.g.\r\n\r\n- https://ignite.apache.org/\r\n- https://vitess.io/\r\n- https://www.yugabyte.com/\r\n- https://www.cockroachlabs.com/\r\n- https://pingcap.com/en/\r\n- https://crate.io/\r\n- https://clickhouse.tech/\r\n- https://druid.apache.org/\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 115,
    "metadata": {
      "issue_number": 166,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 12,
      "created_at": "2020-04-18T07:46:22Z",
      "updated_at": "2022-10-31T17:38:10Z",
      "closed_at": null,
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e5b16ed7f4ad",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/57",
    "title": "What's data update story?",
    "text": "# What's data update story?\n\nI found that /imp does not overwrite or append existing data, at least judging by web console results.   A new table can be created with new data, but this is not practical in most cases. Is there an update story in this DB, or is this meant to be a load/analyze/drop thing?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 57,
    "metadata": {
      "issue_number": 57,
      "state": "closed",
      "labels": [],
      "comments_count": 12,
      "created_at": "2016-12-30T02:21:11Z",
      "updated_at": "2017-01-03T12:17:00Z",
      "closed_at": "2017-01-03T12:17:00Z",
      "author": "julianrz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ec3e8fbf8dac",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/32",
    "title": "nfsdb vs mapdb?",
    "text": "# nfsdb vs mapdb?\n\nThis is a question rather than an issue.\nHow it compares to https://github.com/jankotek/mapdb ?\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 18,
    "metadata": {
      "issue_number": 32,
      "state": "closed",
      "labels": [],
      "comments_count": 12,
      "created_at": "2015-08-17T05:27:22Z",
      "updated_at": "2015-08-19T21:31:42Z",
      "closed_at": "2015-08-19T21:31:41Z",
      "author": "gembin",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-499b449a1fa7",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6113",
    "title": "ILP HTTP ingestion not supporting . in column names",
    "text": "# ILP HTTP ingestion not supporting . in column names\n\n### To reproduce\n\n1. Create data file of ilp format titled data.txt:\n`cpu_temp,foo=bar gauge=87.332\nhttp_requests_total,method=post,code=200 counter=1027\nhttp_requests_total,method=post,code=400 counter=3\nhttp_request_duration_seconds 0.05=24054,0.1=33444,0.2=100392,0.5=129389,1=133988,sum=53423,count=144320,min=0,max=10\nrpc_duration_seconds 0.01=3102,0.05=3272,0.5=4773,0.9=9001,0.99=76656,sum=1.7560473e+07,count=2693`\n\n2. Input command to send to questdb:\n`curl -i -XPOST 'http://localhost:9000/write' --data-binary @data.txt`\n\n3. See expected return:\n`HTTP/1.1 400 Bad request\nServer: questDB/1.0\nDate: Mon, 8 Sep 2025 21:19:47 GMT\nTransfer-Encoding: chunked\nContent-Type: application/json; charset=utf-8\n\n{\"code\":\"invalid\",\"message\":\"failed to parse line protocol:errors encountered on line(s):\\nerror in line 4: table: http_request_duration_seconds; invalid column name: 0.05\",\"line\":4,\"errorId\":\"a56d099acec9-2\"}`\n\n### QuestDB version:\n\n9.0.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 22.04 (Docker)\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nRyan Hoang\n\n### Affiliation:\n\nIndependent\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\nAs ILP supports periods in columns:\nhttps://docs.influxdata.com/influxdb/v2/reference/syntax/line-protocol/#special-characters\n\nI see similarities to: https://github.com/questdb/questdb/issues/5117 however for ILP support would have expected some documentation regarding it in: https://questdb.com/docs/reference/api/ilp/overview/ or for . to be supported via conversion",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 184,
    "metadata": {
      "issue_number": 6113,
      "state": "open",
      "labels": [
        "Documentation"
      ],
      "comments_count": 11,
      "created_at": "2025-09-08T21:30:31Z",
      "updated_at": "2025-12-07T04:55:26Z",
      "closed_at": null,
      "author": "Tsz-Fung-Ryan",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-684ec2757f92",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5609",
    "title": "Exception with response code 500 on invalid query",
    "text": "# Exception with response code 500 on invalid query\n\n### To reproduce\n\n```\n2025-04-22T07:28:52.426459Z C i.q.c.h.p.JsonQueryProcessorState [992175138049] internal error [ex=\njava.lang.NullPointerException: Cannot invoke \"java.lang.CharSequence.length()\" because \"tok\" is null\n\tat io.questdb.griffin.SqlKeywords.isJsonExtract(SqlKeywords.java:909)\n\tat io.questdb.griffin.SqlParser.rewriteJsonExtractCast(SqlParser.java:3444)\n\tat io.questdb.griffin.PostOrderTreeTraversalAlgo.traverse(PostOrderTreeTraversalAlgo.java:86)\n\tat io.questdb.griffin.SqlParser.rewriteKnownStatements(SqlParser.java:3506)\n\tat io.questdb.griffin.SqlParser.expr(SqlParser.java:3675)\n\tat io.questdb.griffin.SqlParser.expr(SqlParser.java:3685)\n\tat io.questdb.griffin.SqlParser.parseFromClause(SqlParser.java:2055)\n\tat io.questdb.griffin.SqlParser.parseDml0(SqlParser.java:1815)\n\tat io.questdb.griffin.SqlParser.parseDml(SqlParser.java:1596)\n\tat io.questdb.griffin.SqlParser.parseSelect(SqlParser.java:2563)\n\tat io.questdb.griffin.SqlParser.parse(SqlParser.java:3711)\n\tat io.questdb.griffin.SqlCompilerImpl.compileExecutionModel(SqlCompilerImpl.java:1878)\n\tat io.questdb.griffin.SqlCompilerImpl.compileUsingModel(SqlCompilerImpl.java:2409)\n\tat io.questdb.griffin.SqlCompilerImpl.compileInner(SqlCompilerImpl.java:1980)\n\tat io.questdb.griffin.SqlCompilerImpl.compile(SqlCompilerImpl.java:308)\n\tat io.questdb.cairo.pool.SqlCompilerPool$C.compile(SqlCompilerPool.java:138)\n\tat io.questdb.cutlass.http.processors.JsonQueryProcessor.compileAndExecuteQuery(JsonQueryProcessor.java:508)\n\tat io.questdb.cutlass.http.processors.JsonQueryProcessor.execute0(JsonQueryProcessor.java:224)\n\tat io.questdb.cutlass.http.processors.JsonQueryProcessor.onRequestComplete(JsonQueryProcessor.java:296)\n\tat io.questdb.cutlass.http.HttpConnectionContext.handleClientRecv(HttpConnectionContext.java:991)\n\tat io.questdb.cutlass.http.HttpConnectionContext.handleClientOperation(HttpConnectionContext.java:307)\n\tat io.questdb.cutlass.http.HttpServer.handleClientOperation(HttpServer.java:345)\n\tat io.questdb.cutlass.http.HttpServer$1.lambda$$0(HttpServer.java:129)\n\tat io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODispatcher.java:216)\n\tat io.questdb.cutlass.http.HttpServer$1.run(HttpServer.java:133)\n\tat io.questdb.mp.Worker.run(Worker.java:152)\n, q=`SELECT * FROM trips\n2025-04-22 08:28:52.426\t\nWHERE pickup_latitude = CAST((SELECT MAX(pickup_latitude) FROM trips) AS DOUBLE PRECISION)`]\n```\n\n### QuestDB version:\n\n8.3.0\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nLinux\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nAlex Pelagenko\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 165,
    "metadata": {
      "issue_number": 5609,
      "state": "open",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 11,
      "created_at": "2025-04-22T08:58:46Z",
      "updated_at": "2025-05-04T11:07:54Z",
      "closed_at": null,
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b39b18a79c20",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5107",
    "title": "Still cannot start on FreeBSD",
    "text": "# Still cannot start on FreeBSD\n\n### To reproduce\n\n1. Build native libs with cmake and cargo\r\n2. Build jar with maven\r\n3. Try to start questdb\n\n### QuestDB version:\n\n8.1.4\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nFreeBSD 14.1-RELEASE-p5 (jail)\n\n### File System, in case of Docker specify Host File System:\n\nZFS\n\n### Full Name:\n\nPeter TKATCHENKO\n\n### Affiliation:\n\nFlytrace\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\nI'm trying to upgrade Questdb 7.4.0 to 8.4.1 on amd64 server.\r\nI've built the native libs but it looks that the jar built with maven is still incomplete.\r\nTrying to start Questdb I get the following error:\r\n`java.lang.module.FindException: Module io.questdb not found`\r\n\r\nPlease, explain in details how can I debug this error and where should I put the native libs before maven build.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 157,
    "metadata": {
      "issue_number": 5107,
      "state": "open",
      "labels": [
        "Won't fix"
      ],
      "comments_count": 11,
      "created_at": "2024-10-24T21:03:23Z",
      "updated_at": "2024-12-24T10:36:28Z",
      "closed_at": null,
      "author": "Peter2121",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1fc480b83364",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4727",
    "title": "Add a volume-weighted moving average function (VWMA)",
    "text": "# Add a volume-weighted moving average function (VWMA)\n\n### Is your feature request related to a problem?\n\nRelated to: https://github.com/questdb/questdb/issues/4620\r\n\r\nWe should support VWMA in QuestDB for finance use-cases.\r\n\r\nVWAP: `sum(price * quantity) / sum(quantity)`\r\nVWMA is similar, but with a moving average window.\r\n\r\nFor a 3 trade window:\r\n\r\n`((price1 * quantity1) + (price2 * quantity2) + (price3 * quantity3)) / (quantity1 + quantity2 + quantity3)`\r\n\n\n### Describe the solution you'd like.\n\nSupport for volume weighted moving average\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nNick Woolmer\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 98,
    "metadata": {
      "issue_number": 4727,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 11,
      "created_at": "2024-06-28T11:26:15Z",
      "updated_at": "2024-11-15T12:19:02Z",
      "closed_at": "2024-11-15T12:19:02Z",
      "author": "nwoolmer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7f586d41c615",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4566",
    "title": "Consider Moving Dark/Light Theme Toggle Button to Top of Page",
    "text": "# Consider Moving Dark/Light Theme Toggle Button to Top of Page\n\n### Is your feature request related to a problem?\n\n### Title: Consider Moving Dark/Light Theme Toggle Button to Top of Page\r\n\r\nDescription:\r\nI noticed that the **dark/light theme toggle button on the questDb.io site** is currently located at the bottom of the page in the footer section. It might be more user-friendly to move this toggle button to the top of the page for easier access. Users would find it more convenient to switch themes without having to scroll down. This small change could enhance the user experience and make the theme switch more accessible.\r\n\r\n**As in Image :-**\r\n\r\n![Screenshot_20240527-141218_Chrome](https://github.com/questdb/questdb/assets/65233567/35e5bb63-3bfb-48fd-bc54-bc2f93b281c6)\r\n\n\n### Describe the solution you'd like.\n\n### For the issue I raised about moving the dark/light theme toggle button to the top of the page on questDb.io\r\n\r\nA possible solution could be to implement a sticky header with the toggle button so that it remains visible as users scroll down the page. This way, users can easily access the theme switch without having to scroll back up or down to find it. This solution would enhance user experience and make the theme switch more accessible throughout the site.\r\n\r\n**Maybe like these:-**\r\n\r\n![20240527_144643_0001](https://github.com/questdb/questdb/assets/65233567/dd8179ef-b59f-46f3-ada9-1d49aeb74ea4)\r\n![20240527_144643_0000](https://github.com/questdb/questdb/assets/65233567/21e4d694-ce15-4b7e-9b9b-49b7e1ba29cf)\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nPawan Kumar\n\n### Affiliation:\n\nitzpa1\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 224,
    "metadata": {
      "issue_number": 4566,
      "state": "closed",
      "labels": [
        "Documentation"
      ],
      "comments_count": 11,
      "created_at": "2024-05-27T09:24:18Z",
      "updated_at": "2024-06-01T14:30:47Z",
      "closed_at": "2024-06-01T14:30:47Z",
      "author": "itzpa1",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-9847ddd817de",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3269",
    "title": "SIGSEGV io.questdb.griffin.engine.functions.columns.ColumnUtils.symbolColumnUpdateKeys(JJJJ)V",
    "text": "# SIGSEGV io.questdb.griffin.engine.functions.columns.ColumnUtils.symbolColumnUpdateKeys(JJJJ)V\n\n### Describe the bug\n\nExperienced a SIGSEGV while importing a fairly big CSV (~500GB / 13bn records) into table with such structure\r\n`CREATE TABLE 'g_voltage_power2' (car_id symbol, gmt TIMESTAMP, value INT) timestamp (gmt) PARTITION BY MONTH WAL;`\r\n\r\nPrevious import with car_id INT was sucessful.\r\n\r\n```\r\nStack: [0x00007f1f693b7000,0x00007f1f694b8000],  sp=0x00007f1f694b6420,  free space=1021k\r\nNative frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nJ 7028% c2 io.questdb.griffin.engine.functions.columns.ColumnUtils.symbolColumnUpdateKeys(JJJJ)V io.questdb@7.1.1 (81 bytes) @ 0x00007f202c8db897 [0x00007f202c8db820+0x0000\r\n000000000077]\r\nj  io.questdb.cutlass.text.TextImportTask$PhaseUpdateSymbolKeys.run(Lio/questdb/std/str/Path;)V+252 io.questdb@7.1.1\r\nj  io.questdb.cutlass.text.TextImportTask.run(Lio/questdb/cutlass/text/TextLexerWrapper;Lio/questdb/cutlass/text/CsvFileIndexer;Lio/questdb/std/str/DirectCharSink;Lio/quest\r\ndb/std/DirectLongList;JJLio/questdb/std/str/Path;Lio/questdb/std/str/Path;)Z+171 io.questdb@7.1.1\r\nj  io.questdb.cutlass.text.TextImportJob.doRun(IJLio/questdb/mp/Job$RunStatus;)Z+47 io.questdb@7.1.1\r\nJ 6947 c2 io.questdb.cutlass.text.ParallelCsvFileImporter.collect(ILjava/util/function/Consumer;)I io.questdb@7.1.1 (80 bytes) @ 0x00007f202c8b24c8 [0x00007f202c8b2420+0x00\r\n000000000000a8]\r\nj  io.questdb.cutlass.text.ParallelCsvFileImporter.phaseUpdateSymbolKeys()V+308 io.questdb@7.1.1\r\nj  io.questdb.cutlass.text.ParallelCsvFileImporter.process(Lio/questdb/cairo/CairoSecurityContext;)V+122 io.questdb@7.1.1\r\nJ 4138 c2 io.questdb.cutlass.text.TextImportRequestJob.runSerially()Z io.questdb@7.1.1 (335 bytes) @ 0x00007f202c3ea6a0 [0x00007f202c3ea380+0x0000000000000320]\r\nJ 2666 c2 io.questdb.mp.SynchronizedJob.run(ILio/questdb/mp/Job$RunStatus;)Z io.questdb@7.1.1 (39 bytes) @ 0x00007f202c2652fc [0x00007f202c2652a0+0x000000000000005c]\r\nJ 2696% c2 io.questdb.mp.Worker.run()V io.questdb@7.1.1 (557 bytes) @ 0x00007f202c26b818 [0x00007f202c26b740+0x00000000000000d8]\r\nv  ~StubRoutines::call_stub\r\nV  [libjvm.so+0x8cf25b]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, Thread*)+0x39b\r\nV  [libjvm.so+0x8cd21d]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, Thread*)+0x1ed\r\nV  [libjvm.so+0x97e46c]  thread_entry(JavaThread*, Thread*)+0x6c\r\nV  [libjvm.so+0xed275a]  JavaThread::thread_main_inner()+0x1ba\r\nV  [libjvm.so+0xecf35f]  Thread::call_run()+0x14f\r\nV  [libjvm.so+0xc6cc26]  thread_native_entry(Thread*)+0xe6\r\n\r\n```\r\n\r\nFull log attached.\n\n### To reproduce\n\n1. Import a large dataset into symbol column.\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.1.1\r\n- **OS**: OpenSuse Leap 15.4 / openjdk version \"11.0.18\" 2023-01-17 / build 11.0.18+0-suse-150000.3.93.1-x8664\r\n- **Browser**: Irrelevant\n```\n\n\n### Additional context\n\n[hs_err_pid+17272.log](https://github.com/questdb/questdb/files/11300574/hs_err_pid%2B17272.log)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 213,
    "metadata": {
      "issue_number": 3269,
      "state": "open",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 11,
      "created_at": "2023-04-22T07:57:15Z",
      "updated_at": "2023-05-05T08:50:31Z",
      "closed_at": null,
      "author": "Roze",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-64697ecbc5a0",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1953",
    "title": "Fix escaping of special characters in JSON response from /exec?query=...",
    "text": "# Fix escaping of special characters in JSON response from /exec?query=...\n\n### Describe the Bug\r\n\r\nThe JSON response sent back over HTTP doesn't encode certain characters correctly.\r\n\r\nFor example, if a value in a string column contains the char `\\1` the byte value is copied verbatim in the response, breaking the client:\r\n\r\n```python\r\n>>> json.loads('\"\\1\"')\r\n...\r\njson.decoder.JSONDecodeError: Invalid control character at: line 1 column 2 (char 1)\r\n```\r\n\r\nInstead, we should escape such chars as per the `escape` section of the [json website](https://www.json.org/json-en.html) and more in detail in the \"String\" section of the [spec](https://www.ecma-international.org/wp-content/uploads/ECMA-404_2nd_edition_december_2017.pdf):\r\n```\r\n>>> json.loads('\"\\\\u0001\"') == '\\1'\r\nTrue\r\n```\r\n\r\nFrom a cursory read, it looks like all characters with codepoints below `\\x20` (space) need escaping, though more may require it.\r\n\r\n```\r\ncharacter\r\n    '0020' . '10FFFF' - '\"' - '\\'\r\n'\\' escape\r\n```\r\n\r\n\r\n### To reproduce\r\n\r\n1. Insert a row (e.g. via the line protocol) with problematic characters.\r\n2. Query via HTTP `/exec?query=tablename`.\r\n3. Observe parsing issue in client. Python's `json` library seems strict enough to pick this up.\r\n\r\n### Expected Behavior\r\n\r\nCharacters should be escaped.\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.2.1 (and earlier)\r\n- **OS**: Any\r\n- **Browser**: Python\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 198,
    "metadata": {
      "issue_number": 1953,
      "state": "open",
      "labels": [
        "Bug",
        "Good first issue",
        "REST API",
        "hacktoberfest"
      ],
      "comments_count": 11,
      "created_at": "2022-03-15T15:02:52Z",
      "updated_at": "2025-09-22T15:00:26Z",
      "closed_at": null,
      "author": "amunra",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-c1c2b825bb85",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1430",
    "title": "Time range with negative multiplier as modifier",
    "text": "# Time range with negative multiplier as modifier\n\n**Describe the bug**\r\nTime range with negative multiplier as the modifier does not work.\r\n```\r\nERROR: Invalid date\r\n```\r\n\r\n**To Reproduce**\r\n```\r\nSELECT count() FROM sometable WHERE ts IN '2021-10-12T14:00:00;-60m';\r\n```\r\n\r\n**Expected behavior**\r\nShould work like:\r\n```\r\nSELECT count() FROM sometable WHERE ts IN '2021-10-12T13:00:00;60m';\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu 20.04 LTS\r\n - Version: 6.0.7.1\r\n - JRE: openjdk-11-jre-headless:amd64 (11.0.11+9-0ubuntu2~20.04)\r\n\r\n**Additional context**\r\n* postgresql-client-12 (12.8-0ubuntu0.20.04.1)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 77,
    "metadata": {
      "issue_number": 1430,
      "state": "closed",
      "labels": [
        "Documentation"
      ],
      "comments_count": 11,
      "created_at": "2021-10-12T18:09:26Z",
      "updated_at": "2021-10-13T11:53:49Z",
      "closed_at": "2021-10-13T11:53:48Z",
      "author": "robocoder",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-79885a6de4e9",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1359",
    "title": "Timestamps have wrong date when inserting data using Postgres JDBC",
    "text": "# Timestamps have wrong date when inserting data using Postgres JDBC\n\n**Describe the bug**\r\nTimestamps insert with the wrong date over Postgres JDBC.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Unzip and run the attached Kotlin script using the following command: `kotlin questdb-timestamp-fail.main.kts`. If you do not have Kotlin installed you can transcribe the script to Java instead, it should be very simple to do.\r\n2. Observe that the original `java.sql.Timestamp` we insert into the database differs drastically from the `java.sql.Timestamp` object we get from the database when querying our inserted data.\r\n3. The above can also be observed from the QuestDB web UI as well.\r\n\r\n**Expected behavior**\r\nThe timestamp we insert into the database should match the timestamp we get when we query for the inserted data.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Arch Linux 5.14.7-zen1-1-zen x86_64\r\n - Browser: Firefox Developer 93.0b9 (64-bit)\r\n - QuestDB Version: 6.0.6 (docker). I've also tested 6.0.5 and the issue is present there as well.\r\n - Postgres driver version: PostgreSQL JDBC Driver 42.2.24 (https://jdbc.postgresql.org/)\r\n - JVM version: `AdoptOpenJDK version \"17\"`\r\n\r\n**Additional context**\r\nI have not ruled out whether or not this is a bug in the Java Postgres JDBC driver.\r\n\r\n**Attachments:**\r\n[script.zip](https://github.com/questdb/questdb/files/7230351/script.zip)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 201,
    "metadata": {
      "issue_number": 1359,
      "state": "closed",
      "labels": [
        "Bug",
        "Help wanted"
      ],
      "comments_count": 11,
      "created_at": "2021-09-25T21:09:40Z",
      "updated_at": "2022-04-27T13:01:14Z",
      "closed_at": "2022-04-27T13:01:14Z",
      "author": "null-dev",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-6e3037e4c22f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/875",
    "title": "QuestDB spam",
    "text": "# QuestDB spam\n\nFew say ago I received this mail from the person who claims to be the CTO of QuestDB.\r\n```\r\nVlad Ilyushchenko <vlad.ilyushchenko@questdb.tech>\r\n\r\nHey anatoly,\r\n\r\n\r\nI helped develop an alternative to InfluxDB, which is faster, more reliable, uses standard SQL and supports InfluxDB line protocol. It's an Open Source time series database that should be easy to try out with your existing stack. Here's the GitHub link if you'd like to check it out: https://github.com/questdb/questdb\r\n\r\n\r\nQuestDB is built from the ground up to be the most performant open source time series database available. Would love to hear what you think!\r\n\r\n\r\nThanks,\r\n\r\nVlad / CTO @ QuestDB\r\n\r\n\r\nIf you don't want to hear from me again, please let me know :)\r\n```\r\nThe two links provided in the mail - to GitHub and to \"let me know\" are links trackers. The domain leads to `track.questdb.tech` which is different from https://questdb.io/ that this project uses.\r\n\r\nSo I want to ask what all of this mean?\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 165,
    "metadata": {
      "issue_number": 875,
      "state": "closed",
      "labels": [],
      "comments_count": 11,
      "created_at": "2021-03-21T14:28:18Z",
      "updated_at": "2021-04-19T17:27:58Z",
      "closed_at": "2021-03-21T14:30:55Z",
      "author": "techtonik",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-197d65197475",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/600",
    "title": "inner join query time is too long ",
    "text": "# inner join query time is too long \n\n**Describe the bug**\r\nA clear and concise description of what the bug is.\r\ni use orders table have 1 billion data, and rdiers table have 1 million data. then, execute the following sql statement\r\n`select * from orders inner join riders on orders.rider_id = riders.uid;`\r\ni hope get the results quickly, however very time consuming\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 63,
    "metadata": {
      "issue_number": 600,
      "state": "closed",
      "labels": [
        "Performance"
      ],
      "comments_count": 11,
      "created_at": "2020-09-14T08:43:55Z",
      "updated_at": "2021-09-29T17:21:34Z",
      "closed_at": "2021-09-29T17:21:34Z",
      "author": "dybxin",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-97214180407d",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/599",
    "title": "batch insert data error arrayIndexOutOfBoundsException",
    "text": "# batch insert data error arrayIndexOutOfBoundsException\n\n**Describe the bug**\r\n` java\r\njava.lang.ArrayIndexOutOfBoundsException: Index -2 out of bounds for length 16\r\n\tat io.questdb.std.ObjList.getQuick(ObjList.java:117)\r\n\tat io.questdb.griffin.engine.functions.rnd.RndStringListFunctionFactory$Func.getStr(RndStringListFunctionFactory.java:87)\r\n\tat io.questdb.cairo.sql.VirtualRecord.getStr(VirtualRecord.java:119)\r\n\tat io.questdb.griffin.rowcopier/0x000000080020e840.copy(Unknown Source)\r\n\tat io.questdb.griffin.SqlCompiler.copyUnordered(SqlCompiler.java:1331)\r\n\tat io.questdb.griffin.SqlCompiler.insertAsSelect(SqlCompiler.java:1663)\r\n\tat io.questdb.griffin.SqlCompiler.executeWithRetries(SqlCompiler.java:1459)\r\n\tat io.questdb.griffin.SqlCompiler.compileUsingModel(SqlCompiler.java:1239)\r\n\tat io.questdb.griffin.SqlCompiler.compile(SqlCompiler.java:710)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.processQuery(PGConnectionContext.java:1414)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.parse(PGConnectionContext.java:976)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.handleClientOperation(PGConnectionContext.java:304)\r\n\tat io.questdb.cutlass.pgwire.PGJobContext.handleClientOperation(PGJobContext.java:79)\r\n\tat io.questdb.cutlass.pgwire.PGWireServer$1.lambda$$0(PGWireServer.java:69)\r\n\tat io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODispatcher.java:151)\r\n\tat io.questdb.cutlass.pgwire.PGWireServer$1.run(PGWireServer.java:82)\r\n\tat io.questdb.mp.Worker.run(Worker.java:107)\r\n`\r\n**Additional context**\r\n\r\nsql = `\r\n\t\tINSERT INTO orders\r\n\t\t\tSELECT\r\n\t\t\t\tx uid, --increasing integer\r\n\t\t\t\trnd_long(1, 100000000, 0) rider_id,\r\n\t\t\t\trnd_long(1, 10000000, 0) taxi_id,\r\n\t\t\t\trnd_long(1, 10000000, 0) driver_id,\r\n\t\t\t\trnd_timestamp(to_timestamp('2017', 'yyyy'), to_timestamp('2018', 'yyyy'), 0)start_time,\r\n\t\t\t\trnd_timestamp(to_timestamp('2019', 'yyyy'), to_timestamp('2020', 'yyyy'), 0) end_time,\r\n\t\t\t\trnd_str('alipay', 'wechatpay', 'cashpay') payment_type,\r\n\t\t\t\trnd_double() fare,\r\n\t\t\t\trnd_str('xxxxx', 'xxxxxx', 'xxxxx') start_location,\r\n\t\t\t\trnd_str('xxxxx', 'xxxxx', 'xxxxx') end_location\r\n\t\t\tFROM long_sequence(10000000) x\r\n\t\t;\r\n\t`\r\n\r\nc := time.Tick(5 * time.Second)\r\n    for next := range c {\r\n\tfmt.Printf(\"%v \\n\", next)\r\n\t_, err = db.Exec(sql)\r\n\r\n\tif err != nil {\r\n\t\tfmt.Println(err)\r\n\t}\r\n   }\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 137,
    "metadata": {
      "issue_number": 599,
      "state": "closed",
      "labels": [],
      "comments_count": 11,
      "created_at": "2020-09-14T01:12:25Z",
      "updated_at": "2020-09-15T08:49:31Z",
      "closed_at": "2020-09-15T08:49:31Z",
      "author": "dybxin",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-136969006db8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/55",
    "title": "Requesting homebrew formula for installation",
    "text": "# Requesting homebrew formula for installation\n\nProvide a homebrew package installer (http://brew.sh/index.html)\n\nsay 'brew install questdb' && 'brew service start questdb'  to bring up the webui on osx\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 28,
    "metadata": {
      "issue_number": 55,
      "state": "closed",
      "labels": [],
      "comments_count": 11,
      "created_at": "2016-09-28T18:53:52Z",
      "updated_at": "2016-10-16T20:23:32Z",
      "closed_at": "2016-10-15T11:42:05Z",
      "author": "aahmed-se",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-fd2896ba90f9",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/54",
    "title": "With index JVM crashes",
    "text": "# With index JVM crashes\n\nI have been using QuestDB embedded in my program and I love its speed however when I tried to create a symbol column with indexing it crashes the JVM like so:\n# A fatal error has been detected by the Java Runtime Environment:\n# \n# EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000000006d7d15c0, pid=7512, tid=5912\n# \n# JRE version: Java(TM) SE Runtime Environment (8.0_45-b15) (build 1.8.0_45-b15)\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode windows-amd64 compressed oops)\n# Problematic frame:\n# V  [jvm.dll+0x2715c0]\n# \n# Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\n# \n# An error report file with more information is saved as:\n# C:\\Users\\musta_000\\Desktop\\Test\\hs_err_pid7512.log\n\nCompiled method (nm)    2050  349     n 0       sun.misc.Unsafe::copyMemory (native)\n total in heap  [0x0000000002bd1ad0,0x0000000002bd1e50] = 896\n relocation     [0x0000000002bd1bf0,0x0000000002bd1c38] = 72\n main code      [0x0000000002bd1c40,0x0000000002bd1e50] = 528\nCompiled method (c1)    2063  433       3       sun.misc.Unsafe::copyMemory (11 bytes)\n total in heap  [0x0000000002c07b10,0x0000000002c07f28] = 1048\n relocation     [0x0000000002c07c30,0x0000000002c07c70] = 64\n main code      [0x0000000002c07c80,0x0000000002c07e00] = 384\n stub code      [0x0000000002c07e00,0x0000000002c07ea8] = 168\n metadata       [0x0000000002c07ea8,0x0000000002c07eb0] = 8\n scopes data    [0x0000000002c07eb0,0x0000000002c07ee0] = 48\n scopes pcs     [0x0000000002c07ee0,0x0000000002c07f20] = 64\n dependencies   [0x0000000002c07f20,0x0000000002c07f28] = 8\n# \n# If you would like to submit a bug report, please visit:\n# http://bugreport.java.com/bugreport/crash.jsp\n# \n\nI am able to append approximately 9,586,980 rows to the Journal before the crash. (using a test case as my data takes a while to generate) I have been messing around with the JournalConfigurationBuilder options to try an alleviate this to no avail.\nMy configuration build looks as such:\npublic static final JournalConfigurationBuilder CONFIG = new JournalConfigurationBuilder() {{\n        $(Primers.class)\n                .keyColumn(\"Cluster\").txCountHint(100000000)\n                .$sym(\"Cluster\").index().size(60).valueCountHint(100000000).noCache()\n        ;\n    }};\nPrimers.class has these columns:\nprivate long Sequence;\nprivate String Strain;\nprivate String Cluster;\nprivate boolean Hairpin;\n\nI been trying my hardest to figure it out but I only have a basic understanding of how questdb works. I personally think it is running out of space for either the index or the journal file but I'm not sure. I can post the hs_err_pid7512.log if necessary. Thank you for your time. \n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 338,
    "metadata": {
      "issue_number": 54,
      "state": "closed",
      "labels": [],
      "comments_count": 11,
      "created_at": "2016-07-20T17:59:08Z",
      "updated_at": "2016-07-26T20:22:33Z",
      "closed_at": "2016-07-26T20:22:33Z",
      "author": "charlesgregory",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e508b6215818",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/30",
    "title": "ArrayIndexOutOfBoundsException when trying to add 2^14 Strings with index",
    "text": "# ArrayIndexOutOfBoundsException when trying to add 2^14 Strings with index\n\nHello,\n\nI've encountered the following error on version 3.0.0-20150216.031900-5: ArrayIndexOutOfBoundsException: -64, when trying to add 2^14 (16384) elements with indexed String column. Without index everything works fine, also $int with index does work normally - unfortunately only $str/$sym seem to be queryable by key/value.\n\nTest that reproduces the error: https://gist.github.com/user16558789/d1b0781f1d5ca21bd637\n\nOn a side note: is there a way in 3.0.0 to query by int values (like \"select \\* from table where id = 123;\")? The code in https://github.com/NFSdb/nfsdb/releases/tag/2.0.1 does not work anymore, unfortunately.\n\nThank you!\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 95,
    "metadata": {
      "issue_number": 30,
      "state": "closed",
      "labels": [],
      "comments_count": 11,
      "created_at": "2015-03-02T20:39:10Z",
      "updated_at": "2015-03-15T03:34:39Z",
      "closed_at": "2015-03-03T00:58:39Z",
      "author": "user16558789",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7b7f356962a4",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/17",
    "title": "Replay mode",
    "text": "# Replay mode\n\nSometimes you want to replay events or emit back events with the exact time spacing or a multiple of it. This would be a good option to add.\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 31,
    "metadata": {
      "issue_number": 17,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 11,
      "created_at": "2014-08-30T04:19:38Z",
      "updated_at": "2014-09-03T13:05:00Z",
      "closed_at": "2014-09-03T13:05:00Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2e687802a045",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6144",
    "title": "OOM exception is not well handled from HTTP Server",
    "text": "# OOM exception is not well handled from HTTP Server\n\n### To reproduce\n\nHTTP connection times out when server experiences OOM here:\n```\n2025-09-15T13:10:47.0890734Z 0000-00-00T00:00:00.000000Z C Unhandled exception in worker minhttp_0\n2025-09-15T13:10:47.0891595Z io.questdb.cairo.CairoException: [-1] global RSS memory limit exceeded [usage=7330513230, RSS_MEM_LIMIT=7330513230, size=64, memoryTag=33]\n2025-09-15T13:10:47.0892945Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cairo.CairoException.instance(CairoException.java:375)\n2025-09-15T13:10:47.0894187Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cairo.CairoException.nonCritical(CairoException.java:133)\n2025-09-15T13:10:47.0895176Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.Unsafe.checkAllocLimit(Unsafe.java:331)\n2025-09-15T13:10:47.0897091Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.Unsafe.malloc(Unsafe.java:248)\n2025-09-15T13:10:47.0898003Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cutlass.http.HttpHeaderParser$BoundaryAugmenter.<init>(HttpHeaderParser.java:979)\n2025-09-15T13:10:47.0899411Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cutlass.http.HttpHeaderParser.<init>(HttpHeaderParser.java:58)\n2025-09-15T13:10:47.0901270Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cutlass.http.DefaultHttpHeaderParserFactory.newParser(DefaultHttpHeaderParserFactory.java:35)\n2025-09-15T13:10:47.0902612Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cutlass.http.HttpConnectionContext.<init>(HttpConnectionContext.java:148)\n2025-09-15T13:10:47.0904103Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.cutlass.http.HttpServer$HttpContextFactory.lambda$new$0(HttpServer.java:371)\n2025-09-15T13:10:47.0905338Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.WeakMutableObjectPool.newInstance(WeakMutableObjectPool.java:69)\n2025-09-15T13:10:47.0906921Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.WeakMutableObjectPool.newInstance(WeakMutableObjectPool.java:31)\n2025-09-15T13:10:47.0908022Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.WeakObjectPoolBase.fill(WeakObjectPoolBase.java:92)\n2025-09-15T13:10:47.0909432Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.WeakMutableObjectPool.<init>(WeakMutableObjectPool.java:38)\n2025-09-15T13:10:47.0911026Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.network.IOContextFactoryImpl.lambda$new$0(IOContextFactoryImpl.java:43)\n2025-09-15T13:10:47.0911858Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.std.ThreadLocal.get(ThreadLocal.java:46)\n2025-09-15T13:10:47.0913245Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.network.IOContextFactoryImpl.setup(IOContextFactoryImpl.java:86)\n2025-09-15T13:10:47.0914564Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.network.AbstractIODispatcher.setup(AbstractIODispatcher.java:243)\n2025-09-15T13:10:47.0915401Z \tat io.questdb@9.0.4-SNAPSHOT/io.questdb.mp.Worker.run(Worker.java:136)\n2025-09-15T13:11:47.1173458Z 2025-09-15T13:11:47.112274Z I server-main QuestDB is shutting down...\n```\n\n### QuestDB version:\n\nlatest\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nLinux (arm)\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nVlad Ilyushechenko\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 174,
    "metadata": {
      "issue_number": 6144,
      "state": "open",
      "labels": [
        "hacktoberfest"
      ],
      "comments_count": 10,
      "created_at": "2025-09-15T13:45:13Z",
      "updated_at": "2025-10-11T03:48:59Z",
      "closed_at": null,
      "author": "bluestreak01",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-69ea664fb615",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5644",
    "title": "Bug: Cannot Set TTL",
    "text": "# Bug: Cannot Set TTL\n\n### To reproduce\n\nRun this SQL `ALTER TABLE table_name SET TTL 7 DAYS`\n\nGot error : `[2:14:45 PM GMT+07:00] 'param' or 'type' expected`\n\nLog : `compute.internal questdb[240484]: 2025-04-30T07:12:24.120071Z E i.q.g.e.QueryProgress err [id=-1, sql=`ALTER TABLE table_name SET TTL 7 DAYS;`, principal=admin, cache=false, jit=true, time=91156, msg='param' or 'type' expected, errno=0, pos=34]`\n\nThis also occurs when creating table with TTL.\n\n### QuestDB version:\n\n12.3 (questdb)\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nAmazon Linux 2023.6.20241121\n\n### File System, in case of Docker specify Host File System:\n\nAMI\n\n### Full Name:\n\nKaisar Bumi\n\n### Affiliation:\n\nIndependent Developer\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n<img width=\"589\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/baefec46-f58d-450e-86fb-367a9e6a65b7\" />\n\n\n<img width=\"1257\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0b6314ae-3b63-4537-8aed-ac0169e1ad7b\" />",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 141,
    "metadata": {
      "issue_number": 5644,
      "state": "open",
      "labels": [],
      "comments_count": 10,
      "created_at": "2025-04-30T07:25:13Z",
      "updated_at": "2025-08-03T13:39:27Z",
      "closed_at": null,
      "author": "bsdrpst",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-c11812a2b6b6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5602",
    "title": "Confusing error when an incorrect `sample by` time unit is specified",
    "text": "# Confusing error when an incorrect `sample by` time unit is specified\n\n### To reproduce\n\n```\nadmin=> select timestamp,symbol, avg(price)  from trades sample by 1min;\nERROR:  Invalid unit: n\n```\n'1min' should be '1m' here\n\nlog\n\n```\n2025-04-17T10:05:38.142517Z E i.q.g.e.QueryProgress err [id=-1, sql=`select timestamp,symbol, avg(price)  from trades sample by 1min;`, principal=admin, cache=false, jit=true, time=1749000, msg=Invalid unit: n, errno=0, pos=-1]\n2025-04-17T10:05:38.143018Z E i.q.c.p.m.PGConnectionContextModern failed to parse message [err: ``]\n```\n\n### QuestDB version:\n\nmaster\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nmacos\n\n### File System, in case of Docker specify Host File System:\n\nApfs\n\n### Full Name:\n\nVictor\n\n### Affiliation:\n\nVictor\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 135,
    "metadata": {
      "issue_number": 5602,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL",
        "Good first issue"
      ],
      "comments_count": 10,
      "created_at": "2025-04-17T10:09:20Z",
      "updated_at": "2025-06-02T15:50:34Z",
      "closed_at": "2025-06-02T15:50:34Z",
      "author": "kafka1991",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-ea470e851f95",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5114",
    "title": "Add an optional timezone parameter to `dateadd`.",
    "text": "# Add an optional timezone parameter to `dateadd`.\n\n### Is your feature request related to a problem?\n\nUsing `dateadd` works in other timezones than UTC, but we need to convert to the timezone and back and so the code is too lengthy.\n\n### Describe the solution you'd like.\n\nAn optional timezone parameter added to `dateadd`.\r\n\r\n```SQL\r\nDATEADD('w', 1, '2024-10-21', 'Europe/Bratislava')\r\n```\n\n### Describe alternatives you've considered.\n\n```SQL\r\nTO_UTC(DATEADD('w', 1, TO_TIMEZONE('2024-10-21', 'Europe/Bratislava')), 'Europe/Bratislava')\r\n```\n\n### Full Name:\n\nPeter FaÄko\n\n### Affiliation:\n\nmyself\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 86,
    "metadata": {
      "issue_number": 5114,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 10,
      "created_at": "2024-10-29T15:05:31Z",
      "updated_at": "2025-01-03T01:45:35Z",
      "closed_at": "2025-01-03T01:45:35Z",
      "author": "peter-facko",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e23bb44afa7c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4164",
    "title": "index function does not work. ",
    "text": "# index function does not work. \n\n### To reproduce\n\n> 1. QuestDB brings a powerful storage engine with high storage performance, but querying data is extremely slow. The performance is the same when indexing is created or not, because the entire partition is scanned.\r\n> \r\n> 2. If the partition data is larger, querying the entire table requires huge consumption, and the officially introduced indexes and practical indexes will not work at all.\r\n> \r\n> 3. It is a great pity that a database does not have an index. If QuestDB does not have an index, please delete the introduction to the index in the document.This can easily mislead users into thinking that there is an index function.\r\n\r\n\r\nCreate table example:\r\n`CREATE TABLE demo (\r\n  symb SYMBOL INDEX CAPACITY 256,\r\n  p DOUBLE,\r\n  ts TIMESTAMP\r\n) TIMESTAMP(ts) PARTITION BY DAY;`\r\n\r\n\r\nQuery SQL example:\r\n`SELECT * FROM demo WHERE symb = 'a' and ts in '2024-01-26';`\r\n\r\nAfter creating the index, actually scan the entire partition data of '2024-01-23' on that day.\r\n\r\nThere is no official document shown above, and it is scanned based on the index.\r\nOther users of earlier versions have also reported that the index does not work. Indexing is a fatal problem. I hope that the feedback will be successful and this problem can be fixed. Thanks!\r\nhttps://questdb.io/docs/concept/indexes/\r\n\n\n### QuestDB version:\n\n6-7.3.9\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nall\n\n### File System, in case of Docker specify Host File System:\n\nall\n\n### Full Name:\n\nlunxhsu\n\n### Affiliation:\n\nlunxhsu\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 285,
    "metadata": {
      "issue_number": 4164,
      "state": "open",
      "labels": [],
      "comments_count": 10,
      "created_at": "2024-01-26T12:04:04Z",
      "updated_at": "2024-02-04T14:03:41Z",
      "closed_at": null,
      "author": "lunxhsu",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-427a7b969a83",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3308",
    "title": "coalesce returns null on result from division by zero",
    "text": "# coalesce returns null on result from division by zero\n\n**TLDR Update:** in aggregate functions (AVG / SUM / MAX / etc.), QDB internally handles `-Infinity` and `Infinity` values as nulls and do not calculate them into the final result. Column with 1000 **Infinity** values and two values of 1,3 will have the AVG of 2. which is wrong.\r\n\r\nI see now behavior is changed and its actually returning null when dividing by zero. which can be treaded with a simple coalesce.\r\n\r\n### Describe the bug\r\ncoalesce should convert the nulls to something else.. in this case 0 but the result is all nulls.\r\n\r\nI know some of my tickers may seem weird but this is a real scenario that actually happened to me on my server, I have just reproduced it on demo for your convenience.\r\n\r\n### To reproduce\r\n\r\ntry on [demo questdb](https://demo.questdb.io/)\r\n`SELECT *,A/G,coalesce(A/G,0) FROM (SELECT galon_price A,coalesce((galon_price1),0) G FROM (SELECT * FROM gas_prices b lt join (SELECT * FROM gas_prices a) on timestamp))`\r\n\r\n### Expected Behavior\r\n\r\nresult should be 0\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: demo\r\n- **OS**: ?\r\n- **Browser**: chrome\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 192,
    "metadata": {
      "issue_number": 3308,
      "state": "open",
      "labels": [
        "Question",
        "REST API"
      ],
      "comments_count": 10,
      "created_at": "2023-05-06T12:42:25Z",
      "updated_at": "2024-02-01T10:55:23Z",
      "closed_at": null,
      "author": "superichmann",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-bf2b61cc4faa",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3062",
    "title": "Occasional SIGSEGV on 7.0.1",
    "text": "# Occasional SIGSEGV on 7.0.1\n\n### Describe the bug\r\n\r\nI'm seeing the database crash occasionally when doing lots of inserts using the Java ILP client\r\n\r\n```\r\ntsMax=2023-03-10T18:06:17.807000Z, commitToTimestamp=1969-12-31T23:59:59.999999Z]\r\n2023-03-10T18:06:18.506628Z I i.q.c.TableWriter processing WAL [path=/var/lib/questdb/db/controller-status~8/wal7/0, roLo=35642, seqTxn5951, roHi=35675, tsMin=2023-03-10T18:06:16.798000Z, tsMax=2023-03-10T18:06:17.818000Z, commitToTimestamp=1969-12-31T23:59:59.999999Z]\r\n2023-03-10T18:06:18.514509Z I i.q.c.TableWriter processing WAL [path=/var/lib/questdb/db/controller-status~8/wal6/0, roLo=35991, seqTxn5952, roHi=36023, tsMin=2023-03-10T18:06:16.801000Z, tsMax=2023-03-10T18:06:17.806000Z, commitToTimestamp=294247-01-10T04:00:54.775807Z]\r\n2023-03-10T18:06:18.515401Z I i.q.c.TableWriter sorting WAL [table=controller-status, ordered=false, lagRowCount=258, rowLo=35991, rowHi=36023]\r\n2023-03-10T18:06:18.583126Z I i.q.c.TableWriter closing last partition [table=device-status]\r\n2023-03-10T18:06:18.587159Z I i.q.c.TableWriter merged partition [table=`device-status`, ts=2023-03-10T18:00:00.000000Z, txn=3674]\r\n2023-03-10T18:06:18.592116Z I i.q.c.TableWriter switched partition [path='/var/lib/questdb/db/device-status~5/2023-03-10T18.3674']\r\n2023-03-10T18:06:18.669128Z I i.q.c.TableWriter o3 partition task [table=controller-status, partitionIsReadOnly=false, srcOooBatchRowSize=290, srcOooLo=0, srcOooHi=289, srcOooMax=290, o3RowCount=290, o3LagRowCount=0, srcDataMax=45829, o3TimestampMin=2023-03-10T18:06:15.602000Z, o3Timestamp=2023-03-10T18:06:15.602000Z, o3TimestampMax=2023-03-10T18:06:17.818000Z, partitionTimestamp=2023-03-10T18:00:00.000000Z, partitionIndex=4, partitionSize=46119, maxTimestamp=2023-03-10T18:06:15.643000Z, last=true, append=false, pCount=1, flattenTimestamp=true, memUsed=108346810934]\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00007f2a2c0baa08, pid=1, tid=91\r\n#\r\n# JRE version: OpenJDK Runtime Environment Corretto-17.0.3.6.1 (17.0.3+6) (build 17.0.3+6-LTS)\r\n# Java VM: OpenJDK 64-Bit Server VM Corretto-17.0.3.6.1 (17.0.3+6-LTS, mixed mode, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n# Problematic frame:\r\n# C  [libquestdb16208632089475074569.so+0x6fa08]  merge_copy_var_column_int32_AVX2(index_t*, long, long*, char*, long*, char*, long*, char*, long)+0xa8\r\n#\r\n# Core dump will be written. Default location: /var/lib/questdb/core.1\r\n#\r\n# An error report file with more information is saved as:\r\n# /var/lib/questdb/db/hs_err_pid+1.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/corretto/corretto-17/issues/\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n#\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 7.0.1 (docker image)\r\n- **OS**: Amazon Linux, m6a.16xlarge instance\r\n- **Browser**: N/A (Java ILP client inserts)\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 278,
    "metadata": {
      "issue_number": 3062,
      "state": "closed",
      "labels": [],
      "comments_count": 10,
      "created_at": "2023-03-10T19:01:19Z",
      "updated_at": "2023-03-17T18:29:04Z",
      "closed_at": "2023-03-17T18:29:04Z",
      "author": "andyb-ev",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1ea06ac3e759",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1595",
    "title": "Multiple thread write error",
    "text": "# Multiple thread write error\n\n### Describe the bug\r\n\r\nHello. I used QuestDB 6.1.1 in Springboot. I use one client bean to write data, and the bean is used in 16 threads. When the QuestDb works on CentOS , there is a error \r\n\r\n```\r\n2021-11-17T03:29:22.759826Z E i.q.c.l.t.LineTcpConnectionContext [100] could not process line data [table=gs, msg=could not mmap  [size=12288, offset=0, fd=120, memUsed=3361135068, fileLen=12288], errno=19]\r\n``` \r\n\r\nThen I deploy the QuestDB on Windows 10, version 21H2, I got another error, the log on windows just as the attachment\r\n[stdout-2021-11-19T15-16-53.txt](https://github.com/questdb/questdb/files/7569112/stdout-2021-11-19T15-16-53.txt)\r\n:\r\n\r\nOn line 231, I find that there is a lot of whitespace at the front of  tablename. \r\nIn addition, the error occurs after my program starts about 40s.\r\n\r\nHere is my code:\r\n\r\n```\r\n@Autowired\r\nprivate LineTcpSender sender;\r\n\r\n@Override\r\npublic void addPoint(HistoryDataDTO hisData) {\r\n    sender.put(getLine(hisData));\r\n}\r\n\r\nprivate CharSequence getLine(HistoryDataDTO hisData) {\r\n    return String.format(\"%s,tagName=%s,objectName=%s value=%f,quality=\\\"%s\\\" %d\\n\",\r\n            hisData.getTenantName(),\r\n            hisData.getTagName(),\r\n            hisData.getMetricName(),\r\n            Double.parseDouble(hisData.getValue().toString()),\r\n            hisData.getQuality(),\r\n            hisData.getTime() * 1000000);\r\n}\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 178,
    "metadata": {
      "issue_number": 1595,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 10,
      "created_at": "2021-11-19T08:41:33Z",
      "updated_at": "2021-11-26T19:55:39Z",
      "closed_at": "2021-11-26T19:55:39Z",
      "author": "Gszekt",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-25f1b4f64692",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1565",
    "title": "Add decimal with fixed precision/scale as datatype",
    "text": "# Add decimal with fixed precision/scale as datatype\n\nDecimal numbers are missing between supported questdb datatypes. Given that one of the use cases for the db is storing market price data, and these usually have the decimal format, I believe this would be frequently used.\r\n\r\nE.g. decimal(p,s) in mssql: https://docs.microsoft.com/en-us/sql/t-sql/data-types/decimal-and-numeric-transact-sql?view=sql-server-ver15",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 50,
    "metadata": {
      "issue_number": 1565,
      "state": "open",
      "labels": [
        "New feature",
        "Core"
      ],
      "comments_count": 10,
      "created_at": "2021-11-12T08:42:59Z",
      "updated_at": "2025-08-04T08:37:11Z",
      "closed_at": null,
      "author": "xrust01",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-5dc9c0260595",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1012",
    "title": "Crash: EXCEPTION_ILLEGAL_INSTRUCTION",
    "text": "# Crash: EXCEPTION_ILLEGAL_INSTRUCTION\n\n**Describe the bug**\r\nWhen I run the following query, QuestDB crashes:\r\n\r\n`SELECT sensorID, max(reading) from temperatures;`\r\n\r\nLog:\r\n\r\n2021-05-13T07:32:47.790498Z I http-server connected [ip=127.0.0.1, fd=1580]\r\n2021-05-13T07:32:53.766413Z I i.q.c.h.p.JsonQueryProcessorState [1532] exec [q='SELECT sensorID, max(reading) from temperatures']\r\n2021-05-13T07:32:53.766472Z I i.q.c.h.p.QueryCache miss [thread=questdb-worker-3, sql=SELECT sensorID, max(reading) from temperatures]\r\n2021-05-13T07:32:53.767357Z I i.q.g.SqlCompiler plan [q=`select-group-by sensorID, max(reading) max from (select [sensorID, reading] from temperatures timestamp (ts))`, fd=1532]\r\n2021-05-13T07:32:53.776687Z I i.q.c.h.p.JsonQueryProcessorState [1532] execute-new [skip: 0, stop: 1000]\r\n\r\n A fatal error has been detected by the Java Runtime Environment:\r\n\r\n  EXCEPTION_ILLEGAL_INSTRUCTION (0xc000001d) at pc=0x0000000066a9c192, pid=25432, tid=11796\r\n\r\n JRE version: Java(TM) SE Runtime Environment 18.9 (11.0.8+10) (build 11.0.8+10-LTS)\r\n Java VM: Java HotSpot(TM) 64-Bit Server VM 18.9 (11.0.8+10-LTS, mixed mode, tiered, compressed oops, parallel gc, windows-amd64)\r\n Problematic frame:\r\n C  [libquestdb17581070583141605799.dll+0x1c192]\r\n\r\n No core dump will be written. Minidumps are not enabled by default on client versions of Windows\r\n\r\n An error report file with more information is saved as:\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - Windows 10\r\n - questdb 6.0.0\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 159,
    "metadata": {
      "issue_number": 1012,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 10,
      "created_at": "2021-05-13T07:48:09Z",
      "updated_at": "2021-05-21T12:57:06Z",
      "closed_at": "2021-05-21T12:57:06Z",
      "author": "Yussef77",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-87502beffca3",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/903",
    "title": "Add support for IP types such as IPV4 / IPV6 similar to 'inet'",
    "text": "# Add support for IP types such as IPV4 / IPV6 similar to 'inet'\n\n**Is your feature request related to a problem? Please describe.**\r\nCurrently to store IP addresses, I must use the `string` type.\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to store IP addresses in QuestDB using dedicated types so that I can more efficiently store these values and run queries which have the correct semantics without string conversion.\r\n\r\nIn Postgres this looks like the following:\r\n\r\n```sql\r\ncreate table inet_test (address inet);\r\ninsert into inet_test values ('192.168.2.1');\r\ninsert into inet_test values ('192.168.2.1/24');\r\nselect * from inet_test;\r\n\r\n address\r\n----------------\r\n 192.168.2.1\r\n 192.168.2.1/24\r\n ```\r\n\r\n**Additional context**\r\nIPV4 is a 4-byte host address (`binary(4)`) in dotted-decimal notation  - four decimal numbers, each ranging from 0 to 255, separated by dots):\r\n\r\n```\r\n192.168.0.1\r\n```\r\n\r\nIPV6 is a 16-byte host address (`binary(16)`) in eight groups of four hexadecimal digits separated by colons:\r\n\r\n```\r\n2001:0db8:3c4d:0015:0000:0000:1a2f:1a2b\r\n```\r\n\r\n__Considerations__\r\n\r\n* IPv6 allows using the two-colon (::) notation to represent contiguous 16-bit fields of zeros. \r\n* Fields of zeros can be represented as a single 0\r\n* Leading zeros in a field may be omitted, such as changing `0db8` to `db8`\r\n\r\nTherefore, the following are all equivalent:\r\n\r\n```\r\n2001:0db8:3c4d:0015:0000:0000:1a2f:1a2b\r\n2001:0db8:3c4d:0015::1a2f:1a2b\r\n2001:db8:3c4d:15::1a2f:1a2b\r\n```\r\n\r\n* The ability to support for block suffixes should be considered, i.e.:\r\n    * 192.168.100.0/24\r\n    * 2001:db8:3c4d::/48\r\n\r\nTo compare with other systems, in PostgreSQL, there are two data types `inet` and `cidr`\r\n\r\n> The essential difference between inet and cidr data types is that inet accepts values with nonzero bits to the right of the netmask, whereas cidr does not. For example, 192.168.0.1/24 is valid for inet but not for cidr.\r\n\r\n__References:__\r\n\r\n* [IPV6 overview](https://docs.oracle.com/cd/E23823_01/html/816-4554/ipv6-overview-10.html)\r\n* [Postgres network address types](https://www.postgresql.org/docs/current/datatype-net-types.html)\r\n\r\n__Additional functionality:__\r\n\r\nBy supporting types of this kind, there is the possibility to allow the use of network address functions and operators which are used like so:\r\n\r\n| operator| description|\r\n|---|---|\r\n|<<| `inet << inet â†’ boolean` - Is subnet strictly contained by subnet? (`inet '192.168.1.5' << inet '192.168.1/24' â†’ t`)\r\n| <<= | `inet <<= inet â†’ boolean` - Is subnet contained by or equal to subnet? (`inet '192.168.1/24' <<= inet '192.168.1/24' â†’ t`)\r\n\r\nFor more details, see [Postgres Network address Functions and Operators](https://www.postgresql.org/docs/current/functions-net.html)\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 372,
    "metadata": {
      "issue_number": 903,
      "state": "open",
      "labels": [
        "New feature",
        "Core"
      ],
      "comments_count": 10,
      "created_at": "2021-04-08T16:57:29Z",
      "updated_at": "2026-02-06T15:17:25Z",
      "closed_at": null,
      "author": "bsmth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-4a7afa004d21",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/28",
    "title": "Network example produces different results on Linux vs Windows",
    "text": "# Network example produces different results on Linux vs Windows\n\nI ran the SimpleReplication server/client example as outlined in this URL:\nhttp://github.nfsdb.org/\n\nThe key difference between the example shown on the website and mine is that I declare a serverConfig or clientConfig with a setIdName(\"eth0\"):\n\n[server]\nJournalServer server = new JournalServer(new ServerConfig() {{\n            setIfName(\"eth0\");\n        }}, factory);\n\n[client]\nfinal JournalClient client = new JournalClient(new ClientConfig() {{\n            setIfName(\"eth0\");\n        }},factory);\n\nExpected Behavior:\n1. As soon as I declare a JournalReader on the client, the db file must be immediately synced.\n2. As the server publishes new data, it should be synchronized on the client side.\n\nOn Windows (as expected), as soon as I run the SimpleReplicationClientMain, I can see the sync messages as shown below:\n\nC:\\VG\\dev\\NFSDBTest\\src>java SimpleReplicationClientMain C:\\VG\\dev\\NFSDBTest\\data\nJan 05, 2015 4:51:42 PM com.nfsdb.journal.net.mcast.AbstractOnDemandPoller\nINFO: Polling on name:eth0 (Intel(R) 82579LM Gigabit Network Connection)\nJan 05, 2015 4:51:42 PM com.nfsdb.journal.net.config.ClientConfig\nINFO: Connected to /192.168.1.12:7075\nClient started\ntook: 1978, count=1000000\nJan 05, 2015 4:51:42 PM com.nfsdb.journal.net.StatsCollectingReadableByteChannel\n\nINFO: received 39000097 bytes @ 153.059222 MB/s from: /192.168.1.12:7075 [1939 c\nalls]\ntook: 406, count=1000000\n\nWhereas running the same on a Linux machine, you can see that the client connected fine but no sync messages for a long time. And then about 15-min later, you start seeing the sync messages but the count remains zero. The debug messages keeps repeating for a minute and then pauses for another minute and this keeps repeating in a cycle.\n\nDEBUG on {LINUX} SERVER:\nINFO: sent 71 bytes @ Infinity MB/s to: /192.168.1.19:43112 [4 calls]\nJan 05, 2015 7:55:51 PM com.nfsdb.journal.net.StatsCollectingWritableByteChannel\n\nDEBUG on {LINUX} CLIENT:\ntook: 1420505748619, count=0\nJan 05, 2015 7:55:48 PM com.nfsdb.journal.net.StatsCollectingReadableByteChannel\nINFO: received 57 bytes @ 0.001394 MB/s from: /192.168.1.19:7075 [3 calls]\n\n**\\* Version Details ***\nnfsdb-core-2.1.1-SNAPSHOT.jar\ndisruptor-3.3.0.jar\n\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID:    Ubuntu\nDescription:    Ubuntu 14.04.1 LTS\nRelease:    14.04\nCodename:    trusty\n\n$ java -version\njava version \"1.8.0_25\"\nJava(TM) SE Runtime Environment (build 1.8.0_25-b17)\nJava HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)\n\n**\\* Noticed the same behavior on Ubuntu running OpenJDK **\\* \n\nPlease advise. I can try this on a CentOS system as well in the next few days and publish my results.\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 359,
    "metadata": {
      "issue_number": 28,
      "state": "closed",
      "labels": [],
      "comments_count": 10,
      "created_at": "2015-01-06T01:09:36Z",
      "updated_at": "2015-01-07T01:48:41Z",
      "closed_at": "2015-01-07T01:48:41Z",
      "author": "vguhesan",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-628a2b144863",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6077",
    "title": "Error querying data when a partition is skipped because it hasn't been written to during its period.",
    "text": "# Error querying data when a partition is skipped because it hasn't been written to during its period.\n\n### To reproduce\n\nI have a system that was entering entries for several days. Turned it off for a day or so. Then turned it back on. When it came back on it started tracking items just fine. But when I attempt to query the table I get an error when attempting to read the partition for the day that was off. There isn't a partition for that day since no data was tracked and the system was powered off.\n\nThe table is partitioned by day.\n\nThe queries are not exactly for that partition. its to simple SQL Query to filter data:\n\nThe first query is: \n\"SELECT * FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4') ORDER BY occurred DESC\"\n\nand the second is:\n\"SELECT count() FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4')\"\n\n\nThe odd thing is this query works successfully:\n\"SELECT count() FROM alarmHistory`\"\n\n\n```\n2025-08-25T23:43:39.160463Z I i.q.g.e.QueryProgress exe [id=10, sql=`SELECT * FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4') ORDER BY occurred DESC`, principal=admin, cache=false, jit=false]\n\n2025-08-25T23:43:39.160929Z E i.q.c.TableReader open partition failed, partition does not exist on the disk [path=C:\\Development\\Servers\\finstack6\\var\\proj\\demo\\alarms\\history\\alarmHistory~1\\2025-08-24.14158]\n\n2025-08-25T23:43:39.160995Z E i.q.g.e.QueryProgress err [id=10, sql=`SELECT * FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4') ORDER BY occurred DESC`, principal=admin, cache=false, jit=true, time=539400, msg=Partition '2025-08-24' does not exist in table 'alarmHistory' directory. Run [ALTER TABLE alarmHistory FORCE DROP PARTITION LIST '2025-08-24'] to repair the table or the database from the backup., errno=0, pos=0]\n\n[2025-08-25 16:43:39] <QuestHistoryDB> {} [err] Error reading history database\n  sys::Err: io.questdb.cairo.CairoException: [0] Partition '2025-08-24' does not exist in table 'alarmHistory' directory. Run [ALTER TABLE alarmHistory FORCE DROP PARTITION LIST '2025-08-24'] to repair the table or the database from the backup.\n\n2025-08-25T23:43:39.162009Z I i.q.g.e.QueryProgress exe [id=11, sql=`SELECT count() FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4')`, principal=admin, cache=false, jit=false]\n\n2025-08-25T23:43:39.162219Z E i.q.c.TableReader open partition failed, partition does not exist on the disk [path=C:\\Development\\Servers\\finstack6\\var\\proj\\demo\\alarms\\history\\alarmHistory~1\\2025-08-24.14158]\n\n2025-08-25T23:43:39.162256Z E i.q.g.e.QueryProgress err [id=11, sql=`SELECT count() FROM alarmHistory WHERE sourceRef in('@p:demo:r:1eeb0a6a-c9a53af4')`, principal=admin, cache=false, jit=true, time=247699, msg=Partition '2025-08-24' does not exist in table 'alarmHistory' directory. Run [ALTER TABLE alarmHistory FORCE DROP PARTITION LIST '2025-08-24'] to repair the table or the database from the backup., errno=0, pos=0]\n```\n\n\nExpectations:\nI'd expect it to continue to work as if no data is written for that day.\n\n### QuestDB version:\n\n9.0.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nWindows 11\n\n### File System, in case of Docker specify Host File System:\n\nNTFS\n\n### Full Name:\n\nMichael Rochelle\n\n### Affiliation:\n\nSiemens Industry\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 444,
    "metadata": {
      "issue_number": 6077,
      "state": "open",
      "labels": [],
      "comments_count": 9,
      "created_at": "2025-08-25T23:50:51Z",
      "updated_at": "2025-09-11T18:06:16Z",
      "closed_at": null,
      "author": "EliteScientist",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-bdb227bac7a8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5447",
    "title": "Web Console redirect does't work behind nginx reverse proxy",
    "text": "# Web Console redirect does't work behind nginx reverse proxy\n\n### To reproduce\n\nI've my QuestDB instance running behind an Nginx's Reverse Proxy since there's no built-in auth and I need to protect a little bit the console.\n\nUntil 8.2.1 everything was fine, a GET request to the `/` correctly returns a moved permanently to `/index.html` with and without nginx.\n\nWith the latest version (`8.2.2`) the web server just don't answer. When I try to GET the `/` through nginx the request goes in timeout, while if I try to GET directly `/index.html` it successfully works. The redirect `/ -> /index.html` however works if the request is not made through Nginx.\n\nI guess that the culprit is #5297 which might have broke something, or am I missing some new settings?\n\n### QuestDB version:\n\n8.2.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nHost OS: Ubuntu 22.0.4.5, Docker Image (eclipse-temurin:23-noble)\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nFrancesco Marongiu\n\n### Affiliation:\n\nNone\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 203,
    "metadata": {
      "issue_number": 5447,
      "state": "open",
      "labels": [],
      "comments_count": 9,
      "created_at": "2025-03-06T09:25:41Z",
      "updated_at": "2025-12-03T13:25:31Z",
      "closed_at": null,
      "author": "fgnm",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-b577e0745fb2",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5427",
    "title": "Performance issue with NOT IN filter that includes NULL values inside a CTE",
    "text": "# Performance issue with NOT IN filter that includes NULL values inside a CTE\n\n### To reproduce\n\nWhen using a `NOT IN` filter inside a CTE, which is subsequently aggregated with `SAMPLE BY`, including NULL values with `Field NOT IN ('A', 'B') OR Field IS NOT NULL` results in significantly worse performance.\n\nTo reproduce, create a table with a symbol, numeric, and timestamp field and populate a large number of rows. Fill the symbol column with values including NULL.\n\nThen, test the following query:\n```sql\nWITH FilteredData AS (\n    (SELECT *\n    FROM Test\n    WHERE Timestamp BETWEEN '2024-09-29' AND '2024-10-04' \n    AND Qualifiers NOT IN ('Test1', 'Test2') OR Qualifiers IS NULL\n    --  AND coalesce(Qualifiers, '') NOT IN ('Test1', 'Test2')   \n    --  AND Qualifiers NOT IN ('Test1', 'Test2') \n) timestamp(Timestamp)\n),\nSampledData AS (\n    SELECT Timestamp, first(Price) AS Open\n    FROM FilteredData \n    SAMPLE BY 5m \n)\nSELECT Timestamp AS Timestamp, Open\nFROM SampledData\n;\n```\n\nWhen using the `Qualifiers NOT IN ('Test1', 'Test2') OR Qualifiers IS NULL` filter, the query plan looks like this:\n\n![Image](https://github.com/user-attachments/assets/3b95eebd-81e9-48f8-8e67-87aaeaa1ab44)\n\n(Query time: 4920ms)\n\nCompared to without including NULL:\n\n![Image](https://github.com/user-attachments/assets/ac2ff741-e83c-4cd1-9bc1-92da5f206d42)\n\n(Query time: 750ms)\n\nFor now, it is possible to workaround using Coalesce, but the performance is still not as good as without including NULL:\n\n![Image](https://github.com/user-attachments/assets/7ce2878c-1ab7-4310-ba89-787b1a61e2bb)\n\n(Query time: 2660ms)\n\n\nFinally, it appears that there is no performance issue when doing this outside of a CTE that is subsequently aggregated. E.g. running just the FilteredData CTE query by itself takes ~800ms and has the following query plan:\n\n![Image](https://github.com/user-attachments/assets/ab509c46-3255-4db4-99e1-188a29ddc803)\n\n### QuestDB version:\n\n8.2.1\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nDocker\n\n### File System, in case of Docker specify Host File System:\n\nZFS\n\n### Full Name:\n\nOliver Zhu\n\n### Affiliation:\n\nBRG\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 312,
    "metadata": {
      "issue_number": 5427,
      "state": "open",
      "labels": [
        "SQL",
        "Performance"
      ],
      "comments_count": 9,
      "created_at": "2025-02-27T12:55:30Z",
      "updated_at": "2025-02-28T15:23:00Z",
      "closed_at": null,
      "author": "oz-brg",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-5281f9bd1fce",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5215",
    "title": "NPE on AtomicBooleanCircuitBreaker.cancelledFlag",
    "text": "# NPE on AtomicBooleanCircuitBreaker.cancelledFlag\n\n### To reproduce\r\n\r\nWhen I was using version 8.2, the following error message would appear during the query. I have confirmed that there is no problem with the query statement, but the problem is accidental, and the error message is as follows:\r\n\r\n`Cannot invoke java.util.concurrent.atomic.AtomicBoolean.get() because \"this.cancelledFlag\" is null`\r\n\r\nI have never experienced this problem with version 7.4\r\n\r\n### QuestDB version:\r\n\r\n8.2\r\n\r\n### OS, in case of Docker specify Docker and the Host OS:\r\n\r\ncentos 7.9\r\n\r\n### File System, in case of Docker specify Host File System:\r\n\r\nlinux centos\r\n\r\n### Full Name:\r\n\r\njian peng\r\n\r\n### Affiliation:\r\n\r\n1332606303@qq.com\r\n\r\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\r\n\r\n- [X] Yes, I have\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 131,
    "metadata": {
      "issue_number": 5215,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 9,
      "created_at": "2024-12-02T07:23:16Z",
      "updated_at": "2024-12-05T14:22:40Z",
      "closed_at": "2024-12-05T14:22:40Z",
      "author": "Dapeng960208",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-37dfb2119ea1",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4900",
    "title": "Support negative numbers in `approx_percentile` function",
    "text": "# Support negative numbers in `approx_percentile` function\n\nCurrently, `approx_percentile` only supports non-negative numbers. This limitation comes from the HdrHistogram library, which we use for approx_precentile. The library is designed for latency measurements, hence the limitation. HdrHistogram doesn't support negative numbers, so we'll need to modify the library.\r\n\r\n## Original text\r\n\r\n### To reproduce\r\n\r\n```sql\r\nCREATE TABLE archmag (a double, ts timestamp)timestamp(ts) PARTITION BY DAY WAL;\r\nINSERT INTO archmag (a, ts) VALUES (-100.0,systimestamp()), (-20.0,systimestamp()), (-5.0,systimestamp());\r\nselect approx_percentile(a, 0.1) from archmag;\r\n```\r\n\r\n### QuestDB version:\r\n\r\n8.1.0\r\n\r\n### OS, in case of Docker specify Docker and the Host OS:\r\n\r\nAmazon Linux 2023.4.20240528\r\n\r\n### File System, in case of Docker specify Host File System:\r\n\r\next4\r\n\r\n### Full Name:\r\n\r\nJacek Juraszek\r\n\r\n### Affiliation:\r\n\r\narencibia\r\n\r\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\r\n\r\n- [X] Yes, I have\r\n\r\n### Additional context\r\n\r\nerror log: _The value -100.0 is out of bounds for histogram, current covered range [6.668014432879854E240, 1.3656093558537942E244) cannot be extended any further. Caused by: [-1] The value -100.0 is out of bounds for histogram, current covered range [6.668014432879854E240, 1.3656093558537942E244) cannot be extended any further. Caused by:_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 192,
    "metadata": {
      "issue_number": 4900,
      "state": "open",
      "labels": [
        "Enhancement",
        "SQL",
        "hacktoberfest"
      ],
      "comments_count": 9,
      "created_at": "2024-08-28T11:21:09Z",
      "updated_at": "2025-10-02T08:53:44Z",
      "closed_at": null,
      "author": "jjuraszek",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-78b274582155",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4060",
    "title": "WAL table is suspended when non-existent partition drop executed in 7.3.7",
    "text": "# WAL table is suspended when non-existent partition drop executed in 7.3.7\n\n### Describe the bug\n\nAfter upgrading from 7.3.5 to 7.3.7 questdb service stops to write records after making snapshot  \n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version** 7.3.7:\r\n- **OS** Almalinux 9:\r\n- **Browser** Firefox:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 61,
    "metadata": {
      "issue_number": 4060,
      "state": "closed",
      "labels": [],
      "comments_count": 9,
      "created_at": "2023-12-12T13:17:23Z",
      "updated_at": "2023-12-20T11:53:32Z",
      "closed_at": "2023-12-20T11:53:32Z",
      "author": "smyshliakov",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-631ceb44b3fe",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3296",
    "title": "Same UPDATE query does work in standard table but doesn't in WAL table",
    "text": "# Same UPDATE query does work in standard table but doesn't in WAL table\n\n### Describe the bug\n\nFollowing query:\r\n\r\n\"UPDATE table SET  metric_name = 'deleted' WHERE metric_name like  'Isl00001Lib0%';\"\r\n\r\n\n\n### To reproduce\n\n1. execute UPDATE table SET  metric_name = 'deleted' WHERE metric_name like  'Isl00001Lib0%';\r\n2. SELECT * from table WHERE metric_name LIKE 'Isl00001Lib0%';\r\n3. the matching row has not changed\n\n### Expected Behavior\n\nthe matching rows should be updated\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.1.1\r\n- **OS**: Linux\r\n- **Browser**: Firefox\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 89,
    "metadata": {
      "issue_number": 3296,
      "state": "closed",
      "labels": [],
      "comments_count": 9,
      "created_at": "2023-05-03T14:23:03Z",
      "updated_at": "2023-05-09T20:47:58Z",
      "closed_at": "2023-05-09T20:47:57Z",
      "author": "tolap22",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e221e8d06b89",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2575",
    "title": "SIGSEGV fatal error when Randomizing",
    "text": "# SIGSEGV fatal error when Randomizing\n\n### Describe the bug\n\nWhen I create a specific table, insert data into it and then `select * from table`. QuestDB crashes with the following error:\r\n\r\n```\r\n#\r\n2022-10-03T16:50:31.007887380Z # A fatal error has been detected by the Java Runtime Environment:\r\n2022-10-03T16:50:31.007892370Z #\r\n2022-10-03T16:50:31.007895120Z #  SIGSEGV (0xb) at pc=0x00007f0b09b87fe2, pid=1, tid=102\r\n2022-10-03T16:50:31.007897700Z #\r\n2022-10-03T16:50:31.007899970Z # JRE version: OpenJDK Runtime Environment Corretto-17.0.3.6.1 (17.0.3+6) (build 17.0.3+6-LTS)\r\n2022-10-03T16:50:31.007902130Z # Java VM: OpenJDK 64-Bit Server VM Corretto-17.0.3.6.1 (17.0.3+6-LTS, mixed mode, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n2022-10-03T16:50:31.007904350Z # Problematic frame:\r\n2022-10-03T16:50:31.007906540Z # J 1932 c1 sun.misc.Unsafe.getInt(J)I jdk.unsupported@17.0.3 (8 bytes) @ 0x00007f0b09b87fe2 [0x00007f0b09b87f40+0x00000000000000a2]\r\n2022-10-03T16:50:31.007908730Z #\r\n2022-10-03T16:50:31.007910730Z # No core dump will be written. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n2022-10-03T16:50:31.007913070Z #\r\n2022-10-03T16:50:31.008852587Z # An error report file with more information is saved as:\r\n2022-10-03T16:50:31.008871857Z # /var/lib/questdb/db/hs_err_pid+1.log\r\n2022-10-03T16:50:31.045703534Z Compiled method (c1)   44543 1932       3       sun.misc.Unsafe::getInt (8 bytes)\r\n2022-10-03T16:50:31.045724344Z  total in heap  [0x00007f0b09b87d90,0x00007f0b09b88140] = 944\r\n2022-10-03T16:50:31.045728034Z  relocation     [0x00007f0b09b87ef0,0x00007f0b09b87f28] = 56\r\n2022-10-03T16:50:31.045732774Z  main code      [0x00007f0b09b87f40,0x00007f0b09b88080] = 320\r\n2022-10-03T16:50:31.045735204Z  stub code      [0x00007f0b09b88080,0x00007f0b09b880b0] = 48\r\n2022-10-03T16:50:31.045737024Z  oops           [0x00007f0b09b880b0,0x00007f0b09b880b8] = 8\r\n2022-10-03T16:50:31.045738794Z  metadata       [0x00007f0b09b880b8,0x00007f0b09b880c8] = 16\r\n2022-10-03T16:50:31.045740414Z  scopes data    [0x00007f0b09b880c8,0x00007f0b09b880f8] = 48\r\n2022-10-03T16:50:31.045742344Z  scopes pcs     [0x00007f0b09b880f8,0x00007f0b09b88138] = 64\r\n2022-10-03T16:50:31.045744164Z  dependencies   [0x00007f0b09b88138,0x00007f0b09b88140] = 8\r\n2022-10-03T16:50:31.076943547Z 2022-10-03T16:50:31.002398Z I i.q.c.h.p.JsonQueryProcessorState [90] exec [q=''sensor_logs'']\r\n2022-10-03T16:50:31.076967997Z 2022-10-03T16:50:31.002442Z I i.q.c.h.p.QueryCache miss [thread=questdb-worker-26, sql='sensor_logs']\r\n2022-10-03T16:50:31.076971767Z 2022-10-03T16:50:31.005385Z I i.q.g.SqlCompiler plan [q=`select-choose object_id, key, value, ts from (select [object_id, key, value, ts] from sensor_logs timestamp (ts))`, fd=90]\r\n2022-10-03T16:50:31.076973917Z 2022-10-03T16:50:31.005493Z I i.q.c.h.p.JsonQueryProcessorState [90] execute-new [skip: 0, stop: 1000]\r\n2022-10-03T16:50:31.076976987Z 2022-10-03T16:50:31.005677Z I i.q.c.TableReader open partition /var/lib/questdb/db/sensor_logs/2022-10-03 [rowCount=100, partitionNameTxn=-1, transientRowCount=100, partitionIndex=0, partitionCount=1]\r\n2022-10-03T16:50:31.600310034Z #\r\n2022-10-03T16:50:31.600351294Z # If you would like to submit a bug report, please visit:\r\n2022-10-03T16:50:31.600355744Z #   https://github.com/corretto/corretto-17/issues/\r\n2022-10-03T16:50:31.600357844Z #\r\n2022-10-03T16:50:31.600359404Z \r\n2022-10-03T16:50:31.600361004Z [error occurred during error reporting (), id 0xb, SIGSEGV (0xb) at pc=0x00007f0b1f2bf602]\r\n```\r\n\r\nFor some reason, this only occurs if the timestamp is randomized. I can insert any value within the randomized range even out of order and it still works fine.\r\n\r\nFor example, this works fine:\r\n```kotlin\r\nval sender: Sender ...\r\nval instant = Instant.now()\r\nfor (i in 1..100) {\r\n    sender.table(\"sensor_logs\")\r\n        .symbol(\"object_id\", \"id\")\r\n        .symbol(\"key\", \"key\")\r\n        .stringColumn(\"value\", (12 + Random.nextDouble()).toString())\r\n        .at(instant.plusSeconds(1000).toNanos())\r\n}\r\n\r\nsender.flush()\r\n\r\nprivate fun Instant.toNanos(): Long =\r\n    TimeUnit.SECONDS.toNanos(epochSecond) + nano\r\n```\n\n### To reproduce\n\n1. Spin up a Docker of Quest with docker compose:\r\n```yaml\r\ndb:\r\n  image: questdb/questdb:6.5.3\r\n  container_name: quest\r\n  ports:\r\n    - '9000:9000' # REST API\r\n    - '8812:8812' # PostgreSQL\r\n    - '9009:9009' # InfluxDB\r\n    - '9003:9003' # Prometheus Metrics\r\n  environment:\r\n    - QDB_METRICS_ENABLED=true\r\n    - QDB_PG_PASSWORD=password\r\n    - QDB_PG_USER=user\r\n  volumes:\r\n    - ./data/quest:/var/lib/questdb\r\n```\r\n\r\n2. Create a table with:\r\n```sql\r\ncreate table 'sensor_logs'\r\n(\r\n  'object_id' symbol,\r\n  'key' symbol,\r\n  'value' string,\r\n  'ts' timestamp\r\n) timestamp(ts) PARTITION by day;\r\n```\r\n\r\n3. Insert into the database from Java (more specifically Kotlin) with:\r\n```kotlin\r\nval sender: Sender ...\r\nval instant = Instant.now()\r\nfor (i in 1..100) {\r\n    sender.table(\"sensor_logs\")\r\n        .symbol(\"object_id\", \"id\")\r\n        .symbol(\"key\", \"key\")\r\n        .stringColumn(\"value\", (12 + Random.nextDouble()).toString())\r\n        .at(instant.plusSeconds(Random.nextLong(1000)).toNanos()) // It specifically has something to do with randomizing the number here\r\n}\r\n\r\nsender.flush()\r\n\r\nprivate fun Instant.toNanos(): Long =\r\n    TimeUnit.SECONDS.toNanos(epochSecond) + nano\r\n```\r\n\r\n4. Select from the table with `'sensor_logs'`\n\n### Expected Behavior\n\nInsert and select all the rows.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.5.3\r\n- **OS**: Docker on Windows 10\r\n- **Browser**: Google Chrome\n```\n\n\n### Additional context\n\n[quest.log](https://github.com/questdb/questdb/files/9699523/quest.log)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 542,
    "metadata": {
      "issue_number": 2575,
      "state": "closed",
      "labels": [
        "Question"
      ],
      "comments_count": 9,
      "created_at": "2022-10-03T17:19:27Z",
      "updated_at": "2022-10-10T14:19:04Z",
      "closed_at": "2022-10-10T14:19:04Z",
      "author": "Andavin",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-69d1d6f2ac74",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2565",
    "title": "Support CREATE TABLE LIKE syntax",
    "text": "# Support CREATE TABLE LIKE syntax\n\n### Is your feature request related to a problem?\n\nPostgres supports the following syntax\r\n```sql\r\nCREATE TABLE tab_2 (LIKE tab_1);\r\n```\r\nto avoid having to copy-paste schema from another table when exactly same schema is required. We could support this [syntax](https://www.postgresql.org/docs/current/sql-createtable.html) too, for now without `like_options`.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 72,
    "metadata": {
      "issue_number": 2565,
      "state": "closed",
      "labels": [
        "New feature",
        "Good first issue"
      ],
      "comments_count": 9,
      "created_at": "2022-09-30T06:53:22Z",
      "updated_at": "2022-10-08T17:04:36Z",
      "closed_at": "2022-10-08T17:04:36Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2c87ce01d8d6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2425",
    "title": "could not open read-only [file=H:\\BuleDb\\QuestDb\\bin\\qdbroot\\db\\BoolTest\\default\\a.d]",
    "text": "# could not open read-only [file=H:\\BuleDb\\QuestDb\\bin\\qdbroot\\db\\BoolTest\\default\\a.d]\n\ncould not open read-only  ï¼ŒRestarts have no effect and appear randomly\r\n\r\n.NET CORE     PGSQL",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 20,
    "metadata": {
      "issue_number": 2425,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 9,
      "created_at": "2022-08-13T22:42:39Z",
      "updated_at": "2022-08-20T14:55:36Z",
      "closed_at": "2022-08-20T14:55:36Z",
      "author": "DotNetNext",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-ca15439818c8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2420",
    "title": "Data cannot be recovered after the server is powered off",
    "text": "# Data cannot be recovered after the server is powered off\n\n### Describe the bug\n\nData cannot be recovered after the server is powered off\r\n\r\n![image](https://user-images.githubusercontent.com/3843463/184275536-73e72a21-c623-494e-bcd7-937f39749c1f.png)\r\n![image](https://user-images.githubusercontent.com/3843463/184275611-f608f414-61ab-4404-8a0a-04f666159350.png)\r\n\n\n### To reproduce\n\nthe server is powered off\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**:6.4.3\r\n- **OS**:windows server 2019 datacenter\r\n- **Browser**:Google Chrome\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 60,
    "metadata": {
      "issue_number": 2420,
      "state": "closed",
      "labels": [
        "Question",
        "durability issue"
      ],
      "comments_count": 9,
      "created_at": "2022-08-12T02:50:32Z",
      "updated_at": "2023-05-15T16:22:25Z",
      "closed_at": "2023-05-15T16:22:25Z",
      "author": "pinzi",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7562c9e3ad74",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2182",
    "title": "Inconsistent results between 6.3 and 6.4",
    "text": "# Inconsistent results between 6.3 and 6.4\n\n### Describe the bug\r\n\r\nFrom the description of the queries below, you will see that:\r\n\r\n`Query A1` with QuestDB 6.4 returns an incorrect number of rows\r\n`Query A1` and `A2` are perform very poorly (and never even finish with larger databases)\r\n`Query B1` results in a puzzling extra row\r\n\r\nA copy of the database can be made available upon request. \r\n\r\n```sql\r\nSELECT * FROM bus WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n`2442 rows`\r\n\r\n```sql\r\nSELECT DISTINCT * FROM bus WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n`2442 rows`\r\n\r\n```sql\r\nSELECT * FROM branch WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n`4106 rows`\r\n\r\n```sql\r\nSELECT DISTINCT * FROM bus WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n`4106 rows`\r\n\r\n### Query A1\r\n```sql\r\nSELECT DISTINCT * FROM bus WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND\r\n(busId IN (SELECT \"from\" FROM branch WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND logical) OR\r\nbusId IN (SELECT \"to\" FROM branch WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND logical))\r\n```\r\n### Query A2\r\n```sql\r\nSELECT * FROM bus WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND\r\n(busId IN (SELECT \"from\" FROM branch WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND logical) OR\r\nbusId IN (SELECT \"to\" FROM branch WHERE datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm') AND logical))\r\n```\r\n\r\n### Query B1\r\n```sql\r\nSELECT * FROM bus INNER JOIN ((SELECT datetime,\"from\" as busId FROM branch WHERE logical) UNION (SELECT datetime,\"to\" as busId FROM branch WHERE logical)) u ON bus.busId = u.busId AND bus.datetime = u.datetime WHERE bus.datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n### Query B2\r\n```sql\r\nSELECT * FROM bus INNER JOIN ((SELECT DISTINCT datetime,\"from\" as busId FROM branch WHERE logical) UNION (SELECT DISTINCT datetime,\"to\" as busId FROM branch WHERE logical)) u ON bus.busId = u.busId AND bus.datetime = u.datetime WHERE bus.datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n\r\n### Query B3\r\n```sql\r\nSELECT DISTINCT * FROM bus INNER JOIN ((SELECT datetime,\"from\" as busId FROM branch WHERE logical) UNION (SELECT datetime,\"to\" as busId FROM branch WHERE logical)) u ON bus.busId = u.busId AND bus.datetime = u.datetime WHERE bus.datetime = to_timestamp('20201031_2330','yyyyMMdd_HHmm')\r\n```\r\n### QuestDB 6.3\r\nQuery A1: 862 rows in 9.78s\r\nQuery A2: 862 rows in 10.01s\r\n\r\nQuery B1: 863 rows in 29ms\r\nQuery B2: 862 rows in 79ms\r\nQuery B3: 862 rows in 83ms\r\n\r\n### QuestDB 6.4\r\nQuery A1: 1818 rows in 14.09s\r\nQuery A2: 862 rows in 13.92s\r\n\r\nQuery B1: 863 rows in 89ms\r\nQuery B2: 862 rows in 138ms\r\nQuery B3: 862 rows in 69ms",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 388,
    "metadata": {
      "issue_number": 2182,
      "state": "open",
      "labels": [
        "Bug"
      ],
      "comments_count": 9,
      "created_at": "2022-06-01T11:52:39Z",
      "updated_at": "2022-07-07T15:24:18Z",
      "closed_at": null,
      "author": "lucgirardin",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-c7eae36f2c93",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1784",
    "title": "ILP tcp: sockets not closed under high load",
    "text": "# ILP tcp: sockets not closed under high load\n\n### Describe the bug\n\nWhen inserting a lot of data points a a relatively high rate (~10kHz) via the Influx protocol, the QuestDB blocks at some point in time with `Connection refused`. After a couple of minutes, new points can be added to the DB again.\r\nThe reason for that is that it runs out of file/socket descriptors. However the writing application properly opens and closes the sockets and also writes synchronously.\r\n\r\nThe data-points are inserted in batches of ~5000 points / batch. The configuration is as stated below.\r\n\r\nQuestDB config:\r\n\r\n```\r\nQDB_LINE_TCP_WORKER_COUNT=1\r\nQDB_LINE_TCP_MAINTENANCE_JOB_INTERVAL=1000\r\nQDB_PG_SHARED_WORKER_COUNT=2\r\nQDB_CAIRO_COMMIT_LAG=1000\r\nQDB_CAIRO_MAX_UNCOMMITTED_ROWS=10000\r\n```\r\n\r\nAttached you find a sample INFO level output of the issue.\r\nDo you have any idea to debug this further?\r\n\r\n```\r\n2022-01-13T15:25:39.209109Z I tcp-line-server scheduling disconnect [fd=21, reason=0]\r\n2022-01-13T15:25:39.209112Z I tcp-line-server disconnected [ip=127.0.0.1, fd=21, src=queue]\r\n2022-01-13T15:25:39.256019Z I tcp-line-server connected [ip=127.0.0.1, fd=21]\r\n2022-01-13T15:25:39.256077Z I i.q.c.l.t.LineTcpMeasurementScheduler idle table going active [tableName=tablea]\r\n2022-01-13T15:25:39.256080Z I i.q.c.l.t.TableUpdateDetails closing table writer [tableName=tablea]\r\n2022-01-13T15:25:39.256081Z I i.q.c.l.t.TableUpdateDetails closing table parsers [tableName=tablea]\r\n2022-01-13T15:25:39.256087Z I i.q.c.l.t.TableUpdateDetails closing table parsers [tableName=tablea]\r\n2022-01-13T15:25:39.256113Z I i.q.c.p.WriterPool >> [table=`tablea`, thread=22]\r\n2022-01-13T15:25:39.256139Z I i.q.c.l.t.LineTcpMeasurementScheduler assigned tablea to thread 0\r\n2022-01-13T15:25:39.256146Z I i.q.c.l.t.TableUpdateDetails network IO thread using table [workerId=1, tableName=tablea, nNetworkIoWorkers=1]\r\n2022-01-13T15:25:39.256209Z I tcp-line-server scheduling disconnect [fd=21, reason=0]\r\n2022-01-13T15:25:39.256211Z I tcp-line-server disconnected [ip=127.0.0.1, fd=21, src=queue]\r\n2022-01-13T15:25:39.257062Z I i.q.c.l.t.LineTcpWriterJob assigned table to writer thread [tableName=tablea, threadId=0]\r\n2022-01-13T15:25:39.309473Z I tcp-line-server connected [ip=127.0.0.1, fd=21]\r\n2022-01-13T15:25:39.309617Z I tcp-line-server connected [ip=127.0.0.1, fd=103]\r\n2022-01-13T15:25:39.310436Z I tcp-line-server scheduling disconnect [fd=103, reason=0]\r\n2022-01-13T15:25:39.310439Z I tcp-line-server disconnected [ip=127.0.0.1, fd=103, src=queue]\r\n2022-01-13T15:25:39.310482Z I tcp-line-server scheduling disconnect [fd=21, reason=0]\r\n2022-01-13T15:25:39.310483Z I tcp-line-server disconnected [ip=127.0.0.1, fd=21, src=queue]\r\n2022-01-13T15:25:39.313028Z I i.q.c.TableWriter sorting o3 [table=tableb]\r\n2022-01-13T15:25:39.313056Z I i.q.c.TableWriter sorted [table=tableb]\r\n2022-01-13T15:25:39.411316Z I tcp-line-server connected [ip=127.0.0.1, fd=105]\r\n2022-01-13T15:25:39.501122Z I i.q.c.TableWriter closing last partition [table=tableb]\r\n2022-01-13T15:25:39.502389Z I i.q.c.TableWriter merged partition [table=`tableb`, ts=2022-01-13T00:00:00.000000Z, txn=864197]\r\n2022-01-13T15:25:39.502468Z I i.q.c.TableWriter switched partition [path='/root/.questdb/db/tableb/2022-01-13.864197]\r\n2022-01-13T15:25:39.511308Z I tcp-line-server connected [ip=127.0.0.1, fd=21]\r\n2022-01-13T15:25:39.612058Z I tcp-line-server connected [ip=127.0.0.1, fd=103]\r\n2022-01-13T15:25:39.712843Z I tcp-line-server connected [ip=127.0.0.1, fd=104]\r\n2022-01-13T15:25:39.813628Z I tcp-line-server connected [ip=127.0.0.1, fd=106]\r\n2022-01-13T15:25:39.914350Z I tcp-line-server connected [ip=127.0.0.1, fd=107]\r\n2022-01-13T15:25:40.015118Z I tcp-line-server connected [ip=127.0.0.1, fd=108]\r\n2022-01-13T15:25:40.115838Z I tcp-line-server connected [ip=127.0.0.1, fd=109]\r\n2022-01-13T15:25:40.216615Z I tcp-line-server connected [ip=127.0.0.1, fd=110]\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.1.3 (container)\r\n- **OS**: Debian Bullseye\r\n- **System**: 16 cores (8*2 hyperthreading), 32GB RAM\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 389,
    "metadata": {
      "issue_number": 1784,
      "state": "closed",
      "labels": [
        "Question"
      ],
      "comments_count": 9,
      "created_at": "2022-01-13T16:35:32Z",
      "updated_at": "2022-01-27T11:21:48Z",
      "closed_at": "2022-01-20T12:12:51Z",
      "author": "fmoessbauer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-477ef454bab4",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1701",
    "title": "Ban/ignore null literals for non-nullable types",
    "text": "# Ban/ignore null literals for non-nullable types\n\n### Describe the bug\r\n\r\nATM the following query\r\n```sql\r\nselect * from t where abyte = null;\r\n```\r\nexecutes just fine and returns rows with `abyte` column equal to zero. The same happens for short columns.\r\n\r\nBoolean, byte, short and char column types don't support nulls (important note: geobyte and geoshort types support nulls), so we need to do something with this behavior. We could either report an error for queries involving byte/short null comparisons, or always evaluate such checks to `false` (even if it's something like `abyte <> null`).\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:\r\n- **OS**:\r\n- **Browser**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 123,
    "metadata": {
      "issue_number": 1701,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL",
        "Good first issue"
      ],
      "comments_count": 9,
      "created_at": "2021-12-16T13:24:08Z",
      "updated_at": "2022-10-01T12:25:04Z",
      "closed_at": "2022-10-01T12:25:04Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-1189a0834514",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1213",
    "title": "ILP does not handle duplicate fields in the message",
    "text": "# ILP does not handle duplicate fields in the message\n\n**Describe the bug**\r\nILP messes data if message has duplicate filed tags\r\n\r\n**To Reproduce**\r\nSend\r\n```\r\nweather alpha=0.1,speed=1,speed=2\r\nweather alpha=0.2\r\nweather timestamp=9831289\r\n```\r\n\r\nThis will result in messed data at line 2 having `speed` = 2 and line 3 `timestamp` in 1970\r\n\r\n**Expected behavior**\r\nReject line or take either first or last value of duplicated fileds. Reject lines where designated timestamp is specified as a field\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment (please complete the following information):**\r\n - Version: [e.g. 5.0.4]\r\n\r\n**Additional context**\r\nrelated to #1212 \r\n\r\nCan be reproduced with following test in LineTcpServerTest.java \r\n\r\n```Java\r\n    @Test\r\n    public void testDubFieldValues() throws Exception {\r\n        String lineData =\r\n                \"weather g=1,a=0.1,windspeed=1.0 0\\n\" +\r\n                \"weather g=2,windspeed=2.0,windspeed=3.0 60000000000\\n\" +\r\n                \"weather g=3,a=0.4,a=0.5 120000000000\\n\"+\r\n                \"weather g=4,a=0.7,windspeed=67.72 180000000000\\n\";\r\n\r\n        runInContext(() -> {\r\n            send(lineData, \"weather\", true, false);\r\n\r\n            String expected =\r\n                    \"g\\ta\\twindspeed\\ttimestamp\\n\" +\r\n                            \"1.0\\t0.1\\t1.0\\t1970-01-01T00:00:00.000000Z\\n\" +\r\n                            \"2.0\\tNaN\\t2.0\\t1970-01-01T00:01:00.000000Z\\n\" +\r\n                            \"3.0\\t0.4\\tNaN\\t1970-01-01T00:02:00.000000Z\\n\" +\r\n                            \"4.0\\t0.67\\t67.72\\t1970-01-01T0:03:00.000000Z\\n\";\r\n            assertTable(expected, \"weather\");\r\n        });\r\n    }\r\n\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 158,
    "metadata": {
      "issue_number": 1213,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 9,
      "created_at": "2021-07-21T10:18:31Z",
      "updated_at": "2021-12-13T16:23:22Z",
      "closed_at": "2021-12-13T16:23:22Z",
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-9005de3a7b1e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/915",
    "title": "Stop the Mail spam",
    "text": "# Stop the Mail spam\n\nAs issues in #875 as well as in #906, I also received cold spam mails pointing to this project sent from the address `vlad.ilyushchenko@questdb.tech`. Are the team members of this project aware of this spam methods? And can you please stop this? Especially because this practice is [illegal in some countries like Germany](https://www.gesetze-im-internet.de/englisch_uwg/englisch_uwg.html#p0099), where I come from.\r\n\r\nAnd by the way, really friendly and nice community management by blocking people who are actually pissed of by this crude and intruding spam.\r\n![brave_nXeGhyxFzf](https://user-images.githubusercontent.com/16734205/115232071-9ec51280-a116-11eb-8be7-3fb104f5decb.png)\r\n\r\nI think, depending on the reactions to this issue, I also will file a report to GitHub related to this issue.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 108,
    "metadata": {
      "issue_number": 915,
      "state": "closed",
      "labels": [],
      "comments_count": 9,
      "created_at": "2021-04-19T11:54:47Z",
      "updated_at": "2021-04-20T20:00:39Z",
      "closed_at": "2021-04-20T20:00:39Z",
      "author": "zekroTJA",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-695e940c7d54",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/39",
    "title": "$long index",
    "text": "# $long index\n\nHi Vlad,\n\nOnce again, thanks for your work on NFSdb - I am looking forward to the next release.\n\nQuick question: Are you planning to add support for 64bit integer index? Current version (2.1.0) supports only $int and $str. I currently use $int but I am afraid that 32bit integer range won't be enough for me in the near future. I was thinking of using $str instead. I've tested performance of $int vs $str: (using an object with int/string key + 80bytes of raw data). These are results of my testing:\n\nInt key:\n5,000,000 items added in 315ms\nLookup time is 37ms\n\nString key:\n5,000,000 items added in 616ms\nLookup time is 38ms\n\nThe lookup time is almost the same, but append time is approximately two times slower - that is why I decided not to use $str.\n\nWhat do you think?\n\nThanks,\nJaromir\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 147,
    "metadata": {
      "issue_number": 39,
      "state": "closed",
      "labels": [],
      "comments_count": 9,
      "created_at": "2016-03-01T08:55:50Z",
      "updated_at": "2016-09-05T21:19:13Z",
      "closed_at": "2016-09-05T21:19:13Z",
      "author": "jaromirs",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-bdf050008398",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6411",
    "title": "Index on a column can not be created if column name end with 'symbol'",
    "text": "# Index on a column can not be created if column name end with 'symbol'\n\n### To reproduce\n\n```\nALTER MATERIALIZED VIEW candles_market_spot_0_5m ALTER COLUMN candle_symbol ADD INDEX;\nERROR:  'capacity' keyword expected\n```\n\n```\nALTER MATERIALIZED VIEW candles_market_spot_0_5m ALTER COLUMN candle_symbol CAPACITY 8192 ADD INDEX;\nERROR:  'symbol capacity', 'add index' or 'drop index' expected\nLINE 1: ...ndles_market_spot_0_5m ALTER COLUMN candle_symbol CAPACITY 8...\n```\n\n### QuestDB version:\n\n9.2.0\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 22.04 Docker\n\n### File System, in case of Docker specify Host File System:\n\nZFS\n\n### Full Name:\n\nMaksim Tulupov\n\n### Affiliation:\n\nCoinmetrics\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 132,
    "metadata": {
      "issue_number": 6411,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2025-11-19T05:26:26Z",
      "updated_at": "2025-11-21T19:37:19Z",
      "closed_at": "2025-11-21T19:37:19Z",
      "author": "tulupov",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-a39d021f7c2c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5839",
    "title": "Malformed start boundary when using quoted boundary in multipart/form-data",
    "text": "# Malformed start boundary when using quoted boundary in multipart/form-data\n\nTo Reproduce\n.NET code:\n\ncsharp\nCopy\nEdit\n`using` var fileStream = file.OpenRead();\nusing var request = new HttpRequestMessage(HttpMethod.Post, \"http://localhost:9000/imp\");\n\nvar form = new MultipartFormDataContent\n{\n    {\n        new StreamContent(fileStream)\n        {\n            Headers = { ContentType = new System.Net.Http.Headers.MediaTypeHeaderValue(\"text/plain\") }\n        },\n        \"data\"\n    }\n};\n\nrequest.Content = form;\n\nusing var response = await httpClient.SendAsync(request, cancellationToken);\n`response.EnsureSuccessStatusCode();`\nProduces a header like:\n\nhttp\nCopy\nEdit\nContent-Type: multipart/form-data; boundary=\"abc123\"\nâŒ Current Behavior\nQuestDB responds with:\n\nsql\nCopy\nEdit\nMalformed start boundary\nâœ… Expected Behavior\nQuestDB should accept boundary values whether quoted or unquoted, as allowed by the MIME spec (RFC 2046).\n\nðŸ› ï¸ Workaround\nManually strip quotes from the boundary string in C#:\n\ncsharp\nCopy\nEdit\nvar boundary = form.Headers.ContentType.Parameters.First(p => p.Name == \"boundary\");\nboundary.Value = boundary.Value.Replace(\"\\\"\", \"\");\nðŸ“¦ Environment\nQuestDB version: 8.3.3\n\nOS: Windows 11\n\nFile System: NTFS\n\nDocker: Not applicable\n\n.NET Version: 7+\n\nâœ… Kernel Configuration\nYes, maximum open files and virtual memory limits have been configured.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 161,
    "metadata": {
      "issue_number": 5839,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2025-07-06T08:41:57Z",
      "updated_at": "2025-07-06T15:50:56Z",
      "closed_at": "2025-07-06T15:47:08Z",
      "author": "itsmeopitmus",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3bcd6fbd53e3",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5483",
    "title": "Mat views: applied base table txn is misleading",
    "text": "# Mat views: applied base table txn is misleading\n\n### To reproduce\n\nWhen a materialized view is created, or the schema of the base table changed, `applied_base_table_txn` increases, but `base_table_txn` stays the same. The behavior is the reverse of what we expect (`base_table_txn` should increase immediately, and `applied_base_table_txn` should converge to that gradually).\n```sql\nCREATE TABLE btc_trades1 ( \n\tsymbol SYMBOL CAPACITY 256 CACHE,\n\tside SYMBOL CAPACITY 256 CACHE,\n\tprice DOUBLE, \n\tamount DOUBLE,\n\ttimestamp TIMESTAMP\n) timestamp(timestamp) PARTITION BY DAY WAL\nWITH maxUncommittedRows=500000, o3MaxLag=2000000us;\n\nCREATE MATERIALIZED VIEW 'btc_mv1' REFRESH INCREMENTAL AS ( \n\n\tselect timestamp, avg(price)\n\tfrom btc_trades1\n\tsample by 1m\n\n) PARTITION BY WEEK;\n\nselect base_table_txn, applied_base_table_txn from materialized_views where view_name = 'btc_mv1'; -- base: -1, applied: 0\n\nalter table btc_trades1 add column new_column string;\n\nselect base_table_txn, applied_base_table_txn from materialized_views where view_name = 'btc_mv1'; -- base: -1, applied: 1\n```\n\n### QuestDB version:\n\nlatest master\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nMacOS\n\n### File System, in case of Docker specify Host File System:\n\nAPFS\n\n### Full Name:\n\nEmre Berk Kaya\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 208,
    "metadata": {
      "issue_number": 5483,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2025-03-18T11:49:44Z",
      "updated_at": "2025-04-15T13:48:20Z",
      "closed_at": "2025-04-15T13:48:20Z",
      "author": "emrberk",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e374057d5097",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5417",
    "title": "SQL: between does not work with SHORT column type",
    "text": "# SQL: between does not work with SHORT column type\n\n### To reproduce\n\n```SQL\nCREATE TABLE \"tu\" ( \n\ttimestamp TIMESTAMP,\n\tcreated TIMESTAMP,\n\tid LONG256,\n\tevent SHORT,\n\torigin SHORT,\n\torganization STRING\n) timestamp(timestamp);\n\n\nselect \nCASE  \n        WHEN event = -30 THEN 10\n        WHEN event = -31 THEN 50\n        WHEN event = -32 THEN 100\n        WHEN event = -33 THEN 500\n        WHEN event = -34 THEN 1000\n        WHEN event = -35 THEN 10000\n        WHEN event = -36 THEN 11000\n    END\nas sz,\n* from tu \nwhere origin = 1 and event between -30 and -40\nlimit -10;\n```\n\nResult:\n\n```\n2025-02-25T12:27:31.686503Z E i.q.g.e.QueryProgress err [id=-1, sql=`select \nCASE  \n        WHEN event = -30 THEN 10\n        WHEN event = -31 THEN 50\n        WHEN event = -32 THEN 100\n        WHEN event = -33 THEN 500\n        WHEN event = -34 THEN 1000\n        WHEN event = -35 THEN 10000\n        WHEN event = -36 THEN 11000\n    END\nas sz,\n* from tu \nwhere origin = 1 and event between -30 and -40\nlimit -10`, principal=admin, cache=false, jit=true, time=3244500, msg=argument type mismatch for function `between` at #1 expected: TIMESTAMP, actual: SHORT, errno=0, pos=303]\n```\n\n### QuestDB version:\n\n8.2.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nNA\n\n### File System, in case of Docker specify Host File System:\n\nNA\n\n### Full Name:\n\nAlex Pelagenko\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 250,
    "metadata": {
      "issue_number": 5417,
      "state": "open",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 8,
      "created_at": "2025-02-25T12:29:07Z",
      "updated_at": "2025-03-05T08:30:35Z",
      "closed_at": null,
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-61291eb430f1",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4877",
    "title": "Memory usage is too high and not released in Java (embedded) mode.",
    "text": "# Memory usage is too high and not released in Java (embedded) mode.\n\n### To reproduce\r\n\r\n1. Start the application I built with Spring Boot 3, with JVM parameters set to -Xmx2G -Xms2G.\r\n![image](https://github.com/user-attachments/assets/5853ffa1-2151-4ea8-a85f-1e57b1d152ed)\r\n\r\n2. I will create globally `CairoConfiguration`,`CairoEngine`, and `CairoEngine` during the project startup.\r\n![image](https://github.com/user-attachments/assets/09f62bcd-a675-4526-97f0-32ddcd754e5d)\r\n\r\n3. There is a table with 20 million records, and I will perform several group aggregation statistics on this table based on different conditions.\r\n\r\n4. It will be observed that the JVM garbage collection is normal, but the RES usage shown by the top command is very high and does not get released even after garbage collection.\r\n![image](https://github.com/user-attachments/assets/92d2a37f-e092-4e36-b613-be038ec0cc21)\r\n![image](https://github.com/user-attachments/assets/02f8bd23-fc79-49a9-95e1-0e12e80c683c)\r\n\r\n5. I tried using NMT to check the memory situation, but did not find any issues.\r\n[native_memory.txt](https://github.com/user-attachments/files/16657036/native_memory.txt)\r\n\r\n6. I suspect that there might be some places in QuestDB where memory is not being released. Please help me.\r\n\r\n7. Because it is deployed alongside other programs, the memory usage cannot be so high, even if it means sacrificing some performance.\r\n\r\n### QuestDB version:\r\n\r\n8.1.0\r\n\r\n### OS, in case of Docker specify Docker and the Host OS:\r\n\r\nCentOS 8.2.2004\r\n\r\n### File System, in case of Docker specify Host File System:\r\n\r\next4\r\n\r\n### Full Name:\r\n\r\nxtadg\r\n\r\n### Affiliation:\r\n\r\nSugon\r\n\r\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\r\n\r\n- [X] Yes, I have\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 232,
    "metadata": {
      "issue_number": 4877,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2024-08-19T07:26:44Z",
      "updated_at": "2025-01-10T01:13:42Z",
      "closed_at": "2025-01-10T01:13:41Z",
      "author": "xtadg",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7e55898c5da8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4504",
    "title": "Support of Decimal64 type wanted",
    "text": "# Support of Decimal64 type wanted\n\n### Is your feature request related to a problem?\n\nOur services are full of financial data that require 16-digit precision operations and compact 64-bit representation, so Decimal64 fits perfectly for that.\n\n### Describe the solution you'd like.\n\nAdd decimal64 type support, use efficient implementation for arithmetical operations and conversions to other types like here: https://github.com/epam/DFP\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nAndriy Plokhotnyuk\n\n### Affiliation:\n\nAP ITS\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 82,
    "metadata": {
      "issue_number": 4504,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 8,
      "created_at": "2024-05-14T18:16:03Z",
      "updated_at": "2026-01-16T19:09:58Z",
      "closed_at": "2026-01-16T19:09:58Z",
      "author": "plokhotnyuk",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-63fc9c973963",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4483",
    "title": "Web Console not available after 7.3.7",
    "text": "# Web Console not available after 7.3.7\n\n### To reproduce\r\n\r\n1) Go to localhost:9000 to access the Web Console.\r\n2) The Web Console does not load in Browser (same with wget)\r\n\r\n### QuestDB version:\r\n\r\n7.3.8\r\n\r\n### OS, in case of Docker specify Docker and the Host OS:\r\n\r\nUbuntu 20.04.6 (Docker)\r\n\r\n### File System, in case of Docker specify Host File System:\r\n\r\next4\r\n\r\n### Full Name:\r\n\r\nJulien\r\n\r\n### Affiliation:\r\n\r\nPrivate\r\n\r\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\r\n\r\n- [X] Yes, I have\r\n\r\n### Additional context\r\n\r\nWith the same configuration, it works for all versions below 7.3.8. \r\nLast working Version is 7.3.7\r\n\r\nWhen switching back between 7.3.7 and 7.3.8, it always works on 7.3.7, but not on 7.3.8\r\n\r\nI am running questdb in a docker environment. The available/published ports are the same, with each version.\r\nAccording to the Release-Notes no configuration regarding the Web Console changed. At least as far as I could find!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 165,
    "metadata": {
      "issue_number": 4483,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2024-05-08T06:50:06Z",
      "updated_at": "2024-05-09T08:50:30Z",
      "closed_at": "2024-05-09T08:50:30Z",
      "author": "juocal",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-7570bd2e5098",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4352",
    "title": "QuestDB doesn't release deleted file handles unless restarted",
    "text": "# QuestDB doesn't release deleted file handles unless restarted\n\n### To reproduce\n\n1. set configuration file:\r\n```\r\nconfig.validation.strict=true\r\nhttp.enabled=true\r\nhttp.net.bind.to=0.0.0.0:9000\r\nhttp.net.connection.limit=1000\r\nquery.timeout.sec=60\r\ncairo.o3.column.memory.size=64M\r\ncairo.o3.partition.split.min.size=10M\r\ncairo.o3.last.partition.max.splits=100\r\nline.tcp.enabled=true\r\nline.tcp.net.bind.to=0.0.0.0:9009\r\nline.tcp.net.connection.limit=1000\r\npg.enabled=true\r\npg.net.bind.to=0.0.0.0:8812\r\npg.net.connection.limit=1000\r\npg.worker.count=4\r\ncairo.wal.enabled.default=true\r\nwal.apply.worker.count=4\r\nwal.apply.worker.affinity=0,1,2,3\r\nmetrics.enabled=true\r\n```\r\n\r\n2. create table:\r\n``` sql\r\nCREATE TABLE 'aws_iot_data_rule' (\r\n  ts TIMESTAMP,\r\n  region SYMBOL capacity 8 CACHE,\r\n  product_id SYMBOL capacity 128 CACHE,\r\n  device_uid SYMBOL capacity 131072 CACHE index capacity 128,\r\n  dtype SYMBOL capacity 256 CACHE index capacity 524288,\r\n  dvalue STRING\r\n) timestamp (ts) PARTITION BY HOUR WAL;\r\n```\r\n\r\n3. After writing data for some time, execute shell command: `lsof -w|grep deleted|more`:\r\n![image](https://github.com/questdb/questdb/assets/7621696/8c4d4b15-e459-46f5-940b-7ad4585d8acb)\r\n\r\nAs shown in the picture above, there are many deleted but not released file handles, and the number fluctuates and increases over time until the QuestDB service is restarted.\r\n\r\nFor more details, please see: [https://questdb.slack.com/archives/C1NFJEER0/p1711683096175669](url)\n\n### QuestDB version:\n\n7.4\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nLinux  6.1.79-99.167.amzn2023.aarch64 #1 SMP Tue Mar 12 18:15:29 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux (Docker)\n\n### File System, in case of Docker specify Host File System:\n\nxfs\n\n### Full Name:\n\nPok Liu\n\n### Affiliation:\n\nHesungâ€„Innovationâ€„Corp\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 216,
    "metadata": {
      "issue_number": 4352,
      "state": "open",
      "labels": [
        "Core"
      ],
      "comments_count": 8,
      "created_at": "2024-03-29T10:27:02Z",
      "updated_at": "2025-05-13T07:29:08Z",
      "closed_at": null,
      "author": "pokliu",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-051cd9a3b5bc",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3986",
    "title": "DB stopped to write data into tables silently",
    "text": "# DB stopped to write data into tables silently\n\n### Describe the bug\n\nI've discovered that my DB stopped to write data into tables but WAL still reported activity and clients writing into DB using ILP didn't recognize any problem.\r\nRead queries (both GUI and Postgres) never ended even if there is timeout defined in configuration.\r\nRestart of the DB solved the problem\r\nAttached is chart with WAL performance and logs\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\nHard to say ... DB shouldn't stop own job silently.\r\nIf DB is not able to persist data into table, at least clients writing data MUST be notified\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.3.4\r\n- **OS**: RHEL 9.0\r\n- **Browser**:\n```\n\n\n### Additional context\n\nChange of WAL performance chart around the moment of the problem\r\n![image](https://github.com/questdb/questdb/assets/16635431/26da25bb-d080-4436-944f-f713573f691e)\r\nLonger time period to see \"the shape\"\r\n![image](https://github.com/questdb/questdb/assets/16635431/23b0b88d-858d-4f81-97dc-48605619403e)\r\nLogs\r\n[questdb-rolling.log.zip](https://github.com/questdb/questdb/files/13427764/questdb-rolling.log.zip)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 146,
    "metadata": {
      "issue_number": 3986,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2023-11-21T15:11:32Z",
      "updated_at": "2024-10-25T10:22:51Z",
      "closed_at": "2024-10-25T10:22:51Z",
      "author": "mrosi",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-65c102696218",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3943",
    "title": "Query Fails to Execute - 'WITH LT JOIN'",
    "text": "# Query Fails to Execute - 'WITH LT JOIN'\n\n### Describe the bug\n\nSince QDB 7.3.4 `WITH` queries that contain more than 41 `LT JOIN`s are returning the following error message instead of the results:\r\n`Npgsql.PostgresException (0x80004005): 00000: SQL model is too complex to evaluate`\r\n\r\nQDB Log contains nothing except for\r\n```\r\n2023-11-10T07:29:09.622345Z I pg-server connected [ip=127.0.0.1, fd=48088]\r\n2023-11-10T07:29:09.622965Z I i.q.g.SqlCompilerImpl parse ........\r\n```\r\n\n\n### To reproduce\n\nrun on demo\r\n```\r\nWITH Y AS (SELECT * FROM trades WHERE symbol='BTC-USD'),\r\nX AS (SELECT * FROM ((Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y LT JOIN (Y) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol) ON symbol))  WHERE date_trunc('day', timestamp) = date_trunc('day', timestamp111111111111111111111111111111111111111111))\r\nSELECT * FROM X\r\n```\r\n\r\nsee error message.\n\n### Expected Behavior\n\nThe query results should be returned, same as in 7.3.3\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.3.4\r\n- **OS**: windows 10\r\n- **Browser**: npgsql\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 343,
    "metadata": {
      "issue_number": 3943,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2023-11-10T07:48:50Z",
      "updated_at": "2023-11-17T11:47:43Z",
      "closed_at": "2023-11-17T11:47:43Z",
      "author": "superichmann",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-11e23dd083c9",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3904",
    "title": "QuestDB stops listening to ports after a short while",
    "text": "# QuestDB stops listening to ports after a short while\n\n### Describe the bug\n\nI have problems running QuestDB on macOS installed with Homebrew. Everything seems fine but a few seconds after restart all QuestDB listening ports will be closed. There are no message in the log / stdout and the process keeps running.\r\n\r\nThe ports will be opened for listening after start:\r\n\r\n```\r\nCOMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\r\njava    10424  gnn   98u  IPv4 0x8fed7be36c0007dd      0t0  TCP *:9000 (LISTEN)\r\njava    10424  gnn  102u  IPv4 0x8fed7be36bff719d      0t0  TCP *:8812 (LISTEN)\r\njava    10424  gnn  104u  IPv4 0x8fed7be36a250b5d      0t0  TCP *:9009 (LISTEN)\r\n```\r\n\r\nBut after a short while they will be closed magically without any message:\r\n\r\n```\r\nCOMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\r\njava    10424  gnn   98u  IPv4 0x8fed7be36c0007dd      0t0  TCP *:9000 (CLOSED)\r\njava    10424  gnn  102u  IPv4 0x8fed7be36bff719d      0t0  TCP *:8812 (CLOSED)\r\njava    10424  gnn  104u  IPv4 0x8fed7be36a250b5d      0t0  TCP *:9009 (CLOSED)\r\n```\r\n\r\nSo no new connections are possible (e.g. web console). I have also tried to deactivate macOS integrated firewall but that makes no difference.\r\n\r\nThe curios thing is that the process seems to be running normal:\r\n```\r\nbrew services info questdb\r\nquestdb (homebrew.mxcl.questdb)\r\nRunning: âœ”\r\nLoaded: âœ”\r\nSchedulable: âœ˜\r\nUser: gnn\r\nPID: 11010\r\n```\r\n\r\n```\r\nquestdb status -d /usr/local/var/questdb -f\r\n\r\n  ___                  _   ____  ____\r\n / _ \\ _   _  ___  ___| |_|  _ \\| __ )\r\n| | | | | | |/ _ \\/ __| __| | | |  _ \\\r\n| |_| | |_| |  __/\\__ \\ |_| |_| | |_) |\r\n \\__\\_\\\\__,_|\\___||___/\\__|____/|____/\r\n                        www.questdb.io\r\n\r\nPID: 11029\r\n```\r\n\r\n```\r\n  UID   PID  PPID   C STIME   TTY           TIME CMD\r\n  501 11029 11010   0 10:30am ??         0:23.50 /usr/local/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home/bin/java -DQuestDB-Runtime-66535 -ea -Dnoebug -XX:ErrorFile=/usr/local/var/questdb/db/hs_err_pid+%p.log -XX:+UnlockExperimentalVMOptions -XX:+AlwaysPreTouch -XX:+UseParallelGC -XX:-MaxFDLimit -p /usr/local/Cellar/questdb/7.3.3/libexec/questdb.jar -m io.questdb/io.questdb.ServerMain -d /usr/local/var/questdb -f -n\r\n```\n\n### To reproduce\n\n1. Install QuestDB: `brew install questdb`\r\n2. Start QuestDB: `brew services start questdb` (NOTE: Same if I start the QuestDB directly by `questdb start -d /usr/local/var/questdb -f`)\r\n3. Watch open ports: `lsof -nP -i tcp:9000,9009,8812`\n\n### Expected Behavior\n\nQuestDB remains listening on ports 9000, 9009 and 8812 until stopped.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.3.3\r\n- **OS**: macOS Ventura 13.6 (Intel Mac)\r\n- **Browser**: Firefox 119.0 (64-Bit), Safari Version 17.0\n```\n\n\n### Additional context\n\n[stdout-2023-11-01T10-30-20.txt](https://github.com/questdb/questdb/files/13225560/stdout-2023-11-01T10-30-20.txt)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 379,
    "metadata": {
      "issue_number": 3904,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2023-11-01T09:50:04Z",
      "updated_at": "2023-11-02T10:48:44Z",
      "closed_at": "2023-11-02T10:48:44Z",
      "author": "x-gnn",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-6a5fc63fab94",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3868",
    "title": "Telegraf plugin for QuestDB Influx Line Protocol Authentication",
    "text": "# Telegraf plugin for QuestDB Influx Line Protocol Authentication\n\n### Is your feature request related to a problem?\n\nI'd like to feed data from Telegraf to QuestDB over ILP. This currently works only when ILP does not have any authentication configured.\r\n\r\nI'd like to use the plugin together with ILP authentication. This likely requires a custom Telegraf plugin, it should not be too difficult to implement one. \n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 87,
    "metadata": {
      "issue_number": 3868,
      "state": "closed",
      "labels": [
        "New feature",
        "ILP",
        "hacktoberfest",
        "integration"
      ],
      "comments_count": 8,
      "created_at": "2023-10-19T08:22:27Z",
      "updated_at": "2024-06-04T08:50:30Z",
      "closed_at": "2024-06-04T08:50:29Z",
      "author": "jerrinot",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-775831d1769e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3748",
    "title": "Python module quest-connect not compatible with Windows",
    "text": "# Python module quest-connect not compatible with Windows\n\n### Describe the bug\n\nHi. \r\n\r\nI am starting development of a script to push data to questdb, using SQLAlchemy. But, I got the following failure:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\ws\\GIT\\toolanalysis\\python\\test_sqlalchemy.py\", line 8, in <module>\r\n    engine = create_engine(\"questdb://admin:quest@localhost:8812/qdb\")\r\n  File \"<string>\", line 2, in create_engine\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py\", line 375, in warned\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 518, in create_engine\r\n    entrypoint = u._get_entrypoint()\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\site-packages\\sqlalchemy\\engine\\url.py\", line 662, in _get_entrypoint\r\n    cls = registry.load(name)\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 341, in load\r\n    return impl.load()\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\importlib\\metadata.py\", line 77, in load\r\n    module = import_module(match.group('module'))\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 790, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"C:\\TCC\\Tools\\python3\\3.9.1-2_WIN64\\lib\\site-packages\\questdb_connect\\__init__.py\", line 67, in <module>\r\n    time.tzset()\r\nAttributeError: module 'time' has no attribute 'tzset' \r\n```\r\n\r\nI have googled the issue, and looks like it is because \"tzset\" is not supported on Windows. \r\n\r\nCould you please provide an update of the quest-connect module, to add support of Windows? \r\n\r\nThanks!\n\n### To reproduce\n\nUse SQLAlchemy for questdb on Windows.\r\nMy tests were done with the example provided in the official documentation.\n\n### Expected Behavior\n\nPython scripts runs without failure.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.3.1\r\n- **OS**: Windows 10\r\n- **Browser**: Google Chrome\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 276,
    "metadata": {
      "issue_number": 3748,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2023-09-15T06:48:41Z",
      "updated_at": "2023-11-14T13:54:17Z",
      "closed_at": "2023-11-14T13:54:17Z",
      "author": "Shionigami",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-9bfe87e20781",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3674",
    "title": "Optimise `select distinct col from table` so that it uses only the metadata files to compute the result",
    "text": "# Optimise `select distinct col from table` so that it uses only the metadata files to compute the result\n\nCreate a narrow/big table:\r\n\r\n```\r\nCREATE TABLE event_records AS (\r\n    SELECT\r\n    timestamp_sequence(to_timestamp('2023-05-15T22:00:00.123456Z', 'yyyy-MM-ddTHH:mm:ss.SSSUUUz'), 10000L) ts,\r\n    rnd_symbol('dev0', 'dev1', 'dev2') device_name,\r\n    rnd_double(2) value\r\n    FROM long_sequence(100000000)\r\n) TIMESTAMP(ts) PARTITION BY DAY WAL;\r\n```\r\n\r\nInsert a few more rows:\r\n\r\n```\r\nINSERT INTO event_records \r\n    SELECT\r\n    timestamp_sequence(to_timestamp('2023-10-15T22:00:00.123456Z', 'yyyy-MM-ddTHH:mm:ss.SSSUUUz'), 10000L),\r\n    rnd_symbol('dev0', 'dev1', 'dev2'),\r\n    rnd_double(100000000)\r\n    FROM long_sequence(100000000);\r\n```\r\n\r\n(you can modify the above insert's date and run it again, and again to add even more data).\r\n\r\n\r\nRun this query:\r\n\r\n```\r\nselect distinct device_name from event_records;\r\n```\r\n\r\nIt will take seconds to complete. For this query, as it is a symbol column, we can workout the number of distinct symbols from the metadata files alone, and the runtime would reduce to milliseconds.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 132,
    "metadata": {
      "issue_number": 3674,
      "state": "open",
      "labels": [
        "Enhancement"
      ],
      "comments_count": 8,
      "created_at": "2023-08-21T11:13:25Z",
      "updated_at": "2023-10-03T10:07:50Z",
      "closed_at": null,
      "author": "marregui",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-101a71b3f505",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3598",
    "title": "How to implement the removal of adjacent data with the same coordinates when querying coordinate data within a certain time range",
    "text": "# How to implement the removal of adjacent data with the same coordinates when querying coordinate data within a certain time range\n\n### Is your feature request related to a problem?\n\nHow to implement the removal of adjacent data with the same coordinates when querying coordinate data within a certain time range\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 72,
    "metadata": {
      "issue_number": 3598,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 8,
      "created_at": "2023-07-26T03:14:57Z",
      "updated_at": "2023-08-03T08:16:58Z",
      "closed_at": "2023-08-03T08:16:58Z",
      "author": "feipengcheng",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-6a30edfc1627",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3500",
    "title": "Cannot alter table and add/remove column to non-WAL to WAL converted tables",
    "text": "# Cannot alter table and add/remove column to non-WAL to WAL converted tables\n\n### Describe the bug\r\n\r\nI have table which was created as non-WAL table (before WAL was introduced). After WAL was introduced i enabled WAL for the table. Now when i try to alter the table and add new boolean column using the below statement (from QuestDB console UI):\r\n`ALTER TABLE sensor_sample ADD COLUMN presence BOOLEAN;`\r\nI am getting the following exception:\r\n\r\n> cannot read alter statement serialized, data is too short to read 10 bytes header [offset=90, size=0]\r\n\r\nThe above issue is not appearing when i create new WAL table from scratch.\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 7.1.2\r\n- **OS**: Windows 11\r\n- **Browser**: Any\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 135,
    "metadata": {
      "issue_number": 3500,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2023-06-21T08:30:55Z",
      "updated_at": "2023-06-22T15:29:37Z",
      "closed_at": "2023-06-22T15:29:37Z",
      "author": "100le",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-9ed7a2124b42",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3326",
    "title": "Too Much Input Exception When SELECTing Many AVG Columns",
    "text": "# Too Much Input Exception When SELECTing Many AVG Columns\n\n### Describe the bug\r\n\r\nWhen SELECTing 6000 columns AVG on the same SELECT there is an error.\r\nServerSide:\r\n```\r\n023-05-12T09:12:00.131416Z E i.q.s.BytecodeAssembler Too much input to generate io.questdb.griffin.engine.groupby.GroupByFunctionsUpdater. Bytecode is too long\r\n2023-05-12T09:12:00.131435Z C i.q.c.p.PGWireServer internal error [ex=\r\nio.questdb.std.ex.BytecodeException: Error in bytecode\r\n\tat io.questdb.std.ex.BytecodeException.<clinit>(BytecodeException.java:28)\r\n\tat io.questdb.std.BytecodeAssembler.endMethodCode(BytecodeAssembler.java:199)\r\n\tat io.questdb.griffin.engine.groupby.GroupByFunctionsUpdaterFactory.generateUpdateNew(GroupByFunctionsUpdaterFactory.java:220)\r\n\tat io.questdb.griffin.engine.groupby.GroupByFunctionsUpdaterFactory.getInstance(GroupByFunctionsUpdaterFactory.java:110)\r\n\tat io.questdb.griffin.engine.groupby.GroupByNotKeyedRecordCursorFactory.<init>(GroupByNotKeyedRecordCursorFactory.java:64)\r\n\tat io.questdb.griffin.SqlCodeGenerator.generateSelectGroupBy(SqlCodeGenerator.java:3336)\r\n\tat io.questdb.griffin.SqlCodeGenerator.generateSelect(SqlCodeGenerator.java:2665)\r\n\tat io.questdb.griffin.SqlCodeGenerator.generateQuery0(SqlCodeGenerator.java:2272)\r\n\tat io.questdb.griffin.SqlCodeGenerator.generateQuery(SqlCodeGenerator.java:2259)\r\n\tat io.questdb.griffin.SqlCodeGenerator.generate(SqlCodeGenerator.java:186)\r\n\tat io.questdb.griffin.SqlCompiler.generate(SqlCompiler.java:2731)\r\n\tat io.questdb.griffin.SqlCompiler.compileUsingModel(SqlCompiler.java:1322)\r\n\tat io.questdb.griffin.SqlCompiler.compileInner(SqlCompiler.java:1271)\r\n\tat io.questdb.griffin.SqlCompiler.compile(SqlCompiler.java:268)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.compileQuery(PGConnectionContext.java:1192)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.parseQueryText(PGConnectionContext.java:1699)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.processParse(PGConnectionContext.java:2358)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.parse(PGConnectionContext.java:1631)\r\n\tat io.questdb.cutlass.pgwire.PGConnectionContext.handleClientOperation(PGConnectionContext.java:418)\r\n\tat io.questdb.cutlass.pgwire.PGJobContext.handleClientOperation(PGJobContext.java:89)\r\n\tat io.questdb.cutlass.pgwire.PGWireServer$1.lambda$$0(PGWireServer.java:91)\r\n\tat io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODispatcher.java:189)\r\n\tat io.questdb.cutlass.pgwire.PGWireServer$1.run(PGWireServer.java:128)\r\n\tat io.questdb.mp.Worker.run(Worker.java:118)\r\n]\r\n```\r\n\r\nClientSide (.NET):\r\n```\r\n    Npgsql.NpgsqlException: Exception while reading from stream ---> System.IO.EndOfStreamException: Attempted to read past the end of the stream.\r\n\r\n  Stack Trace:â€‰\r\n    NpgsqlReadBuffer.<Ensure>g__EnsureLong|42_0(NpgsqlReadBuffer buffer, Int32 count, Boolean async, Boolean readingNotifications)\r\n    --- End of inner exception stack trace ---\r\n    NpgsqlReadBuffer.<Ensure>g__EnsureLong|42_0(NpgsqlReadBuffer buffer, Int32 count, Boolean async, Boolean readingNotifications)\r\n    NpgsqlConnector.<ReadMessage>g__ReadMessageLong|232_0(NpgsqlConnector connector, Boolean async, DataRowLoadingMode dataRowLoadingMode, Boolean readingNotifications, Boolean isReadingPrependedMessage)\r\n    NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)\r\n    NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)\r\n    NpgsqlDataReader.NextResult()\r\n    NpgsqlCommand.ExecuteReader(CommandBehavior behavior, Boolean async, CancellationToken cancellationToken)\r\n    NpgsqlCommand.ExecuteReader(CommandBehavior behavior, Boolean async, CancellationToken cancellationToken)\r\n    NpgsqlCommand.ExecuteReader(CommandBehavior behavior)\r\n```\r\n\r\n### To reproduce\r\n\r\nCreate a table with a lot of columns.\r\nSELECT AVG of each of them:\r\n`SELECT AVG(col1), AVG(col2), .... , AVG(colX) from tbl`\r\n\r\n### Expected Behavior\r\n\r\nthe results of the query should be returned.\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:711\r\n- **OS**:windows\r\n- **Browser**:chrome\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 233,
    "metadata": {
      "issue_number": 3326,
      "state": "closed",
      "labels": [
        "Enhancement",
        "SQL"
      ],
      "comments_count": 8,
      "created_at": "2023-05-12T09:29:23Z",
      "updated_at": "2026-01-22T21:56:26Z",
      "closed_at": "2026-01-22T21:56:26Z",
      "author": "superichmann",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2dd7b44dd1fd",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3090",
    "title": "Function to_timestamp without format does not work in v 7",
    "text": "# Function to_timestamp without format does not work in v 7\n\n### Describe the bug\r\n\r\nThis used to work in 6.5.3\r\n\r\n```sql\r\nselect to_timestamp('2023-03-03 10:36:11.228932+00:00') \r\n```\r\n\r\nbut in 7.0.1 returns `null`\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:7.0.1\r\n```\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 38,
    "metadata": {
      "issue_number": 3090,
      "state": "open",
      "labels": [
        "SQL",
        "Compatibility"
      ],
      "comments_count": 8,
      "created_at": "2023-03-23T09:26:05Z",
      "updated_at": "2024-02-01T10:55:07Z",
      "closed_at": null,
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-c03c973b7cee",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3088",
    "title": "Timestamp Generator with Intervals",
    "text": "# Timestamp Generator with Intervals\n\n### Is your feature request related to a problem?\r\n\r\nCurrently, the only way to generate a timestamp series is by using a base timestamp and a `long` type for a step. Current way of generating timestamps:\r\n\r\n`SELECT x, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), 100000L)`\r\n\r\nThis is a problem when I need to generate a weekly or monthly series. \r\n\r\n### Describe the solution you'd like.\r\n\r\nThe solution would be similar to this:\r\n\r\n`GENERATE_DATE_ARRAY(DATE('2020-01-01'), CURRENT_DATE(), INTERVAL 1 DAY)`\r\n\r\nideally would look something more QuestDB-y.\r\n\r\n`timestamp_sequence(ts, 1M)`\r\n\r\n### Describe alternatives you've considered.\r\n\r\nThere is a hacky way to get around this with selecting a min, max union then sample by INTERVAL with FILL. \r\n\r\n### Additional context.\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 117,
    "metadata": {
      "issue_number": 3088,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 8,
      "created_at": "2023-03-22T15:36:40Z",
      "updated_at": "2024-08-21T11:58:31Z",
      "closed_at": null,
      "author": "EmmettM",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-a074a52fefb8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2460",
    "title": "Cannot drop partitions while inserting data using UDP sender",
    "text": "# Cannot drop partitions while inserting data using UDP sender\n\n### Describe the bug\n\nI have a service that is inserting data into quest using UDP ILP sender. As part of our data retention service we would like to drop partitions a few days old. However, when I run a simple drop partition command like the following using curl or through quest's UI: \r\n\r\nALTER TABLE measurements DROP PARTITION LIST '2019-05-18';\r\n\r\nI get a response saying:\r\n```\r\nJsonQueryProcessorState [179] waiting for update query\r\nJsonQueryProcessor [fd=179] Resource busy, will retry\r\nJsonQueryProcessorState [179] syntax-error [q=`ALTER TABLE '{table_name}' DROP PARTITION LIST '2022-08-15'`, at=0, message=`Query timeout. Please add HTTP header 'Statement-Timeout' with timeout in msQuery timeout. Please add HTTP header 'Statement-Timeout' with timeout in msTimeout expired on waiting for the async command execution result [instance=8]Timeout expired on waiting for the async command execution result [instance=9]Query timeout. Please add HTTP header 'Statement-Timeout' with timeout in msQuery timeout. Please add HTTP header 'Statement-Timeout' with timeout in msQuery timeout. Please add HTTP header 'Statement-Timeout' with timeout in msQuery timeout. Please add HTTP header 'Statement-Timeout' with timeout in ms`]\r\n```\r\n\r\nIncreasing the timeout just makes the request run longer with the same 'Resource busy, will retry' error as above:\r\n`cairo.writer.alter.busy.wait.timeout.micro=50000000` \r\n\r\n\n\n### To reproduce\n\n1. Create quest instance that accepts udp messages on port 9009\r\n2. Inserting metrics into the quest instance using the udp line protocol sender\r\n3. Try to run drop partition commands while the insertion service runs in the background\n\n### Expected Behavior\n\nDrop partition command runs as excepted and does not constantly throw a resource busy error\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.4.2\r\n- **OS**: Debian GNU/Linux 11\r\n- **Browser**:Chrome 104.0.5112.79 (Official Build) (arm64)\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 285,
    "metadata": {
      "issue_number": 2460,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2022-08-25T10:16:55Z",
      "updated_at": "2022-09-09T12:53:55Z",
      "closed_at": "2022-09-09T12:53:55Z",
      "author": "sherwinmascarenhas",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-d1f597651c87",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2267",
    "title": "An undeletable partition appears in questdb",
    "text": "# An undeletable partition appears in questdb\n\n### Describe the bug\r\n\r\nRun: select * from aws_iot_device_shadow_update order by ts desc limit 10;\r\n\r\n[19:47:20] Partition '54448-04-21' does not exist in table 'aws_iot_device_shadow_update' directory. Run [ALTER TABLE aws_iot_device_shadow_update DROP PARTITION LIST '54448-04-21'] to repair the table or restore the partition directory.\r\n\r\nRun: ALTER TABLE aws_iot_device_shadow_update DROP PARTITION LIST '54448-04-21';\r\n\r\n[19:50:19] 'YYYY-MM-DD' expected[errno=0]\r\n\r\n### To reproduce\r\n\r\nI'm not sure, but I recreated the table structure, migrated the data by creating a new table name, executing insert into select, then removing the old table and renaming the new table\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 6.2.1\r\n- **OS**: Linux 10-10-20-115 4.18.0-348.2.1.el8_5.x86_64 #1 SMP Tue Nov 16 14:42:35 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n- **Browser**:  Chrome\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 133,
    "metadata": {
      "issue_number": 2267,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2022-06-24T11:56:25Z",
      "updated_at": "2022-09-09T02:08:39Z",
      "closed_at": "2022-09-09T02:08:39Z",
      "author": "pokliu",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-f000091e8add",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1525",
    "title": "QuestDB demo does not work at all.",
    "text": "# QuestDB demo does not work at all.\n\n### Describe the bug\n\nI'm trying to run Q4 from https://tech.marksblogg.com/benchmarks.html\r\n\r\n```\r\nSELECT passenger_count, year(pickup_datetime) AS year, round(trip_distance) AS distance, count(*) AS c\r\nFROM trips\r\nGROUP BY passenger_count, year, distance\r\nORDER BY year, c DESC\r\n```\r\n\r\nIt is running for about ten seconds and then gives\r\n```\r\nQuestDB is not reachable [503]\r\n```\n\n### To reproduce\n\nSee above.\n\n### Expected Behavior\n\nThe claim\r\n\r\n> Query our demo dataset with 1.6 billion rows in milliseconds\r\n\r\nfrom the QuestDB front page should be more real.\n\n### Environment\n\n```markdown\n- **QuestDB 6.0.9**:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 103,
    "metadata": {
      "issue_number": 1525,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 8,
      "created_at": "2021-11-07T01:50:48Z",
      "updated_at": "2024-01-05T18:56:58Z",
      "closed_at": "2024-01-05T18:56:58Z",
      "author": "alexey-milovidov",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-e4613406f728",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1517",
    "title": "Can't connect to QuestDB using Python and psycopg2 on Windows 10",
    "text": "# Can't connect to QuestDB using Python and psycopg2 on Windows 10\n\n### Describe the bug\n\nTrying to connect to an up and running QuestDB local instance on Windows 10 using the Python code found in the [official documentation](https://questdb.io/docs/develop/connect#postgres-compatibility) results in:\r\n\r\n```\r\nError while connecting to QuestDB using PostgreSQL protocol server closed the connection unexpectedly\r\n\tThis probably means the server terminated abnormally\r\n\tbefore or while processing the request.\r\n```\r\n\r\nI tried increasing the timeout (`http.keep-alive.timeout`) in _server.conf_ to no avail.\n\n### To reproduce\n\n1. Download _questdb-6.0.9-rt-windows-amd64.tar_ from the [official download page](https://questdb.io/get-questdb/)\r\n2. Extract .tar file downloaded in step 1. using elevated (\"Run as Administrator\") command prompt via `tar -xvzf C:\\Users\\<username>\\Downloads\\questdb-6.0.9-rt-windows-amd64.tar -C C:\\Windows\\System32\\questdb`. A prerequisite is to create a folder named _questdb_ under _C:\\Windows\\System32_. I tried other locations too, and got the same result.\r\n3. Navigate to _C:\\Windows\\System32\\questdb\\bin_ in an elevated command prompt and run `questdb.exe install`, then `questdb.exe start`.\r\n4. Verify that QuestDB is running by typing `questdb.exe status` in command prompt, which results in in \"Service QuestDB is RUNNING\". The running service can also be seen in services app and the web console at http://localhost:9000/ is working as expected.\r\n5. Using `netstat -ab` in command prompt check that TCP communication for JAVA.EXE at 127.0.0.1:8812 is ESTABLISHED.\r\n6. Run the Python code which tries to connect to QuestDB via its Postgres endpoint as found in the [official documentation](https://questdb.io/docs/develop/connect#postgres-compatibility).\n\n### Expected Behavior\n\nPython connection using psycopg2 to QuestDB via its Postgres protocol is successful, with details of its DSN parameters printed, and the server version retrieved.\n\n### Environment\n\n```markdown\n- **QuestDB version**: questdb-6.0.9-rt-windows-amd64\r\n- **OS**: Windows 10 Home Version 21H1, OS build 19043.1288\r\n- **Browser**: Microsoft Edge Version 95.0.1020.40 (Official build) (64-bit)\r\n- **Python**: 3.8.5\r\n- **psycopg2**: 2.8.6\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 293,
    "metadata": {
      "issue_number": 1517,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2021-11-04T08:22:07Z",
      "updated_at": "2022-03-13T13:20:45Z",
      "closed_at": "2022-03-13T13:20:45Z",
      "author": "bugfoot",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-359beaa95e29",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1382",
    "title": "questdb-ilp-rust - Rust port of LineProtoSender",
    "text": "# questdb-ilp-rust - Rust port of LineProtoSender\n\nThe goal is to create thin, easy to use Rust client for sending ILP messages. We already have a Java client:\r\n\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/AbstractLineSender.java\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/LineTcpSender.java\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/LineUdpSender.java\r\n\r\nPerhaps port this one maintaining the same API",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 42,
    "metadata": {
      "issue_number": 1382,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "Good first issue"
      ],
      "comments_count": 8,
      "created_at": "2021-10-04T13:05:24Z",
      "updated_at": "2022-09-29T11:55:38Z",
      "closed_at": "2022-09-29T11:55:38Z",
      "author": "bluestreak01",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-f40c35dc3514",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1381",
    "title": "questdb-ilp-python - Python port of LineProtoSender",
    "text": "# questdb-ilp-python - Python port of LineProtoSender\n\nThe goal is to create thin, easy to use Python client for sending ILP messages. We already have a Java client:\r\n\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/AbstractLineSender.java\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/LineTcpSender.java\r\n* https://github.com/questdb/questdb/blob/master/core/src/main/java/io/questdb/cutlass/line/LineUdpSender.java\r\n\r\nPerhaps port this one maintaining the same API",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 42,
    "metadata": {
      "issue_number": 1381,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "Good first issue"
      ],
      "comments_count": 8,
      "created_at": "2021-10-04T13:04:14Z",
      "updated_at": "2022-09-13T10:50:53Z",
      "closed_at": "2022-09-13T10:50:52Z",
      "author": "bluestreak01",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-1d498d88ba38",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1257",
    "title": "Support timestamp fieldsets in ILP",
    "text": "# Support timestamp fieldsets in ILP\n\n**Is your feature request related to a problem? Please describe.**\r\nI have a time series forecasting usecase where I need to store two timestamp columns in a table (one is designated). Unfortunately ILP does not seem to support timestamp columns (I tried several formats and just get conversion errors), so I am forced to resort to the Postgres client for inserting new data which works fine, but based on everything I've seen is not as fast for QuestDB.\r\n\r\n**Describe the solution you'd like**\r\nIt would be great if the ILP could be extended to include timestamp data types for fieldsets.\r\n\r\n**Describe alternatives you've considered**\r\nFor now I am using a Postgres client for all writes and it works acceptably, but has noticeable lag when I'm trying to write anything of decent volume.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 138,
    "metadata": {
      "issue_number": 1257,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 8,
      "created_at": "2021-08-14T10:32:59Z",
      "updated_at": "2021-10-06T09:42:47Z",
      "closed_at": "2021-10-06T09:42:47Z",
      "author": "devenjarvis",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-09fcf4188404",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1227",
    "title": "Table names are case sensitive on Linux and insensitive on Windows, Mac",
    "text": "# Table names are case sensitive on Linux and insensitive on Windows, Mac\n\n**To Reproduce**\r\n```SQL\r\ncreate table geo as (select 1 from long_sequence(1))\r\n\r\n-- maybe restart needed\r\n\r\nselect * from GEO\r\n```\r\n\r\n**Expected behavior**\r\nSelect should work on all platforms. Doesn't work on Linux atm\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS:  Linux, MacOS, Windows\r\n - Version: 6.0.4\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 60,
    "metadata": {
      "issue_number": 1227,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2021-08-02T17:26:45Z",
      "updated_at": "2023-05-22T10:07:49Z",
      "closed_at": "2023-05-22T10:00:52Z",
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b28436379a02",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1211",
    "title": "Embedded: Very high memory usage on startup",
    "text": "# Embedded: Very high memory usage on startup\n\n**Describe the bug**\r\nI'm not sure there is a bug here, however, when simply initializing QuestDB as an embedded database, a whopping 220MB of extra heap space is used (measured AFTER full GC). No DB operations are performed other than startup and no database is present (empty). I understand that to be high performance might be a trade off with higher memory usage, but it seems like memory should only be 'paid for' if used? Or is this some sort of startup cache that can be tuned?\r\n\r\n**To Reproduce**\r\n1. Startup any Java (or in my case, Kotlin) application - can be \"hello world\" as long as it doesn't terminate\r\n2. Open `VisualVM` - perform a full GC and note down heap usage\r\n3. Close program, and modify with these lines in one of the classes (ensuring the references are not released of course)\r\n\r\n```Java\r\n    private var config = new DefaultCairoConfiguration(\"./\")\r\n    private var engine = new CairoEngine(config)\r\n```\r\n\r\n4. Open `VisualVM` - perform a full GC and note down heap usage\r\n\r\nThe difference between heap usage in steps 4 and step 1 is 220MB (at least in my application, 30MB before, 250MB after)\r\n\r\n**Expected behavior**\r\nI expected heap usage to be minimal until I performed data transactions at which point I'd hope it would be tunable ideally based on my desire for in app caching\r\n\r\n**Screenshots**\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux (amd64), OpenJDK 11 64-bit\r\n - Browser: not relevant\r\n - Version: PopOS 20.04\r\n\r\n**Additional context**\r\nUsing QuestDB 6.0.4\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 260,
    "metadata": {
      "issue_number": 1211,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2021-07-20T22:36:10Z",
      "updated_at": "2021-09-24T08:11:15Z",
      "closed_at": "2021-09-24T08:11:15Z",
      "author": "nu11ptr",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e5e85eb4c950",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1073",
    "title": "Unable to start QuestDB service on Windows 10 (no JVM version)",
    "text": "# Unable to start QuestDB service on Windows 10 (no JVM version)\n\nI have managed to successfully install QuestDB 6.0.2 (no JVM version) from the binaries on my local machine, which is running Windows 10. However, I am unable to start the service (as it can be inferred from the screenshot below), even though I am running the command line as an administrator:\r\n![Immagine 2021-05-31 123401](https://user-images.githubusercontent.com/60734967/120180848-929b9d00-c20c-11eb-9746-34e56b0b4919.png)\r\nThe contents of the destination folder `\"C:\\QuestDB\\\"` are the following:\r\n![Immagine 2021-05-31 123846](https://user-images.githubusercontent.com/60734967/120181448-5e74ac00-c20d-11eb-89f7-b512b2d373ae.png)\r\n\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 79,
    "metadata": {
      "issue_number": 1073,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2021-05-31T10:43:02Z",
      "updated_at": "2021-06-03T12:08:06Z",
      "closed_at": "2021-06-03T12:08:06Z",
      "author": "riccardogiro",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-887fe660c923",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/532",
    "title": "Add Prometheus metrics endpoint",
    "text": "# Add Prometheus metrics endpoint\n\n**Is your feature request related to a problem? Please describe.**\r\n\r\nIt would be useful to know if QuestDB is misbehaving and how it is performing.\r\n\r\n**Describe the solution you'd like**\r\n\r\nA common way to do this is with Prometheus metrics, providing a /metrics endpoint with various counters would be helpful (HTTP requests, SQL execution count, exceptions, errors), potentially histograms of execution times as well as some gauges (e.g. current number of running queries, open FDs, open TCP connections).\r\n\r\n**Describe alternatives you've considered**\r\n\r\nProviding a SQL command to get stats and then writing an exporter would be an option (maybe useful to have a stats command anyway), however built-in support would be good (and very common for cloud native tools).",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 124,
    "metadata": {
      "issue_number": 532,
      "state": "closed",
      "labels": [
        "New feature",
        "Compatibility"
      ],
      "comments_count": 8,
      "created_at": "2020-08-05T19:51:28Z",
      "updated_at": "2022-06-17T08:32:22Z",
      "closed_at": "2022-06-17T08:32:22Z",
      "author": "dgl",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-c0b8b7700e11",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/457",
    "title": "IODispatcherTest failed tests and Error:Hash of jdk.unsupported ",
    "text": "# IODispatcherTest failed tests and Error:Hash of jdk.unsupported \n\n**Describe the bug(s):**\r\n\r\nAfter running mvn clean and mvn package i get the error (using release 5.0.1):\r\n\r\n```\r\nError: Hash of jdk.unsupported (1fd4ffb477c0fb59022daeab3a0bf32a4681e8f8d540396f3641503915fd54ca) differs to expected hash (2f3dae909f60914354efbc2acbff4a6c976ac1fb47851afa737262816b828381) recorded in java.base\r\n[\u001b[1;31mERROR\u001b[m] Command execution failed.\r\n```\r\n\r\nOn another machine in docker, we get the following error:\r\n\r\n```\r\nFailed tests: \r\n  IODispatcherTest.testJsonQueryBadUtf8:1899->testJsonQuery:4986->testJsonQuery:4966->testJsonQuery0:4993->lambda$testJsonQuery0$38:5079->lambda$testJsonQuery$37:4974->sendAndReceive:4862->sendAndReceive:4895->sendAndReceive:4946 Error at: 208, local=10\r\n  IODispatcherTest.testJsonQueryLimitColumnsUtf8:2398->testJsonQuery:4986->testJsonQuery:4966->testJsonQuery0:4993->lambda$testJsonQuery0$38:5079->lambda$testJsonQuery$37:4974->sendAndReceive:4862->sendAndReceive:4895->sendAndReceive:4946 Error at: 213, local=24\r\n  IODispatcherTest.testJsonUtf8EncodedColumnName:3250->testJsonQuery:4966->testJsonQuery0:4993->lambda$testJsonQuery0$38:5079->lambda$testJsonQuery$37:4974->sendAndReceive:4862->sendAndReceive:4895->sendAndReceive:4946 Error at: 208, local=19\r\n  IODispatcherTest.testJsonUtf8EncodedQuery:3279->testJsonQuery:4986->testJsonQuery:4966->testJsonQuery0:4993->lambda$testJsonQuery0$38:5079->lambda$testJsonQuery$37:4974->sendAndReceive:4862->sendAndReceive:4895->sendAndReceive:4946 Error at: 220, local=31\r\n\r\nTests run: 4192, Failures: 4, Errors: 0, Skipped: 6\r\n```\r\n\r\n**complete log file for first error:**\r\nhttps://gist.github.com/CatspersCoffee/4bc593bbc9326ccb70074b73184f994c\r\n\r\n**complete log file for second set of test errors:**\r\nhttps://gist.github.com/mortenpi/65741ee47038b0b698d6df8159449118\r\n\r\n**To Reproduce**\r\ninstalled and cloned as per the instructions: https://github.com/questdb/questdb\r\nTested using master branch and also release 5.0.1 both produce the same errors respectively. \r\n\r\n\r\n\r\n**Environment**\r\nplease see links above, complete environment details provided.\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 137,
    "metadata": {
      "issue_number": 457,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 8,
      "created_at": "2020-06-29T04:31:11Z",
      "updated_at": "2020-07-31T07:59:05Z",
      "closed_at": "2020-07-31T07:59:05Z",
      "author": "CatspersCoffee",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-88c5ddd7023f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/61",
    "title": "`questdb.sh start` fails under Java 9",
    "text": "# `questdb.sh start` fails under Java 9\n\nI'm on macOS 10.13.3, with Java 9.0.4 installed.\r\n\r\nStarting up Questdb 1.0.4 fails.\r\n\r\nLooks like it's due to `questdb.sh` using a `-XX:+PrintGCApplicationStoppedTime` option that's no longer supported under Oracle's Java 9 JRE.\r\n\r\n```\r\n$ wget https://www.questdb.org/download/questdb-1.0.4-bin.tar.gz\r\n--2018-02-12 07:03:03--  https://www.questdb.org/download/questdb-1.0.4-bin.tar.gz\r\nResolving www.questdb.org (www.questdb.org)... 104.28.24.7, 104.28.25.7, 2400:cb00:2048:1::681c:1907, ...\r\nConnecting to www.questdb.org (www.questdb.org)|104.28.24.7|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 2659295 (2.5M) [application/gzip]\r\nSaving to: â€˜questdb-1.0.4-bin.tar.gzâ€™\r\n\r\nquestdb-1.0.4-bin.tar.gz      100%[=================================================>]   2.54M  2.43MB/s    in 1.0s\r\n\r\n2018-02-12 07:03:05 (2.43 MB/s) - â€˜questdb-1.0.4-bin.tar.gzâ€™ saved [2659295/2659295]\r\n\r\n[~/tmp]\r\n$ tar xf questdb-1.0.4-bin.tar.gz\r\n[~/tmp]\r\n$ cd questdb-1.0.4\r\n[ ~/tmp/questdb-1.0.4]\r\n$ ./questdb.sh start\r\n\r\n  ___                  _   ____  ____\r\n / _ \\ _   _  ___  ___| |_|  _ \\| __ )\r\n| | | | | | |/ _ \\/ __| __| | | |  _ \\\r\n| |_| | |_| |  __/\\__ \\ |_| |_| | |_) |\r\n \\__\\_\\\\__,_|\\___||___/\\__|____/|____/\r\n                       www.questdb.org\r\n\r\nCreated QuestDB ROOT directory: /usr/local/var/questdb\r\nUnrecognized VM option 'PrintGCApplicationStoppedTime'\r\nError: Could not create the Java Virtual Machine.\r\nError: A fatal exception has occurred. Program will exit.\r\n[~/tmp/questdb-1.0.4]\r\n$ java -version\r\njava version \"9.0.4\"\r\nJava(TM) SE Runtime Environment (build 9.0.4+11)\r\nJava HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)\r\n```\r\n\r\nWhen I tried editing the startup script to get it working, these options were also unrecognized:\r\n* `-XX:+PrintGCTimeStamps`\r\n* `-XX:+PrintGCDateStamps`\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 215,
    "metadata": {
      "issue_number": 61,
      "state": "closed",
      "labels": [],
      "comments_count": 8,
      "created_at": "2018-02-12T12:07:41Z",
      "updated_at": "2018-04-17T20:47:20Z",
      "closed_at": "2018-04-17T20:47:20Z",
      "author": "apjanke",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-bde86cdae8e1",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6618",
    "title": "Incorrect result when using WHERE filter on TIMESTAMP with GROUP BY",
    "text": "# Incorrect result when using WHERE filter on TIMESTAMP with GROUP BY\n\n### To reproduce\n\nWhen applying a WHERE condition on a TIMESTAMP column together with GROUP BY, QuestDB sometimes returns an incorrect or empty result even though matching rows exist.\n\n### QuestDB version:\n\n7.3.x\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 20.04\n\n### File System, in case of Docker specify Host File System:\n\next8\n\n### Full Name:\n\nKrish \n\n### Affiliation:\n\nNone\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 108,
    "metadata": {
      "issue_number": 6618,
      "state": "open",
      "labels": [],
      "comments_count": 7,
      "created_at": "2026-01-09T04:39:36Z",
      "updated_at": "2026-01-16T09:25:45Z",
      "closed_at": null,
      "author": "krishbansal9966-ux",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-62afb3c112ce",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6536",
    "title": "NPE with invalid SQL",
    "text": "# NPE with invalid SQL\n\n### To reproduce\n\nfound via fuzzing:\n```sql\nSELECT ts, col6 AS a1, CASE WHEN col5 <> '192.168.150.217' THEN 754326 WHEN col1 = false THEN '' WHEN ts <= ''''qocvwy'''' THEN col4 ELSE sym END, col4, -109639495026, '1986-05-10T21:39:17Z', '2026-07-22T10:32:41.213795Z',) col2 AS a2, 'p', 0.0 AS a3 FROM t1 a4 LT JOIN t2 a5 ON a4.timestamp = a5.timestamp LEFT OUTER JOIN t3 ON a4.col3 = t3.col3 LEFT col7 <= false OR a4.sym <> col2 GROUP BY a5.col4, col4, a5.col7, col10 ORDER BY col7 ASC NULLS FIRST, a5.col10 DESC, a5.col3 DESC NULLS FIRST, 3 DESC LIMIT 194\n```\n\n```\n2025-12-14T18:36:53.377325Z E i.q.t.f.s.SqlParserFuzzTest SQL: SELECT ts, col6 AS a1, CASE WHEN col5 <> '192.168.150.217' THEN 754326 WHEN col1 = false THEN '' WHEN ts <= ''''qocvwy'''' THEN col4 ELSE sym END, col4, -109639495026, '1986-05-10T21:39:17Z', '2026-07-22T10:32:41.213795Z',) col2 AS a2, 'p', 0.0 AS a3 FROM t1 a4 LT JOIN t2 a5 ON a4.timestamp = a5.timestamp LEFT OUTER JOIN t3 ON a4.col3 = t3.col3 LEFT col7 <= false OR a4.sym <> col2 GROUP BY a5.col4, col4, a5.col7, col10 ORDER BY col7 ASC NULLS FIRST, a5.col10 DESC, a5.col3 DESC NULLS FIRST, 3 DESC LIMIT 194\n2025-12-14T18:36:53.377397Z E i.q.t.f.s.SqlParserFuzzTest \njava.lang.NullPointerException: Cannot invoke \"java.lang.CharSequence.length()\" because \"seq\" is null\n\tat io.questdb.std.Chars.indexOfLastUnquoted(Chars.java:847)\n\tat io.questdb.griffin.SqlParser.createColumnAlias(SqlParser.java:468)\n\tat io.questdb.griffin.SqlParser.generateColumnAlias(SqlParser.java:681)\n\tat io.questdb.griffin.SqlParser.parseSelectClause(SqlParser.java:3410)\n\tat io.questdb.griffin.SqlParser.parseDml0(SqlParser.java:1990)\n\tat io.questdb.griffin.SqlParser.parseDml(SqlParser.java:1880)\n\tat io.questdb.griffin.SqlParser.parseSelect(SqlParser.java:2972)\n\tat io.questdb.griffin.SqlParser.parse(SqlParser.java:4329)\n\tat io.questdb.griffin.SqlCompilerImpl.compileExecutionModel(SqlCompilerImpl.java:2564)\n\tat io.questdb.griffin.SqlCompilerImpl.testCompileModel(SqlCompilerImpl.java:569)\n\tat io.questdb.cairo.pool.SqlCompilerPool$C.testCompileModel(SqlCompilerPool.java:221)\n\tat io.questdb.test.fuzz.sql.SqlParserFuzzTest.testParse(SqlParserFuzzTest.java:302)\n\tat io.questdb.test.fuzz.sql.SqlParserFuzzTest.lambda$runFuzzTest$0(SqlParserFuzzTest.java:239)\n```\n\n\nanother NPE - 2nd root cause\n```sql\nSELECT  \n  CASE \n    WHEN col10 > '3975' THEN col7 \n    WHEN col5 < '4045' FILL -3754167527262084558 \n  END\nFROM t1 \nORDER BY col10,, col8, col6;\n```\n\n```\njava.lang.NullPointerException: Cannot invoke \"java.lang.CharSequence.isEmpty()\" because \"n.token\" is null\n\tat io.questdb.griffin.SqlParser.parseFromClause(SqlParser.java:2489)\n\tat io.questdb.griffin.SqlParser.parseDml0(SqlParser.java:2099)\n\tat io.questdb.griffin.SqlParser.parseDml(SqlParser.java:1880)\n\tat io.questdb.griffin.SqlParser.parseAsSubQuery(SqlParser.java:4373)\n\tat io.questdb.griffin.ExpressionParser.processLambdaQuery(ExpressionParser.java:244)\n\tat io.questdb.griffin.ExpressionParser.parseExpr(ExpressionParser.java:1025)\n\tat io.questdb.griffin.SqlParser.expr(SqlParser.java:4292)\n\tat io.questdb.griffin.SqlParser.expr(SqlParser.java:4307)\n\tat io.questdb.griffin.SqlParser.parseSelectFrom(SqlParser.java:3427)\n\tat io.questdb.griffin.SqlParser.parseFromClause(SqlParser.java:2320)\n\tat io.questdb.griffin.SqlParser.parseDml0(SqlParser.java:2099)\n\tat io.questdb.griffin.SqlParser.parseDml(SqlParser.java:1880)\n\tat io.questdb.griffin.SqlParser.parseSelect(SqlParser.java:2972)\n\tat io.questdb.griffin.SqlParser.parse(SqlParser.java:4360)\n\tat io.questdb.griffin.SqlCompilerImpl.compileExecutionModel(SqlCompilerImpl.java:2564)\n\tat io.questdb.griffin.SqlCompilerImpl.testCompileModel(SqlCompilerImpl.java:569)\n\tat io.questdb.cairo.pool.SqlCompilerPool$C.testCompileModel(SqlCompilerPool.java:221)\n\tat io.questdb.test.fuzz.sql.SqlParserFuzzTest.testParse(SqlParserFuzzTest.java:302)\n[...]\n```\n### QuestDB version:\n\nc2343aad\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nNA\n\n### File System, in case of Docker specify Host File System:\n\nNA\n\n### Full Name:\n\nJaromir Hamala\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 374,
    "metadata": {
      "issue_number": 6536,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 7,
      "created_at": "2025-12-14T18:38:20Z",
      "updated_at": "2026-01-26T11:51:04Z",
      "closed_at": "2026-01-26T11:51:04Z",
      "author": "jerrinot",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-630b96c3c393",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6202",
    "title": "Allow SAMPLE BY to use a long, rather than an INT for the units",
    "text": "# Allow SAMPLE BY to use a long, rather than an INT for the units\n\n### Is your feature request related to a problem?\n\nAt the moment the SAMPLE BY statement allows for an INT unit, which can limit the interval users want to use, specially if queries are programmatic rather than interactive. This can be a problem with nanoseconds support, as in nanoseconds you get to the limit of the INT in just about 2 seconds.\n\nIf I do `SAMPLE BY 3000000000n`, I am getting `the number is too large (invalid tolerance value [value=3000000000n, maximum=2147483647n])`\n\nUsers haven't complained about this just yet, but one user already complained about a similar issue with dateadd, (`there is no matching function `dateadd` with the argument types: (CHAR, LONG, TIMESTAMP)`) so I thought I'd add this here\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\nchange to a different unit. Which is workable when doing interactive queries, not so much for programmatic\n\n### Full Name:\n\njavier\n\n### Affiliation:\n\nquestdb\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 176,
    "metadata": {
      "issue_number": 6202,
      "state": "open",
      "labels": [
        "New feature",
        "SQL",
        "Good first issue",
        "internal"
      ],
      "comments_count": 7,
      "created_at": "2025-09-29T14:07:18Z",
      "updated_at": "2025-11-14T22:03:08Z",
      "closed_at": null,
      "author": "javier",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-5ed5b90aaea3",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6094",
    "title": "Add index creation during mat view creation",
    "text": "# Add index creation during mat view creation\n\n### Is your feature request related to a problem?\n\nWe [already support](https://github.com/questdb/questdb/pull/5961) indexing symbols on matviews, but only after view is created. It would be nice to have them available also during matview creation. Note when we add this, the show create statement will need to be updated as well\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\njavier\n\n### Affiliation:\n\nquestdb\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 85,
    "metadata": {
      "issue_number": 6094,
      "state": "open",
      "labels": [
        "Enhancement",
        "SQL",
        "hacktoberfest",
        "internal",
        "Materialized View"
      ],
      "comments_count": 7,
      "created_at": "2025-09-01T15:06:31Z",
      "updated_at": "2025-10-02T07:08:31Z",
      "closed_at": null,
      "author": "javier",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-46dba466c57c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5815",
    "title": "Flaky LineHttpSenderMockServerTest.testTimeoutConfString",
    "text": "# Flaky LineHttpSenderMockServerTest.testTimeoutConfString\n\n### To reproduce\n\nwindows-other:\n```\n<<<<= io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testTimeoutConfString duration_ms=125\n2025-07-04T17:10:57.767427Z E i.q.t.TestListener ***** Test Failed ***** io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testTimeoutConfString duration_ms=125 : \nio.questdb.cutlass.line.LineSenderException: Failed to detect server line protocol version\n\tat io.questdb.cutlass.line.http.AbstractLineHttpSender.createLineSender(AbstractLineHttpSender.java:222)\n\tat io.questdb.client.Sender$LineSenderBuilder.build(Sender.java:687)\n\tat io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.lambda$testWithMock$38(LineHttpSenderMockServerTest.java:580)\n\tat io.questdb.test.tools.TestUtils.assertMemoryLeak(TestUtils.java:738)\n\tat io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testWithMock(LineHttpSenderMockServerTest.java:551)\n\tat io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testWithMock(LineHttpSenderMockServerTest.java:543)\n\tat io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testTimeoutConfString(LineHttpSenderMockServerTest.java:433)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:316)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:240)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:214)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:155)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385)\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)\n\tat org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)\nCaused by: io.questdb.cutlass.http.client.HttpClientException: timed out [errno=0]\n\tat io.questdb.cutlass.http.client.HttpClient.dieWaiting(HttpClient.java:221)\n\tat io.questdb.cutlass.http.client.HttpClientWindows.ioWait(HttpClientWindows.java:64)\n\tat io.questdb.cutlass.http.client.HttpClient.recvOrD\n2025-07-04T17:10:57.768148Z I i.q.t.TestListener <<<< io.questdb.test.cutlass.http.line.LineHttpSenderMockServerTest.testTimeoutConfString duration_ms=125\n****\n```\n\n### QuestDB version:\n\nlatest master\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nWindows\n\n### File System, in case of Docker specify Host File System:\n\nntfs\n\n### Full Name:\n\nAndrei Pechkurov\n\n### Affiliation:\n\nQuestDB\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 202,
    "metadata": {
      "issue_number": 5815,
      "state": "open",
      "labels": [
        "SchrÃ¶dinger's bug"
      ],
      "comments_count": 7,
      "created_at": "2025-07-04T18:13:53Z",
      "updated_at": "2025-07-13T12:16:36Z",
      "closed_at": null,
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-d4e76c44619b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5763",
    "title": "[Enhancement] Add IF EXISTS to truncate table",
    "text": "# [Enhancement] Add IF EXISTS to truncate table\n\n### Is your feature request related to a problem?\n\nA small usability improvement would be to have `IF EXISTS` in `TRUNCATE TABLE` so it will not error when table is missing, as we have with `DROP TABLE`\n\n### Describe the solution you'd like.\n\nBoth\n```\nTRUNCATE TABLE ratings;\n```\nand\n```\nTRUNCATE TABLE IF EXISTS ratings;\n```\n\n### Describe alternatives you've considered.\n\nAt the moment I truncate and get an error. It'd be nice if I didn't need error control when I choose to ignore the fact table might yet not exist.\n\n### Full Name:\n\njavier ramirez\n\n### Affiliation:\n\nquestdb\n\n### Additional context\n\nVery minor thing. If implementation is not trivial, happy to ignore it.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 123,
    "metadata": {
      "issue_number": 5763,
      "state": "closed",
      "labels": [
        "Enhancement",
        "SQL",
        "Good first issue",
        "Friction"
      ],
      "comments_count": 7,
      "created_at": "2025-06-19T08:03:32Z",
      "updated_at": "2025-09-22T14:55:04Z",
      "closed_at": "2025-09-22T14:55:04Z",
      "author": "javier",
      "top_comments": [],
      "is_feature_request": true,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e3bc312e72c6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5630",
    "title": "SQL: Overload for sum(boolean)",
    "text": "# SQL: Overload for sum(boolean)\n\n### Is your feature request related to a problem?\n\nThis allows a nice-to-read SQL such as `SELECT SUM(spread > 10) as high_spread, SUM(spread <= 10) as low_spread FROM order_book;`\n\n### Describe the solution you'd like.\n\nThis is inspired by this DuckDB feature: https://github.com/duckdb/duckdb/pull/15042\n\n### Describe alternatives you've considered.\n\nthere is some overlap with https://github.com/questdb/questdb/issues/5335\nSum(boolean) is more specialised, significantly simpler to implement and arguably the SQL is easy to read. \n\n### Full Name:\n\nJaromir Hamala\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 88,
    "metadata": {
      "issue_number": 5630,
      "state": "open",
      "labels": [
        "New feature",
        "SQL",
        "Java"
      ],
      "comments_count": 7,
      "created_at": "2025-04-28T08:33:57Z",
      "updated_at": "2025-10-31T11:41:21Z",
      "closed_at": null,
      "author": "jerrinot",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3dad8851f3bd",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/5411",
    "title": "UUID is treated as signed byte for ordering",
    "text": "# UUID is treated as signed byte for ordering\n\n### To reproduce\n\nGenerate any number of UUIDs and sort the table by that value. The result seems to be in order of signed byte, not unsigned as one would expect. This only happens for the first byte, other bytes are treated as unsigned and the order is what one would expect.\n\nThis is probably not much of an issue when using UUID4, which is random, but could cause issues if people use other versions. For me, problems arose in tests when I was trying to compare the generated values in my Python code with the one returned from the DB and I was trying to ensure consistent ordering.\nI guess this could be an optimization to start from the middle of the value space, but if that is the case, it should be documented.\n\nThis query generates the output below. I've added the first byte in signed representation in brackets behind it.\n\nSELECT id FROM uuid_test ORDER BY id\n\n80000000-0000-0000-0000-000000000000 (-128)\nbb000000-0000-0000-0000-000000000000 (-69)\nff000000-0000-0000-0000-000000000000 (-1)\n00000000-0000-0000-0000-000000000000 (0)\n38000000-0000-0000-0000-000000000000 (56)\n7f000000-0000-0000-0000-000000000000 (127)\n\nSince I wasn't allowed to upload a Jupyter and I'm not sure how GitHub likes it if I just append '.txt' (even tho the upload went through, technically), I just copied the 4 cells into a txt. Have fun.\n\n[questdb_uuid.txt](https://github.com/user-attachments/files/18933119/questdb_uuid.txt)\n\n### QuestDB version:\n\n8.2.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nDocker in WSL Ubuntu 22.04.5\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nDaniel BÃ¼cherl\n\n### Affiliation:\n\nUniversity of South Bohemia (Student)\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [x] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 296,
    "metadata": {
      "issue_number": 5411,
      "state": "closed",
      "labels": [
        "SQL",
        "Tidy up"
      ],
      "comments_count": 7,
      "created_at": "2025-02-23T20:24:42Z",
      "updated_at": "2025-02-28T15:52:44Z",
      "closed_at": "2025-02-28T15:52:44Z",
      "author": "dprbook",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-0d881493e6d5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4894",
    "title": "Unable to paginate using PostgreSQL in Java code with QuestDB.",
    "text": "# Unable to paginate using PostgreSQL in Java code with QuestDB.\n\n### To reproduce\n\n# env\r\n|item|version|\r\n|---|---|\r\n|ubuntu|24.04|\r\n|questdb|8.1.0|\r\n|postgresql|42.2.25|\r\n\r\n# How did I discover this issue?\r\n\r\nI have a requirement to use the PostgreSQL driver in Java code to connect to QuestDB and query a single table with pagination. However, I found that QuestDB doesn't seem to work correctly and doesn't return paginated data as expected.\r\n\r\n## My code didn't work as expected\r\n### the java code\r\n```\r\npublic class QueryInQuestDB {\r\n    public static void main(String[] args) throws SQLException {\r\n        Connection connection = DriverManager.getConnection(\"jdbc:postgresql://192.168.182.165:8812/qdb\", \"admin\", \"admin\");\r\n        connection.setAutoCommit(false);\r\n        String sql =\"SELECT * from tt_1 where status = ? order by created_time desc limit ?,?\";\r\n        PreparedStatement preparedStatement = connection.prepareStatement(sql);\r\n        preparedStatement.setInt(1,1);\r\n        preparedStatement.setInt(2,10);\r\n        preparedStatement.setInt(3,40);\r\n        ResultSet resultSet = preparedStatement.executeQuery();\r\n        while (resultSet.next()){\r\n            String groupId = resultSet.getString(\"group_id\");\r\n            System.out.println(\"group_id:\"+groupId);\r\n        }\r\n        preparedStatement.close();\r\n        connection.close();\r\n    }\r\n}\r\n```\r\n\r\n### Error response message.\r\n```\r\nException in thread \"main\" org.postgresql.util.PSQLException: ERROR: Cannot invoke \"io.questdb.griffin.engine.functions.constants.ConstantFunction.getLong(io.questdb.cairo.sql.Record)\" because \"defaultValue\" is null\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2565)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2297)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:322)\r\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:481)\r\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:401)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:164)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:114)\r\n\tat com.cosmo.questdb.QueryInQuestDB.main(QueryInQuestDB.java:18)\r\n```\r\n\r\n### Questdb log\r\n[stdout-2024-08-26T09-57-04.txt](https://github.com/user-attachments/files/16747143/stdout-2024-08-26T09-57-04.txt)\r\n\r\n## Some interesting things.\r\n### If I change the second parameter of `LIMIT` to be concatenated as a string\r\n\r\nThe following code runs successfully and returns the result I need.\r\n\r\n```\r\npublic class QueryInQuestDB {\r\n    public static void main(String[] args) throws SQLException {\r\n        Connection connection = DriverManager.getConnection(\"jdbc:postgresql://192.168.182.165:8812/qdb\", \"admin\", \"quest\");\r\n        connection.setAutoCommit(false);\r\n        String sql =\"SELECT * from tt_1 where status = ? order by created_time desc limit ?,\"+40;\r\n        PreparedStatement preparedStatement = connection.prepareStatement(sql);\r\n        preparedStatement.setInt(1,1);\r\n        preparedStatement.setInt(2,10);\r\n//        preparedStatement.setInt(3,40);\r\n        ResultSet resultSet = preparedStatement.executeQuery();\r\n        while (resultSet.next()){\r\n            String groupId = resultSet.getString(\"group_id\");\r\n            System.out.println(\"group_id:\"+groupId);\r\n        }\r\n        preparedStatement.close();\r\n        connection.close();\r\n    }\r\n}\r\n```\r\n\n\n### QuestDB version:\n\n8.1.0\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu24.04\n\n### File System, in case of Docker specify Host File System:\n\next4\n\n### Full Name:\n\nchao\n\n### Affiliation:\n\nzhuc@jointsky.com\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 335,
    "metadata": {
      "issue_number": 4894,
      "state": "closed",
      "labels": [
        "Bug",
        "Postgres Wire"
      ],
      "comments_count": 7,
      "created_at": "2024-08-26T10:04:09Z",
      "updated_at": "2024-08-28T18:15:46Z",
      "closed_at": "2024-08-28T18:15:46Z",
      "author": "chaoxxx",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-85cc3e591952",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4833",
    "title": "Rename the wal_tables() parameter \"writerLagTxnCount\"",
    "text": "# Rename the wal_tables() parameter \"writerLagTxnCount\"\n\n### Is your feature request related to a problem?\n\nThere can be an offset between `writerTxn` and `sequencerTxn`. This is often seen during a table suspension, when ingestion can continue but not be applied to the table. \r\n\r\nThe difference between these numbers tells you how many transactions behind the table is.\r\n\r\nA few times now, people have been confused by another field, `writerLagTxnCount`. This is a transient value that can change frequently. When many transactions come in, especially small ones, they may be briefly held and then applied as a group. The number of transactions held before application is `writerLagTxnCount`.\r\n\r\nSome read this as the lag between the writer position and sequencer txn. \n\n### Describe the solution you'd like.\n\nRenaming the column to something less confusing.\r\n\r\nFor example, `bufferedTxnSize` or `txnBufferSize` or similar.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nNick Woolmer\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 159,
    "metadata": {
      "issue_number": 4833,
      "state": "closed",
      "labels": [
        "Help wanted",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 7,
      "created_at": "2024-07-31T11:57:32Z",
      "updated_at": "2025-04-23T08:41:18Z",
      "closed_at": "2025-04-23T08:41:18Z",
      "author": "nwoolmer",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-112833223a56",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4751",
    "title": "Add ILP traffic volume metrics",
    "text": "# Add ILP traffic volume metrics\n\n### Is your feature request related to a problem?\n\nWe should add new \"total bytes\" metrics for ILP/TCP and ILP/HTTP to monitor the ingestion network traffic.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Full Name:\n\nAndrei Pechkurov\n\n### Affiliation:\n\nQuestDB\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 60,
    "metadata": {
      "issue_number": 4751,
      "state": "closed",
      "labels": [
        "New feature",
        "Good first issue",
        "ILP"
      ],
      "comments_count": 7,
      "created_at": "2024-07-08T18:17:15Z",
      "updated_at": "2024-10-15T16:50:43Z",
      "closed_at": "2024-10-15T16:50:42Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-07aee784177e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4481",
    "title": "Symbol incorrectly written to table in 7.4.2 when multiple processes sending to QuestDB",
    "text": "# Symbol incorrectly written to table in 7.4.2 when multiple processes sending to QuestDB\n\n### To reproduce\n\n# Description:\r\nAfter updating QuestDB from version 7.3.7 to 7.4.2 using Docker, an issue occurred is in the same table defining different symbol names are incorrectly writing. The symbol is erroneously written as the default one.\r\n\r\nThe write method uses `from questdb.ingress import Sender` and is performed by two separate Python processes. The symbol names have been verified to be correct. Over the past three days, this issue has occurred 3 times, each time after approximately 160 to 240 records have been written by each process. During the occurrence, which lasts for about 5 minutes, almost all writes are incorrectly attributed to the other symbol.\r\n\r\nFor example below, the symbols represent different exchanges, with one process responsible for Binance and the other for Upbit. During normal operation, without any process interruption, when each process has written after ablut 160 to 240 records, all Upbit records are incorrectly written as Binance. It has been confirmed that the sent Exchange name is indeed Upbit.\r\n![image](https://github.com/questdb/questdb/assets/1086156/b617e924-568e-4082-ae25-2882b9714ca5)\r\n\r\n![image](https://github.com/questdb/questdb/assets/1086156/e46aca10-c949-4c49-89c9-82e9e60557f4)\r\n\r\nThis issue started occurring after updating from version 7.3.7 to 7.4.2. Only the storage location moved, and the server IP changed, all other settings and envirment remained unchanged.\r\n\r\n## Steps to reproduce:\r\nAfter updating QuestDB from version 7.3.7 to 7.4.2 using Docker\r\n1. Set up two Python processes, each defining a different symbol name (e.g., Binance and Upbit).\r\n2. Use `from questdb.ingress import Sender` to write data to the same table from both processes.\r\n3. Monitor the table after each process has written after about 160 to 240 records. (in my case about 16 to 24 hours)\r\n\r\n## Expected behavior:\r\nEach process should correctly write its respective symbol name to the table.\r\n\r\n## Actual behavior:\r\nAfter each process has written after about 160 to 240 records, in 5 minutes almost all records from one process (e.g., Upbit) are incorrectly written with the symbol name (written as the default one? e.g., Binance).\r\n\r\n\r\nPlease assist me in resolving this issue or point out any areas that need modification which I may have overlooked, to ensure consistent writing of correct symbol names into tables when using multiple processes in QuestDB. Thank you.\r\n\n\n### QuestDB version:\n\n7.4.2\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nUbuntu 22.04 (Docker)\n\n### File System, in case of Docker specify Host File System:\n\nzfs\n\n### Full Name:\n\nJay\n\n### Affiliation:\n\nsevenjay.tw\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 438,
    "metadata": {
      "issue_number": 4481,
      "state": "closed",
      "labels": [
        "Bug",
        "ILP"
      ],
      "comments_count": 7,
      "created_at": "2024-05-08T05:01:21Z",
      "updated_at": "2024-05-20T21:32:10Z",
      "closed_at": "2024-05-20T21:32:09Z",
      "author": "sevenjay",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b80640cc3bd5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/4159",
    "title": "Query intermittently returns null",
    "text": "# Query intermittently returns null\n\n### To reproduce\n\nI have a table with cols: ticker, date, ohlcv. if I run the below query, occasionally, it returns nulls for min(date). if I run the query again, it returns valid values, and maybe null again at a later stage. please note, I am updating the table in another process.\r\n\r\n`SELECT ticker, min(date), max(date) from mkt_ohlcv order by 1`\n\n### QuestDB version:\n\n7.3.9\n\n### OS, in case of Docker specify Docker and the Host OS:\n\nMac\n\n### File System, in case of Docker specify Host File System:\n\nMac\n\n### Full Name:\n\nRoh Codeur\n\n### Affiliation:\n\npersonal\n\n### Have you followed Linux, MacOs kernel configuration steps to increase Maximum open files and Maximum virtual memory areas limit?\n\n- [X] Yes, I have\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 133,
    "metadata": {
      "issue_number": 4159,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2024-01-25T02:28:17Z",
      "updated_at": "2024-04-13T18:49:32Z",
      "closed_at": "2024-04-13T18:49:32Z",
      "author": "Roh-codeur",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-a983fd8941b5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3916",
    "title": "insert failed with java.lang.ArrayIndexOutOfBoundsException",
    "text": "# insert failed with java.lang.ArrayIndexOutOfBoundsException\n\n### Describe the bug\n\nMy table hava 9 million lines. When I insert new line into it, it can't insert successfully. here is my error Log:\r\n![image](https://github.com/questdb/questdb/assets/16832207/7f260e48-2ecb-4412-bb60-4a0e3468906a)\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**:7.3.3\r\n- **OS**:ubuntu 20\r\n- **Browser**:Chrome\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 59,
    "metadata": {
      "issue_number": 3916,
      "state": "open",
      "labels": [
        "Bug",
        "REST API"
      ],
      "comments_count": 7,
      "created_at": "2023-11-06T03:00:26Z",
      "updated_at": "2023-11-19T12:16:36Z",
      "closed_at": null,
      "author": "wanyuetian",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-e8b7cfe364e6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3848",
    "title": "Spill data to disk (disk spill) support for FastMap",
    "text": "# Spill data to disk (disk spill) support for FastMap\n\n### Is your feature request related to a problem?\r\n\r\n**Disclamer**. While contributions on this one are very much appreciated, this is not an easy frag.\r\n\r\n[`FastMap`](https://github.com/questdb/questdb/blob/c35187666472a18f3bc2624c7e0c761cb20ce45a/core/src/main/java/io/questdb/cairo/map/FastMap.java) is QuestDB's native memory hash table used for hash joins, as well as for storing analytic functions state in GROUP BY/SAMPLE BY queries. Refer to the class' javadoc for the high-level design overview of the hash table.\r\n\r\nApart from other aspects, `FastMap` uses a chunk of native memory as a grow-only heap that stores key-value pairs. Such design has a few advantages, among which disk spill support friendliness. Disk spill is nothing more but an ability to store the data structure on disk when it grows too large to fit into RAM. It's not something unique and can be met in some databases.\r\n\r\nIn our case, `FastMap` could be modified to migrate from \"ordinary\" native memory (i.e. anonymous mmapped memory) to mmapped memory (see `io.questdb.cairo.vm` package) when it reaches certain heap size threshold. Other than that, we should make sure to clean up the temporary files when we no longer need them and on start-up.\r\n\r\n### Describe the solution you'd like.\r\n\r\n_No response_\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context.\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 211,
    "metadata": {
      "issue_number": 3848,
      "state": "closed",
      "labels": [
        "New feature",
        "hacktoberfest"
      ],
      "comments_count": 7,
      "created_at": "2023-10-15T08:22:42Z",
      "updated_at": "2024-06-02T07:48:49Z",
      "closed_at": "2024-06-02T07:48:49Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-d60bdff07ec8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3560",
    "title": "Database backup fails",
    "text": "# Database backup fails\n\nThis issue is regarding the same tables that were reported at -> https://github.com/questdb/questdb/issues/3500 \r\n\r\nAfter applying the suggested workaround the tables were altered successfully but now the database backup fails constantly.\r\nI upgraded now to the latest version 7.2.1 were the mentioned issue was fixed, i converted the tables again to Non-WAL and then back to WAL but i cannot still do a successful backup running the query 'BACKUP database;' from the QuestDB Console.\r\n\r\nError message: \"backup dir already exists [path=/backup/questdb/tmp/sensor_sample_431886/, table=sensor_sample_431886]\"\r\n\r\nI removed manually all directories in the backup/questdb directory and tried again but again the above issue is shown.\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: 7.2.1\r\n- **OS**: CentOS\r\n- **Browser**: Any\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nLogs from questdb:\r\n\r\n```2023-07-11T19:41:42.514687Z C i.q.c.h.p.JsonQueryProcessorState [319] Uh-oh. Err                                                                                                                                                             or!\r\njava.lang.AssertionError: index out of bounds, 14 >= 14\r\n        at io.questdb.std.ObjList.getQuick(ObjList.java:173)\r\n        at io.questdb.cairo.AbstractRecordMetadata.getColumnMetadata(AbstractRec                                                                                                                                                             ordMetadata.java:63)\r\n        at io.questdb.cairo.AbstractRecordMetadata.getColumnType(AbstractRecordM                                                                                                                                                             etadata.java:73)\r\n        at io.questdb.griffin.RecordToRowCopierUtils.generateCopier(RecordToRowC                                                                                                                                                             opierUtils.java:156)\r\n        at io.questdb.griffin.SqlCompiler$DatabaseBackupAgent.backupTable(SqlCom                                                                                                                                                             piler.java:3031)\r\n        at io.questdb.griffin.SqlCompiler$DatabaseBackupAgent.sqlDatabaseBackup(                                                                                                                                                             SqlCompiler.java:3124)\r\n        at io.questdb.griffin.SqlCompiler$DatabaseBackupAgent.sqlBackup(SqlCompi                                                                                                                                                             ler.java:3110)\r\n        at io.questdb.griffin.SqlCompiler.lambda$registerKeywordBasedExecutors$1                                                                                                                                                             (SqlCompiler.java:2667)\r\n        at io.questdb.griffin.SqlCompiler.compileInner(SqlCompiler.java:1231)\r\n        at io.questdb.griffin.SqlCompiler.compile(SqlCompiler.java:219)\r\n        at io.questdb.cutlass.http.processors.JsonQueryProcessor.compileQuery(Js                                                                                                                                                             onQueryProcessor.java:421)\r\n        at io.questdb.cutlass.http.processors.JsonQueryProcessor.execute0(JsonQu                                                                                                                                                             eryProcessor.java:184)\r\n        at io.questdb.cutlass.http.processors.JsonQueryProcessor.onRequestComple                                                                                                                                                             te(JsonQueryProcessor.java:234)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.handleClientRecv(HttpCo                                                                                                                                                             nnectionContext.java:627)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.handleClientOperation(H                                                                                                                                                             ttpConnectionContext.java:209)\r\n        at io.questdb.cutlass.http.HttpServer$1.lambda$$0(HttpServer.java:86)\r\n        at io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODisp                                                                                                                                                             atcher.java:189)\r\n        at io.questdb.cutlass.http.HttpServer$1.run(HttpServer.java:101)\r\n        at io.questdb.mp.Worker.run(Worker.java:118)\r\n\r\n2023-07-11T19:41:42.515921Z I i.q.c.h.HttpConnectionContext kicked out [fd=319]\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 207,
    "metadata": {
      "issue_number": 3560,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2023-07-11T19:52:05Z",
      "updated_at": "2023-08-10T18:31:42Z",
      "closed_at": "2023-08-10T18:31:42Z",
      "author": "100le",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-6fcdf8ae40ac",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3512",
    "title": "High idle CPU usage v7.2",
    "text": "# High idle CPU usage v7.2\n\n### Describe the bug\n\nIDLE CPU is 100-200% in the machine with 8 cores.\r\nI have waited a couple of hours and the CPU usage remains the same. \r\n\n\n### To reproduce\n\n- download and unpack questdb-7.2\r\n- ./questdb.sh start\r\n- connect with psql and create a new table\r\n- No queries are executed\n\n### Expected Behavior\n\n- idle CPU usage is about 0%\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.2\r\n- **OS**: Ubuntu 22.04\r\n- **Browser**: n/a\r\n\r\nBoth questdb-7.2-rt-linux-amd64.tar.gz and questdb-7.2-no-jre-bin.tar.gz\n```\n\n\n### Additional context\n\nVery similar to https://github.com/questdb/questdb/issues/269",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 95,
    "metadata": {
      "issue_number": 3512,
      "state": "open",
      "labels": [],
      "comments_count": 7,
      "created_at": "2023-06-24T04:27:11Z",
      "updated_at": "2025-01-05T12:40:36Z",
      "closed_at": null,
      "author": "akamick86",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-5fb7e6d1637f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3439",
    "title": "Stuck in table lock while writing using HTTP REST API",
    "text": "# Stuck in table lock while writing using HTTP REST API\n\n### Describe the bug\n\nI started writing into the table using HTTP REST API, it was working fine and after sometime I noticed that the table is getting stuck and the writer is not able to write to the table .\r\n\r\nIf I write the same data in a new table then it is able to write to it, the issue starts to coming out after the row count in the table starts increasing.\r\n\r\nI also tried to write using ILP protocol, then I'm getting Error 104, Connection reset by peer.\n\n### To reproduce\n\n1. Start writing to a table using HTTP REST API through python\r\n2. After the table has say more than 150k records, the issue seems to be coming.\r\n\n\n### Expected Behavior\n\nIt should write to the table instead of acquiring lock and getting stuck forever .\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.0.1\r\n- **OS**: Ubuntu 22.04.2 LTS\n```\n\n\n### Additional context\n\n2023-05-31T12:48:51.297492Z A server-main enjoy\r\n2023-05-31T12:52:31.324541Z I http-server connected [ip=193.148.250.63, fd=114]\r\n2023-05-31T12:52:31.325823Z I i.q.c.t.TextLoader configured [table=`9839131_preprocessed_V_L_1_0`, overwrite=false, durable=false, atomicity=2, partitionBy=NONE, timestamp=null, timestampFo\r\nrmat=null]\r\n2023-05-31T12:52:31.342409Z I i.q.c.t.TextDelimiterScanner scan result [table=`9839131_preprocessed_V_L_1_0`, delimiter=',', priority=10, mean=1045.0, stddev=0.0]\r\n2023-05-31T12:52:31.343215Z I i.q.s.ObjectPool pool resize [class=io.questdb.std.str.DirectByteCharSequence$Factory, size=128]\r\n2023-05-31T12:52:31.343861Z I i.q.s.ObjectPool pool resize [class=io.questdb.std.str.DirectByteCharSequence$Factory, size=256]\r\n2023-05-31T12:52:31.345559Z I i.q.s.ObjectPool pool resize [class=io.questdb.std.str.DirectByteCharSequence$Factory, size=512]\r\n2023-05-31T12:52:31.348577Z I i.q.s.ObjectPool pool resize [class=io.questdb.std.str.DirectByteCharSequence$Factory, size=1024]\r\n2023-05-31T12:52:31.350696Z I i.q.s.ObjectPool pool resize [class=io.questdb.std.str.DirectByteCharSequence$Factory, size=2048]\r\n2023-05-31T12:52:31.417019Z I i.q.c.t.AbstractTextLexer resizing 1024 -> 5508 [table=9839131_preprocessed_V_L_1_0]\r\n2023-05-31T12:52:31.418995Z I i.q.c.p.WriterPool open [table=`9839131_preprocessed_V_L_1_0`, thread=42]\r\n2023-05-31T12:52:31.419066Z I i.q.c.TableWriter open '9839131_preprocessed_V_L_1_0'\r\n2023-05-31T12:52:31.419239Z I i.q.c.TableUtils locked '/home/saurabh/.questdb/db/9839131_preprocessed_V_L_1_0.lock' [fd=115]\r\n2023-05-31T12:52:31.497434Z I i.q.c.TableWriter switched partition [path='/home/saurabh/.questdb/db/9839131_preprocessed_V_L_1_0/default']\r\n2023-05-31T12:52:31.513019Z I i.q.c.p.WriterPool >> [table=`9839131_preprocessed_V_L_1_0`, thread=42]\r\n2023-05-31T12:52:31.513130Z I i.q.c.t.CairoTextWriter mis-detected [table=9839131_preprocessed_V_L_1_0, column=0, type=STRING]\r\n2023-05-31T12:52:31.516692Z I i.q.c.t.CairoTextWriter cannot update metadata attributes o3MaxLag and maxUncommittedRows when the table exists and parameter overwrite is false\r\n2023-05-31T12:52:31.566860Z C server-main unhandled error [job=io.questdb.cutlass.http.HttpServer$1@6581dc0a, ex=\r\njava.lang.AssertionError\r\n        at io.questdb.cairo.TableWriter.setRowValueNotNull(TableWriter.java:6073)\r\n        at io.questdb.cairo.TableWriter$RowImpl.putDouble(TableWriter.java:6911)\r\n        at io.questdb.cutlass.text.types.DoubleAdapter.write(DoubleAdapter.java:61)\r\n        at io.questdb.cutlass.text.CairoTextWriter.onField(CairoTextWriter.java:237)\r\n        at io.questdb.cutlass.text.CairoTextWriter.onFieldsNonPartitioned(CairoTextWriter.java:162)\r\n        at io.questdb.cutlass.text.AbstractTextLexer.triggerLine(AbstractTextLexer.java:394)\r\n        at io.questdb.cutlass.text.AbstractTextLexer.onLineEnd(AbstractTextLexer.java:436)\r\n        at io.questdb.cutlass.text.CsvTextLexer.doSwitch(CsvTextLexer.java:42)\r\n        at io.questdb.cutlass.text.AbstractTextLexer.parse0(AbstractTextLexer.java:303)\r\n        at io.questdb.cutlass.text.AbstractTextLexer.parse(AbstractTextLexer.java:102)\r\n        at io.questdb.cutlass.text.AbstractTextLexer.parse(AbstractTextLexer.java:97)\r\n        at io.questdb.cutlass.text.TextLoader.parse(TextLoader.java:222)\r\n        at io.questdb.cutlass.text.TextLoader.parseStructure(TextLoader.java:337)\r\n        at io.questdb.cutlass.text.TextLoader.parse(TextLoader.java:226)\r\n        at io.questdb.cutlass.http.processors.TextImportProcessor.onChunk(TextImportProcessor.java:97)\r\n        at io.questdb.cutlass.http.HttpMultipartContentParser.onChunkWithRetryHandle(HttpMultipartContentParser.java:258)\r\n        at io.questdb.cutlass.http.HttpMultipartContentParser.parse(HttpMultipartContentParser.java:222)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.parseMultipartResult(HttpConnectionContext.java:683)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.continueConsumeMultipart(HttpConnectionContext.java:396)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.consumeMultipart(HttpConnectionContext.java:380)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.handleClientRecv(HttpConnectionContext.java:588)\r\n        at io.questdb.cutlass.http.HttpConnectionContext.handleClientOperation(HttpConnectionContext.java:203)\r\n        at io.questdb.cutlass.http.HttpServer$1.lambda$$0(HttpServer.java:86)\r\n        at io.questdb.network.AbstractIODispatcher.processIOQueue(AbstractIODispatcher.java:189)\r\n        at io.questdb.cutlass.http.HttpServer$1.run(HttpServer.java:101)\r\n        at io.questdb.mp.Worker.run(Worker.java:118)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 357,
    "metadata": {
      "issue_number": 3439,
      "state": "open",
      "labels": [
        "Bug",
        "REST API"
      ],
      "comments_count": 7,
      "created_at": "2023-06-02T05:10:42Z",
      "updated_at": "2023-06-09T15:28:03Z",
      "closed_at": null,
      "author": "saurabhk3",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b8af03cdadd8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/3010",
    "title": "UNION shows weird behaviour when combined with order by",
    "text": "# UNION shows weird behaviour when combined with order by\n\n### Describe the bug\n\nIf we UNION several statements then wrap around a select with order by, the results show inconsistent (and unexpected) behaviour. \r\n\r\nAdding some queries below to show the problem\n\n### To reproduce\n\n1. go to demo site and run this query\r\n\r\n```\r\nWITH full_range AS (  \r\n  SELECT timestamp, galon_price FROM (\r\n    SELECT * FROM (\r\n      SELECT * FROM gas_prices\r\n      UNION\r\n      SELECT to_timestamp('1999-01-01', 'yyyy-MM-dd'), NULL as galon_price -- First Date\r\n      UNION \r\n      SELECT to_timestamp('2023-02-20', 'yyyy-MM-dd'), NULL  as galon_price -- Last Date\r\n    ) AS unordered_data order by timestamp\r\n  ) TIMESTAMP(timestamp)\r\n) \r\nselect * from full_range\r\n```\r\n\r\nEven if the statements are not in order inside the UNION, when we do the final `select * from full_range` we see results are ordered. So far, this is what I would expect, as after the UNION we are doing a `AS unordered_data order by timestamp`\r\n\r\nNow let's just add order by to the external select, as in\r\n\r\n```\r\nWITH full_range AS (  \r\n  SELECT timestamp, galon_price FROM (\r\n    SELECT * FROM (\r\n      SELECT * FROM gas_prices\r\n      UNION\r\n      SELECT to_timestamp('1999-01-01', 'yyyy-MM-dd'), NULL as galon_price -- First Date\r\n      UNION \r\n      SELECT to_timestamp('2023-02-20', 'yyyy-MM-dd'), NULL  as galon_price -- Last Date\r\n    ) AS unordered_data order by timestamp\r\n  ) TIMESTAMP(timestamp)\r\n) \r\nselect * from full_range order by timestamp\r\n```\r\n\r\nNow results are not in order. We see all the results from the original gas_prices table first, then the results from the other two statements in the end. If we` order by timestamp ASC` we see the same problem, but interestingly `order by timestamp DESC `works as expected. However, when we do the order by timestamp DESC the console is showing results in white, meaning it is not interpreted as a designated timestamp\r\n\r\n![image](https://user-images.githubusercontent.com/3839/220626409-4cc10e11-fc9b-4c60-b067-2cf9309a0fa9.png)\r\n\r\nJust to make sure it was not happening because the two \"literal queries\" don't have a designated timestamp, I rewrote the query as \r\n\r\n```\r\nWITH first_date AS (\r\nselect * from (select * FROM (SELECT to_timestamp('1999-01-01', 'yyyy-MM-dd') as timestamp, NULL as galon_price)) timestamp(timestamp)\r\n),\r\nlast_date AS (\r\nselect * from (select * FROM (SELECT to_timestamp('2023-02-20', 'yyyy-MM-dd') as timestamp, NULL as galon_price)) timestamp (timestamp)\r\n),\r\nfull_range AS (  \r\n  SELECT timestamp, galon_price FROM (\r\n    SELECT * FROM (\r\n      SELECT * FROM gas_prices\r\n     UNION\r\n      SELECT * FROM first_date\r\n      UNION\r\n      SELECT * FROM last_date\r\n    ) order by timestamp \r\n  ) TIMESTAMP(timestamp)\r\n) \r\nselect * from full_range  order by timestamp desc;\r\n```\r\nNow every statement there should have a designated timestamp, however the behaviour is the same as above. Order by ASC fails, order by DESC misses the designated timestamp, no order by produces good results.\r\n\r\nIf however we change the queries (with or without designated timestamps) so the UNION statements are in chronological order, the results are always in the right order (without order by or with and without ASC and DESC), but however the DESC statement still misses the designated timestamp meaning.\r\n\r\nThe next two versions of the query (both with and without extra designated timestamp statements) return those results\r\n\r\n```\r\nWITH full_range AS (  \r\n  SELECT timestamp, galon_price FROM (\r\n    SELECT * FROM (\r\n      SELECT to_timestamp('1999-01-01', 'yyyy-MM-dd') as timestamp, NULL as galon_price -- First Date\r\n      UNION\r\n      SELECT * FROM gas_prices           \r\n      UNION \r\n      SELECT to_timestamp('2023-02-20', 'yyyy-MM-dd') as timestamp, NULL  as galon_price -- Last Date\r\n    )  AS unordered_data order by timestamp\r\n  ) TIMESTAMP(timestamp)\r\n) \r\nselect * from full_range order by timestamp ASC\r\n```\r\n\r\n```\r\nWITH first_date AS (\r\nselect * from (select * FROM (SELECT to_timestamp('1999-01-01', 'yyyy-MM-dd') as timestamp, NULL as galon_price)) timestamp(timestamp)\r\n),\r\nlast_date AS (\r\nselect * from (select * FROM (SELECT to_timestamp('2023-02-20', 'yyyy-MM-dd') as timestamp, NULL as galon_price)) timestamp (timestamp)\r\n),\r\nfull_range AS (  \r\n  SELECT timestamp, galon_price FROM (\r\n    SELECT * FROM (\r\n      SELECT * FROM first_date\r\n      UNION \r\n      SELECT * FROM gas_prices\r\n      UNION\r\n      SELECT * FROM last_date\r\n    ) order by timestamp \r\n  ) TIMESTAMP(timestamp)\r\n) \r\nselect * from full_range  order by timestamp ASC;\r\n```\r\n\r\n\r\n\r\n\n\n### Expected Behavior\n\nI would expect since we are forcing ORDER BY after the UNION results should be in order. If ORDER BY cannot be used in combination with UNION then it should probably throw an error\n\n### Environment\n\n```markdown\n- **QuestDB version**: 7.0.1 (dev version on the demo site)\r\n- **OS**: demo site\r\n- **Browser**:chrome\n```\n\n\n### Additional context\n\nThis was surfaced at the public slack conversation at https://questdb.slack.com/archives/C1NFJEER0/p1676890276428389",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 712,
    "metadata": {
      "issue_number": 3010,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2023-02-22T13:09:29Z",
      "updated_at": "2023-03-21T09:32:23Z",
      "closed_at": "2023-03-21T09:32:22Z",
      "author": "javier",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-6dc259287b3e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2976",
    "title": "Multiple LT JOINs error on SYMBOL column",
    "text": "# Multiple LT JOINs error on SYMBOL column\n\n### Describe the bug\r\n\r\non this table:\r\n```\r\nCREATE TABLE 'tests' (\r\n  Ticker SYMBOL capacity 256 CACHE,\r\n  H FLOAT,\r\n  P_T FLOAT,\r\n  timestamp TIMESTAMP\r\n) timestamp (timestamp) PARTITION BY MONTH;\r\n```\r\nAfter inserting data, calling from webui:\r\n`tests LT JOIN (tests LT JOIN (tests) ON (Ticker)) ON (Ticker)`\r\nresulting Ticker type changes to STRING instead of SYMBOL on Ticker1\r\nThis causes further problems when adding more ON statements. In the following select:\r\n`tests LT JOIN (tests LT JOIN (tests LT JOIN (tests)) ON (Ticker)) ON (Ticker)`\r\nThe following error message occurs:\r\n`right side column 'Ticker1' is of unsupported type`\r\nWhich is the default error message when trying to LT JOIN ON STRING column\r\n\r\n### To reproduce\r\n\r\n1. Create the table as outlined in description\r\n2. insert data\r\n3. perform the following query:`tests LT JOIN (tests LT JOIN (tests LT JOIN (tests)) ON (Ticker)) ON (Ticker)`\r\n4. the error should be reproduced\r\n\r\n### Expected Behavior\r\n\r\nThe data should be returned from the query\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**: Updated to 7 but webui still shows 6.6.1\r\n- **OS**:  Microsoft Windows 10 Pro\r\n- **Browser**: Google Chrome\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n![image](https://user-images.githubusercontent.com/79865853/217516347-49f360e8-d5a8-40f8-ae49-dad065e977cd.png)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 198,
    "metadata": {
      "issue_number": 2976,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2023-02-08T11:21:46Z",
      "updated_at": "2023-04-02T10:53:07Z",
      "closed_at": "2023-03-30T13:24:33Z",
      "author": "superichmann",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-ed3389996b64",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2917",
    "title": "Implement and test ISO week date format parser",
    "text": "# Implement and test ISO week date format parser\n\n### Is your feature request related to a problem?\n\n#2779 added partition by week support. We should make sure that timestamp filters such as `ts in '2023-W01'` as well as `YYYY-Www` format parsing work as expected.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 65,
    "metadata": {
      "issue_number": 2917,
      "state": "open",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2023-01-12T18:05:36Z",
      "updated_at": "2023-03-09T05:04:03Z",
      "closed_at": null,
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-858318b0109a",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2836",
    "title": ".Net Core BUG",
    "text": "# .Net Core BUG\n\n### Describe the bug\r\n\r\n![image](https://user-images.githubusercontent.com/12455385/204773465-32fefebe-f7dd-4f8b-be2c-8af8675e49ba.png)\r\n\r\n\r\nNpgsql.dll  Is greater than  6.0   error\r\n\r\n5.0 Normal\r\n \r\n ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 17,
    "metadata": {
      "issue_number": 2836,
      "state": "open",
      "labels": [
        "Bug"
      ],
      "comments_count": 7,
      "created_at": "2022-11-30T10:35:01Z",
      "updated_at": "2023-01-20T13:27:06Z",
      "closed_at": null,
      "author": "DotNetNext",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-1d1f4f3989c6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2509",
    "title": "Support backward interval generation in IN operator",
    "text": "# Support backward interval generation in IN operator\n\n### Is your feature request related to a problem?\n\nIN operator support interval generation:\r\n```\r\nselect timestamp, price\r\nfrom trades\r\nwhere timestamp in '2018-01-01T13:00:00;1h;24h;10000'\r\n```\r\n\r\nThe above query would return results from 13:00-14:00 intervals for 10K days starting with 2018-01-01, so the results are in future when compared with the start date.\r\n\r\nWould be nice to support backward iteration syntax, e.g. '2018-01-01T13:00:00;1h;-24h;10000', which would return intervals in the past.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 97,
    "metadata": {
      "issue_number": 2509,
      "state": "closed",
      "labels": [
        "Help wanted",
        "New feature",
        "SQL",
        "Good first issue"
      ],
      "comments_count": 7,
      "created_at": "2022-09-07T14:30:57Z",
      "updated_at": "2023-02-03T18:53:20Z",
      "closed_at": "2023-02-03T18:53:20Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-efd13b6e5d6f",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2477",
    "title": "Automatic log file deletion in LogRollingFileWriter",
    "text": "# Automatic log file deletion in LogRollingFileWriter\n\n### Is your feature request related to a problem?\n\n`LogRollingFileWriter` should support automatic log file deletion based on time or space. The check can take place each time when a new log file is rolled.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\nA cron job that would delete older log files does the trick, but having a built-in feature is better.\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 79,
    "metadata": {
      "issue_number": 2477,
      "state": "closed",
      "labels": [
        "New feature",
        "Good first issue"
      ],
      "comments_count": 7,
      "created_at": "2022-08-29T10:21:37Z",
      "updated_at": "2023-01-18T15:51:14Z",
      "closed_at": "2023-01-18T15:51:14Z",
      "author": "puzpuzpuz",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-aecc870c22d6",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2396",
    "title": "Allow for a init script to initialize a table with preferred data types",
    "text": "# Allow for a init script to initialize a table with preferred data types\n\n### Is your feature request related to a problem?\n\nIn a dockerized environment I'd like to be able to script an initialization of a quest database with a predefined table and preferred column data types - similar to vanilla Postgres. \n\n### Describe the solution you'd like.\n\nSomething like Postgres & Docker where I can define:\r\n\r\n**init.sql**\r\n```\r\nCREATE TABLE if not exists example(id symbol, value float .... etc)\r\n```\r\n\r\n**Dockerfile**\r\n```\r\nFROM questdb/questdb:latest\r\n\r\nCOPY init.sql /docker-entrypoint-initdb.d/\r\n```\r\n\r\nOr a similar mechanism for automating the initial table setup.\r\n\n\n### Describe alternatives you've considered.\n\n**Option 1) NOT initializing a table and just letting Questdb create the table automatically on first insert**\r\nThis is useful however it's not guaranteed that Quest will choose the correct datatypes for the columns. \r\n\r\n**Option 2) Initializing the table using a curl command to the quest insert API**\r\nThis is useful as well, however quest's default image does not include curl, so it manually adding a `RUN apt-get update && apt-get install curl -y` is required.\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 187,
    "metadata": {
      "issue_number": 2396,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 7,
      "created_at": "2022-08-04T21:10:01Z",
      "updated_at": "2025-11-16T19:00:34Z",
      "closed_at": null,
      "author": "mrpher",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-d5a716497f9a",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2379",
    "title": "Casting bool -> numeric data types yields unexpected results",
    "text": "# Casting bool -> numeric data types yields unexpected results\n\n### Describe the bug\n\nFor some aggregation, I would like to do a \"sum where condition holds\" calculation. In order to do so, I am currently using the approach described below. When casting bool to int or float numbers, FALSE unexpectedly gets mapped to the value 1, destroying this attempt.  I thus will try doing a where first.\r\n\r\nIn pandas for example, we can use sum/mean etc easily on booleans to get the percentage of true/false values, and I would hope that \r\na) Casting bool -> float/int will continue yielding the correct semantics\r\nb) Averaging over bools will attempt using (a) to calculate aggregated statistics.\r\n\r\nProbably b) is more like a feature request, though.\n\n### To reproduce\n\nselect cast((150 < 120) as float)\n\n### Expected Behavior\n\n-> returns  FALSE -> 0\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.4.3 Docker\r\n- **OS**:\r\n- **Browser**:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 159,
    "metadata": {
      "issue_number": 2379,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2022-08-01T16:16:05Z",
      "updated_at": "2022-08-05T17:34:12Z",
      "closed_at": "2022-08-05T17:34:12Z",
      "author": "Zahlii",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-2fc9823d671c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2361",
    "title": "dateadd converts a designated timestamp to a regular timestamp",
    "text": "# dateadd converts a designated timestamp to a regular timestamp\n\n### Describe the bug\n\nWhen I do a `dateadd` on a timestamp, it's converted to a regular timestamp.\r\nAs a result, sample by no longer works on the regular timestamp.\n\n### To reproduce\n\nGo to demo: https://demo.questdb.io\r\n\r\nThis works as expected resulting in about 144 rows.\r\n```\r\nSELECT timestamp, count() FROM 'trades'\r\nSAMPLE BY 1d\r\nALIGN TO CALENDAR\r\n```\r\n\r\nThis does not work as expected due to the bug I explained above and produces around 86M rows:\r\n```\r\nSELECT dateadd('h', 2, timestamp) timestamp, count() FROM 'trades'\r\nSAMPLE BY 1d\r\nALIGN TO CALENDAR\r\n```\n\n### Expected Behavior\n\nI'd expect the two to work in the same way.\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.4.3\r\n- **OS**: N/A (I use demo)\r\n- **Browser**: Safari\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 138,
    "metadata": {
      "issue_number": 2361,
      "state": "open",
      "labels": [],
      "comments_count": 7,
      "created_at": "2022-07-29T10:49:45Z",
      "updated_at": "2022-07-29T15:32:39Z",
      "closed_at": null,
      "author": "newskooler",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-e75d5b9bd908",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2359",
    "title": "Performance Slowdown With One Additional Column",
    "text": "# Performance Slowdown With One Additional Column\n\n### Describe the bug\r\n\r\nGetting an unexpectedly high latency on a relatively simple query:\r\n\r\n`select ts, last(close) from 'ohlc_exchanges' where pair in 'btc-usd' and exchange in 'coinbase' and ts >= cast(1608864755 * 1000000L AS TIMESTAMP) and ts <= cast(1656125555 * 1000000L AS TIMESTAMP) sample by 1d align to calendar with offset '00:00' limit 5000`\r\n\r\nTable has ~65M rows. \"Pair\" and \"exchange\" are indexed and the table is partitioned by month. QDB 6.4.2. Query takes about 6000ms.\r\n\r\nI have a similar table w/ ~25M rows that does not have the exchange column, and similar queries on this table take around 200ms.\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\nExpecting similar performance to the second table.\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version 6.4.2**:\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 135,
    "metadata": {
      "issue_number": 2359,
      "state": "open",
      "labels": [],
      "comments_count": 7,
      "created_at": "2022-07-28T15:53:28Z",
      "updated_at": "2022-08-01T00:03:07Z",
      "closed_at": null,
      "author": "EmmettM",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-c6c17c5ef22b",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2161",
    "title": "SIGILL at select where-query",
    "text": "# SIGILL at select where-query\n\n### Describe the bug\n\nexecuting the following query via web interface crashes questdb\r\n\r\nselect date_time, e10 from prices where e10 = 0 limit 500\r\n\r\nexcerpt from log file, omitted duplicate error lines:\r\n\r\n> 2022-05-28 07:17:08,stdout,\"2022-05-28T07:17:08.249920Z I i.q.g.SqlCompiler plan [q=`select-choose date_time, e10 from (select [date_time, e10] from prices timestamp (date_time) where e10 = 0) order by date_time desc limit 500`, fd=7]\"\r\n> 2022-05-28 07:17:08,stdout,limit 500]\r\n> 2022-05-28 07:17:08,stdout,\"2022-05-28T07:17:08.254639Z I i.q.g.SqlCodeGenerator JIT enabled for (sub)query [tableName=prices, fd=7]\"\r\n> 2022-05-28 07:17:08,stdout,\"2022-05-28T07:17:08.256408Z I i.q.c.h.p.JsonQueryProcessorState [7] execute-new [skip: 0, stop: 1000]\"\r\n> 2022-05-28 07:17:08,stdout,# A fatal error has been detected by the Java Runtime Environment:\r\n> 2022-05-28 07:17:08,stdout,#\r\n> 2022-05-28 07:17:08,stdout,\"#  SIGILL (0x4) at pc=0x00007fdb25cd2026, pid=1, tid=22\r\n> 2022-05-28 07:17:08,stdout,#\r\n> 2022-05-28 07:17:08,stdout,# JRE version: OpenJDK Runtime Environment (17.0.1+12) (build 17.0.1+12-39)\r\n> 2022-05-28 07:17:08,stdout,\"# Java VM: OpenJDK 64-Bit Server VM (17.0.1+12-39, mixed mode, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)\r\n> 2022-05-28 07:17:08,stdout,# Problematic frame:\r\n> 2022-05-28 07:17:08,stdout,# C  0x00007fdb25cd2026\r\n> 2022-05-28 07:17:08,stdout,#\r\n> 2022-05-28 07:17:08,stdout,# Core dump will be written. Default location: Core dumps may be processed with \"/usr/syno/sbin/syno-dump-core.sh /volume1 %p %s\" (or dumping to /root/.questdb/core.1)\r\n> 2022-05-28 07:17:08,stdout,#\r\n> 2022-05-28 07:17:08,stdout,# An error report file with more information is saved as:\r\n> 2022-05-28 07:17:08,stdout,# /root/.questdb/hs_err_pid1.log\r\n> 2022-05-28 07:17:08,stdout,#\r\n> 2022-05-28 07:17:08,stdout,\r\n> 2022-05-28 07:17:08,stdout,\"[error occurred during error reporting (), id 0xb, SIGSEGV (0xb) at pc=0x00007fdb3e099529]\r\n> \r\n> ...\r\n> \r\n> 2022-05-28 07:17:08,stdout,\"[Too many errors, abort]\r\n\r\n`\n\n### To reproduce\n\n1. CREATE TABLE prices (ID LONG, station_id STRING, e5 DOUBLE, e10 DOUBLE, diesel DOUBLE, date_time TIMESTAMP)\r\n2. SELECT  date_time , e10 from 'prices' where e10 = 0\n\n### Expected Behavior\n\nsuccessful query and query results\n\n### Environment\n\n```markdown\n- QuestDB 6.3.1\r\n- server: docker on linux, client: macOS 12.4\r\n- Safari 15.5 (17613.2.7.1.8)\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 302,
    "metadata": {
      "issue_number": 2161,
      "state": "closed",
      "labels": [
        "Bug",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2022-05-28T10:41:20Z",
      "updated_at": "2022-05-30T10:38:23Z",
      "closed_at": "2022-05-30T10:38:23Z",
      "author": "operatorpk",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-8598d6495d29",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/2025",
    "title": "Add params to REST to fix SQL injection",
    "text": "# Add params to REST to fix SQL injection\n\n### Is your feature request related to a problem?\r\n\r\nWe've seen code like this in one of the tutorials:\r\n\r\n```python\r\nrequests.get('http://localhost:9000/exec', params={\r\n    'query': f'insert into excel_rates values({date}, {usd}, {inr})'})\r\n```\r\n\r\nString interpolation in the query exposes the customer (and database) to SQL injection.\r\n\r\n\r\n### Describe the solution you'd like.\r\n\r\nWe should provide an additional way to support this via, for example, a `params` query parameter list and avoid this issue:\r\n\r\n```python\r\n# Note: This is just an idea to illustrate the point. Not the only solution.\r\n\r\nrequests.get('http://localhost:9000/exec', params={\r\n    'query': 'insert into excel_rates values(?, ?, ?)',\r\n    'params': [date, usd, inr]})\r\n```\r\n\r\nI.e. this would expand to, for example:\r\n\r\n```\r\nhttp://localhost:9000/exec?query=insert+into+excel_rates+values%28%3F%2C+%3F%2C+%3F%29&params=2022-08-06&params=10.4&params=1034.24\r\n```\r\n\r\n### Describe alternatives you've considered.\r\n\r\nThis can be worked around with client-side SQL encoding, but it's unrealistic and error-prone to expect our users to do this.\r\n\r\n### Additional context.\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 150,
    "metadata": {
      "issue_number": 2025,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 7,
      "created_at": "2022-04-11T16:37:27Z",
      "updated_at": "2022-05-29T04:25:50Z",
      "closed_at": null,
      "author": "amunra",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-60ac71dafbdc",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1846",
    "title": "drop partition does not work on 6.2",
    "text": "# drop partition does not work on 6.2\n\nWe're running these statements:\r\n\r\n`alter table [name] drop partition where at < dateadd('d', -4, now());`\r\n\r\nThey work fine on 6.0.4 but have no effect on 6.2.\r\n\r\nHere's the PR on our side where we try to upgrade: https://github.com/vespa-engine/vespa/pull/20971\r\n(You have looked at this code before and suggested to use the above expression - thanks!)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 62,
    "metadata": {
      "issue_number": 1846,
      "state": "closed",
      "labels": [
        "Documentation"
      ],
      "comments_count": 7,
      "created_at": "2022-01-31T14:40:04Z",
      "updated_at": "2022-02-01T07:29:30Z",
      "closed_at": "2022-01-31T22:21:13Z",
      "author": "bratseth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-debaa456b058",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1723",
    "title": "Questdb server-main [errno=19] cannot join group",
    "text": "# Questdb server-main [errno=19] cannot join group\n\n### Describe the bug\n\nI shutdown down questdb with ./questdb.sh stop while it probably still had some data to flush, now it refuses to start up again with the below log:\r\n```\r\n\r\ntail -f /root/.questdb/log/stdout-2021-12-21T17-19-31.txt \r\n2021-12-21T16:19:34.356606Z D i.q.c.v.MemoryCMRImpl open /root/.questdb/db/telemetry_config/default/package.d [fd=56, pageSize=4, size=4]\r\n2021-12-21T16:19:34.356907Z A i.q.TelemetryJob instance [id=0x05d3a81bb951a7000010dcde9b0526, enabled=true]\r\n2021-12-21T16:19:34.357030Z D i.q.c.TxnScoreboard release  [p=140357182423040, txn=1]\r\n2021-12-21T16:19:34.357061Z D i.q.c.p.ReaderPool 'telemetry_config' is back [at=0:0, thread=1]\r\n2021-12-21T16:19:34.405102Z A http-server listening on 0.0.0.0:9000 [fd=57]\r\n2021-12-21T16:19:34.489865Z A http-min-server listening on 0.0.0.0:9003 [fd=59]\r\n2021-12-21T16:19:34.505107Z A pg-server listening on 0.0.0.0:8812 [fd=61]\r\n2021-12-21T16:19:34.584379Z I i.q.c.l.u.AbstractLineProtoUdpReceiver closed [fd=63]\r\n2021-12-21T16:19:34.584438Z I i.q.c.l.u.AbstractLineProtoUdpReceiver closed [fd=63]\r\n2021-12-21T16:19:34.584469Z E server-main [errno=19] cannot join group [fd=63, bind=0, group=-402587133]\r\n```\r\n\r\n\n\n### To reproduce\n\n1. insert a series of lines inside db\r\n2. shutdown down questdb with ./questdb.sh stop \r\n3. start questdb with ./questdb.sh\r\n4. check it didn't start up with ./questdb.sh status\r\n5. check log file for \"cannot join group\" error\n\n### Expected Behavior\n\n1. server should start up again even if it has been stopped while an insertion was under way\n\n### Environment\n\n```markdown\n- **QuestDB version**: questdb-6.1.2-rt-linux-amd6\r\n- **OS**: Linux Mint\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 191,
    "metadata": {
      "issue_number": 1723,
      "state": "closed",
      "labels": [
        "Question"
      ],
      "comments_count": 7,
      "created_at": "2021-12-21T16:28:19Z",
      "updated_at": "2022-01-24T16:31:31Z",
      "closed_at": "2022-01-20T12:12:24Z",
      "author": "italy-dude",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-148de8b45ab8",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1707",
    "title": "live demo dies on SQL: select * from 'trips' where fare_amount<15.0 order by pickup_datetime;",
    "text": "# live demo dies on SQL: select * from 'trips' where fare_amount<15.0 order by pickup_datetime;\n\n### Describe the bug\n\nlive demo dies on SQL\r\n```\r\n  select * from 'trips' where fare_amount<15.0 order by pickup_datetime;\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Environment\n\n```markdown\n- **QuestDB version**:\r\n- **OS**:\r\n- **Browser**:\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 61,
    "metadata": {
      "issue_number": 1707,
      "state": "closed",
      "labels": [
        "Performance"
      ],
      "comments_count": 7,
      "created_at": "2021-12-17T13:34:30Z",
      "updated_at": "2022-05-09T10:24:43Z",
      "closed_at": "2022-05-09T10:24:43Z",
      "author": "qlbit-com",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-0a7c857903ff",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1577",
    "title": "Metabase Integration doesn't work",
    "text": "# Metabase Integration doesn't work\n\n### Describe the bug\n\nMetabase via the query:\r\n\r\n`SELECT TRUE AS \"_\" FROM \"public\".\"table\" WHERE 1 <> 1 LIMIT 0`\r\n\r\nFails to check that the table exists, it seems removing the \"public\" of the SQL query fixes this issue in QDB.\r\n\r\n`SELECT TRUE AS \"_\" FROM \"table\" WHERE 1 <> 1 LIMIT 0`\n\n### To reproduce\n\n1. Go to Metabase latest version\r\n2. Go to Admin\r\n3. Connect to QDB via the PSQL connector.\n\n### Expected Behavior\n\nQuestDB returns true on a table name\n\n### Environment\n\n```markdown\n- **QuestDB version**: 6.0.7.1\r\n- **OS**: Ubuntu 20.04\n```\n\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 106,
    "metadata": {
      "issue_number": 1577,
      "state": "open",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2021-11-16T10:44:44Z",
      "updated_at": "2023-03-15T11:04:17Z",
      "closed_at": null,
      "author": "DylanDKnight",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-152fa6ad9471",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1572",
    "title": "LineTcpSender ingest data terminated ",
    "text": "# LineTcpSender ingest data terminated \n\n### Describe the bug\r\n\r\nI used the following code to simulate data injection into questdb.\r\nI want to simulate the data of 100,000 sensors a weekï¼Œand the frequency of a sensor is one per second.\r\nTotal records :`100,000*60*60*24*7=60.48 billion`.\r\nHowever, the simulation program always terminates after a few hours.\r\nWhy and what should i do ?\r\n\r\n```java\r\nString hostIPv4 = \"127.0.0.1\";\r\nint port = 8009;\r\nint bufferCapacity = 256 * 1024;\r\ntry {\r\n            LineTcpSender sender = new LineTcpSender(Net.parseIPv4(hostIPv4), port, bufferCapacity);\r\n            while (beginTs<=endTs){\r\n                long startTime = System.currentTimeMillis();\r\n                for (int i = 1; i <= 100; i++) {\r\n                    for(int j =1 ;j<=1000;j++){\r\n                        sender.metric(\"object\"+i).tag(\"tagName\", \"tag\"+j)\r\n                                .field(\"value\", rnd.nextDouble())\r\n                                .$(beginTs);\r\n                    }\r\n                    sender.flush();\r\n                }\r\n\r\n                LocalDateTime localDateTime = new Date(beginTs/1000000).toInstant().atOffset(ZoneOffset.UTC).toLocalDateTime();\r\n                beginTs+=1000000000;\r\n            }\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\nI hope that the connection can continue to write data, or can there be a reconnection mechanism when the connection is disconnected\r\n\r\n\r\n\r\n### Environment\r\n\r\n```markdown\r\n- **QuestDB version**:6.1.1\r\n- **OS**:CentOS Linux release 7.9.2009\r\n- **Browser**:Micro Edge 87.0.664.75\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 172,
    "metadata": {
      "issue_number": 1572,
      "state": "open",
      "labels": [
        "Bug",
        "ILP"
      ],
      "comments_count": 7,
      "created_at": "2021-11-15T08:56:43Z",
      "updated_at": "2022-03-07T10:34:54Z",
      "closed_at": null,
      "author": "yiwojiu",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-b4363e70d9ff",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1419",
    "title": "Latest by returns incorrect results when applied with filters",
    "text": "# Latest by returns incorrect results when applied with filters\n\n**To Reproduce**\r\nData set:\r\n```\r\ncreate table numeric_column_value_1(device_id symbol index, column_name symbol index, value double, timestamp timestamp) timestamp(timestamp) partition by day;\r\n\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c1', 101.1, '2021-10-05T11:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c1', 101.2, '2021-10-05T12:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c1', 101.3, '2021-10-05T13:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c1', 101.4, '2021-10-05T14:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c2', 102.1, '2021-10-05T11:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c2', 102.2, '2021-10-05T12:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c2', 102.3, '2021-10-05T13:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c2', 102.4, '2021-10-05T14:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d1', 'c2', 102.5, '2021-10-05T15:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d2', 'c1', 201.1, '2021-10-05T11:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d2', 'c1', 201.2, '2021-10-05T12:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d2', 'c1', 201.3, '2021-10-05T13:31:35.878Z');\r\ninsert into numeric_column_value_1 (device_id, column_name, value, timestamp) values ('d2', 'c1', 201.4, '2021-10-05T14:31:35.878Z');\r\n```\r\n\r\nQuery: ` select * from numeric_column_value_1 latest by device_id, column_name`\r\nResult: \r\n![image](https://user-images.githubusercontent.com/15086720/136762406-9255e26f-79ed-4d38-ad03-6453b8c32913.png)\r\nStatus: Correct\r\n\r\nQuery: `select * from numeric_column_value_1 latest by device_id, column_name where device_id = 'd1'`\r\nResult: \r\n![image](https://user-images.githubusercontent.com/15086720/136762623-86dd143f-1185-4cf3-95cc-8feda4d68005.png)\r\nStatus: Incorrect\r\n\r\nQuery: `select * from numeric_column_value_1 latest by device_id, column_name where device_id = 'd1' and column_name in ('c1', 'c2')`\r\nResult:  No data\r\n![image](https://user-images.githubusercontent.com/15086720/136762830-169843c5-8f44-4a52-a9c5-2edfa3d7278c.png)\r\nStatus: Incorrect\r\n\r\nQuery: `select * from numeric_column_value_1 latest by device_id, column_name where timestamp >= '2021-10-05T14:31:35.878Z'`\r\nResult: \r\n![image](https://user-images.githubusercontent.com/15086720/136763409-0aa1d40c-e13b-4e34-bcab-a31920db35f9.png)\r\nStatus: Incorrect\r\n\r\n**Environment**:\r\nVersion: 6.0.7.1\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 263,
    "metadata": {
      "issue_number": 1419,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 7,
      "created_at": "2021-10-11T09:05:19Z",
      "updated_at": "2021-11-13T01:23:15Z",
      "closed_at": "2021-11-13T01:23:15Z",
      "author": "arina-ielchiieva",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-99d453144675",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1338",
    "title": "JDK core dump when doing o3 sort.",
    "text": "# JDK core dump when doing o3 sort.\n\nI have a case in which (I don't know why) it goes for \"o3\" sort:\r\n\r\n```\r\n2021-09-19T16:59:21.132415Z I i.q.c.p.WriterPool >> [table=`effr_rates`, thread=56]\r\n2021-09-19T16:59:21.132678Z I i.q.c.TableWriter switched to o3 [table=effr_rates]\r\n2021-09-19T16:59:21.133946Z I i.q.c.TableWriter sorting o3 [table=effr_rates]\r\n```\r\n\r\nWhen that happens, there is a core dump from the JDK:\r\n\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000066aacdc3, pid=32636, tid=32648\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment 18.9 (11.0.12+8) (build 11.0.12+8-LTS-237)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM 18.9 (11.0.12+8-LTS-237, mixed mode, tiered, compressed oops, g1 gc, windows-amd64)\r\n# Problematic frame:\r\n# C  [libquestdb11394819330336213871.dll+0x2cdc3]\r\n#\r\n# No core dump will be written. Minidumps are not enabled by default on client versions of Windows\r\n#\r\n# An error report file with more information is saved as:\r\n# D:\\...\\hs_err_pid32636.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://bugreport.java.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n#\r\n```\r\n\r\nHere are the contents of the log file:\r\n\r\n[gh-1338.log](https://github.com/questdb/questdb/files/7195561/1338.log)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 194,
    "metadata": {
      "issue_number": 1338,
      "state": "closed",
      "labels": [
        "Bug"
      ],
      "comments_count": 7,
      "created_at": "2021-09-19T17:03:33Z",
      "updated_at": "2021-09-24T11:34:48Z",
      "closed_at": "2021-09-24T10:22:29Z",
      "author": "jfarjona",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-1ff4528481d4",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1100",
    "title": "PgWire: compatibility with python's asyncpg Postgres library",
    "text": "# PgWire: compatibility with python's asyncpg Postgres library\n\nDoes QuestDB support [asyncpg](https://github.com/MagicStack/asyncpg) library?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 13,
    "metadata": {
      "issue_number": 1100,
      "state": "closed",
      "labels": [
        "Postgres Wire",
        "Compatibility"
      ],
      "comments_count": 7,
      "created_at": "2021-06-07T11:36:53Z",
      "updated_at": "2022-01-04T09:17:49Z",
      "closed_at": "2022-01-04T09:17:17Z",
      "author": "riccardogiro",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-8d92ec504734",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1075",
    "title": "Column alias 'time' does not work",
    "text": "# Column alias 'time' does not work\n\n**Describe the bug**\r\nSometimes alias time does not work \r\n\r\n**To Reproduce**\r\n```sql\r\ncreate table tst (\r\ntimestamp timestamp\r\n)\r\n\r\nselect timestamp time from tst\r\n```\r\n\r\n**Expected behavior**\r\nResult column name is timestamp but should be time\r\n\r\n**Screenshots**\r\n\r\n**Environment (please complete the following information):**\r\n - Version: 6.0.2\r\n\r\n**Additional context**\r\nThis is the workaround syntax with AS\r\n```sql\r\nselect timestamp AS time from tst\r\n```\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 70,
    "metadata": {
      "issue_number": 1075,
      "state": "closed",
      "labels": [
        "Bug",
        "Good first issue",
        "hacktoberfest"
      ],
      "comments_count": 7,
      "created_at": "2021-06-01T10:57:03Z",
      "updated_at": "2021-11-29T15:39:25Z",
      "closed_at": "2021-11-29T15:39:25Z",
      "author": "ideoma",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-166540db69fa",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/1060",
    "title": "Add Javadoc",
    "text": "# Add Javadoc\n\nIt would be very helpful if your interfaces defined their behavior in Javadoc.\r\nAs it is we need to guess, or try to deduce it from the code ... but there are many implementations.\r\n\r\nFor example, I'm facing an issue where myRecord.getStr(0) returns null. What does that mean? Or is it a bug? Very hard to tell.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 60,
    "metadata": {
      "issue_number": 1060,
      "state": "closed",
      "labels": [
        "Documentation"
      ],
      "comments_count": 7,
      "created_at": "2021-05-25T05:40:58Z",
      "updated_at": "2021-06-14T19:32:29Z",
      "closed_at": "2021-06-14T19:32:29Z",
      "author": "bratseth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-9c098df22e74",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/975",
    "title": "Last row is missing from UI",
    "text": "# Last row is missing from UI\n\n**Describe the bug**\r\nLast row is not visible on UI scroll.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to UI\r\n2. Click on `my_table`\r\n3. Run this SQL with table name (e.g. `my_table`)\r\n4. Scroll to the bottom and you will see that the last row is different form the last raw using `SELECT * FROM my_table LIMIT -1`\r\n\r\n**Expected behavior**\r\nI'd expect to see it. It seems omitted.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux\r\n - Browser: Safari\r\n - Version: `questdb:6.0.0.b2`\r\n \r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "negative",
    "word_count": 93,
    "metadata": {
      "issue_number": 975,
      "state": "closed",
      "labels": [
        "Bug",
        "UI",
        "hacktoberfest"
      ],
      "comments_count": 7,
      "created_at": "2021-05-05T15:48:10Z",
      "updated_at": "2021-10-14T16:59:52Z",
      "closed_at": "2021-10-14T14:30:04Z",
      "author": "newskooler",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": true
    }
  },
  {
    "id": "questdb-github_issue-bd2f4a0702e5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/853",
    "title": "All contributors edit",
    "text": "# All contributors edit\n\nWe had some merge conflicts by adding too quickly to our [all contributors megathread](https://github.com/questdb/questdb/issues/252) so this is a temp ticket for adding you guys in one go! ðŸ˜‚ \r\n\r\nThanks again for the contributions!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 37,
    "metadata": {
      "issue_number": 853,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2021-03-11T14:41:52Z",
      "updated_at": "2021-04-28T09:36:58Z",
      "closed_at": "2021-03-11T14:46:54Z",
      "author": "bsmth",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-da6844437878",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/593",
    "title": "SQL for bulk insert",
    "text": "# SQL for bulk insert\n\nsupport syntax for insert of multiple rows at once:\r\n\r\n```sql\r\nINSERT INTO table_name (column_list)\r\nVALUES\r\n    (value_list_1),\r\n    (value_list_2),\r\n    ...\r\n    (value_list_n);\r\n```\r\n\r\n## Unknowns\r\n\r\nInvestigate if PostgreSQL supports bind variables in statement like that.\r\n\r\n## Implementation\r\n\r\nIt is very likely this insert will need to be executed _without_ creating intermediate model. The existing QuestDb's implementation of Insert is also deficient and this is an opportunity to prove it.\r\n\r\n## Considerations\r\n\r\n- insert should convert constant types similarly to how BindVariableService does. In that it should be possible to insert `null` into primitive types, such as `int`, `long` etc. It should also be possible to insert  constant `int` into `short` and `byte` column, numeric value into `char` column etc.\r\n\r\n- The implementation should be efficient and performant for frequent execution of insert statements\r\n\r\n- The implementation must not generate grabage",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 142,
    "metadata": {
      "issue_number": 593,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2020-09-08T12:47:59Z",
      "updated_at": "2022-05-25T06:46:17Z",
      "closed_at": "2021-11-24T17:07:00Z",
      "author": "bluestreak01",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-6d7e69b4f94e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/261",
    "title": "Apache Arrow in-memory format(data structure conversion or native table 'arrow' type)",
    "text": "# Apache Arrow in-memory format(data structure conversion or native table 'arrow' type)\n\n**Is your feature request related to a problem? Please describe.**\r\nTo support large amount of existing analytical, ML and big data solutions data interchange please consider support of Apache Arrow in-memory data layout format.\r\n\r\n**Describe the solution you'd like**\r\nDatabase should reuse existing ML, statistical data processing , GPGPU analytics function acceleration. Also this in-memory format support read/write arrow buffers into/from [parquet](https://github.com/apache/arrow/blob/master/c_glib/parquet-glib/arrow-file-reader.h)/[orc](https://github.com/apache/arrow/blob/master/c_glib/arrow-glib/orc-file-reader.h) columnar format.\r\n\r\n**Describe alternatives you've considered**\r\nImplement all required function in QuestDB. Write custom adapter tailored for Apache Spark integration. \r\n\r\n**Additional context**\r\nhttp://gpuopenanalytics.com/\r\nhttps://rapids.ai/\r\nhttps://arrow.apache.org/docs/format/Columnar.html\r\nhttps://github.com/apache/arrow\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 100,
    "metadata": {
      "issue_number": 261,
      "state": "open",
      "labels": [
        "New feature",
        "Compatibility"
      ],
      "comments_count": 7,
      "created_at": "2020-05-01T16:10:02Z",
      "updated_at": "2022-08-25T11:27:28Z",
      "closed_at": null,
      "author": "igor-suhorukov",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7e857ab264ef",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/219",
    "title": "User Defined Functions and Triggers",
    "text": "# User Defined Functions and Triggers\n\nIs it possible to have a framework to write user-defined functions and triggers?\r\n\r\nThis also will be useful for building pipelines in: https://github.com/questdb/questdb/issues/218. \r\n\r\nSome UDF can also be repurposed to write data sinks.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 39,
    "metadata": {
      "issue_number": 219,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 7,
      "created_at": "2020-04-24T04:42:04Z",
      "updated_at": "2026-02-04T20:18:17Z",
      "closed_at": null,
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-bec1779835db",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/165",
    "title": "Continuous Queries + Reactive Queries",
    "text": "# Continuous Queries + Reactive Queries\n\n**Is your feature request related to a problem? Please describe.**\r\n\r\nMuch of current systems have to deal with reactive data and data which continuously change. Currently one has to poll to see data changes and update.\r\n\r\n**Describe the solution you'd like**\r\n\r\nMake the DB push the changes and new data to the clients.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- https://rethinkdb.com/\r\n- https://ksqldb.io/\r\n- https://github.com/pipelinedb/pipelinedb\r\n- https://geode.apache.org/\r\n- https://crate.io/\r\n- https://ignite.apache.org/\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 75,
    "metadata": {
      "issue_number": 165,
      "state": "closed",
      "labels": [
        "New feature",
        "SQL"
      ],
      "comments_count": 7,
      "created_at": "2020-04-18T07:31:07Z",
      "updated_at": "2022-09-17T22:10:01Z",
      "closed_at": "2021-04-20T11:02:53Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-7ae9ecb17dea",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/163",
    "title": "Add Update and Delete",
    "text": "# Add Update and Delete\n\n**Is your feature request related to a problem? Please describe.**\r\n\r\nEven when dealing with time-series data, not all data will be TS data unless is very specific domains and application. Instead of using another DB for regular data, it will be good if you can UPDATE and DELETE data. Maybe you can specify certain tables as append-only which will further optimise these tables.\r\n\r\n**Describe the solution you'd like**\r\n\r\nSupport TS tables and regular tables with UPDATE and DELETE.\r\n\r\nThis way you can use 1 DB for all your needs than different DBs as speciality solution when you need TS capabilities.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n[Apache Ignite](https://ignite.apache.org/), [CrateDB](https://crate.io/) has the ability to deal with time series and regular data.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 123,
    "metadata": {
      "issue_number": 163,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2020-04-18T07:04:15Z",
      "updated_at": "2021-11-14T21:32:45Z",
      "closed_at": "2021-08-30T16:32:07Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-3f1a25dbfbbf",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/29",
    "title": "Advise on building a fail-over solution on top of NFSdb (with two+ nodes)",
    "text": "# Advise on building a fail-over solution on top of NFSdb (with two+ nodes)\n\nHello:\nI've been testing the replication options available in NFSdb.\n- I like the benchmark numbers I see on NFSdb\n- I like the fact that I can setup a client to replicate the data onto one or many nodes\n\nIn order to use NFSdb in a production environment, fail-over and data-redundancy becomes an question.\n\nIs there any plans to implement a fail-over implementation or sample code/pseudo-code that can be shared that will help me in building a fail-over solution on top of NFSdb?\n\nScenario:\n- {Node1} has a server running NFSdb\n- {Node2} is running a client replicating the database\n- Node1 goes down, I need Node2 to switch from a client mode to a server mode.\n- More data is appended into Node2 and at some later point of time Node1 is started\n- Node1 needs to be aware that there is already a master running on Node2 and join it as a client (instead of a server)\n\nIs this possible using the (multicast) foundation in NFSdb.\n\nAny advise on how this can be achieved?\n\nThank you and looking forward to your feedback.\n\nVenkatt\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 200,
    "metadata": {
      "issue_number": 29,
      "state": "closed",
      "labels": [
        "New feature"
      ],
      "comments_count": 7,
      "created_at": "2015-01-12T20:40:35Z",
      "updated_at": "2015-02-15T05:02:31Z",
      "closed_at": "2015-02-15T05:02:31Z",
      "author": "vguhesan",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-305dbe9c891c",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/18",
    "title": "Lazy / Cached Spill Over to Disk",
    "text": "# Lazy / Cached Spill Over to Disk\n\nWhen reliability can be traded for speed it might be good to have an option to have a delayed spill over to disk and use in memory caching to achieve more speed. \n\nFor the example given, I about 15m to 30m objects per second is achievable for the given example, if not more.\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 61,
    "metadata": {
      "issue_number": 18,
      "state": "closed",
      "labels": [],
      "comments_count": 7,
      "created_at": "2014-09-03T05:24:45Z",
      "updated_at": "2014-09-04T14:26:19Z",
      "closed_at": "2014-09-04T13:02:32Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-29d4d3aab93e",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/14",
    "title": "Messaging Patterns",
    "text": "# Messaging Patterns\n\nIs it possible to have messaging pattern abstraction implementation on top of NFSdb?\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 16,
    "metadata": {
      "issue_number": 14,
      "state": "closed",
      "labels": [
        "Question"
      ],
      "comments_count": 7,
      "created_at": "2014-08-29T16:52:34Z",
      "updated_at": "2014-10-13T01:39:43Z",
      "closed_at": "2014-10-13T01:39:43Z",
      "author": "sirinath",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  },
  {
    "id": "questdb-github_issue-09eadeee7ae5",
    "origin": "questdb",
    "source_type": "github_issue",
    "url": "https://github.com/questdb/questdb/issues/6631",
    "title": "cairo.max.commit.rows",
    "text": "# cairo.max.commit.rows\n\n### Is your feature request related to a problem?\n\nYes heavy WALs\n\n\n### Describe the solution you'd like.\n\nWas wondering if there has been any discussion around having a configuration that would let users tell the database the largest number of rows it is allowed to commit at a time.\n\nThe idea here being if I am replaying historical data coming from disparate sources and possibly a mix of timestamps I could set as an example\n\n```\ncairo.max.commit.rows:      10,000\ncairo.max.uncommitted.rows: 100,000\ncairo.o3.max.lag:           60s\n```\n\nIf time is exceeded 10k is commited 90k survive in the o3 buffers, same could happen if max uncommitted is reached (only 10k is drained). This could help reduce the write amplification because one would in effect be allowing the oldest rows to bubble to the front and only that smaller amount gets flushed and newer rows continue to survive.\n\n### Describe alternatives you've considered.\n\nPushing some of this work into the clients\n\n### Full Name:\n\nErick Arce\n\n### Affiliation:\n\nClear Street\n\n### Additional context\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 173,
    "metadata": {
      "issue_number": 6631,
      "state": "open",
      "labels": [
        "New feature"
      ],
      "comments_count": 6,
      "created_at": "2026-01-13T04:03:23Z",
      "updated_at": "2026-01-15T12:55:39Z",
      "closed_at": null,
      "author": "earce-clearstreet",
      "top_comments": [],
      "is_feature_request": false,
      "is_bug": false
    }
  }
]