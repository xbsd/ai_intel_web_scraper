[
  {
    "id": "questdb-official_docs-ee5158df020b",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs",
    "title": "Introduction | QuestDB",
    "text": "QuestDB is an open source time-series database engineered for low latency. It uses a column-oriented, time-partitioned storage engine with memory-mapped files and vectorized (SIMD) execution to support high-throughput ingestion and millisecond-level analytical queries. The system is built from scratch with a zero-GC Java core and focused C++/Rust components, in a compact codebase optimized for cache locality and predictable tail latency. SQL is extended with time-series operators such as\nSAMPLE BY\n,\nLATEST ON\n,\nASOF JOIN\n, and\nWINDOW JOIN\n. See\nArchitecture\nfor details.\nQuick start\nLive demo\nTest your skills\n\n## About this documentation‚Äã\n\nThis documentation covers both\nQuestDB Open Source\nand\nQuestDB Enterprise\n.\nQuestDB Enterprise builds on top of QuestDB Open Source, using it as its core\nlibrary. Everything in open source works in Enterprise, but not the other way\naround. Enterprise adds features like high availability, advanced security, RBAC,\nautomated backups, and multi-tier storage with seamless object storage integration.\n\n## Get started‚Äã\n\n- Quick start- Install and run QuestDB\n- Schema design- Design your tables\n- Ingest data- Bring your data using QuestDB clients\n- Query data- Analyze with SQL\n\n## Guides‚Äã\n\n\n### Create database\n\nSet up your first QuestDB database and start storing time-series data.\nRead more\n\n### Capacity planning\n\nSelect a storage medium, plan, size and compress your QuestDB deployment.\nRead more\n\n### Working with time\n\nIt's about time. Learn how to work with timestamps and timezones in QuestDB.\nRead more\n\n### Backup and restore\n\nSee the methods to backup and restore your QuestDB deployment.\nRead more\n\n## Resources‚Äã\n\n\n### Query overview\n\nLearn about our powerful extended SQL and how to use it to query QuestDB.\n\n### Language clients\n\nExplore our language clients and how to use them to ingest data into QuestDB.\n\n### Configuration\n\nSee all of our available configuration options and fine-tune to match your use case.\n\n### Third-Party Tools\n\nOur recommended third-party tools can aid you in analyzing and visualizing your data.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 322,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-d3696d6e8ad0",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/getting-started/create-database",
    "title": "Create a sample database | QuestDB",
    "text": "This guide walks you through creating a sample dataset.\nIt utilizes\nrnd_\nfunctions and basic SQL grammar to generate 'mock' data of\nspecific types.\nFor most applications, you will import your data using methods like the InfluxDB\nLine Protocol, CSV imports, or integration with third-party tools such as\nTelegraf,\nKafka\n, or Prometheus. If your interest lies in data ingestion rather\nthan generation, refer to our\ningestion overview\n.\nAlternatively, the\nQuestDB demo instance\noffers a\npractical way to explore data creation and manipulation without setting up your\ndataset.\nAll that said, in this tutorial you will learn how to:\n- Create tables\n- Populate tables with sample data\n- Run simple and advanced queries\n- Delete tables\n\n### Before we begin...‚Äã\n\nAll commands are run through the\nWeb Console\naccessible at\nhttp://localhost:9000\n.\nYou can also run the same SQL via the\nPostgres endpoint\nor the\nREST API\n.\nIf QuestDB is not running locally, checkout the\nquick start\n.\n\n### Creating a table‚Äã\n\nWith QuestDB running, the first step is to create a table.\nWe'll start with one representing financial market data. Then in the insert\nsection, we'll create another pair of tables representing temperature sensors\nand their readings.\nLet's start by creating the\ntrades\ntable:\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL,    side SYMBOL,    price DOUBLE,    amount DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAYDEDUP UPSERT KEYS(timestamp, symbol);\n```\n\nThis table uses QuestDB's key time-series features:\n- TIMESTAMP(timestamp)‚Äî Designates the time column. QuestDB physically\nsorts data by this column, enabling sub-millisecond time-range queries.\n- PARTITION BY DAY‚Äî Splits data into daily partitions for efficient\nqueries and data lifecycle management.\n- SYMBOL‚Äî Optimized type for repeated strings like tickers.\n- DEDUP UPSERT KEYS‚Äî Prevents duplicate rows.\nFor a deeper understanding, see\nSchema design\n.\nWe've done all of this to match the nature of how we'll query this data. We're\nfocused on a the flow of the market, the pulse of the market's day-to-day, hence\nwe've partitioned it as such. We're also leery of duplicates, for accuracy of\ndata, so we'll ensure that if timestamps are identical that we do not create a\nduplicate. Timestamps are essential for time-series analysis.\nWe'll proceed forward to INSERT.\n\n### Inserting data‚Äã\n\n\n#### Financial market data‚Äã\n\nLet's populate our\ntrades\ntable with procedurally-generated data:\nInsert as SELECT\n\n```questdb-sql\nINSERT INTO trades    SELECT        timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024        rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols        rnd_str('buy', 'sell') side, -- Random side (BUY or SELL)        rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0,        rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1    FROM long_sequence(10000) x;\n```\n\nOur\ntrades\ntable now contains 10,000 randomly-generated trades. The\ncomments indicate how we've structured our random data. We picked a few\ncompanies, BUY vs. SELL, and created a timestamp every minute. We've dictated\nthe overall number of rows generated via\nlong_sequence(10000)\n. We can bump\nthat up, if we want.\nWe've also conservatively generated a timestamp per minute, even though in\nreality trades against these companies are likely much more frequent. This helps\nkeep our basic examples basic.\nNow let's look at the table and its data:\n\n```questdb-sql\n'trades';\n```\n\nIt will look similar to this, albeit with alternative randomized values.\n\n| timestamp | symbol | side | price | amount |\n| --- | --- | --- | --- | --- |\n| 2024-01-01T00:00:00.000000Z | BTC-USD | sell | 483.904143675277 | 139.449481016294 |\n| 2024-01-01T00:00:00.060000Z | ETH-USD | sell | 920.296245196274 | 920.296245196274 |\n| 2024-01-01T00:00:00.180000Z | UNI-USD | sell | 643.277468441839 | 643.277468441839 |\n| 2024-01-01T00:00:00.360000Z | LTC-USD | buy | 218.0920768859 | 729.81119178972 |\n| 2024-01-01T00:00:00.600000Z | BTC-USD | sell | 157.596416931116 | 691.081778396176 |\n\nThat's some fake market data. Let's create more tables to demonstrate joins.\n\n### Quotes and instruments‚Äã\n\nThis next example will create and populate two more tables. One table will\ncontain price quotes, and the other will contain instrument metadata. In both\ncases, we will create the table and generate the data at the same time.\nThis combines the CREATE & SELECT operations to perform a create-and-insert:\nCreate table as, quotes\n\n```questdb-sql\nCREATE TABLE quotesAS(    SELECT        x ID,        timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts,        rnd_double(0)*80 + 100 price,        rnd_long(0, 10000, 0) instrument_id    FROM long_sequence(10000000) x)TIMESTAMP(ts)PARTITION BY MONTH DEDUP UPSERT KEYS(ts, instrument_id);\n```\n\nThis table uses the same time-series features:\n- TIMESTAMP(ts)‚Äî Designates the time column for fast time-range queries.\n- PARTITION BY MONTH‚Äî Monthly partitions (use larger partitions for\nlower-volume data).\n- DEDUP UPSERT KEYS(ts, instrument_id)‚Äî One quote per timestamp per\ninstrument.\nThe generated data will look like the following:\n\n| ID | ts | price | instrument_id |\n| --- | --- | --- | --- |\n| 1 | 2019-10-17T00:00:00.000000Z | 145.37 | 9160 |\n| 2 | 2019-10-17T00:00:00.600000Z | 162.91 | 9671 |\n| 3 | 2019-10-17T00:00:01.400000Z | 128.58 | 8731 |\n| 4 | 2019-10-17T00:00:01.500000Z | 131.69 | 3447 |\n| 5 | 2019-10-17T00:00:01.600000Z | 155.68 | 7985 |\n| ... | ... | ... | ... |\n\nNice - and our next table, which includes the instruments themselves and their\ndetail:\nCreate table as, instruments\n\n```questdb-sql\nCREATE TABLE instrumentsAS(    SELECT        x ID, -- Increasing integer        rnd_str('NYSE', 'NASDAQ', 'LSE', 'TSE', 'HKEX') exchange, -- Random exchange        rnd_str('Tech', 'Finance', 'Energy', 'Healthcare', 'Consumer') sector -- Random sector    FROM long_sequence(10000) x)\n```\n\nThis\ninstruments\ntable has no designated timestamp ‚Äî it's a static lookup\ntable with no time dimension. This is the exception; most QuestDB tables should\nhave a designated timestamp to enable time-series optimizations.\nWith these two new tables, and our prior financial market data table, we've got\na lot of useful queries we can test.\n\n### Running queries‚Äã\n\nOur financial market data table is a great place to test various\naggregate functions\n, to compute price\nover time intervals, and similar analysis.\nLet's expand on the\nquotes\nand\ninstruments\ntables.\nFirst, let's look at\nquotes\n, running our shorthand for\nSELECT * FROM quotes;\n:\n\n```questdb-sql\nquotes;\n```\n\nLet's then select the\ncount\nof records from\nquotes\n:\n\n```questdb-sql\nSELECT count() FROM quotes;\n```\n\n\n| count |\n| --- |\n| 10,000,000 |\n\nAnd then the average price:\n\n```questdb-sql\nSELECT avg(price) FROM quotes;\n```\n\n\n| average |\n| --- |\n| 139.99217780895 |\n\nWe can now use the\ninstruments\ntable alongside the\nquotes\ntable to get more\ninteresting results using a\nJOIN\n:\n\n```questdb-sql\nSELECT *FROM quotesJOIN(    SELECT ID inst_id, exchange, sector    FROM instruments)ON quotes.instrument_id = inst_id;\n```\n\nThe results should look like the table below:\n\n| ID | ts | price | instrument_id | inst_id | exchange | sector |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | 2019-10-17T00:00:00.000000Z | 146.47 | 3211 | 3211 | LSE | Tech |\n| 2 | 2019-10-17T00:00:00.100000Z | 136.59 | 2319 | 2319 | NASDAQ | Finance |\n| 3 | 2019-10-17T00:00:00.100000Z | 160.29 | 8723 | 8723 | NYSE | Tech |\n| 4 | 2019-10-17T00:00:00.100000Z | 170.94 | 885 | 885 | HKEX | Healthcare |\n| 5 | 2019-10-17T00:00:00.200000Z | 149.34 | 3200 | 3200 | NASDAQ | Energy |\n| 6 | 2019-10-17T00:00:01.100000Z | 160.95 | 4053 | 4053 | TSE | Consumer |\n\nNote the timestamps returned as we've JOIN'd the tables together.\nLet's try another type of aggregation:\nAggregation keyed by sector\n\n```questdb-sql\nSELECT sector, max(price)FROM quotesJOIN(    SELECT ID inst_id, sector    FROM instruments) aON quotes.instrument_id = a.inst_id;\n```\n\nThe results should look like the table below:\n\n| sector | max |\n| --- | --- |\n| Tech | 179.99998786398 |\n| Finance | 179.99998138348 |\n| Energy | 179.9999994818 |\n| Healthcare | 179.99991705861 |\n| Consumer | 179.99999233377 |\n\nBack to time, given we have one table (\nquotes\n) partitioned by time, let's\nsee what we can do when we JOIN the tables together to perform an aggregation\nbased on an hour of time:\nAggregation by hourly time buckets\n\n```questdb-sql\nSELECT ts, sector, exchange, avg(price)FROM quotes timestamp(ts)JOIN    (SELECT ID inst_id, sector, exchange    FROM instruments    WHERE sector='Tech' AND exchange='NYSE') aON quotes.instrument_id = a.inst_idWHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next daySAMPLE BY 1h -- aggregation by hourly time bucketsALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n```\n\nThe results should look like the table below:\n\n| ts | sector | exchange | average |\n| --- | --- | --- | --- |\n| 2019-10-21T00:00:00.000000Z | Tech | NYSE | 140.004285872 |\n| 2019-10-21T00:01:00.000000Z | Tech | NYSE | 136.68436714 |\n| 2019-10-21T00:02:00.000000Z | Tech | NYSE | 135.24368409 |\n| 2019-10-21T00:03:00.000000Z | Tech | NYSE | 137.19398410 |\n| 2019-10-21T00:04:00.000000Z | Tech | NYSE | 150.77868682 |\n| ... | ... | ... | ... |\n\nFor more information about these statements, please refer to the\nSELECT\n,\nJOIN\nand\nSAMPLE BY\npages.\n\n### Deleting tables‚Äã\n\nWe can now clean up the demo data by using\nDROP TABLE\nSQL. Be careful using this statement\nas QuestDB cannot recover data that is deleted in this way:\n\n```questdb-sql\nDROP TABLE quotes;DROP TABLE instruments;DROP TABLE trades;\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1536,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-5f68e54c889c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/getting-started/quick-start",
    "title": "Quick start | QuestDB",
    "text": "This guide will get your first QuestDB instance running.\nAs the goal is to, well,\nstart quickly\n, we'll presume defaults.\nOnce running, we'll provide guides for inserting data, configuration and\nproduction hosting.\nQuestDB already running?Jump ahead!Select a first-party client or ingest method.\n\n## Install QuestDB‚Äã\n\nChoose from the following options:\n- Docker\n- Homebrew\n- Binaries\n\n### Docker‚Äã\n\nTo use Docker, one must have Docker. You can find installation guides for your\nplatform on the\nofficial documentation\n.\nOnce Docker is installed, pull and run QuestDB:\n\n```shell\ndocker run -p 9000:9000 -p 8812:8812 -p 9003:9003 questdb/questdb:9.3.2\n```\n\nThis exposes the Web Console (9000), PostgreSQL wire (8812), and health endpoint (9003).\nFor deeper instructions, see the\nDocker deployment guide\n.\n\n### Homebrew‚Äã\n\nTo install QuestDB via\nHomebrew\n, run the following command:\n\n```shell\nbrew install questdb\n```\n\nOn macOS, the location of the root directory of QuestDB and\nserver configuration\nfiles depend on the chip:\n- Apple Silicon (M1/M2/M*) chip:/opt/homebrew/var/questdb\n- Intel chip:/usr/local/var/questdb\n\n### Binaries‚Äã\n\nDownload and run QuestDB via binaries.\nSelect your platform of choice:\n- Linux\n- Windows\n- Any (no JVM)\nDownload the binary:\nquestdb-9.3.2-rt-linux-x86-64.tar.gz\nNext, unpack it:\n\n```shell\ntar -xvf questdb-9.3.2-rt-linux-x86-64.tar.gz\n```\n\nThe default directory becomes:\n\n```shell\n$HOME/.questdb\n```\n\nDownload the executable:\nquestdb-9.3.2-rt-windows-x86-64.tar.gz\nThe default root directory becomes:\n\n```shell\nC:\\Windows\\System32\\qdbroot\n```\n\nDownload the binary:\nquestdb-9.3.2-no-jre-bin.tar.gz\nThis package does not embed Java.\nUse this if there is no package for your platform, such as ARM Linux.\nRequires local Java 17.\nTo check your installed version:\n\n```shell\njava -version\n```\n\nIf you do not have Java, install one of the following:\n- AdoptOpenJDK\n- Amazon Corretto\n- OpenJDK\n- Oracle Java\nOther Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible.\nFor environment variable, point\nJAVA_HOME\nto your Java 17 installation folder.\n\n## Run QuestDB‚Äã\n\n- Linux/No JVM\n- macOS (Homebrew)\n- Windows\n\n```shell\n./questdb.sh start\n```\n\nTo stop:\n./questdb.sh stop\n| To check status:\n./questdb.sh status\nAll options./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]OptionDescription-dExpects adirdirectory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see thedefault rootsection below.-tExpects atagstring value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will bequestdb.-fForce re-deploying theWeb Console. Without this option, theWeb Consoleis cached and deployed only when missing.-nDo not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it.\n\n```shell\nquestdb start\n```\n\nTo stop:\nquestdb stop\n| To check status:\nquestdb status\nAll optionsquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]OptionDescription-dExpects adirdirectory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see thedefault rootsection below.-tExpects atagstring value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will bequestdb.-fForce re-deploying theWeb Console. Without this option, theWeb Consoleis cached and deployed only when missing.-nDo not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it.\n\n```shell\nquestdb.exe start\n```\n\nTo stop:\nquestdb.exe stop\n| To check status:\nquestdb.exe status\nAll optionsquestdb.exe [start|stop|status|install|remove] \\[-d dir] [-f] [-j JAVA_HOME] [-t tag]OptionDescriptioninstallInstalls the Windows QuestDB service. The service will start automatically at startup.removeRemoves the Windows QuestDB service. It will no longer start at startup.-dExpects adirdirectory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see thedefault rootsection below.-tExpects atagstring value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will bequestdb.-fForce re-deploying theWeb Console. Without this option, theWeb Consoleis cached and deployed only when missing.-jWindows only!This option allows to specify a path toJAVA_HOME.\n\n## Verify installation‚Äã\n\nOpen\nhttp://localhost:9000\nin your browser.\nYou should see the QuestDB\nWeb Console\n.\nTry running your first query in the SQL editor:\n\n```questdb-sql\nCREATE TABLE test (ts TIMESTAMP, val DOUBLE) TIMESTAMP(ts);INSERT INTO test VALUES (now(), 42.5);SELECT * FROM test;\n```\n\nThe\nTIMESTAMP(ts)\nclause makes this a time-series table ‚Äî it's what enables\nQuestDB's sub-millisecond time-range queries. See\nSchema design\nto understand this and other\nkey concepts.\nIt works? You're ready to bring your data.\nDefault ports:\n\n| Port | Service |\n| --- | --- |\n| 9000 | REST APIandWeb Console |\n| 9009 | InfluxDB Line Protocol (ILP)- Legacy TCP, use HTTP instead |\n| 8812 | PostgreSQL Wire Protocol |\n| 9003 | Health endpoint |\n\n\n## Bring your data‚Äã\n\nNow... Time to really blast-off. üöÄ\nNext up: Bring your data - the\nlife blood\nof any database.\nChoose from one of our premium ingest-only language clients:\n\n### C & C++\n\nHigh-performance client for systems programming and embedded applications.\nRead more\n\n### .NET\n\nCross-platform client for building applications with .NET technologies.\nRead more\n\n### Go\n\nAn open-source programming language supported by Google with built-in concurrency.\nRead more\n\n### Java\n\nPlatform-independent client for enterprise applications and Android development.\nRead more\n\n### Node.js\n\nNode.js¬Æ is an open-source, cross-platform JavaScript runtime environment.\nRead more\n\n### Python\n\nPython is a programming language that lets you work quickly and integrate systems more effectively.\nRead more\n\n### Rust\n\nSystems programming language focused on safety, speed, and concurrency.\nRead more\nWant more options? See theingestion overview.\n\n### Create new data‚Äã\n\nNo data yet and still want to trial QuestDB?\nThere are several quick options:\n- QuestDB demo instance: Hosted, fully loaded and\nready to go. Quickly explore the Web Console and SQL syntax.\n- Create my first data set guide: create\ntables, usernd_functions and make your own data.\n- Sample dataset repos: IoT,\ne-commerce, finance or git logs? Check them out!\n- Quick start repos:\nCode-based quick starts that cover ingestion, querying and data visualization\nusing common programming languages and use cases. Also, a cat in a tracksuit.\n- Time series streaming analytics template:\nA handy template for near real-time analytics using open source technologies.\n\n## Learn QuestDB‚Äã\n\nNew to QuestDB? Start here:\n- Schema design‚Äî Understand designated\ntimestamps, partitions, and SYMBOL types. These are the foundations of fast\ntime-series queries.\nFor production deployments:\n- Capacity planning‚Äî Recommended\nconfigurations for production\n- Configuration‚Äî All server.conf options\n- Visualize with Grafana‚Äî Create\ndashboards from your data (example)",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1090,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-e558badc20b1",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/timestamps-timezones",
    "title": "Working with time zones | QuestDB",
    "text": "QuestDB stores all timestamps in UTC without time zone information. To query\ndata at your local time, use\nTICK syntax\n. To\ndisplay results in local time, use conversion functions.\nKey Points\n- All timestamps are stored in UTC ‚Äî no time zone information is preserved\n- UseTICK syntaxwith@timezoneto query data at your local time\n- Prefer full time zone names (America/New_York) over abbreviations (EST)\n- Useto_timezone()only when displaying local time in results\n\n## How to refer to time zones‚Äã\n\nQuestDB uses the\nIANA tz database\n.\nSpecify time zones by geographic region or UTC offset:\n\n| Format | Example | Recommended? |\n| --- | --- | --- |\n| Geographic region | America/New_York | ‚úÖ Best |\n| UTC offset | +02:00,-05:00 | ‚úÖ Good |\n| Abbreviation | EST,CST | ‚ö†Ô∏è Avoid |\n\nAvoid abbreviations\n‚Äî the same abbreviation often maps to multiple time\nzones. For example,\nCST\ncould mean U.S. Central Standard Time or China\nStandard Time. QuestDB can only recognize one, leading to unexpected results.\nFor valid time zone names, see the\nIANA time zone database\n.\nnote\nThe tz database includes historic transitions. QuestDB applies the correct\noffset based on the timestamp value, accounting for historical daylight saving\ntime changes.\n\n## Querying by local time‚Äã\n\nYou're in New York and want trades from 9am your time. Use\nTICK syntax\nwith\n@timezone\n:\n\n```questdb-sql\nSELECT * FROM tradesWHERE ts IN '2024-01-15T09:00@America/New_York;1h';\n```\n\nTICK converts your local time to UTC intervals, enabling efficient\ninterval scans\n. More examples:\n\n```questdb-sql\n-- London business hours (09:00-17:00) for January workdaysSELECT * FROM tradesWHERE ts IN '2024-01-[01..31]T09:00@Europe/London#wd;8h';-- NYSE trading hours (09:30-16:00 Eastern)SELECT * FROM tradesWHERE ts IN '2024-01-[01..31]T09:30@America/New_York#wd;6h30m';-- Last 5 business days, Tokyo morning sessionSELECT * FROM tradesWHERE ts IN '[$today-5bd..$today-1bd]T09:00@Asia/Tokyo;2h30m';\n```\n\nTICK handles DST transitions automatically ‚Äî a 9 AM start time in New York\nmaps to different UTC times in winter vs summer.\n\n### Why TICK instead of conversion functions‚Äã\n\nTICK generates UTC intervals at query planning time, enabling binary search.\nConverting each row forces a full table scan:\n\n```questdb-sql\n-- Efficient: interval scan (sub-millisecond on billions of rows)WHERE ts IN '2024-01-[01..31]T09:00@Europe/London;8h'-- Inefficient: full table scan (must read every row)WHERE extract(hour FROM to_timezone(ts, 'Europe/London')) BETWEEN 9 AND 17\n```\n\n\n## Converting timestamps for display‚Äã\n\nWhen you need local time in query results (not filtering), use\nto_timezone()\n:\n\n```questdb-sql\nSELECT    to_timezone(ts, 'Europe/Berlin') as local_time,    symbol,    priceFROM tradesWHERE ts IN '2024-01-15';\n```\n\n\n| local_time | symbol | price |\n| --- | --- | --- |\n| 2024-01-15T10:30:00.000000Z | BTC-USD | 42000 |\n\n\n### to_utc() for ingestion‚Äã\n\nIf source data arrives in local time, convert to UTC before storing:\n\n```questdb-sql\nINSERT INTO tradesSELECT to_utc(local_ts, 'America/New_York'), symbol, priceFROM source_data;\n```\n\nThis ensures consistent ordering and avoids ambiguity during DST transitions.\n\n## See also‚Äã\n\n- TICK intervals‚Äî Complete@timezonesyntax reference\n- Designated timestamp‚Äî How timestamps define table structure\n- Date/time functions‚Äîto_timestamp(),to_utc(),to_timezone()",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 478,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-c562726abf0c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/overview",
    "title": "Third-Party Tools Overview | QuestDB",
    "text": "QuestDB integrates well with a range of third-party tools, offering\ncompatibility with systems for visualization, data ingestion, analytics, and\nmore.\n\n## Visualization Tools‚Äã\n\nInteract with and visualize your QuestDB data using these powerful visualization\nplatforms:\n- Grafana:Create stunning dashboards\nand interactive graphs fortime-series datavisualization.\n- Superset: Build interactive\nvisualizations and perform ad-hoc data analysis.\n- PowerBI:Create interactive data visualizations and dashboards.\n- qStudio: A free SQL GUI for query\nexecution, table browsing, and result charting.\n\n## Data Ingestion and Streaming‚Äã\n\nIngest, store, and process high-throughput and real-time data streams with these\nintegrations:\n- Apache Kafka:A distributed\nevent streaming platform for high-throughput data pipelines.\n- Telegraf: Collect and report metrics from\nvarious sources.\n- Redpanda:A Kafka-compatible streaming\nplatform for simplified data pipelines.\n- Apache Flink: Process real-time data streams\nefficiently.\ntasks at scale.\n\n## Workflow Orchestrators‚Äã\n\nAutomate your data pipelines with these workflow orchestrators:\n- Apache Airflow: A workflow automation tool for\nscheduling and monitoring tasks through directed acyclic graphs (DAGs).\n- Dagster: A modern workflow orchestrator for\ndata pipelines.\n\n## Analytics and Processing‚Äã\n\nEnhance your data analysis and processing capabilities with QuestDB through\nthese tools:\n- Pandas: Analyzetime-series datain Python\nwith powerful data structures.\n- MindsDB: Build machine learning models for\npredictive analytics ontime-series data.\n- Apache Spark: Handle complex data processing\n\n## Tooling and Interfaces‚Äã\n\nImprove your interactions with QuestDB using these tools and interfaces:\n- Prometheus:Efficiently store and\nanalyze monitoring metrics.\n- SQLAlchemy: Utilize Python's ORM\ncapabilities for database interactions.\nIs there an integration you'd like to see that's not listed? Let us know by\nopening an issue on\nQuestDB Github\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 264,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-b2fba54e67b5",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/operations/backup",
    "title": "Backup and restore | QuestDB",
    "text": "You should back up QuestDB to be prepared for the case where your original\ndatabase or data is lost, or if your database or table is corrupted. Backups are\nalso required to create\nreplica instances\nin\nQuestDB Enterprise.\n\n## Overview‚Äã\n\nQuestDB supports two backup methods:\n- Built-in incremental backup(Enterprise only): Fully automated‚Äîconfigure\nonce, set a schedule, and backups run automatically. Supports point-in-time\nrecovery to any backup timestamp.\n- Manual checkpoint backup(OSS and Enterprise): Relies on external tools to copy data. Requires manual\ncoordination:CHECKPOINT CREATE‚Üí copy data with external tools ‚ÜíCHECKPOINT RELEASE. Works well with cloud disk snapshots (AWS EBS, Azure\ndisks, etc.) where you simply trigger a snapshot. For on-premises environments\nwithout snapshot capabilities, you'll need external tools or custom scripts\n(e.g., rsync), which do not provide point-in-time recovery.\n\n## QuestDB Enterprise: built-in backup and restore‚Äã\n\nQuestDB Enterprise provides an incremental backup system that stores your data\nin object storage. Backups are incremental‚Äîonly changed data is uploaded‚Äîmaking\nthem fast and bandwidth-efficient. You can monitor progress and check for errors\nwhile backups run.\n\n### Prerequisites‚Äã\n\n\n#### License‚Äã\n\nBuilt-in backup requires QuestDB Enterprise. This feature is not available in\nthe open source version. See\nQuestDB Enterprise\nfor licensing information.\n\n#### Supported storage backends‚Äã\n\nBackup supports the following storage backends:\n- Amazon S3and S3-compatible storage (MinIO, etc.)\n- Azure Blob Storage\n- Google Cloud Storage (GCS)\n- Filesystem- Local or network-attached storage (NFS, etc.). Backup is not\nsensitive to the underlying filesystem type.\nSee\nConfigure object storage\nfor connection string formats.\n\n#### Permissions‚Äã\n\nThe backup process requires write access to the target storage. Authentication\nis optional‚Äîyou can use instance credentials (IAM roles, managed identities) or\nprovide explicit credentials in the connection string.\n\n#### Network‚Äã\n\nNetwork requirements depend on your chosen storage backend. Ensure QuestDB can\nreach the storage endpoint on the appropriate port (typically HTTPS/443 for\ncloud storage).\n\n#### Storage capacity‚Äã\n\nPlan your backup storage before starting. A safe estimate is\n2√ó your\nuncompressed database size\n. See\nEstimate backup storage\nfor detailed calculations.\n\n### Quick start‚Äã\n\nMinimal configuration to enable backups:\n\n```conf\nbackup.enabled=truebackup.object.store=s3::bucket=my-bucket;region=eu-west-1;access_key_id=...;secret_access_key=...;\n```\n\nThen run\nBACKUP DATABASE;\nin SQL. See\nRun a backup\nfor details.\n\n### Configure‚Äã\n\n\n#### Backup retention‚Äã\n\nControl how many backups to keep before automatic cleanup removes older ones:\n\n```conf\nbackup.cleanup.keep.latest.n=7\n```\n\n\n#### Filesystem backups‚Äã\n\nFor local testing or air-gapped environments, you can back up to a local\nfilesystem path instead of cloud object storage:\n\n```conf\nbackup.object.store=fs::root=/mnt/backups;atomic_write_dir=/mnt/backups/atomic;\n```\n\nThe\natomic_write_dir\nparameter is required for filesystem backends and\nspecifies a directory for atomic write operations during backup.\n\n#### Configuration reference‚Äã\n\n\n| Property | Description | Default |\n| --- | --- | --- |\n| backup.enabled | Enable backup functionality | false |\n| backup.object.store | Object store connection string | None (required) |\n| backup.schedule.cron | Cron expression forscheduled backups | None (manual only) |\n| backup.schedule.tz | IANA timezonefor cronschedule | UTC |\n| backup.cleanup.keep.latest.n | Number of backups to retain | 5 |\n| backup.compression.level | Compression level (1-22) | 5 |\n| backup.compression.threads | Threads for compression | CPU count |\n| backup.enable.partition.hashes | Compute BLAKE3 hashes during backup | false |\n| backup.verify.partition.hashes | Verify hashes during restore | false |\n\n\n### Run a backup‚Äã\n\nOnce configured, you can run a backup at any time using the following command:\nBackup database\n\n```questdb-sql\nBACKUP DATABASE;\n```\n\nExample output:\n\n| backup_timestamp |\n| --- |\n| 2024-08-24T12:34:56.789123Z |\n\nThe backup captures the committed database state at the moment the command\nexecutes. In-flight transactions are not included.\n\n### Monitor and abort‚Äã\n\nYou can monitor backup progress and history using the\nbackups()\ntable function:\nBackup history\n\n```questdb-sql\nSELECT * FROM backups();\n```\n\nExample output:\n\n| status | progress_percent | start_ts | end_ts | backup_error | cleanup_error |\n| --- | --- | --- | --- | --- | --- |\n| backup complete | 100 | 2025-07-30T12:49:30.554262Z | 2025-07-30T16:19:48.554262Z |  |  |\n| backup complete | 100 | 2025-08-06T14:15:22.882130Z | 2025-08-06T17:09:57.882130Z |  |  |\n| backup failed | 35 | 2025-08-20T11:58:03.675219Z | 2025-08-20T12:14:07.675219Z | connection error |  |\n| backup in progress | 10 | 2025-08-27T15:42:18.281907Z |  |  |  |\n| cleanup in progress | 100 | 2025-08-13T13:37:41.103729Z | 2025-08-13T16:44:25.103729Z |  |  |\n\nStatus values:\n\n| Status | Meaning | Action |\n| --- | --- | --- |\n| backup in progress | Backup is currently running | Wait or runBACKUP ABORT |\n| backup complete | Backup finished successfully | None required |\n| backup failed | Backup encountered an error | Checkbackup_errorcolumn |\n| cleanup in progress | Old backup data is being removed | Wait for completion |\n| cleanup complete | Cleanup finished successfully | None required |\n| cleanup failed | Cleanup encountered an error | Checkcleanup_errorcolumn |\n\nTo abort a running backup:\nAbort backup\n\n```questdb-sql\nBACKUP ABORT;\n```\n\n\n### Scheduled backups‚Äã\n\nYou can configure automatic scheduled backups using cron syntax. The example\nbelow runs a backup every day at midnight UTC.\n\n```conf\nbackup.schedule.cron=0 0 * * *backup.schedule.tz=UTC\n```\n\n\n#### Cron format‚Äã\n\nQuestDB uses the standard\n5-field cron format\n:\n\n```text\n              FIELD            VALUES             SPECIAL CHARS‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0-59 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ * , - /‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0-23 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ * , - /‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of month ‚îÄ‚îÄ‚îÄ 1-31 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ * , - / L W‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1-12 or JAN-DEC ‚îÄ‚îÄ * , - /‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ day of week ‚îÄ‚îÄ‚îÄ‚îÄ 0-7 or SUN-SAT ‚îÄ‚îÄ‚îÄ * , - / L #‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ* * * * *\n```\n\nSpecial character meanings:\n- *‚Äî matches any value\n- ,‚Äî separates multiple values (e.g.,1,15for 1st and 15th)\n- -‚Äî defines a range (e.g.,1-5for Monday through Friday)\n- /‚Äî specifies intervals (e.g.,*/15for every 15 units)\n- L‚Äî last day of the month, or last specific weekday (e.g.,5L= last Friday)\n- W‚Äî nearest weekday to the given day (e.g.,15W= nearest weekday to the 15th)\n- #‚Äî nth weekday of the month (e.g.,5#3= third Friday)\nFor day-of-week, 0 and 7 both represent Sunday; 1-6 represent Monday through Saturday.\ntip\nUse\ncrontab.guru\nto build and validate your cron expressions.\n\n#### Timezone‚Äã\n\nThe\nbackup.schedule.tz\nproperty accepts any valid\nIANA timezone name\n(e.g.,\nAmerica/New_York\n,\nEurope/London\n) or\nUTC\n.\nIf\nbackup.schedule.tz\nnot specified, the default is\nUTC\n.\n\n#### Resetting schedule without restart‚Äã\n\nThe\nbackup.schedule.cron\nand\nbackup.schedule.tz\nsettings can be modified in\nserver.conf\nand hot-reloaded without\nrestarting the server:\n\n```questdb-sql\nSELECT reload_config();\n```\n\nYou can also use this to enable and disable the schedule by adding or commenting out the\nbackup.schedule.cron\nconfig setting.\n\n### Backup instance name‚Äã\n\nEach QuestDB instance has a backup instance name (three random words like\ngentle-forest-orchid\n). This name is generated on the first backup and\norganizes backups in the object store under\nbackup/<backup_instance_name>/\n.\nTo find your instance name, run:\n\n```questdb-sql\nSELECT backup_instance_name;\n```\n\nReturns\nnull\nif no backup has been run yet.\n\n### Performance characteristics‚Äã\n\nBackup is designed to prioritize database availability over backup speed. Key\ncharacteristics:\n- Pressure-sensitive: Backup automatically throttles itself to avoid\noverwhelming the database instance during normal operations\n- Batch uploads: Data uploads in batches rather than continuously - you may\nsee surges of activity followed by quieter periods in logs\n- Compressed: Data is compressed before upload to reduce transfer time and\nstorage costs\n- Multi-threaded: Backup uses multiple threads but is deliberately\nthrottled to maintain instance reliability\nBackup duration depends on data size. Large databases (1TB+) may take several\nhours for a full initial backup. Subsequent incremental backups are faster as\nonly changed data is uploaded.\n\n### Estimate backup storage‚Äã\n\nA safe estimate for total backup storage is\n2√ó your uncompressed database\nsize on disk\n. This provides headroom for the full backup plus incremental\nhistory and edge cases.\n\n#### How storage accumulates‚Äã\n\n\n| Backup type | What's uploaded | Estimated size |\n| --- | --- | --- |\n| Initial (full) | Entire database | DB size √∑ 4 (default compression) |\n| Incremental | Changed partitions only | Changed data √∑ 4 |\n\nTotal storage = full backup + (average incremental √ó retention count)\nThe default compression level (5) achieves approximately 4√ó reduction. Higher\nbackup.compression.level\nvalues (up to 22) improve compression at the cost\nof CPU time.\n\n#### Partition-level granularity‚Äã\n\nPartitions are the smallest backup unit. Any modification to a partition‚Äîeven\na single row or column update‚Äîcauses the entire partition to be re-uploaded in\nthe next incremental backup.\nThis means:\n- Append-only workloads(typical time-series): Very efficient. Only the\nlatest partition changes between backups.\n- Cross-partition updates: Less efficient. AnUPDATEwithout a\nconstrainingWHEREclause touches all partitions, causing them all to be\nre-uploaded.\n- Schema changes: Column type changes cause affected partitions to be\nre-uploaded.\n\n#### Example calculation‚Äã\n\nA 500 GB database with daily backups, 7-day retention, and ~5% daily change:\n\n| Component | Calculation | Size |\n| --- | --- | --- |\n| Full backup | 500 GB √∑ 4 | 125 GB |\n| Daily incremental | 25 GB √∑ 4 | ~6 GB |\n| 7 incrementals | 6 GB √ó 7 | ~42 GB |\n| Total |  | ~170 GB |\n\nIn this example, actual usage (~170 GB) is well under the 2√ó planning estimate\n(1 TB). The 2√ó rule is intentionally conservative‚Äîuse it for initial capacity\nplanning before you know your actual change patterns, then refine based on\nobserved usage.\n\n#### Check actual usage‚Äã\n\nTo verify your estimates against actual storage, browse your backup data in\nthe object store. Backups are stored under\nbackup/<backup_instance_name>/\n.\nTo find your instance name, see\nBackup instance name\n.\n\n### Limitations‚Äã\n\n- Database-wide only: Backup captures the entire database. You cannot\nexclude tables or backup selected tables individually. Every backup includes\nall user tables, materialized views, and metadata.\n- One backup at a time: Only one backup can run at any given time. Starting\na new backup while one is running will return an error.\n- Primary and replica backups are separate: Each QuestDB instance has its\nownbackup_instance_name, so backing up both\na primary and its replica creates two separate backup sets in the object\nstore. Typically, backing up the primary is sufficient since replicas sync\nfrom the same data.\n\n### Backup validation‚Äã\n\nBackup integrity is verified during restore, not as a standalone operation.\n\n#### Verification during restore‚Äã\n\nQuestDB performs the following checks when restoring:\n- Transaction log verification: Header, hash, and size validation of\ntransaction log entries (always enabled)\n- Partition hash verification: Optional BLAKE3 hash comparison for each\nfile in every partition\n- Manifest validation: Version compatibility and path safety checks\nTo enable partition hash verification, set these properties in\nserver.conf\n:\n\n```conf\nbackup.enable.partition.hashes=true    # Compute hashes during backupbackup.verify.partition.hashes=true    # Verify hashes during restore\n```\n\nIf verification fails, restore stops immediately with an error such as:\nhash mismatch [path=col1.d, expected=..., actual=...]\n\n#### What's not available‚Äã\n\n- No standaloneVALIDATE BACKUPcommand\n- No dry-run restore option\n- Object store integrity relies on the storage provider (e.g., S3's built-in\nchecksums)\n\n### Restore‚Äã\n\nRestore is fast‚Äîapproximately 1.8 TiB can be restored in under 20 minutes,\ndepending on network bandwidth and storage performance.\ncaution\nEnterprise backup restore uses a different trigger file (\n_backup_restore\n) than\nOSS checkpoint restore (\n_restore\n). Do not confuse these two mechanisms.\nTo restore from an object store backup, create a\n_backup_restore\nfile in the\nQuestDB install root. This is a properties file with the object store\nconfiguration and optional selector fields. On startup, QuestDB reads this file,\nselects the requested backup timestamp (or the latest available), downloads the\nbackup data, and reconstructs the local database state.\n\n```conf\nbackup.object.store=s3::bucket=my-bucket;region=eu-west-1;access_key_id=...;secret_access_key=...;backup.instance.name=gentle-forest-orchidbackup.restore.timestamp=2024-08-24T12:34:56.789123Z\n```\n\nParameters:\n\n| Parameter | Required | Description |\n| --- | --- | --- |\n| backup.object.store | Sometimes | Object store connection string; required unless already specified inserver.conf |\n| backup.instance.name | Sometimes | Required when multiple instance names exist in the bucket; seeBackup instance name |\n| backup.restore.timestamp | No | Timestamp for point-in-time recovery; omit for latest backup |\n\n\n#### Point-in-time recovery‚Äã\n\nUse\nbackup.restore.timestamp\nto restore to a specific point in time. QuestDB\nfinds the most recent successful backup at or before the specified timestamp.\nTo find available backup timestamps, query the source instance:\n\n```questdb-sql\nSELECT start_ts FROM backups() WHERE status = 'backup complete';\n```\n\nYou can also specify an arbitrary timestamp (e.g., just before an accidental\ndeletion). QuestDB restores from the nearest available backup before that time.\nIf no backup exists at or before the specified timestamp, QuestDB fails to start\nwith the error:\nbackup restore error: No backup timestamp found that is <=\n.\nwarning\nRestore requires an empty database directory. If the target database already\nhas data (indicated by the presence of\ndb/.data_id\n), restore fails with:\n\"The local database is not empty.\" Use a fresh installation directory for\nrestore operations.\nThe QuestDB version performing the restore must have the same major version as\nthe version that created the backup (e.g., 8.1.0 and 8.1.1 are compatible).\nRestart QuestDB. If restore succeeds,\n_backup_restore\nis removed automatically.\n\n#### Restore failure recovery‚Äã\n\nIf restore fails, QuestDB creates artifacts to help diagnose and recover:\n\n| Artifact | Purpose |\n| --- | --- |\n| .restore_failed/ | Directory containing tables that failed to restore |\n| _restore_failed | File listing the names of failed tables |\n\nTo recover from a failed restore:\n- Check the.restore_failed/directory and_restore_failedfile for details\n- Investigate and fix the underlying issue (connectivity, permissions, etc.)\n- Remove both.restore_failed/directory and_restore_failedfile\n- Restart QuestDB to retry the restore\nIf you see the error \"Failed restore directory found\", a previous restore\nattempt failed. Remove the artifacts listed above before restarting.\n\n### Create a replica from a backup‚Äã\n\nYou can use a backup to bootstrap a new replica instance instead of relying\nsolely on WAL replay from the object store. This is faster when the backup is\nmore recent than the oldest available WAL data.\n- Ensure the primary is running and has replication configuredThe primary must havereplication.role=primaryand a configuredreplication.object.store.\n- Create a_backup_restorefile on the new replica machinePoint it to the same backup location used by the primary:backup.object.store=s3::bucket=my-bucket;region=eu-west-1;access_key_id=...;secret_access_key=...;backup.instance.name=gentle-forest-orchid\n- Configure the replicaSetreplication.role=replicaand ensurereplication.object.storepoints\nto the same object store as the primary.\n- Start the replicaQuestDB restores from the backup first, then switches to WAL replay to catch\nup with the primary.\nFor more details on replication setup, see the\nreplication guide\n.\n\n### Troubleshooting‚Äã\n\nIf you encounter errors during backup or restore:\n- ER007 - Data ID mismatch: The local database and object store have\ndifferent Data IDs. Seeerror code ER007for resolution steps.\n- Backup stuck at 0%: Check network connectivity to the object store and\nverify credentials are correct.\n- \"Failed restore directory found\": A previous restore attempt failed.\nRemove the.restore_failed/directory and_restore_failedfile, then\nrestart. SeeRestore failure recovery.\n- \"The local database is not empty\": Restore requires an empty database\ndirectory. Use a fresh installation or remove the existingdb/directory.\n\n## QuestDB OSS: manual backups with checkpoints‚Äã\n\nThe OSS workflow relies on the\nCHECKPOINT\nmode and external snapshot or file\ncopy tools. When in\nCHECKPOINT\nmode, QuestDB remains available for reads and\nwrites, but some housekeeping tasks are paused. This is safe in principle, but\ndatabase writes may consume more space than normal. When the database exits\nCHECKPOINT\nmode, it resumes the housekeeping tasks and reclaims disk space.\nYou must create a copy of the database using a tool of your choice. These are\nsome suggestions:\n- Cloud snapshot, e.g. EBS volume snapshot on AWS, Premium SSD Disk snapshot on\nAzure etc\n- On-prem backup tools and software you typically use\n- Basic command line tools, such ascporrsync\n\n### Data backup checklist‚Äã\n\nBefore backing up QuestDB, consider these items:\n\n#### Pick a good time‚Äã\n\nWe recommend that teams take a database backup when the database write load is\nat its lowest. If the database is under constant write load, a helpful\nworkaround is to ensure that the disk has at least 50% free space. The more free\nspace, the safer it is to enter the checkpoint mode.\n\n#### Determine backup frequency‚Äã\n\nWe recommend daily backups for disaster recovery purposes.\n\n#### Choose your data copy method‚Äã\n\nWhen choosing the right copy method, consider the following goals:\n- Minimize the time QuestDB spends in checkpoint mode\n- Ensure that the copy time remains sustainable as the database grows\nQuestDB backup lends itself relatively well to all types of differential data\ncopying. Due to time partitioning, older data is often unmodified, at both block\nand file levels.\n\n##### Cloud snapshots‚Äã\n\nIf you're using cloud disks, such as EBS on AWS, SSD on Azure, or similar, we\nstrongly recommend using their existing cloud\nsnapshot\ninfrastructure. The\nadvantages of this approach are that:\n- Cloud snapshots minimize the time QuestDB spends in checkpoint mode\n- Cloud snapshots are differential and can be restored cleanly\nSee the following guides for volume snapshot creation on the following cloud\nplatforms:\n- AWS-\ncreating EBS snapshots\n- Azure-\ncreating snapshots of a virtual hard disk\n- GCP- working\nwith persistent disk snapshots\nCloud snapshot-based systems usually break down their backup process into two\nsteps:\n- Take a snapshot\n- Back up the snapshot\nExit theCHECKPOINTmode as soon as the snapshotting stage is complete.\nSpecifically, exit checkpoint mode at the following snapshot stage:\n\n| Cloud Provider | State | Exit checkpoint mode |\n| --- | --- | --- |\n| Google Cloud(GCP) | RUNNING (UPLOADING) | When RUNNING substate changes from CREATING to UPLOADING |\n| Amazon Web Services(AWS) | PENDING | When status is PENDING |\n| Microsoft Azure | PENDING | Before the longer running \"CREATING\" stage |\n\n\n##### Volume snapshots‚Äã\n\nWhen the database is on-prem, we recommend using existing file system backup\ntools. For example, volume snapshots can be taken via\nLVM\n.\n\n##### File copy‚Äã\n\nIf filesystem or volume snapshots are not available, consider using a file copy\nmethod to back up the QuestDB server root directory. We recommend using a copy\ntool that can skip copying files based on the modification date. One such\npopular tool to accomplish this is\nrsync\n.\n\n### Steps in the backup procedure‚Äã\n\nWhile explaining the steps, we'll assume the database root directory is\n/var/lib/questdb\n.\n\n#### Enter checkpoint mode‚Äã\n\nTo enter the checkpoint mode:\nCreating a Checkpoint\n\n```questdb-sql\nCHECKPOINT CREATE\n```\n\nYou can create only one checkpoint. Attempting to create a second checkpoint\nwill fail.\n\n#### Check checkpoint status‚Äã\n\nYou can double-check at any time that the database is in the checkpoint mode:\nChecking Checkpoint Status\n\n```questdb-sql\nSELECT * FROM checkpoint_status();\n```\n\nHaving confirmed that QuestDB has entered the checkpoint mode, we now create the\nbackup.\n\n#### Take a snapshot or begin file copy‚Äã\n\nAfter a checkpoint is created and before it is released, you may safely access\nthe file system using tools external to the database instance. In other words,\nyou're now OK to begin your backup.\nIf your data copy method is a volume snapshot, you can exit the checkpoint mode\nas soon as the snapshot is taken (which takes a minute or two).\nMake sure to back up the entire server root directory, including thedb,snapshot, and all other directories.\nFile copy may take longer to back up files compared to snapshot. You will have\nto wait until the data transfer is fully complete before exiting checkpoint\nmode.\nIt is very important to exit the checkpoint mode regardless of whether the\ncopy operation succeeded or failed!\n\n#### Exit checkpoint mode‚Äã\n\nWith your backup complete, exit checkpoint mode:\nReleasing a Checkpoint\n\n```questdb-sql\nCHECKPOINT RELEASE\n```\n\nThis concludes the backup process.\nNow, with our additional copy, we're ready to restore QuestDB.\n\n### Restore to a saved checkpoint‚Äã\n\nRestoring from a local checkpoint will restore the entire database.\ncaution\nOSS checkpoint restore uses the\n_restore\ntrigger file. This is different from\nEnterprise backup restore which uses\n_backup_restore\n.\nFollow these steps:\n- Ensure your QuestDB version matches the one that did the backup\n- Restore QuestDB root directory contents (/var/lib/questdb/) from the backup\n- Touch the_restorefile\n- Start the database using the restored root directory\n\n#### Database versions‚Äã\n\nRestoring data is only possible if the backup and restore QuestDB versions have\nthe same major version number, for example:\n8.1.0\nand\n8.1.1\nare compatible.\n8.1.0\nand\n7.5.1\nare not compatible.\n\n#### Restore the root directory‚Äã\n\nWhen using cloud tools, create a new disk from the snapshot. The entire disk\ncontents of the original database will be available when the compute instance\nstarts.\nwarning\nAWS EBS lazy loading\n: By default, EBS volumes created from snapshots load\ndata lazily (on first access), which can cause slow reads after restore. To\nmitigate this:\n- Option 1: EnableFast Snapshot Restore (FSR)on the snapshot before creating the volume\n- Option 2: Pre-warm the volume by reading all blocks after restore:sudo fio --filename=/dev/nvme1n1 --rw=read --bs=1M --iodepth=32 \\--ioengine=libaio --direct=1 --name=volume-initialize\nThis issue may also affect other cloud providers with similar snapshot behavior.\nIf you are not using cloud tools, you have to make sure that you restore the\nroot from the backup using your own tools of choice!\n\n#### The trigger file‚Äã\n\nWhen you are starting the database from the backup for the first time, the\ndatabase must perform a restore procedure. This ensures the data is consistent\nand can be read and written. It only takes place on startup, and requires a\nspecific blank file to exist as the indication of user intent.\nTouch the\n_restore\nfile in the root directory. The following command will do\nthe trick:\n\n```bash\ntouch /var/lib/questdb/_restore\n```\n\n\n#### Start the database‚Äã\n\nStart the database using the root directory as usual. When the\n_restore\nfile\nis present, the database will perform the restore procedure. There are two\npossible outcomes:\n- Restore is successful: the database continues to run normally and is ready to\nuse; the_restorefile is removed to prevent the same procedure running\ntwice\n- Restore fails: the database exits and the_restorefile remains in place. An\nerror message appears instderr. If it can be resolved, starting the\ndatabase again will retry the restore procedure\n\n## Further reading‚Äã\n\n- BACKUPSQL reference- Enterprise backup command syntax\n- CHECKPOINTSQL reference- OSS checkpoint command syntax",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3658,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-49c16f9510ef",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/overview",
    "title": "Ingestion overview | QuestDB",
    "text": "For high-throughput data ingestion, use our\nfirst-party clients\nwith the\nInfluxDB Line Protocol (ILP)\n. This is the recommended method for production\nworkloads.\n\n## First-party clients‚Äã\n\nOur first-party clients are\nthe fastest way to insert data\n. They excel\nwith high-throughput, low-latency data streaming and are the recommended choice\nfor production deployments.\nTo start quickly, select your language:\n\n### C & C++\n\nHigh-performance client for systems programming and embedded applications.\nRead more\n\n### .NET\n\nCross-platform client for building applications with .NET technologies.\nRead more\n\n### Go\n\nAn open-source programming language supported by Google with built-in concurrency.\nRead more\n\n### Java\n\nPlatform-independent client for enterprise applications and Android development.\nRead more\n\n### Node.js\n\nNode.js¬Æ is an open-source, cross-platform JavaScript runtime environment.\nRead more\n\n### Python\n\nPython is a programming language that lets you work quickly and integrate systems more effectively.\nRead more\n\n### Rust\n\nSystems programming language focused on safety, speed, and concurrency.\nRead more\nOur clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only\nprotocol that bypasses SQL\nINSERT\nstatements, thus achieving significantly\nhigher throughput. It also provides some key benefits:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\nAn example of \"data-in\" - via the line - appears as:\n\n```shell\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\ntrades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\ntrades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n\n```\n\nOnce inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names\nmust follow the QuestDB\nnaming rules\n.\n\n### Ingestion characteristics‚Äã\n\nQuestDB is optimized for both throughput and latency. Send data when you have\nit - there's no need to artificially batch on the client side.\n\n| Mode | Throughput (per connection) |\n| --- | --- |\n| Batched writes | ~400k rows/sec |\n| Single-row writes | ~60-80k rows/sec |\n\nClients control batching via explicit\nflush()\ncalls. Each flush ends a batch\nand sends it to the server. If your data arrives one row at a time, send it one\nrow at a time - QuestDB handles this efficiently. If data arrives in bursts,\nbatch it naturally and flush when ready.\nServer-side, WAL processing is asynchronous. Transactions are grouped into\nsegments that roll based on size or row count, requiring no client-side tuning.\n\n## Message brokers and queues‚Äã\n\nIf you already have Kafka, Flink, or another streaming platform in your stack,\nQuestDB integrates seamlessly.\nSee our integration guides:\n- Flink\n- Kafka\n- Redpanda\n- Telegraf\n\n## CSV import‚Äã\n\nFor bulk imports or one-time data loads, use the\nImport CSV tab\nin the\nWeb Console\n:\nFor all CSV import methods, including using the APIs directly, see the\nCSV Import Guide\n.\n\n## Create new data‚Äã\n\nNo data yet? Just starting? No worries. We've got you covered.\nThere are several quick scaffolding options:\n- QuestDB demo instance: Hosted, fully loaded and\nready to go. Quickly explore theWeb Consoleand SQL syntax.\n- Create my first data set guide: Create\ntables, usernd_functions and make your own data.\n- Sample dataset repos: IoT,\ne-commerce, finance or git logs? Check them out!\n- Quick start repos:\nCode-based quick starts that cover ingestion, querying and data visualization\nusing common programming languages and use cases. Also, a cat in a tracksuit.\n- Time series streaming analytics template:\nA handy template for near real-time analytics using open source technologies.\n\n## Next step - queries‚Äã\n\nDepending on your infrastructure, it should now be apparent which ingestion\nmethod is worth pursuing.\nOf course, ingestion (data-in) is only half the battle.\nYour next best step? Learn how to query and explore data-out from theQuery & SQL Overview.\nIt might also be a solid bet to review\ntimestamp basics\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 653,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-19d119916f1b",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/schema-design-essentials",
    "title": "Schema design | QuestDB",
    "text": "New to QuestDB? This guide covers the essential concepts for designing efficient time-series tables.\n\n## Your first table‚Äã\n\nHere's a minimal, well-designed QuestDB table:\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL,    side SYMBOL,    price DOUBLE,    quantity DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nKey elements:\n- TIMESTAMP(timestamp)‚Äî designates the time column (required for time-series)\n- PARTITION BY DAY‚Äî splits data into daily partitions for efficient queries\n- SYMBOL‚Äî optimized type for categorical data like tickers\n\n## Designated timestamp‚Äã\n\nEvery time-series table needs a\ndesignated timestamp\n. This column:\n- Determines physical storage order (data is sorted by this column)\n- Enables partition pruning (queries skip irrelevant time ranges)\n- Powers time-series functions likeSAMPLE BYandLATEST ON\n\n```questdb-sql\nCREATE TABLE market_data (    ts TIMESTAMP,        -- Will be the designated timestamp    symbol SYMBOL,    price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY;\n```\n\nWithout a designated timestamp, you lose most of QuestDB's performance benefits.\nSee\nDesignated Timestamp\nfor details.\n\n## Partitioning‚Äã\n\nPartitioning splits your table into time-based chunks. Choose based on your data volume:\n\n| Data volume | Recommended partition |\n| --- | --- |\n| < 100K rows/day | MONTHorYEAR |\n| 100K - 10M rows/day | DAY |\n| 10M - 100M rows/day | HOUR |\n| > 100M rows/day | HOUR(consider multiple tables) |\n\nGuidelines:\n- Each partition should be a few hundred MB to a few GB\n- Too many small partitions = more file operations\n- Too few large partitions = slower queries and more memory usage\n\n```questdb-sql\n-- High-volume tick dataCREATE TABLE trades (...)TIMESTAMP(ts) PARTITION BY HOUR;-- Lower-volume end-of-day pricesCREATE TABLE eod_prices (...)TIMESTAMP(ts) PARTITION BY MONTH;\n```\n\nSee\nPartitions\nfor details.\n\n## Data types‚Äã\n\n\n### SYMBOL vs VARCHAR‚Äã\n\nWhen to use SYMBOL:\n- Repetitive string values (stock tickers, country codes, status flags)\n- Up to tens of millions of distinct values\n- Columns used inWHEREfilters orGROUP BY\nWhen to use VARCHAR:\n- Unique/high-cardinality values (hundreds of millions of distinct)\n- User-generated text (comments, log messages)\n- UUIDs (though theUUIDtype is better)\nWhy it matters:\n- SYMBOL uses dictionary encoding (integer lookups vs string comparisons)\n- Faster filtering, faster grouping, less storage\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL,       -- Stock ticker: AAPL, GOOGL, etc.    side SYMBOL,         -- BUY or SELL    price DOUBLE,    quantity DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nSee\nSymbol\nfor details.\n\n### Timestamps‚Äã\n\nQuestDB stores all timestamps in\nUTC\nwith microsecond precision.\n\n```questdb-sql\nCREATE TABLE trades (    ts TIMESTAMP,              -- Microsecond precision (recommended)    exchange_ts TIMESTAMP_NS,  -- Nanosecond precision (if needed)    symbol SYMBOL,    price DOUBLE) TIMESTAMP(ts);\n```\n\nUse\nTIMESTAMP\nunless you specifically need nanosecond precision.\nFor timezone handling at query time, see\nWorking with Timestamps and Timezones\n.\n\n### Other types‚Äã\n\n\n| Type | Use case |\n| --- | --- |\n| VARCHAR | Free-text strings |\n| DOUBLE/FLOAT | Floating point numbers |\n| DECIMAL(precision, scale) | Exact decimal numbers (financial data) |\n| LONG/INT/SHORT | Integers |\n| BOOLEAN | True/false flags |\n| UUID | Unique identifiers (more efficient than VARCHAR) |\n| IPv4 | IP addresses |\n| BINARY | Binary data |\n| ARRAY | N-dimensional arrays (e.g.DOUBLE[3][4]) |\n\nNumeric type storage sizes:\n\n| Type | Storage | Range |\n| --- | --- | --- |\n| BYTE | 8 bits | -128 to 127 |\n| SHORT | 16 bits | -32,768 to 32,767 |\n| INT | 32 bits | -2.1B to 2.1B |\n| LONG | 64 bits | -9.2E18 to 9.2E18 |\n| FLOAT | 32 bits | Single precision IEEE 754 |\n| DOUBLE | 64 bits | Double precision IEEE 754 |\n| DECIMAL | 1-32 bytes | Variable based on precision |\n\nChoose the smallest type that fits your data to save storage.\nFor arrays and geospatial data, see\nData Types\n.\n\n### STRING vs VARCHAR‚Äã\n\nQuestDB has two string types:\n\n| Type | Encoding | Status |\n| --- | --- | --- |\n| VARCHAR | UTF-8 | Recommended |\n| STRING | UTF-16 | Legacy, not recommended |\n\nAlways useVARCHARfor new tables.\nThe\nSTRING\ntype exists for backward\ncompatibility but is less efficient. If you have existing tables with\nSTRING\ncolumns, they will continue to work, but consider migrating to\nVARCHAR\nwhen\nconvenient.\n\n## Deduplication‚Äã\n\nQuestDB allows duplicates by default. To enforce uniqueness, use\nDEDUP UPSERT KEYS\n:\n\n```questdb-sql\nCREATE TABLE quotes (    timestamp TIMESTAMP,    symbol SYMBOL,    bid DOUBLE,    ask DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAYDEDUP UPSERT KEYS(timestamp, symbol);\n```\n\nWhen a row arrives with the same\ntimestamp\nand\nsymbol\n, the old row is replaced.\nDeduplication has no noticeable performance penalty.\nSee\nDeduplication\nfor details.\n\n## Data retention with TTL‚Äã\n\nQuestDB doesn't support individual row deletes. Instead, use TTL to automatically\ndrop old partitions:\n\n```questdb-sql\nCREATE TABLE tick_data (    timestamp TIMESTAMP,    symbol SYMBOL,    price DOUBLE,    size LONG) TIMESTAMP(timestamp) PARTITION BY DAY TTL 90 DAYS;\n```\n\nThis keeps the last 90 days of data and automatically removes older partitions.\nSee\nTTL\nfor details.\n\n## Materialized views‚Äã\n\nFor frequently-run aggregations, pre-compute results with materialized views:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW ohlc_1h AS  SELECT    timestamp,    symbol,    first(price) as open,    max(price) as high,    min(price) as low,    last(price) as close,    sum(quantity) as volume  FROM trades  SAMPLE BY 1h;\n```\n\nQuestDB automatically refreshes the view as new data arrives. Queries against\nthe view are instant regardless of base table size.\nSee\nMaterialized Views\nfor details.\n\n## Views‚Äã\n\nWhen query performance is acceptable, or for less frequent queries where you don't need materialization, use views to abstract complex queries:\n\n```questdb-sql\nCREATE VIEW recent_trades AS (  SELECT * FROM trades  WHERE timestamp > dateadd('d', -7, now()));\n```\n\nViews can be parameterized using\nDECLARE OVERRIDABLE\n:\n\n```questdb-sql\nCREATE VIEW trades_above AS (  DECLARE OVERRIDABLE @min_price := 100  SELECT * FROM trades WHERE price >= @min_price);-- Override at query timeDECLARE @min_price := 500 SELECT * FROM trades_above;\n```\n\nSee\nViews\nfor details.\n\n## Common mistakes‚Äã\n\n\n### Using VARCHAR for categorical data‚Äã\n\n\n```questdb-sql\n-- Bad: VARCHAR for repeated valuesCREATE TABLE trades (    timestamp TIMESTAMP,    symbol VARCHAR,        -- Slow filtering and grouping    ...);-- Good: SYMBOL for categorical dataCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL,         -- Fast filtering and grouping    ...);\n```\n\n\n### Wrong partition size‚Äã\n\n\n```questdb-sql\n-- Bad: Yearly partitions for high-volume dataCREATE TABLE trades (...)PARTITION BY YEAR;          -- Partitions will be huge-- Good: Match partition size to data volumeCREATE TABLE trades (...)PARTITION BY HOUR;\n```\n\n\n### Forgetting the designated timestamp‚Äã\n\n\n```questdb-sql\n-- Bad: No designated timestampCREATE TABLE trades (    ts TIMESTAMP,    price DOUBLE);-- Good: Explicit designated timestampCREATE TABLE trades (    ts TIMESTAMP,    price DOUBLE) TIMESTAMP(ts);\n```\n\n\n## Schema changes‚Äã\n\nSome properties\ncannot be changed\nafter table creation:\n\n| Property | Can modify? |\n| --- | --- |\n| Designated timestamp column | No |\n| Partitioning strategy | No |\n| Add new columns | Yes |\n| Drop columns | Yes |\n| Rename columns | Yes |\n| Change column type | Limited |\n\nTo change immutable properties, create a new table and migrate data:\n\n```questdb-sql\n-- 1. Create new table with desired schemaCREATE TABLE trades_new (...) PARTITION BY HOUR;-- 2. Copy dataINSERT INTO trades_new SELECT * FROM trades;-- 3. Swap tablesDROP TABLE trades;RENAME TABLE trades_new TO trades;\n```\n\n\n## Multi-tenancy‚Äã\n\nQuestDB uses a\nsingle database per instance\n. For multi-tenant applications,\nuse table name prefixes:\n\n```questdb-sql\n-- Client-specific tablesCREATE TABLE acme_trades (...);CREATE TABLE globex_trades (...);-- Environment and region tablesCREATE TABLE prod_us_trades (...);CREATE TABLE prod_eu_trades (...);CREATE TABLE staging_trades (...);-- Asset class tablesCREATE TABLE equities_trades (...);CREATE TABLE fx_trades (...);CREATE TABLE crypto_trades (...);\n```\n\nNaming conventions:\n- Use consistent prefixes:{client}_,{env}_{region}_,{asset_class}_\n- Keep names lowercase with underscores\n- Consider query patterns when choosing prefix granularity\nWith\nQuestDB Enterprise\n, you can enforce per-table\npermissions for access control.\n\n## PostgreSQL compatibility‚Äã\n\nQuestDB supports the\nPostgreSQL wire protocol\n,\nso most PostgreSQL client libraries work. However, QuestDB is not PostgreSQL:\n- NoPRIMARY KEY,FOREIGN KEY, orNOT NULLconstraints\n- Limited system catalog compatibility\n- Some PostgreSQL functions may not be available\n\n## Migrating from other databases‚Äã\n\nPostgreSQL / TimescaleDB-- PostgreSQLCREATE TABLE metrics (timestamp TIMESTAMP PRIMARY KEY,name VARCHAR(255) NOT NULL,value DOUBLE PRECISION NOT NULL);INSERT INTO metrics VALUES (...)ON CONFLICT (timestamp) DO UPDATE SET value = EXCLUDED.value;-- QuestDB equivalentCREATE TABLE metrics (timestamp TIMESTAMP,name SYMBOL,value DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAYDEDUP UPSERT KEYS(timestamp, name);\nInfluxDB# InfluxDB line protocolmetrics,name=cpu,region=us value=0.64-- QuestDB equivalentCREATE TABLE metrics (timestamp TIMESTAMP,name SYMBOL,region SYMBOL,value DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\nClickHouse-- ClickHouseCREATE TABLE metrics (timestamp DateTime,name String,value Float64) ENGINE = ReplacingMergeTreeORDER BY (name, timestamp);-- QuestDB equivalentCREATE TABLE metrics (timestamp TIMESTAMP,name SYMBOL,value DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAYDEDUP UPSERT KEYS(timestamp, name);\nDuckDB-- DuckDBCREATE TABLE metrics (timestamp TIMESTAMP,name VARCHAR,value DOUBLE);-- QuestDB equivalentCREATE TABLE metrics (timestamp TIMESTAMP,name SYMBOL,          -- Use SYMBOL for repeated stringsvalue DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n## Schema management‚Äã\n\nFor schema migrations, QuestDB supports\nFlyway\n.\nYou can also use ILP auto-creation for dynamic schemas, though this applies\ndefault settings. See\nILP Overview\nfor details.\n\n## Next steps‚Äã\n\n- Quick Start‚Äî Create your first table and run queries\n- Capacity Planning‚Äî Size your deployment for production\n- Connect & Ingest‚Äî Load data into QuestDB\n- Materialized Views‚Äî Pre-compute aggregations for fast dashboards",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1506,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-1e4470a38f48",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/overview",
    "title": "Query & SQL Overview | QuestDB",
    "text": "Querying - as a base action - is performed in three primary ways:\n- Query via theQuestDB Web Console\n- Query viaPostgreSQL\n- Query viaREST HTTP API\n- Query viaApache Parquet\nFor efficient and clear querying, QuestDB provides SQL with enhanced time series\nextensions. This makes analyzing, downsampling, processing and reading time\nseries data an intuitive and flexible experience.\nQueries can be written into many applications using existing drivers and clients\nof the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged\nheavily by third-party tools to provide visualizations, such as within\nGrafana\n, or for data analysis with dataframe\nlibraries like\nPolars\n.\nNeed to ingest data first? Checkout our\nIngestion overview\n.\n\n## QuestDB Web Console‚Äã\n\nThe Web Console is available by default at\nhttp://localhost:9000\n. The GUI makes it easy to write, return\nand chart queries. There is autocomplete, syntax highlighting, errors, and more.\nIf you want to test a query or interact direclty with your data in the cleanest\nand simplest way, apply queries via the\nWeb Console\n.\nClick to zoom\nFor an example, click\nDemo this query\nin the below snippet. This will run a\nquery within our public demo instance and\nWeb Console\n:\nNavigate time with SQL\nDemo this query\n\n```questdb-sql\nSELECT    timestamp, symbol,    first(price) AS open,    last(price) AS close,    min(price),    max(price),    sum(amount) AS volumeFROM tradesWHERE  timestamp > dateadd('d', -1, now())SAMPLE BY 15m;\n```\n\nIf you see\nDemo this query\non other snippets in this docs, they can be run\nagainst the demo instance.\n\n## PostgreSQL‚Äã\n\nQuery QuestDB using the PostgreSQL endpoint via the default port\n8812\n.\nSee\nPGWire Client overview\nfor details on how to\nconnect to QuestDB using PostgreSQL clients.\nBrief examples in multiple languages are shown below.\n- Python\n- Java\n- NodeJS\n- Go\n- C#\n- C\n- Ruby\n- PHP\n\n```python\nimport psycopg as pgimport time# Connect to an existing QuestDB instanceconn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb'with pg.connect(conn_str, autocommit=True) as connection:    # Open a cursor to perform database operations    with connection.cursor() as cur:        #Query the database and obtain data as Python objects.        cur.execute('SELECT * FROM trades_pg;')        records = cur.fetchall()        for row in records:            print(row)# the connection is now closed\n```\n\n\n```java\npackage com.myco;import java.sql.*;import java.util.Properties;public class App {    public static void main(String[] args) throws SQLException {        Properties properties = new Properties();        properties.setProperty(\"user\", \"admin\");        properties.setProperty(\"password\", \"quest\");        properties.setProperty(\"sslmode\", \"disable\");        final Connection connection = DriverManager.getConnection(            \"jdbc:postgresql://localhost:8812/qdb\", properties);        try (PreparedStatement preparedStatement = connection.prepareStatement(                \"SELECT x FROM long_sequence(5);\")) {            try (ResultSet rs = preparedStatement.executeQuery()) {                while (rs.next()) {                    System.out.println(rs.getLong(1));                }            }        }        connection.close();    }}\n```\n\n\n```javascript\n\"use strict\"const { Client } = require(\"pg\")const start = async () => {  const client = new Client({    database: \"qdb\",    host: \"127.0.0.1\",    password: \"quest\",    port: 8812,    user: \"admin\",  })  await client.connect()  const res = await client.query(\"SELECT x FROM long_sequence(5);\")  console.log(res.rows)  await client.end()}start().catch(console.error)\n```\n\n\n```go\npackage mainimport (  \"database/sql\"  \"fmt\"  _ \"github.com/lib/pq\")const (  host     = \"localhost\"  port     = 8812  user     = \"admin\"  password = \"quest\"  dbname   = \"qdb\")func main() {  connStr := fmt.Sprintf(    \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\",    host, port, user, password, dbname)  db, err := sql.Open(\"postgres\", connStr)  checkErr(err)  defer db.Close()  stmt, err := db.Prepare(\"SELECT x FROM long_sequence(5);\")  checkErr(err)  defer stmt.Close()  rows, err := stmt.Query()  checkErr(err)  defer rows.Close()  var num string  for rows.Next() {    err = rows.Scan(&num)    checkErr(err)    fmt.Println(num)  }  err = rows.Err()  checkErr(err)}func checkErr(err error) {  if err != nil {    panic(err)  }}\n```\n\n\n```c\n// compile with// g++ libpq_example.c -o libpq_example.exe  -I pgsql\\include -L dev\\pgsql\\lib// -std=c++17  -lpthread -lpq#include <libpq-fe.h>#include <stdio.h>#include <stdlib.h>void do_exit(PGconn *conn) {  PQfinish(conn);  exit(1);}int main() {  PGconn *conn = PQconnectdb(      \"host=localhost user=admin password=quest port=8812 dbname=testdb\");  if (PQstatus(conn) == CONNECTION_BAD) {    fprintf(stderr, \"Connection to database failed: %s\\n\",            PQerrorMessage(conn));    do_exit(conn);  }  PGresult *res = PQexec(conn, \"SELECT x FROM long_sequence(5);\");  if (PQresultStatus(res) != PGRES_TUPLES_OK) {    printf(\"No data retrieved\\n\");    PQclear(res);    do_exit(conn);  }  int rows = PQntuples(res);  for (int i = 0; i < rows; i++) {    printf(\"%s\\n\", PQgetvalue(res, i, 0));  }  PQclear(res);  PQfinish(conn);  return 0;}\n```\n\n\n```csharp\nusing Npgsql;string username = \"admin\";string password = \"quest\";string database = \"qdb\";int port = 8812;var connectionString = $@\"host=localhost;port={port};username={username};password={password};database={database};ServerCompatibilityMode=NoTypeLoading;\";await using NpgsqlConnection connection = new NpgsqlConnection(connectionString);await connection.OpenAsync();var sql = \"SELECT x FROM long_sequence(5);\";await using NpgsqlCommand command = new NpgsqlCommand(sql, connection);await using (var reader = await command.ExecuteReaderAsync()) {    while (await reader.ReadAsync())    {        var x = reader.GetInt64(0);    }}\n```\n\n\n```ruby\nrequire 'pg'begin    conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb',                      user: 'admin', password: 'quest' )    rows = conn.exec 'SELECT x FROM long_sequence(5);'    rows.each do |row|        puts row    endrescue PG::Error => e     puts e.messageensure    conn.close if connend\n```\n\n\n```php\n<?phpfunction exceptions_error_handler($severity, $message, $filename, $lineno) {    throw new ErrorException($message, 0, $severity, $filename, $lineno);}set_error_handler('exceptions_error_handler');$db_conn = null;try {        $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin'  password = 'quest' \");        $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' );        while ($row = pg_fetch_assoc($result) ){                print_r($row);                }        pg_free_result($result);} catch (Exception $e) {    echo 'Caught exception: ',  $e->getMessage(), \"\\n\";} finally {        if (!is_null($db_conn)) {                pg_close($db_conn);        }}?>\n```\n\n\n#### Further Reading‚Äã\n\nSee the\nPGWire Client overview\nfor more details on how to use PostgreSQL\nclients to connect to QuestDB.\n\n## REST HTTP API‚Äã\n\nQuestDB exposes a REST API for compatibility with a wide range of libraries and\ntools.\nThe REST API is accessible on port\n9000\nand has the following query-capable\nentrypoints:\nFor details such as content type, query parameters and more, refer to the\nREST HTTP API\nreference.\n\n| Entrypoint | HTTP Method | Description | REST HTTP API Reference |\n| --- | --- | --- | --- |\n| /exp?query=.. | GET | Export SQL Query as CSV | Reference |\n| /exec?query=.. | GET | Run SQL Query returning JSON result set | Reference |\n\n\n#### /exp: SQL Query to CSV‚Äã\n\nThe\n/exp\nentrypoint allows querying the database with a SQL select query and\nobtaining the results as CSV.\nFor obtaining results in JSON, use\n/exec\ninstead, documented next.\n- cURL\n- Python\n\n```bash\ncurl -G --data-urlencode \\    \"query=SELECT * FROM example_table2 LIMIT 3\" \\    http://localhost:9000/exp\n```\n\n\n```csv\n\"col1\",\"col2\",\"col3\"\"a\",10.5,true\"b\",100.0,false\"c\",,true\n```\n\n\n```python\nimport requestsresp = requests.get(    'http://localhost:9000/exp',    {        'query': 'SELECT * FROM example_table2',        'limit': '3,6'   # Rows 3, 4, 5    })print(resp.text)\n```\n\n\n```csv\n\"col1\",\"col2\",\"col3\"\"d\",20.5,true\"e\",200.0,false\"f\",,true\n```\n\n\n#### /exec: SQL Query to JSON‚Äã\n\nThe\n/exec\nentrypoint takes a SQL query and returns results as JSON.\nThis is similar to the\n/exp\nentry point which returns results as CSV.\n\n##### Querying Data‚Äã\n\n- cURL\n- Python\n- NodeJS\n- Go\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\  http://localhost:9000/exec\n```\n\nThe JSON response contains the original query, a\n\"columns\"\nkey with the schema\nof the results, a\n\"count\"\nnumber of rows and a\n\"dataset\"\nwith the results.\n\n```json\n{  \"query\": \"SELECT x FROM long_sequence(5);\",  \"columns\": [{ \"name\": \"x\", \"type\": \"LONG\" }],  \"dataset\": [[1], [2], [3], [4], [5]],  \"count\": 5}\n```\n\n\n```python\nimport sysimport requestshost = 'http://localhost:9000'sql_query = \"select * from long_sequence(10)\"try:    response = requests.get(        host + '/exec',        params={'query': sql_query}).json()    for row in response['dataset']:        print(row[0])except requests.exceptions.RequestException as e:    print(f'Error: {e}', file=sys.stderr)\n```\n\n\n```javascript\nconst fetch = require(\"node-fetch\")const HOST = \"http://localhost:9000\"async function run() {  try {    const query = \"SELECT x FROM long_sequence(5);\"    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    )    const json = await response.json()    console.log(json)  } catch (error) {    console.log(error)  }}run()\n```\n\n\n```go\npackage mainimport (  \"fmt\"  \"io/ioutil\"  \"log\"  \"net/http\"  \"net/url\")func main() {  u, err := url.Parse(\"http://localhost:9000\")  checkErr(err)  u.Path += \"exec\"  params := url.Values{}  params.Add(\"query\", \"SELECT x FROM long_sequence(5);\")  u.RawQuery = params.Encode()  url := fmt.Sprintf(\"%v\", u)  res, err := http.Get(url)  checkErr(err)  defer res.Body.Close()  body, err := ioutil.ReadAll(res.Body)  checkErr(err)  log.Println(string(body))}func checkErr(err error) {  if err != nil {    panic(err)  }}\n```\n\nAlternatively, the\n/exec\nendpoint can be used to create a table and the\nINSERT\nstatement can be used to populate it with values:\n- cURL\n- NodeJS\n- Python\n\n```shell\n# Create Tablecurl -G \\  --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\  http://localhost:9000/exec# Insert a rowcurl -G \\  --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\  http://localhost:9000/exec# Update a rowcurl -G \\  --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\  http://localhost:9000/exec\n```\n\nThe\nnode-fetch\npackage can be installed using\nnpm i node-fetch\n.\n\n```javascript\nconst fetch = require(\"node-fetch\");const HOST = \"http://localhost:9000\";async function createTable() {  try {    const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\";    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    );    const json = await response.json();    console.log(json);  } catch (error) {    console.log(error);  }}async function insertData() {  try {    const query = \"INSERT INTO trades VALUES('abc', 123456)\";    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    );    const json = await response.json();    console.log(json);  } catch (error) {    console.log(error);  }}async function updateData() {  try {    const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\";    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    );    const json = await response.json();    console.log(json);  } catch (error) {    console.log(error);  }}createTable().then(insertData).then(updateData);\n```\n\n\n```python\nimport requestsimport jsonhost = 'http://localhost:9000'def run_query(sql_query):  query_params = {'query': sql_query, 'fmt' : 'json'}  try:    response = requests.get(host + '/exec', params=query_params)    json_response = json.loads(response.text)    print(json_response)  except requests.exceptions.RequestException as e:    print(\"Error: %s\" % (e))# create tablerun_query(\"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\")# insert rowrun_query(\"INSERT INTO trades VALUES('abc', 123456)\")# update rowrun_query(\"UPDATE trades SET value = 9876 WHERE name = 'abc'\")\n```\n\n\n## Apache Parquet‚Äã\n\ninfo\nApache Parquet support is in\nbeta\n. It may not be fit for production use.\nPlease let us know if you run into issues. Either:\n- Email us atsupport@questdb.io\n- Join ourpublic Slack\n- Post on ourDiscourse community\nParquet files can be read and thus queried by QuestDB.\nQuestDB is shipped with a demo Parquet file,\ntrades.parquet\n, which can be\nqueried using the\nread_parquet\nfunction.\nExample:\nread_parquet example\n\n```questdb-sql\nSELECT  *FROM  read_parquet('trades.parquet')WHERE  side = 'buy';\n```\n\nThe trades.parquet file is located in the\nimport\nsubdirectory inside the\nQuestDB root directory. Drop your own Parquet files to the import directory and\nquery them using the\nread_parquet()\nfunction.\nYou can change the allowed directory by setting the\ncairo.sql.copy.root\nconfiguration key.\nFor more information, see the\nParquet documentation\n.\n\n## What's next?‚Äã\n\nNow... SQL! It's query time.\nWhether you want to use the\nWeb Console\n, PostgreSQL or REST HTTP (or both),\nquery construction is rich.\nTo brush up and learn what's unique in QuestDB, consider the following:\n- Data types\n- SQL execution order\nAnd to learn about some of our favourite, most powerful syntax:\n- Window functionsare a powerful analysis\ntool\n- Aggregate functions- aggregations\nare key!\n- Date & time operatorsto learn about\ndate and time\n- SAMPLE BYto summarize data into chunks\nbased on a specified time interval, from a year to a microsecond\n- WHERE INto compress time ranges\ninto concise intervals\n- LATEST ONfor latest values within\nmultiple series within a table\n- ASOF JOINto associate timestamps between\na series based on proximity; no extra indices required\n- Materialized Viewsto pre-compute complex queries\nfor optimal performance\nLooking for visuals?\n- ExploreGrafana\n- Jump quickly into theWeb Console",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1799,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-d91f8b87d896",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/getting-started/capacity-planning",
    "title": "Capacity planning | QuestDB",
    "text": "This guide will help you optimize your QuestDB deployments for peak performance.\nWe cover example scenarios across both edge cases and common setup\nconfigurations.\nMost configuration settings are configured in QuestDB using the\nserver.conf\nconfiguration file, or as environment variables.\nFor more information about applying configuration settings in QuestDB, see the\nconfiguration\npage.\nTo monitor the various metrics of a QuestDB instance, refer to the\nPrometheus monitoring\npage or the\nLogging & Metrics\npage.\n\n## Storage and filesystem‚Äã\n\nSome of the aspects to consider regarding the storage of data and file systems.\n\n### Drive selection‚Äã\n\nIf you're using a physically-attached drive, we strongly recommend using NVMe\ndrives over SATA SSDs.\nNVMe drives offer faster read and write speeds compared to other SSDs. This\ntranslates to overall better performance.\nIf you're using a network-attached drive, like\nAWS EBS\n, please refer to the next section.\n\n### Optimizing IOPS and throughput‚Äã\n\nIOPS is a measure of the number of operations per second. Throughput measures\nthe amount of data transferred per second, e.g. in megabytes per second.\nBoth metrics are important. However, your requirements may vary depending on the\nworkload.\nFor instance, large batch operations might benefit more from higher throughput,\nwhereas real-time query performance might need higher IOPS.\nFor typical loads, particularly when using AWS gp3 volumes, you should aim for\nthe following baseline IOPS and throughput settings:\n- Minimum IOPS: 7000\n- Minimum Throughput: 500 MB/s\nFor optimum performance, utilize the maximum settings:\n- Maximum IOPS: 16000\n- Maximum Throughput: 1 GB/s\n\n### Supported filesystems‚Äã\n\nTo enable compression and to match our recommended performance profile, we\nrecommend using\nZFS file system\n.\nZFS is required for system-level compression.\nWhile ZFS is recommended, QuestDB open source supports the following\nfilesystems:\n- APFS\n- EXT4\n- NTFS\n- OVERLAYFS (used by Docker)\n- XFS (ftype=1only)\n- ZFS\nOther file systems supporting\nmmap\nmay work with QuestDB\nbut they should not be used in production. QuestDB does not test on them.\nWhen you use an unsupported file system, QuestDB logs this warning:\n\n```questdb-sql\n-> UNSUPPORTED (SYSTEM COULD BE UNSTABLE)\"\n```\n\ncaution\nUsers\ncan't use NFS or similar distributed filesystems\ndirectly with a\nQuestDB database.\n\n### Data compression‚Äã\n\nTo enable data compression, filesystem must be ZFS.\nFor instructions on how to do so, see the\nZFS and compression\nguide.\n\n### Write amplification‚Äã\n\nWrite amplification measures how many times data is rewritten during ingestion.\nA value of 1.0 means each row is written once (ideal). Higher values indicate\nrewrites due to out-of-order data merging into existing partitions.\nCalculate it using\nPrometheus metrics\n:\n\n```questdb-sql\nwrite_amplification = questdb_physically_written_rows_total / questdb_committed_rows_total\n```\n\nThese are\ncumulative lifetime counters\n. To measure current write amplification,\ncompare the delta of both values over a time window (e.g., 5 minutes).\n\n| Value | Interpretation |\n| --- | --- |\n| 1.0 ‚Äì 1.5 | Excellent ‚Äì minimal rewrites |\n| 1.5 ‚Äì 3.0 | Normal for moderate out-of-order data |\n| 3.0 ‚Äì 5.0 | Consider reducing partition size |\n| > 5.0 | High ‚Äì reduce partition size or investigate ingestion patterns |\n\nWhen ingesting out-of-order data, high write amplification combined with high\ndisk write rate may reduce database performance.\nFor data ingestion over PostgreSQL Wire Protocol, or as a further step for\nInfluxDB Line Protocol ingestion, using smaller table\npartitions\ncan reduce write amplification. This\napplies in particular to tables with partition directories exceeding several\nhundred MBs on disk. For example,\nPARTITION BY DAY\ncould be reduced to\nPARTIION BY HOUR\n,\nPARTITION BY MONTH\nto\nPARTITION BY DAY\n, and so on.\n\n#### Partition splitting‚Äã\n\nSince QuestDB 7.2, heavily out-of-order commits may split partitions into\nsmaller parts to reduce write amplification. When data is merged into an\nexisting partition due to an out-of-order insert, the partition will be split\ninto two parts: the prefix sub-partition and the suffix sub-partition.\nConsider the following scenario:\n- A partition2023-01-01.1with 1,000 rows every hour, and therefore 24,000\nrows in total.\n- Inserting one row with the timestamp2023-01-01T23:00\nWhen the out-of-order row\n2023-01-01T23:00\nis inserted, the partition is split\ninto 2 parts:\n- Prefix:2023-01-01.1with 23,000 rows\n- Suffix (including the merged row):2023-01-01T75959-999999.2with 1,001 rows\nSee\nSplitting and squashing time partitions\nfor more information.\n\n## CPU and RAM configuration‚Äã\n\nThis section describes configuration strategies based on the forecasted behavior\nof the database.\n\n### RAM size‚Äã\n\nWe recommend having at least 8GB of RAM for basic workloads, and 32GB for more\nadvanced ones.\nFor relatively small datasets i.e 4-40GB, and a read-heavy workload, performance\ncan be improved by maximising use of the OS page cache. Users should consider\nincreasing available RAM to improve the speed of read operations.\n\n### Memory page size configuration‚Äã\n\nWith frequent out-of-order (O3) writes over a large number of columns/tables,\ndatabase performance may be impacted by large memory page sizes, as this\nincreases the demand for RAM. The memory page,\ncairo.o3.column.memory.size\n, is\nset to 8M by default. This means that the table writer uses 16MB (2x8MB) RAM per\ncolumn when it receives O3 writes. O3 write performance, and overall memory\nusage, may be improved by decreasing this value within the range [128K, 8M]. A smaller\npage size allows for a larger number of in-use columns, or otherwise frees up memory\nfor other database processes to use.\n\n### CPU cores‚Äã\n\nBy default, QuestDB tries to use all available CPU cores.\nThe guide on shared worker configuration\nexplains how to change the default settings. Assuming that the disk is not\nbottlenecked on IOPS, the throughput of read-only queries scales proportionally\nwith the number of available cores. As a result, a machine with more cores will\nprovide better query performance.\n\n### Writer page size‚Äã\n\nThe default page size for writers is 16MB. This should be adjusted according to\nyour use case. For example, using a 16MB page-size, to write only 1MB of data is\na waste of resources. To change this default value, set the\ncairo.writer.data.append.page.size\noption in\nserver.conf\n:\nserver.conf\n\n```ini\ncairo.writer.data.append.page.size=1M\n```\n\nFor more horizontal use cases i.e databases with a large number of small tables,\nthe page sizes could be reduced more dramatically. This may better distribute\nresources, and help to reduce write amplification.\n\n### InfluxDB Line Protocol (ILP) over HTTP‚Äã\n\nAs of QuestDB 7.4.2, InfluxDB Line Protocol operates over HTTP instead of TCP.\nAs such, ILP is optimal out-of-the box.\nSee your\nILP client\nfor\nlanguage-specific configurations.\n\n### Postgres Wire Protocol‚Äã\n\nFor clients sending data to QuestDB using the Postgres interface, the following\nconfiguration can be applied, which sets a dedicated worker and pins it with\naffinity\nto a CPU by core ID:\nserver.conf\n\n```ini\npg.worker.count=4pg.worker.affinity=1,2,3,4\n```\n\n\n## Network Configuration‚Äã\n\nFor the InfluxDB Line Protocol, PostgreSQL Wire Protocol and HTTP, there are a\nnumber of configuration settings which control:\n- the number of clients that may connect\n- the internal I/O capacities\n- connection timeout settings\nThese settings are configured in the\nserver.conf\nfile, and follow the naming\nconvention:\n\n```ini\n<protocol>.net.connection.<config>\n```\n\nWhere\n<protocol>\nis one of:\n- http- HTTP connections\n- pg- PostgreSQL Wire Protocol\n- line.tcp- InfluxDB line protocol over TCP\nAnd\n<config>\nis one of the following settings:\n\n| key | description |\n| --- | --- |\n| limit | The number of simultaneous connections to the server. This value is intended to control server memory consumption. |\n| timeout | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses. |\n| hint | Applicable only for Windows, where TCP backlog limit is hit. For example Windows 10 allows max of 200 connection. Even if limit is set higher, without hint=true, it won't be possible to serve more than 200 connections. |\n| sndbuf | Maximum send buffer size on each TCP socket. If value is -1 socket send buffer remains unchanged from OS default. |\n| rcvbuf | Maximum receive buffer size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default. |\n\nFor example, this is a configuration for Linux with a relatively low number of\nconcurrent connections:\nserver.conf InfluxDB Line Protocol network example configuration for a low number of concurrent connections\n\n```ini\n# bind to all IP addresses on port 9009line.tcp.net.bind.to=0.0.0.0:9009# maximum of 30 concurrent connection allowedline.tcp.net.connection.limit=30# nothing to do here, connection limit is quite lowline.tcp.net.connection.hint=false# connections will time out after 60s of no activityline.tcp.net.connection.timeout=60000# receive buffer is 4MB to accomodate large messagesline.tcp.net.rcvbuf=4M\n```\n\nThis is an example for when one would like to configure InfluxDB Line Protocol\nfor a large number of concurrent connections, on Windows:\nserver.conf InfluxDB Line Protocol network example configuration for large number of concurrent connections on Windows\n\n```ini\n# bind to specific NIC on port 9009, NIC is identified by IP addressline.tcp.net.bind.to=10.75.26.3:9009# large number of concurrent connectionsline.tcp.net.connection.limit=400# Windows will not allow 400 client to connect unless we use the \"hint\"line.tcp.net.connection.hint=true# connections will time out after 30s of inactivityline.tcp.net.connection.timeout=30000# receive buffer is 1MB because messages are small, smaller buffer will# reduce memory usage, 400 connections times 1MB = 400MB RAM required to handle inputline.tcp.net.rcvbuf=1M\n```\n\nFor more information on the default settings for the\nhttp\nand\npg\nprotocols,\nrefer to the\nserver configuration page\n.\n\n### Pooled connections‚Äã\n\nConnection pooling should be used for any production-ready use of PostgreSQL\nWire Protocol or InfluxDB Line Protocol over TCP.\nThe maximum number of pooled connections is configurable,\n(\npg.connection.pool.capacity\nfor PostgreSQL Wire Protocol and\n(\nline.tcp.connection.pool.capacity\nfor InfluxDB Line Protocol over TCP. The\ndefault number of connections for both interfaces is 64. Users should avoid\nusing too many connections, as large numbers of concurrent connections will\nincrease overall CPU usage.\n\n## OS configuration‚Äã\n\nChanging system settings on the host OS can improve QuestDB performance. QuestDB\nmay reach system limits relating to maximum open files, and virtual memory\nareas.\nQuestDB writes operating system errors to its logs unchanged. We only recommend\nchanging the following system settings in response to seeing such OS errors in\nthe logs.\n\n### Maximum open files‚Äã\n\nQuestDB uses a\ncolumnar\nstorage model, and\ntherefore its core data structures relate closely to the file system. Columnar\ndata is stored in its own\n.d\nfile, per time partition. In edge cases with\nextremely large tables, frequent out-of-order ingestion, or a high number of\ntable partitions, the number of open files may hit a user or system-wide maximum\nlimit, causing reduced performance and other unwanted behaviours.\nIn Linux/MacOS environments, maximum open file limits for the current user:\n\n```bash\n# Soft limitulimit -Sn# Hard limitulimit -Hn\n```\n\n\n#### Setting the open file limit for the current user:‚Äã\n\nOn a Linux environment, one must increase the hard limit. On MacOS, both the\nhard and soft limits must be set. See\nMax Open Files Limit on MacOS for the JVM\nfor more details.\nModify user limits using\nulimit\n:\n\n```bash\n# Hard limitulimit -H -n 1048576# Soft limitulimit -S -n 1048576\n```\n\nThe system-wide limit should be increased correspondingly.\n\n#### Setting the system-wide open file limit on Linux:‚Äã\n\nTo increase this setting and persist this configuration change, the limit on the\nnumber of concurrently open files can be amended in\n/etc/sysctl.conf\n:\n/etc/sysctl.conf\n\n```ini\nfs.file-max=1048576\n```\n\nTo confirm that this value has been correctly configured, reload\nsysctl\nand\ncheck the current value:\n\n```bash\n# reload configurationsysctl -p# query current settingssysctl fs.file-max\n```\n\n\n#### Extra steps for systemd‚Äã\n\nIf you are running the QuestDB using\nsystemd\n, you will also need to set the\nLimitNOFILE\nproperty in your service file.\nIf you have followed the\nsetup guide\n, then the file should be called\nquestdb.service\nand be located at\n~/.config/systemd/user/questdb.service\n.\nAdd this property to the\n[Service]\nsection, setting it to at least\n1048576\n, or higher if you have set higher OS-wide limits.\nThen restart the service. If you have configured these settings correctly, any warnings in the\nWeb Console\nshould now be cleared.\n\n#### Setting system-wide open file limit on MacOS:‚Äã\n\nOn MacOS, the system-wide limit can be modified by using\nlaunchctl\n:\n\n```shell\nsudo launchctl limit maxfiles 98304 2147483647\n```\n\nTo confirm the change, view the current settings using\nsysctl\n:\n\n```shell\nsysctl -a | grep kern.maxf\n```\n\n\n### Max virtual memory areas limit‚Äã\n\nThe database relies on memory mapping to read and write data to its files. If\nthe host machine has low limits on virtual memory mapping areas, this can cause\nout-of-memory exceptions\n(\nerrno=12\n). To\nincrease this setting and persist this configuration change, mapped memory area\nlimits can be amended in\n/etc/sysctl.conf\n:\n/etc/sysctl.conf\n\n```ini\nvm.max_map_count=1048576\n```\n\nEach mapped area may consume ~128 bytes for each map count i.e 1048576 may use\n1048576*128 = 134MB of kernel memory.\n\n```bash\n# reload configurationsysctl -p# query current settingscat /proc/sys/vm/max_map_count\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2104,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-368acf0c8170",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/configuration/overview",
    "title": "Configuration | QuestDB",
    "text": "This page describes methods for configuring QuestDB server settings.\nConfiguration can be set either:\n- In theserver.confconfiguration file available in theroot directory\n- Using environment variables\nWhen a key is absent from both the config file and the environment variables,\nthe default value is used.\nnote\nFor Windows users\nWhen entering path values, use either\n\\\\\nor\n/\ninstead of the native path\nseparator char\n\\\n.\n- üëçC:\\\\path\\\\to\\\\file\\\\path\n- üëçC:/path/to/file\n- üëéC:\\path\\to\\file\nThe single backslash is interpreted as an escape sequence start within\nJava properties\n.\n\n## Environment variables‚Äã\n\nAll settings in the configuration file can be set or overridden using\nenvironment variables. If a key is set in both the\nserver.conf\nfile and via an\nenvironment variable, the environment variable will take precedence and the\nvalue in the server configuration file will be ignored.\nTo make these configuration settings available to QuestDB via environment\nvariables, they must be in the following format:\n\n```shell\nQDB_<KEY_OF_THE_PROPERTY>\n```\n\nWhere\n<KEY_OF_THE_PROPERTY>\nis equal to the configuration key name. To\nproperly format a\nserver.conf\nkey as an environment variable it must have:\n- QDB_prefix\n- uppercase characters\n- all.period characters replaced with_underscore\nFor example, the server configuration key for query timeout must be passed as\ndescribed below:\n\n| server.confkey | env var |\n| --- | --- |\n| query.timeout | QDB_QUERY_TIMEOUT |\n\nnote\nQuestDB applies these configuration changes on startup and a running instance\nmust be restarted in order for configuration changes to take effect.\n\n### Examples‚Äã\n\nThe following configuration property customizes the query timeout:\nconf/server.conf\n\n```shell\nquery.timeout=120s\n```\n\nCustomizing the query timeout via environment variable\n\n```shell\nexport QDB_QUERY_TIMEOUT=120s\n```\n\n\n## Secrets from files‚Äã\n\nQuestDB supports reading sensitive configuration values from files using the\n_FILE\nsuffix convention. This is useful in containerized environments like\nKubernetes, where secrets are typically mounted as files rather than passed as\nenvironment variables.\nWhen a\n_FILE\nvariant is set, QuestDB reads the secret value from the specified\nfile path. This works with both environment variables and properties in\nserver.conf\n.\n\n### Usage‚Äã\n\nEnvironment variable:\n\n```shell\nQDB_PG_PASSWORD_FILE=/run/secrets/pg-password\n```\n\nProperty file:\nserver.conf\n\n```ini\npg.password.file=/run/secrets/pg-password\n```\n\n\n### Precedence‚Äã\n\nIf both a\n_FILE\nvariant and the direct value are set, the\n_FILE\nvariant\ntakes precedence. For example, if both\nQDB_PG_PASSWORD_FILE\nand\nQDB_PG_PASSWORD\nare set, the value is read from the file.\n\n### File requirements‚Äã\n\nSecret files must meet the following requirements:\n- Maximum size: 64KB\n- Encoding: UTF-8\n- Content handling: Leading and trailing whitespace is automatically trimmed\nThe following paths are not allowed for security reasons:\n- Paths containing..(path traversal)\n- Paths starting with/dev/,/proc/, or/sys/\n- Directories (including symlinks to directories)\nIf a secret file is empty or contains only whitespace, QuestDB logs an advisory\nwarning, as this may weaken authentication.\n\n### Error handling‚Äã\n\nIf a secret file cannot be read at startup, QuestDB fails to start. This\nincludes cases where the file does not exist, is too large, or the path is\nnot allowed.\nDuring runtime, if\nreload_config()\ncannot read a secret file, the reload\nfails and the previous value is retained. This ensures the server continues\noperating if a secret file is temporarily unavailable.\n\n### Reloading secrets‚Äã\n\nSecrets loaded from files support runtime reloading. After updating a secret\nfile, call\nreload_config()\nto apply the new value. See\nReloadable settings\nfor details.\nTo verify that a secret was loaded from a file, run\nSHOW PARAMETERS\nand check\nthe\nvalue_source\ncolumn, which displays\nfile\nfor secrets loaded from files.\n\n### Supported properties‚Äã\n\nThe following properties support the\n_FILE\nsuffix:\n\n| Property | Environment variable |\n| --- | --- |\n| pg.password | QDB_PG_PASSWORD_FILE |\n| pg.readonly.password | QDB_PG_READONLY_PASSWORD_FILE |\n| http.password | QDB_HTTP_PASSWORD_FILE |\n\n\n#### Enterprise properties‚Äã\n\nThe following additional properties are available in\nQuestDB Enterprise\n:\n\n| Property | Environment variable |\n| --- | --- |\n| acl.admin.password | QDB_ACL_ADMIN_PASSWORD_FILE |\n| acl.oidc.tls.keystore.password | QDB_ACL_OIDC_TLS_KEYSTORE_PASSWORD_FILE |\n| replication.object.store | QDB_REPLICATION_OBJECT_STORE_FILE |\n| cold.storage.object.store | QDB_COLD_STORAGE_OBJECT_STORE_FILE |\n| backup.object.store.* | QDB_BACKUP_OBJECT_STORE_*_FILE |\n\nFor Kubernetes-specific examples, see the\nKubernetes deployment guide\n.\n\n## Reloadable settings‚Äã\n\nCertain configuration settings can be reloaded without having to restart the\nserver. To reload a setting, edit its value in the\nserver.conf\nfile and then\nrun the\nreload_config\nSQL function:\nReload server configuration\n\n```questdb-sql\nSELECT reload_config();\n```\n\nIf the value was reloaded successfully, the\nreload_config\nfunction returns\ntrue\nand a message is printed to the server log:\n\n```questdb-sql\n2025-01-02T09:52:40.833848UTC I i.q.DynamicPropServerConfiguration reloaded config option [update, key=http.net.connection.limit, old=100, new=200]\n```\n\nEach key has a\nreloadable\nproperty that indicates whether the key can be\nreloaded. If yes, the\nreload_config\nfunction can be used to reload the\nconfiguration.\nAll reloadable properties can be also queried from the server:\nQuery reloadable properties\n\n```questdb-sql\n(SHOW PARAMETERS) WHERE reloadable = true;\n```\n\n\n## Keys and default values‚Äã\n\nThis section lists the configuration keys available to QuestDB by topic or\nsubsystem. Parameters for specifying buffer and memory page sizes are provided\nin the format\nn<unit>\n, where\n<unit>\ncan be one of the following:\n- mforMB\n- kforkB\nFor example:\nSetting maximum send buffer size to 2MB per TCP socket\n\n```ini\nhttp.net.connection.sndbuf=2m\n```\n\n\n### Shared worker‚Äã\n\nQuestDB uses three specialized worker pools to handle different workloads:\n- Network pool: handles HTTP, PostgreSQL, and ILP server I/O\n- Query pool: executes parallel query operations (filters, group-by)\n- Write pool: manages WAL apply jobs, table writes, materialized view refresh, and housekeeping tasks\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| shared.network.worker.count | max(2, CPU count - 2) if CPU count > 32, max(2, CPU count - 1) if CPU count > 16, otherwise max(2, CPU count) | No | Number of worker threads for the network pool, which handles HTTP, PostgreSQL, and ILP server I/O. Increasing this number will increase network I/O parallelism at the expense of CPU resources. |\n| shared.network.worker.affinity | none | No | Comma-delimited list of CPU ids, one per thread specified inshared.network.worker.count. By default, threads have no CPU affinity. |\n| shared.query.worker.count | max(2, CPU count - 2) if CPU count > 32, max(2, CPU count - 1) if CPU count > 16, otherwise max(2, CPU count) | No | Number of worker threads for the query pool, which executes parallel query operations (filters, group-by). Increasing this number will increase query parallelism at the expense of CPU resources. |\n| shared.query.worker.affinity | none | No | Comma-delimited list of CPU ids, one per thread specified inshared.query.worker.count. By default, threads have no CPU affinity. |\n| shared.write.worker.count | max(2, CPU count - 2) if CPU count > 32, max(2, CPU count - 1) if CPU count > 16, otherwise max(2, CPU count) | No | Number of worker threads for the write pool, which manages WAL apply jobs, table writes, materialized view refresh, and housekeeping tasks. Increasing this number will increase write parallelism at the expense of CPU resources. |\n| shared.write.worker.affinity | none | No | Comma-delimited list of CPU ids, one per thread specified inshared.write.worker.count. By default, threads have no CPU affinity. |\n| shared.worker.haltOnError | false | No | Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n\n\n### HTTP server‚Äã\n\nThis section describes configuration settings for the\nWeb Console\nand the REST API available by default on port\n9000\n. For details on the use of this component, refer to the\nweb console documentation\npage.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| http.enabled | true | No | Enable or disable HTTP server. |\n| http.bind.to | 0.0.0.0:9000 | No | IP address and port of HTTP server. A value of0means that the HTTP server will bind to all network interfaces. You can specify IP address of any individual network interface on your system. |\n| http.user | N/A | No | Username for HTTP Basic Authentication in QuestDB Open Source. QuestDB Enterprise Edition supports more advanced authentication mechanisms: RBAC |\n| http.password | N/A | No | Password for HTTP Basic Authentication in QuestDB Open Source. QuestDB Enterprise Edition supports more advanced authentication mechanisms: RBAC |\n| http.net.connection.limit | 64 | No | The maximum number permitted for simultaneous TCP connection to the HTTP server. The rationale of the value is to control server memory consumption. |\n| http.query.connection.limit | none | No | Soft limit for simultaneous HTTP query connections. When breached, new connections will be rejected but existing connections won't be closed immediately as long as http.net.connection.limit is not exceeded. |\n| http.ilp.connection.limit | none | No | Soft limit for simultaneous ILP connections. When breached, new connections will be rejected but existing connections won't be closed immediately as long as http.net.connection.limit is not exceeded. |\n| http.net.connection.timeout | 300000 | No | TCP connection idle timeout in milliseconds. Connection is closed by HTTP server when this timeout lapses. |\n| http.net.connection.sndbuf | 2M | No | Maximum send buffer size on each TCP socket. If this value is-1, the socket send buffer size remains unchanged from the OS defaults. |\n| http.net.connection.rcvbuf | 2M | No | Maximum receive buffer size on each TCP socket. If this value is-1, the socket receive buffer size remains unchanged from the OS defaults. |\n| http.net.connection.hint | false | No | Windows specific flag to overcome OS limitations on TCP backlog size |\n| http.net.connection.queue.timeout | 5000 | No | Amount of time in milliseconds a connection can wait in the listen backlog queue before it is refused. Connections will be aggressively removed from the backlog until the active connection limit is breached. |\n| http.net.bind.to | 0.0.0.0:9000 | No | IP address and port of HTTP server. |\n| http.connection.pool.initial.capacity | 4 | No | Initial size of pool of reusable objects that hold connection state. The pool should be configured to maximum realistic load so that it does not resize at runtime. |\n| http.connection.string.pool.capacity | 128 | No | Initial size of the string pool shared by the HTTP header and multipart content parsers. |\n| http.multipart.header.buffer.size | 512 | Yes | Buffer size in bytes used by the HTTP multipart content parser. |\n| http.multipart.idle.spin.count | 10000 | No | How long the code accumulates incoming data chunks for column and delimiter analysis. |\n| http.receive.buffer.size | 1M | Yes | Size of receive buffer. |\n| http.request.header.buffer.size | 64K | Yes | Size of internal buffer allocated for HTTP request headers. The value is rounded up to the nearest power of 2. When HTTP requests contain headers that exceed the buffer size server will disconnect the client with HTTP error in server log. |\n| http.worker.count | 0 | No | Number of threads in private worker pool. When0, HTTP server will be using shared worker pool of the server. Values above0switch on private pool. |\n| http.worker.affinity |  | No | Comma separated list of CPU core indexes. The number of items in this list must be equal to the worker count. |\n| http.worker.haltOnError | false | No | Changing the default value is strongly discouraged. Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n| http.send.buffer.size | 2M | Yes | Size of the internal send buffer. Larger buffer sizes result in fewer I/O interruptions the server is making at the expense of memory usage per connection. There is a limit of send buffer size after which increasing it stops being useful in terms of performance. 2MB seems to be optimal value. |\n| http.static.index.file.name | index.html | No | Name of index file for the Web Console. |\n| http.frozen.clock | false | No | Sets the clock to always return zero. This configuration parameter is used for internal testing. |\n| http.allow.deflate.before.send | false | No | Flag that indicates if Gzip compression of outgoing data is allowed. |\n| http.keep-alive.timeout | 5 | No | Used together withhttp.keep-alive.maxto set the value of HTTPKeep-Aliveresponse header. This instructs browser to keep TCP connection open. Has to be0whenhttp.versionis set toHTTP/1.0. |\n| http.keep-alive.max | 10000 | No | Seehttp.keep-alive.timeout. Has to be0whenhttp.versionis set toHTTP/1.0. |\n| http.static.public.directory | public | No | The name of directory for public web site. |\n| http.text.date.adapter.pool.capacity | 16 | No | Size of date adapter pool. This should be set to the anticipated maximum number ofDATEfields a text input can have. The pool is assigned to connection state and is reused alongside of connection state object. |\n| http.text.json.cache.limit | 16384 | No | JSON parser cache limit. Cache is used to compose JSON elements that have been broken up by TCP protocol. This value limits the maximum length of individual tag or tag value. |\n| http.text.json.cache.size | 8192 | No | Initial size of JSON parser cache. The value must not exceedhttp.text.json.cache.limitand should be set to avoid cache resizes at runtime. |\n| http.text.max.required.delimiter.stddev | 0.1222d | No | The maximum standard deviation value for the algorithm that calculates text file delimiter. Usually when text parser cannot recognise the delimiter it will log the calculated and maximum standard deviation for the delimiter candidate. |\n| http.text.max.required.line.length.stddev | 0.8 | No | Maximum standard deviation value for the algorithm that classifies input as text or binary. For the values above configured stddev input will be considered binary. |\n| http.text.metadata.string.pool.capacity | 128 | No | The initial size of pool for objects that wrap individual elements of metadata JSON, such as column names, date pattern strings and locale values. |\n| http.text.roll.buffer.limit | 4M | No | The limit of text roll buffer. Seehttp.text.roll.buffer.sizefor description. |\n| http.text.roll.buffer.size | 1024 | No | Roll buffer is a structure in the text parser that holds a copy of a line that has been broken up by TCP. The size should be set to the maximum length of text line in text input. |\n| http.text.analysis.max.lines | 1000 | No | Number of lines to read on CSV import for heuristics which determine column names & types. Lower line numbers may detect CSV schemas quicker, but possibly with less accuracy. 1000 lines is the maximum for this value. |\n| http.text.lexer.string.pool.capacity | 64 | No | The initial capacity of string fool, which wrapsSTRINGcolumn types in text input. The value should correspond to the maximum anticipated number of STRING columns in text input. |\n| http.text.timestamp.adapter.pool.capacity | 64 | No | Size of timestamp adapter pool. This should be set to the anticipated maximum number ofTIMESTAMPfields a text input can have. The pool is assigned to connection state and is reused alongside of connection state object. |\n| http.text.utf8.sink.size | 4096 | No | Initial size of UTF-8 adapter sink. The value should correspond the maximum individual field value length in text input. |\n| http.json.query.connection.check.frequency | 1000000 | No | Changing the default value is strongly discouraged. The value to throttle check if client socket has been disconnected. |\n| http.json.query.float.scale | 4 | No | The scale value of string representation ofFLOATvalues. |\n| http.json.query.double.scale | 12 | No | The scale value of string representation ofDOUBLEvalues. |\n| http.query.cache.enabled | true | No | Enable or disable the query cache. Cache capacity isnumber_of_blocks * number_of_rows. |\n| http.query.cache.block.count | 4 | No | Number of blocks for the query cache. |\n| http.query.cache.row.count | 16 | No | Number of rows for the query cache. |\n| http.security.readonly | false | No | Forces HTTP read only mode whentrue, disabling commands which modify the data or data structure, e.g. INSERT, UPDATE, or CREATE TABLE. |\n| http.security.max.response.rows | 2^63-1 | No | Limit the number of response rows over HTTP. |\n| http.security.interrupt.on.closed.connection | true | No | Switch to enable termination of SQL processing if the HTTP connection is closed. The mechanism affects performance so the connection is only checked aftercircuit.breaker.throttlecalls are made to the check method. The mechanism also reads from the input stream and discards it since some HTTP clients send this as a keep alive in between requests,circuit.breaker.buffer.sizedenotes the size of the buffer for this. |\n| http.pessimistic.health.check.enabled | false | No | When enabled, the health check returns HTTP 500 for any unhandled errors since the server started. |\n| circuit.breaker.throttle | 2000000 | No | Number of internal iterations such as loops over data before checking if the HTTP connection is still open |\n| circuit.breaker.buffer.size | 32 | No | Size of buffer to read from HTTP connection. If this buffer returns zero and the HTTP client is no longer sending data, SQL processing will be terminated. |\n| http.server.keep.alive | true | No | If set tofalse, the server will disconnect the client after completion of each request. |\n| http.version | HTTP/1.1 | No | Protocol version, other supported value isHTTP/1.0. |\n| http.context.web.console | / | No | Context path for the Web Console. If other REST services remain on the default context paths they will move to the same context path as the Web Console. InfluxDB Line Protocol (ILP) HTTP services are not affected and remain on their default paths. When default context paths are changed, moving the Web Console will not affect the configured paths. QuestDB creates copies of services on the Web Console paths so that both the Web Console and custom services remain operational. |\n| http.context.import | /imp | No | Context path of the file import service. |\n| http.context.table.status | /chk | No | Context path for the table statusservice used by the Import UI in the Web Console. |\n| http.context.export | /exp | No | Context path for the SQL result CSV export service. |\n| http.context.settings | /settings | No | Context path for the service which provides server-side settings to the Web Console. |\n| http.context.execute | /exec | No | Context path for the SQL execution service. |\n| http.context.warnings | /warnings | No | Context path for the Web Console specific service. |\n| http.context.ilp | /write,/api/v2/write | No | Context paths for the Influx Line Protocol (ILP) HTTP services. These are not used by the Web Console. |\n| http.context.ilp.ping | /ping | No | Context path for the Influx Line Protocol (ILP) ping endpoint. |\n| http.redirect.count | 1 | No | Number of HTTP redirects. All redirects are 301 - Moved Permanently. |\n| http.redirect.1 | / -> /index.html | No | Example redirect configuration. Format is 'source -> destination'. |\n\n\n### Cairo engine‚Äã\n\nThis section describes configuration settings for the Cairo SQL engine in\nQuestDB.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| config.reload.enabled | true | No | Whenfalse, disables reload_config() SQL function. |\n| query.timeout.sec | 60 | No | A global timeout (in seconds) for long-running queries. Timeout for each query can override the default by setting HTTP headerStatement-Timeoutor Postgresoptions. |\n| cairo.max.uncommitted.rows | 500000 | No | Maximum number of uncommitted rows per table, when the number of pending rows reaches this parameter on a table, a commit will be issued. |\n| cairo.o3.max.lag | 10 minutes | No | The maximum size of in-memory buffer in milliseconds. The buffer is allocated dynamically through analyzing the shape of the incoming data, ando3MaxLagis the upper limit. |\n| cairo.o3.min.lag | 1 second | No | The minimum size of in-memory buffer in milliseconds. The buffer is allocated dynamically through analyzing the shape of the incoming data, ando3MinLagis the lower limit. |\n| cairo.sql.backup.root | null | No | Output root directory for backups. |\n| cairo.sql.backup.dir.datetime.format | null | No | Date format for backup directory. |\n| cairo.sql.backup.dir.tmp.name | tmp | No | Name of tmp directory used during backup. |\n| cairo.sql.backup.mkdir.mode | 509 | No | Permission used when creating backup directories. |\n| cairo.snapshot.instance.id | empty string | No | Instance id to be included into disk snapshots. |\n| cairo.snapshot.recovery.enabled | true | No | Whenfalse, disables snapshot recovery on database start. |\n| cairo.root | db | No | Directory for storing db tables and metadata. This directory is inside the server root directory provided at startup. |\n| cairo.commit.mode | nosync | No | How changes to table are flushed to disk upon commit. Choices:nosync,async(flush call schedules update, returns immediately),sync(waits for flush on the appended column files to complete). |\n| cairo.rnd.memory.max.pages | 128 | No | Sets the max number of pages for memory used byrnd_functions. Supportsrnd_str()andrnd_symbol(). |\n| cairo.rnd.memory.page.size | 8K | No | Sets the memory page size used byrnd_functions. Supportsrnd_str()andrnd_symbol(). |\n| cairo.create.as.select.retry.count | 5 | No | Number of types table creation or insertion will be attempted. |\n| cairo.default.map.type | fast | No | Type of map used. Options:fast(speed at the expense of storage),compact. |\n| cairo.default.symbol.cache.flag | true | No | Whentrue, symbol values will be cached on Java heap instead of being looked up in the database files. |\n| cairo.default.symbol.capacity | 256 | No | Specifies approximate capacity forSYMBOLcolumns. It should be equal to number of unique symbol values stored in the table and getting this value badly wrong will cause performance degradation. Must be power of 2. |\n| cairo.file.operation.retry.count | 30 | No | Number of attempts to open files. |\n| cairo.idle.check.interval | 300000 | No | Frequency of writer maintenance job in milliseconds. |\n| cairo.inactive.reader.ttl | 120000 | No | TTL (Time-To-Live) to close inactive readers in milliseconds. |\n| cairo.wal.inactive.writer.ttl | 120000 | No | TTL (Time-To-Live) to close inactive WAL writers in milliseconds. |\n| cairo.inactive.writer.ttl | 600000 | No | TTL (Time-To-Live) to close inactive writers in milliseconds. |\n| cairo.index.value.block.size | 256 | No | Approximation of number of rows for a single index key, must be power of 2. |\n| cairo.max.swap.file.count | 30 | No | Number of attempts to open swap files. |\n| cairo.mkdir.mode | 509 | No | File permission mode for new directories. |\n| cairo.parallel.index.threshold | 100000 | No | Minimum number of rows before allowing use of parallel indexation. |\n| cairo.reader.pool.max.segments | 10 | No | Number of segments in the table reader pool. Each segment holds up to 32 readers. |\n| cairo.wal.writer.pool.max.segments | 10 | No | Number of segments in the WAL writer pool. Each segment holds up to 32 writers. |\n| cairo.spin.lock.timeout | 1000 | No | Timeout when attempting to get BitmapIndexReaders in millisecond. |\n| cairo.character.store.capacity | 1024 | No | Size of the CharacterStore. |\n| cairo.character.store.sequence.pool.capacity | 64 | No | Size of the CharacterSequence pool. |\n| cairo.column.pool.capacity | 4096 | No | Size of the Column pool in the SqlCompiler. |\n| cairo.compact.map.load.factor | 0.7 | No | Load factor for CompactMaps. |\n| cairo.expression.pool.capacity | 8192 | No | Size of the ExpressionNode pool in SqlCompiler. |\n| cairo.fast.map.load.factor | 0.5 | No | Load factor for all FastMaps. |\n| cairo.sql.join.context.pool.capacity | 64 | No | Size of the JoinContext pool in SqlCompiler. |\n| cairo.lexer.pool.capacity | 2048 | No | Size of FloatingSequence pool in GenericLexer. |\n| cairo.sql.map.key.capacity | 2M | No | Key capacity in FastMap and CompactMap. |\n| cairo.sql.map.max.resizes | 2^31 | No | Number of map resizes in FastMap and CompactMap before a resource limit exception is thrown, each resize doubles the previous size. |\n| cairo.sql.map.page.size | 4m | No | Memory page size for FastMap and CompactMap. |\n| cairo.sql.map.max.pages | 2^31 | No | Memory max pages for CompactMap. |\n| cairo.model.pool.capacity | 1024 | No | Size of the QueryModel pool in the SqlCompiler. |\n| cairo.sql.sort.key.page.size | 4M | No | Memory page size for storing keys in LongTreeChain. |\n| cairo.sql.sort.key.max.pages | 2^31 | No | Max number of pages for storing keys in LongTreeChain before a resource limit exception is thrown. |\n| cairo.sql.sort.light.value.page.size | 1048576 | No | Memory page size for storing values in LongTreeChain. |\n| cairo.sql.sort.light.value.max.pages | 2^31 | No | Max pages for storing values in LongTreeChain. |\n| cairo.sql.hash.join.value.page.size | 16777216 | No | Memory page size of the slave chain in full hash joins. |\n| cairo.sql.hash.join.value.max.pages | 2^31 | No | Max pages of the slave chain in full hash joins. |\n| cairo.sql.latest.by.row.count | 1000 | No | Number of rows for LATEST BY. |\n| cairo.sql.hash.join.light.value.page.size | 1048576 | No | Memory page size of the slave chain in light hash joins. |\n| cairo.sql.hash.join.light.value.max.pages | 2^31 | No | Max pages of the slave chain in light hash joins. |\n| cairo.sql.sort.value.page.size | 16777216 | No | Memory page size of file storing values in SortedRecordCursorFactory. |\n| cairo.sql.sort.value.max.pages | 2^31 | No | Max pages of file storing values in SortedRecordCursorFactory. |\n| cairo.work.steal.timeout.nanos | 10000 | No | Latch await timeout in nanos for stealing indexing work from other threads. |\n| cairo.parallel.indexing.enabled | true | No | Allows parallel indexation. Works in conjunction with cairo.parallel.index.threshold. |\n| cairo.sql.join.metadata.page.size | 16384 | No | Memory page size for JoinMetadata file. |\n| cairo.sql.join.metadata.max.resizes | 2^31 | No | Number of map resizes in JoinMetadata before a resource limit exception is thrown, each resize doubles the previous size. |\n| cairo.sql.analytic.column.pool.capacity | 64 | No | Size of AnalyticColumn pool in SqlParser. |\n| cairo.sql.create.table.model.batch.size | 1000000 | No | Batch size for non-atomic CREATE AS SELECT statements. |\n| cairo.sql.column.cast.model.pool.capacity | 16 | No | Size of CreateTableModel pool in SqlParser. |\n| cairo.sql.rename.table.model.pool.capacity | 16 | No | Size of RenameTableModel pool in SqlParser. |\n| cairo.sql.with.clause.model.pool.capacity | 128 | No | Size of WithClauseModel pool in SqlParser. |\n| cairo.sql.insert.model.pool.capacity | 64 | No | Size of InsertModel pool in SqlParser. |\n| cairo.sql.insert.model.batch.size | 1000000 | No | Batch size for non-atomic INSERT INTO SELECT statements. |\n| cairo.sql.copy.model.pool.capacity | 32 | No | Size of CopyModel pool in SqlParser. |\n| cairo.sql.copy.buffer.size | 2M | No | Size of buffer used when copying tables. |\n| cairo.sql.double.cast.scale | 12 | No | Maximum number of decimal places that types cast as doubles have. |\n| cairo.sql.float.cast.scale | 4 | No | Maximum number of decimal places that types cast as floats have. |\n| cairo.sql.copy.formats.file | /text_loader.json | No | Name of file with user's set of date and timestamp formats. |\n| cairo.sql.jit.mode | on | No | JIT compilation for SQL queries. May be disabled by setting this value tooff. |\n| cairo.sql.jit.debug.enabled | false | No | Sets debug flag for JIT compilation. When enabled, assembly will be printed intostdout. |\n| cairo.sql.jit.max.in.list.size.threshold | 10 | No | Controls whether or not JIT compilation will be used for a query that uses the IN predicate. If the IN list is longer than this threshold, JIT compilation will be cancelled. |\n| cairo.sql.jit.bind.vars.memory.page.size | 4K | No | Sets the memory page size for storing bind variable values for JIT compiled filter. |\n| cairo.sql.jit.bind.vars.memory.max.pages | 8 | No | Sets the max¬†memory pages for storing bind variable values for JIT compiled filter. |\n| cairo.sql.jit.page.address.cache.threshold | 1M | No | Sets minimum cache size to shrink page address cache after query execution. |\n| cairo.sql.jit.ir.memory.page.size | 8K | No | Sets the memory page size for storing IR for JIT compilation. |\n| cairo.sql.jit.ir.memory.max.pages | 8 | No | Sets max memory pages for storing IR for JIT compilation. |\n| cairo.sql.page.frame.min.rows | 1000 | No | Sets the minimum number of rows in page frames used in SQL queries. |\n| cairo.sql.page.frame.max.rows | 1000000 | No | Sets the maximum number of rows in page frames used in SQL. queries |\n| cairo.sql.sampleby.page.size | 0 | No | SampleBy index query page size. Max values returned in single scan. 0 is default, and it means to use symbol block capacity. |\n| cairo.sql.sampleby.default.alignment.calendar | 0 | No | SampleBy default alignment behaviour. true corresponds to ALIGN TO CALENDAR, false corresponds to ALIGN TO FIRST OBSERVATION. |\n| cairo.date.locale | en | No | The locale to handle date types. |\n| cairo.timestamp.locale | en | No | The locale to handle timestamp types. |\n| cairo.o3.column.memory.size | 256k | No | Memory page size per column for O3 operations. Please be aware O3 will use 2x of the set value per column (therefore a default of 2x256kb). |\n| cairo.writer.data.append.page.size | 16M | No | mmap sliding page size that table writer uses to append data for each column. |\n| cairo.writer.data.index.key.append.page.size | 512K | No | mmap page size for appending index key data; key data is number of distinct symbol values times 4 bytes. |\n| cairo.writer.data.index.value.append.page.size | 16M | No | mmap page size for appending value data. |\n| cairo.writer.misc.append.page.size | 4K | No | mmap page size for mapping small files, default value is OS page size (4k Linux, 64K windows, 16k OSX M1). Overriding this rounds to the nearest (greater) multiple of the OS page size. |\n| cairo.writer.command.queue.capacity | 32 | No | Maximum writer ALTER TABLE and replication command capacity. Shared between all the tables. |\n| cairo.writer.tick.rows.count | 1024 | No | Row count to check writer command queue after on busy writing, e.g. tick after X rows written. |\n| cairo.writer.alter.busy.wait.timeout | 500 | No | Maximum wait timeout in milliseconds forALTER TABLESQL statement run via REST and PostgreSQL Wire Protocol interfaces when statement execution isASYNCHRONOUS. |\n| cairo.sql.column.purge.queue.capacity | 128 | No | Purge column version job queue. Increase the size if column version not automatically cleanup after execution of UPDATE SQL statement. Reduce to decrease initial memory footprint. |\n| cairo.sql.column.purge.task.pool.capacity | 256 | No | Column version task object pool capacity. Increase to reduce GC, reduce to decrease memory footprint. |\n| cairo.sql.column.purge.retry.delay | 10000 | No | Initial delay (Œºs) before re-trying purge of stale column files. |\n| cairo.sql.column.purge.retry.delay.multiplier | 10.0 | No | Multiplier used to increases retry delay with each iteration. |\n| cairo.sql.column.purge.retry.delay.limit | 60000000 | No | Delay limit (Œºs), upon reaching which, the re-try delay remains constant. |\n| cairo.sql.column.purge.retry.limit.days | 31 | No | Number of days purge system will continue to re-try deleting stale column files before giving up. |\n| cairo.volumes | - | No | A comma separated list ofalias -> root-pathpairs defining allowed volumes to be used inCREATE TABLE IN VOLUMEstatements. |\n| cairo.system.table.prefix | sys. | No | Prefix of the tables used for QuestDB internal data storage. These tables are hidden from QuestDB web console. |\n| cairo.wal.enabled.default | true | No | Setting defining whether WAL table is the default when usingCREATE TABLE. |\n| cairo.o3.partition.split.min.size | 50MB | No | The estimated partition size on disk. This setting is one of the conditions to triggerauto-partitioning. |\n| cairo.o3.last.partition.max.splits | 20 | No | The number of partition pieces allowed before the last partition piece is merged back to the physical partition. |\n| cairo.o3.partition.purge.list.initial.capacity | 1 | No | Number of partition expected on average. Initial value for purge allocation job, extended in runtime automatically. |\n| cairo.sql.parallel.groupby.enabled | true | No | Enables parallel GROUP BY execution; requires at least 4 shared worker threads. |\n| cairo.sql.parallel.groupby.merge.shard.queue.capacity | <auto> | No | Merge queue capacity for parallel GROUP BY; used for parallel tasks that merge shard hash tables. |\n| cairo.sql.parallel.groupby.sharding.threshold | 100000 | No | Threshold for parallel GROUP BY to shard the hash table holding the aggregates. |\n| cairo.sql.groupby.allocator.default.chunk.size | 128k | No | Default size for memory buffers in GROUP BY function native memory allocator. |\n| cairo.sql.groupby.allocator.max.chunk.size | 4gb | No | Maximum allowed native memory allocation for GROUP BY functions. |\n| cairo.sql.unordered.map.max.entry.size | 24 | No | Threshold in bytes for switching from single memory buffer hash table (unordered) to a hash table with separate heap for entries (ordered). |\n| cairo.sql.window.max.recursion | 128 | No | Prevents stack overflow errors when evaluating complex nested SQLs. The value is an approximate number of nested SELECT clauses. |\n| cairo.sql.query.registry.pool.size | <auto> | No | Pre-sizes the internal data structure that stores active query executions. The value is chosen automatically based on the number of threads in the shared worker pool. |\n| cairo.sql.analytic.initial.range.buffer.size | 32 | No | Window function buffer size in record counts. Pre-sizes buffer for every windows function execution to contain window records. |\n| cairo.system.writer.data.append.page.size | 256k | No | mmap sliding page size that TableWriter uses to append data for each column specifically for System tables. |\n| cairo.file.descriptor.cache.enabled | true | No | enables or disables the file-descriptor cache |\n| cairo.partition.encoder.parquet.raw.array.encoding.enabled | false | No | determines whether to export arrays in QuestDB-native binary format (true, less compatible) or Parquet-native format (false, more compatible). |\n| cairo.partition.encoder.parquet.version | 1 | No | Output parquet version to use for parquet-encoded partitions. Can be 1 or 2. |\n| cairo.partition.encoder.parquet.statistics.enabled | true | No | Controls whether or not statistics are included in parquet-encoded partitions. |\n| cairo.partition.encoder.parquet.compression.codec | ZSTD | No | Sets the default compression codec for parquet-encoded partitions. Alternatives includeLZ4_RAW,SNAPPY. |\n| cairo.partition.encoder.parquet.compression.level | 9 (ZSTD), 0 (otherwise) | No | Sets the default compression level for parquet-encoded partitions. Dependent on underlying compression codec. |\n| cairo.partition.encoder.parquet.row.group.size | 100000 | No | Sets the default row-group size for parquet-encoded partitions. |\n| cairo.partition.encoder.parquet.data.page.size | 1048576 | No | Sets the default page size for parquet-encoded partitions. |\n\n\n### WAL table configurations‚Äã\n\nThe following WAL tables settings on parallel threads are configurable for\napplying WAL data to the table storage:\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| wal.apply.worker.count | equal to the CPU core count | No | Number of dedicated worker threads assigned to handle WAL table data. |\n| wal.apply.worker.affinity | equal to the CPU core count | No | Comma separated list of CPU core indexes. |\n| wal.apply.worker.haltOnError | false | No | Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n| cairo.wal.purge.interval | 30000 | No | Period in ms of how often WAL-applied files are cleaned up from the disk |\n| cairo.wal.segment.rollover.row.count | 200000 | No | Row count of how many rows are written to the same WAL segment before starting a new segment. Triggers in conjunction withcairo.wal.segment.rollover.size(whichever is first). |\n| cairo.wal.squash.uncommitted.rows.multiplier | 20.0 | No | Multiplier to cairo.max.uncommitted.rows to calculate the limit of rows that can be kept invisible when writing to WAL table under heavy load, when multiple transactions are to be applied. It is used to reduce the number Out-Of-Order (O3) commits when O3 commits are unavoidable by squashing multiple commits together. Setting it very low can increase O3 commit frequency and decrease the throughput. Setting it too high may cause excessive memory usage and increase the latency. |\n| cairo.wal.max.lag.txn.count | 20 | No | Maximum number of transactions that can be kept invisible when writing to WAL table. Once the number is reached, full commit occurs. If not set, defaults to the rounded value of cairo.wal.squash.uncommitted.rows.multiplier. |\n| cairo.wal.apply.parallel.sql.enabled | true | No | When disabled, SQL executed by the WAL apply job will always run single-threaded. |\n\n\n### COPY settings‚Äã\n\n\n#### Import‚Äã\n\nThis section describes configuration settings for using\nCOPY\nto import large\nCSV files, or export parquet files.\nSettings for\nCOPY FROM\n(import):\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| cairo.sql.copy.root | import | No | Input root directory for CSV imports viaCOPYSQL and for Parquet file reading. This path must not overlap with other directory (e.g. db, conf) of running instance, otherwise import may delete or overwrite existing files. Relative paths are resolved against the server root directory. |\n| cairo.sql.copy.work.root | null | No | Temporary import file directory. Defaults toroot_directory/tmpif not set explicitly. |\n| cairo.iouring.enabled | true | No | Enable or disable io_uring implementation. Applicable to newer Linux kernels only. Can be used to switch io_uring interface usage off if there's a kernel bug affecting it. |\n| cairo.sql.copy.buffer.size | 2 MiB | No | Size of read buffers used in import. |\n| cairo.sql.copy.log.retention.days | 3 | No | Number of days to keep import messages insys.text_import_log. |\n| cairo.sql.copy.max.index.chunk.size | 100M | No | Maximum size of index chunk file used to limit total memory requirements of import. Indexing phase should use roughlythread_count * cairo.sql.copy.max.index.chunk.sizeof memory. |\n| cairo.sql.copy.queue.capacity | 32 | No | Size of copy task queue. Should be increased if there's more than 32 import workers. |\n\nCSV import configuration for Docker\nFor QuestDB instances using Docker:\n- cairo.sql.copy.rootmust be defined using one of the following settings:The environment variableQDB_CAIRO_SQL_COPY_ROOT.Thecairo.sql.copy.rootinserver.conf.\n- The path for the source CSV file is mounted.\n- The source CSV file path and the path defined byQDB_CAIRO_SQL_COPY_ROOTare\nidentical.\n- It is optional to defineQDB_CAIRO_SQL_COPY_WORK_ROOT.\nThe following is an example command to start a QuestDB instance on Docker, in\norder to import a CSV file:\n\n```shell\ndocker run -p 9000:9000 \\-v \"/tmp/questdb:/var/lib/questdb\" \\-v \"/tmp/questdb/my_input_root:/var/lib/questdb/questdb_import\" \\-e QDB_CAIRO_SQL_COPY_ROOT=/var/lib/questdb/questdb_import \\questdb/questdb\n```\n\nWhere:\n- -v \"/tmp/questdb/my_input_root:/var/lib/questdb/questdb_import\": Defining a\nsource CSV file location to be/tmp/questdb/my_input_rooton local machine\nand mounting it to/var/lib/questdb/questdb_importin the container.\n- -e QDB_CAIRO_SQL_COPY_ROOT=/var/lib/questdb/questdb_import: Defining the\ncopy root directory to be/var/lib/questdb/questdb_import.\nIt is important that the two path are identical\n(\n/var/lib/questdb/questdb_import\nin the example).\n\n#### Export‚Äã\n\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| cairo.sql.copy.export.root | export | No | Root directory for parquet exports viaCOPY-TOSQL. This path must not overlap with other directory (e.g. db, conf) of running instance, otherwise export may delete or overwrite existing files. Relative paths are resolved against the server root directory. |\n\nParquet export is also generally impacted by query execution and parquet conversion parameters.\nIf not overridden, the following default setting will be used.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| cairo.partition.encoder.parquet.raw.array.encoding.enabled | false | No | determines whether to export arrays in QuestDB-native binary format (true, less compatible) or Parquet-native format (false, more compatible). |\n| cairo.partition.encoder.parquet.version | 1 | No | Output parquet version to use for parquet-encoded partitions. Can be 1 or 2. |\n| cairo.partition.encoder.parquet.statistics.enabled | true | No | Controls whether or not statistics are included in parquet-encoded partitions. |\n| cairo.partition.encoder.parquet.compression.codec | ZSTD | No | Sets the default compression codec for parquet-encoded partitions. Alternatives includeLZ4_RAW,SNAPPY. |\n| cairo.partition.encoder.parquet.compression.level | 9 (ZSTD), 0 (otherwise) | No | Sets the default compression level for parquet-encoded partitions. Dependent on underlying compression codec. |\n| cairo.partition.encoder.parquet.row.group.size | 100000 | No | Sets the default row-group size for parquet-encoded partitions. |\n| cairo.partition.encoder.parquet.data.page.size | 1048576 | No | Sets the default page size for parquet-encoded partitions. |\n\n\n### Parallel SQL execution‚Äã\n\nThis section describes settings that can affect the level of parallelism during\nSQL execution, and therefore can also have an impact on performance.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| cairo.sql.parallel.filter.enabled | true | No | Enable or disable parallel SQL filter execution. JIT compilation takes place only when this setting is enabled. |\n| cairo.sql.parallel.filter.pretouch.enabled | true | No | Enable column pre-touch as part of the parallel SQL filter execution, to improve query performance for large tables. |\n| cairo.page.frame.shard.count | 4 | No | Number of shards for both dispatch and reduce queues. Shards reduce queue contention between SQL statements that are executed concurrently. |\n| cairo.page.frame.reduce.queue.capacity | 64 | No | Reduce queue is used for data processing and should be large enough to supply tasks for worker threads (shared worked pool). |\n| cairo.page.frame.rowid.list.capacity | 256 | No | Row ID list initial capacity for each slot of the reduce queue. Larger values reduce memory allocation rate, but increase minimal RSS size. |\n| cairo.page.frame.column.list.capacity | 16 | No | Column list capacity for each slot of the reduce queue. Used by JIT-compiled filter functions. Larger values reduce memory allocation rate, but increase minimal RSS size. |\n\n\n### Postgres wire protocol‚Äã\n\nThis section describes configuration settings for client connections using\nPostgresSQL wire protocol.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| pg.enabled | true | No | Configuration for enabling or disabling the Postres interface. |\n| pg.net.bind.to | 0.0.0.0:8812 | No | IP address and port of Postgres wire protocol server. 0 means that the server will bind to all network interfaces. You can specify IP address of any individual network interface on your system. |\n| pg.net.connection.limit | 64 | Yes | The maximum number permitted for simultaneous Postgres connections to the server. This value is intended to control server memory consumption. |\n| pg.net.connection.timeout | 300000 | No | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses. |\n| pg.net.connection.rcvbuf | -1 | No | Maximum send buffer size on each TCP socket. If value is -1 socket send buffer remains unchanged from OS default. |\n| pg.net.connection.sndbuf | -1 | No | Maximum receive buffer size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default. |\n| pg.net.connection.hint | false | No | Windows specific flag to overcome OS limitations on TCP backlog size |\n| pg.net.connection.queue.timeout | 300000 | No | Amount of time in milliseconds a connection can wait in the listen backlog queue before it is refused. Connections will be aggressively removed from the backlog until the active connection limit is breached. |\n| pg.security.readonly | false | No | Forces PostgreSQL Wire Protocol read only mode whentrue, disabling commands which modify the data or data structure, e.g. INSERT, UPDATE, or CREATE TABLE. |\n| pg.character.store.capacity | 4096 | No | Size of the CharacterStore. |\n| pg.character.store.pool.capacity | 64 | No | Size of the CharacterStore pool capacity. |\n| pg.connection.pool.capacity | 64 | No | The maximum amount of pooled connections this interface may have. |\n| pg.password | quest | Yes | Postgres database password. |\n| pg.user | admin | Yes | Postgres database username. |\n| pg.readonly.user.enabled | false | Yes | Enable or disable Postgres database read-only user account. When enabled, this additional user can be used to open read-only connections to the database. |\n| pg.readonly.password | quest | Yes | Postgres database read-only user password. |\n| pg.readonly.user | user | Yes | Postgres database read-only user username. |\n| pg.select.cache.enabled | true | No | Enable or disable the SELECT query cache. Cache capacity isnumber_of_blocks * number_of_rows. |\n| pg.select.cache.block.count | 16 | No | Number of blocks to cache SELECT query execution plan against text to speed up execution. |\n| pg.select.cache.row.count | 16 | No | Number of rows to cache for SELECT query execution plan against text to speed up execution. |\n| pg.insert.cache.enabled | true | No | Enable or disable the INSERT query cache. Cache capacity isnumber_of_blocks * number_of_rows. |\n| pg.insert.cache.block.count | 8 | No | Number of blocks to cache INSERT query execution plan against text to speed up execution. |\n| pg.insert.cache.row.count | 8 | No | Number of rows to cache for INSERT query execution plan against text to speed up execution. |\n| pg.update.cache.enabled | true | No | Enable or disable the UPDATE query cache. Cache capacity isnumber_of_blocks * number_of_rows. |\n| pg.update.cache.block.count | 8 | No | Number of blocks to cache UPDATE query execution plan against text to speed up execution. |\n| pg.update.cache.row.count | 8 | No | Number of rows to cache for UPDATE query execution plan against text to speed up execution. |\n| pg.max.blob.size.on.query | 512k | No | For binary values, clients will receive an error when requesting blob sizes above this value. |\n| pg.recv.buffer.size | 1M | Yes | Size of the buffer for receiving data. |\n| pg.send.buffer.size | 1M | Yes | Size of the buffer for sending data. |\n| pg.date.locale | en | No | The locale to handle date types. |\n| pg.timestamp.locale | en | No | The locale to handle timestamp types. |\n| pg.worker.count | 0 | No | Number of dedicated worker threads assigned to handle PostgreSQL Wire Protocol queries. When0, the jobs will use the shared pool. |\n| pg.worker.affinity |  | No | Comma-separated list of thread numbers which should be pinned for Postgres ingestion. Examplepg.worker.affinity=1,2,3. |\n| pg.halt.on.error | false | No | Whether ingestion should stop upon internal error. |\n| pg.daemon.pool | true | No | Defines whether to run all PostgreSQL Wire Protocol worker threads in daemon mode (true) or not (false). |\n| pg.binary.param.count.capacity | 2 | No | Size of the initial capacity for the pool used for binary bind variables. |\n| pg.named.statement.limit | 64 | Yes | Size of the named statement pool. |\n\n\n### InfluxDB Line Protocol (ILP)‚Äã\n\nThis section describes ingestion settings for incoming messages using InfluxDB\nLine Protocol.\n\n| Property | Default | Description |\n| --- | --- | --- |\n| line.default.partition.by | DAY | Table partition strategy to be used with tables that are created automatically by InfluxDB Line Protocol. Possible values are:HOUR,DAY,WEEK,MONTH, andYEAR. |\n| line.auto.create.new.columns | true | When enabled, automatically creates new columns when they appear in the ingested data. When disabled, messages with new columns will be rejected. |\n| line.auto.create.new.tables | true | When enabled, automatically creates new tables when they appear in the ingested data. When disabled, messages for non-existent tables will be rejected. |\n| line.log.message.on.error | true | Controls whether malformed ILP messages are printed to the server log when errors occur. |\n\n\n#### HTTP specific settings‚Äã\n\nILP over HTTP is the preferred way of ingesting data.\n\n| Property | Default | Description |\n| --- | --- | --- |\n| line.http.enabled | true | Enable ILP over HTTP. Default port is 9000. Enabled by default within open source versions, defaults to false and must be enabled for Enterprise. |\n| line.http.ping.version | v2.2.2 | Version information for the ping response of ILP over HTTP. |\n| HTTP properties | Various | SeeHTTP settingsfor general HTTP configuration. ILP over HTTP inherits from HTTP settings. |\n\n\n#### TCP specific settings‚Äã\n\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| line.tcp.enabled | true | No | Enable or disable line protocol over TCP. |\n| line.tcp.net.bind.to | 0.0.0.0:9009 | No | IP address of the network interface to bind listener to and port. By default, TCP receiver listens on all network interfaces. |\n| line.tcp.net.connection.limit | 256 | Yes | The maximum number permitted for simultaneous connections to the server. This value is intended to control server memory consumption. |\n| line.tcp.net.connection.timeout | 300000 | No | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses. |\n| line.tcp.net.connection.hint | false | No | Windows specific flag to overcome OS limitations on TCP backlog size |\n| line.tcp.net.connection.rcvbuf | -1 | No | Maximum buffer receive size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default. |\n| line.tcp.net.connection.queue.timeout | 5000 | No | Amount of time in milliseconds a connection can wait in the listen backlog queue before its refused. Connections will be aggressively removed from the backlog until the active connection limit is breached. |\n| line.tcp.auth.db.path |  | No | Path which points to the authentication db file. |\n| line.tcp.connection.pool.capacity | 64 | No | The maximum amount of pooled connections this interface may have. |\n| line.tcp.timestamp | n | No | Input timestamp resolution. Possible values aren,u,ms,sandh. |\n| line.tcp.msg.buffer.size | 32768 | No | Size of the buffer read from queue. Maximum size of write request, regardless of the number of measurements. |\n| line.tcp.maintenance.job.interval | 1000 | No | Maximum amount of time (in milliseconds) between maintenance jobs committing any uncommitted data on inactive tables. |\n| line.tcp.min.idle.ms.before.writer.release | 500 | No | Minimum amount of idle time (in milliseconds) before a table writer is released. |\n| line.tcp.commit.interval.fraction | 0.5 | No | Commit lag fraction. Used to calculate commit interval for the table according to the following formula:commit_interval = commit_lag ‚àó fraction. The calculated commit interval defines how long uncommitted data will need to remain uncommitted. |\n| line.tcp.commit.interval.default | 1000 | No | Default commit interval in milliseconds. |\n| line.tcp.max.measurement.size | 32768 | No | Maximum size of any measurement. |\n| line.tcp.writer.worker.count |  | No | Number of dedicated I/O worker threads assigned to write data to tables. When0, the writer jobs will use the shared pool. |\n| line.tcp.writer.worker.affinity |  | No | Comma-separated list of thread numbers which should be pinned for line protocol ingestion over TCP. CPU core indexes are 0-based. |\n| line.tcp.writer.worker.sleep.threshold | 1000 | No | Amount of subsequent loop iterations with no work done before the worker goes to sleep. |\n| line.tcp.writer.worker.yield.threshold | 10 | No | Amount of subsequent loop iterations with no work done before the worker thread yields. |\n| line.tcp.writer.queue.capacity | 128 | No | Size of the queue between the IO jobs and the writer jobs, each queue entry represents a measurement. |\n| line.tcp.writer.halt.on.error | false | No | Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n| line.tcp.io.worker.count |  | No | Number of dedicated I/O worker threads assigned to parse TCP input. When0, the writer jobs will use the shared pool. |\n| line.tcp.io.worker.affinity |  | No | Comma-separated list of thread numbers which should be pinned for line protocol ingestion over TCP. CPU core indexes are 0-based. |\n| line.tcp.io.worker.sleep.threshold | 1000 | No | Amount of subsequent loop iterations with no work done before the worker goes to sleep. |\n| line.tcp.io.worker.yield.threshold | 10 | No | Amount of subsequent loop iterations with no work done before the worker thread yields. |\n| line.tcp.disconnect.on.error | true | No | Disconnect TCP socket that sends malformed messages. |\n| line.tcp.acl.enabled | true | No | Enable or disable Access Control List (ACL) authentication for InfluxDB Line Protocol over TCP. Enterprise only. |\n\n\n#### UDP specific settings‚Äã\n\nnote\nThe UDP receiver is deprecated since QuestDB version 6.5.2. We recommend ILP\nover HTTP instead, or less frequently\nILP over TCP\n.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| line.udp.join | 232.1.2.3 | No | Multicast address receiver joins. This values is ignored when receiver is in \"unicast\" mode. |\n| line.udp.bind.to | 0.0.0.0:9009 | No | IP address of the network interface to bind listener to and port. By default UDP receiver listens on all network interfaces. |\n| line.udp.commit.rate | 1000000 | No | For packet bursts the number of continuously received messages after which receiver will force commit. Receiver will commit irrespective of this parameter when there are no messages. |\n| line.udp.msg.buffer.size | 2048 | No | Buffer used to receive single message. This value should be roughly equal to your MTU size. |\n| line.udp.msg.count | 10000 | No | Only for Linux. On Linux, QuestDB will use therecvmmsg()system call. This is the max number of messages to receive at once. |\n| line.udp.receive.buffer.size | 8388608 | No | UDP socket buffer size. Larger size of the buffer will help reduce message loss during bursts. |\n| line.udp.enabled | false | No | Enable or disable UDP receiver. |\n| line.udp.own.thread | false | No | Whentrue, UDP receiver will use its own thread and busy spin that for performance reasons. \"false\" makes receiver use worker threads that do everything else in QuestDB. |\n| line.udp.own.thread.affinity | -1 | No | -1 does not set thread affinity. OS will schedule thread and it will be liable to run on random cores and jump between the. 0 or higher pins thread to give core. This property is only valid when UDP receiver uses own thread. |\n| line.udp.unicast | false | No | Whentrue, UDP will use unicast. Otherwise multicast. |\n| line.udp.timestamp | n | No | Input timestamp resolution. Possible values aren,u,ms,sandh. |\n| line.udp.commit.mode | nosync | No | Commit durability. Available values arenosync,syncandasync. |\n\n\n### Database replication‚Äã\n\nnote\nReplication is\nEnterprise\nonly.\nReplication enables high availability clusters.\nFor setup instructions, see the\nreplication operations\nguide.\nFor an overview of the concept, see the\nreplication concept\npage.\nFor a tuning guide see, the\nreplication tuning guide\n.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| replication.role | none | No | Defaults tononefor stand-alone instances. To enable replication set to one of:primary,replica. |\n| replication.object.store |  | No | A configuration string that allows connecting to an object store. The format isscheme::key1=value;key2=value2;‚Ä¶. The various keys and values are detailed in a later section. Ignored if replication is disabled. No default given variability. |\n| cairo.wal.segment.rollover.size | 2097152 | No | The size of the WAL segment before it is rolled over. Default is2MiB. However, defaults to0unlessreplication.role=primaryis set. |\n| cairo.writer.command.queue.capacity | 32 | No | Maximum writer ALTER TABLE and replication command capacity. Shared between all the tables. |\n| replication.primary.throttle.window.duration | 10000 | No | The millisecond duration of the sliding window used to process replication batches. Default is10000ms. |\n| replication.requests.max.concurrent | 0 | No | A limit to the number of concurrent object store requests. The default is0for unlimited. |\n| replication.requests.retry.attempts | 3 | No | Maximum number of times to retry a failed object store request before logging an error and reattempting later after a delay. Default is3. |\n| replication.requests.retry.interval | 200 | No | How long to wait before retrying a failed operation. Default is200ms. |\n| replication.primary.compression.threads | calculated | No | Max number of threads used to perform file compression operations before uploading to the object store. The default value is calculated as half the number of CPU cores. |\n| replication.primary.compression.level | 1 | No | Zstd compression level. Defaults to1. Valid values are from 1 to 22. |\n| replication.replica.poll.interval | 1000 | No | Millisecond polling rate of a replica instance to check for the availability of new changes. |\n| replication.primary.sequencer.part.txn.count | 5000 | No | Sets the txn chunking size for each compressed batch. Smaller is better for constrained networks (but more costly). |\n| replication.primary.checksum=service-dependent | service-dependent | No | Where a checksum should be calculated for each uploaded artifact. Required for some object stores. Other options: never, always |\n| replication.primary.upload.truncated | true | No | Skip trailing, empty column data inside a WAL column file. |\n| replication.requests.buffer.size | 32768 | No | Buffer size used for object-storage downloads. |\n| replication.summary.interval | 1m | No | Frequency for printing replication progress summary in the logs. |\n| replication.metrics.per.table | true | No | Enable per-table replication metrics on the prometheus metrics endpoint. |\n| replication.metrics.dropped.table.poll.count | 10 | No | How many scrapes of prometheus metrics endpoint before dropped tables will no longer appear. |\n| replication.requests.max.batch.size.fast | 64 | No | Number of parallel requests allowed during the 'fast' process (non-resource constrained). |\n| replication.requests.max.batch.size.slow | 2 | No | Number of parallel requests allowed during the 'slow' process (error/resource constrained path). |\n| replication.requests.base.timeout | 10s | No | Replication upload/download request timeout. |\n| replication.requests.min.throughput | 262144 | No | Expected minimum network speed for replication transfers. Used to expand the timeout and account for network delays. |\n| native.async.io.threads | cpuCount | No | The number of async (network) io threads used for replication (and in the future cold storage). The default should be appropriate for most use cases. |\n| native.max.blocking.threads | cpuCount * 4 | No | Maximum number of threads for parallel blocking disk IO read/write operations for replication (and other). These threads are ephemeral: They are spawned per need and shut down after a short duration if no longer in use. These are not cpu-bound threads, hence the relative large number. The default should be appropriate for most use cases. |\n\n\n### Identity and Access Management (IAM)‚Äã\n\nnote\nIdentity and Access Management is available within\nQuestDB Enterprise\n.\nIdentity and Access Management (IAM) ensures that data can be accessed only by\nauthorized users. The below configuration properties relate to various\nauthentication and authorization features.\nFor a full explanation of IAM, see the\nIdentity and Access Management (IAM) documentation\n.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| acl.enabled | true | No | Enables/disables Identity and Access Management. |\n| acl.admin.user.enabled | true | No | Enables/disables the built-in admin user. |\n| acl.admin.user | admin | No | Name of the built-in admin user. |\n| acl.admin.password | quest | Yes | The password of the built-in admin user. |\n| acl.basic.auth.realm.enabled | false | No | When enabled the browser's basic auth popup window is used instead of the Web Console's login screen. Only present for backwards compatibility. |\n| acl.entity.name.max.length | 255 | No | Maximum length of user, group and service account names. |\n| acl.password.hash.iteration.count | 100000 | No | QuestDB Enterprise never stores passwords in plain text, it stores password hashes only. This is the number of hash iterations used in password hashing. Higher means safer, almost never should be changed. |\n| acl.rest.token.refresh.threshold | 10 | No | When a REST token is created in REFRESH mode, its TTL is extended on every successful authentication, unless the last successful authentication was within this threshold. This setting removes unnecessary overhead of continuously refreshing REST tokens if they are used often. The value is expressed in seconds. |\n| tls.enabled | false | No | Enables/disables TLS encryption globally for all QuestDB interfaces (HTTP endpoints, ILP over TCP). |\n| tls.cert.path |  | No | Path to certificate used for TLS encryption globally. The certificate should be DER-encoded and saved in PEM format. |\n| tls.private.key.path |  | No | Path to private key used for TLS encryption globally. |\n| http.tls.enabled | false | No | Enables/disables TLS encryption for the HTTP server only. |\n| http.tls.cert.path |  | No | Path to certificate used for TLS encryption for the HTTP server only. The certificate should be DER-encoded and saved in PEM format. |\n| http.tls.private.key.path |  | No | Path to private key used for TLS encryption for the HTTP server only. |\n| http.min.tls.enabled | false | No | Enables/disables TLS encryption for the minimal HTTP server only. |\n| http.min.tls.cert.path |  | No | Path to certificate used for TLS encryption for the minimal HTTP server only. The certificate should be DER-encoded and saved in PEM format. |\n| http.min.tls.private.key.path |  | No | Path to private key used for TLS encryption for the minimal HTTP server only. |\n| line.tcp.tls.enabled | false | No | Enables/disables TLS encryption for ILP over TCP only. |\n| line.tcp.tls.cert.path |  | No | Path to certificate used for TLS encryption for ILP over TCP only. The certificate should be DER-encoded and saved in PEM format. |\n| line.tcp.tls.private.key.path |  | No | Path to private key used for TLS encryption for ILP over TCP only. |\n| line.tcp.acl.enabled | true | No | Enables/disables authentication for the ILP over TCP endpoint only. |\n\n\n### OpenID Connect (OIDC)‚Äã\n\nnote\nOpenID Connect is\nEnterprise\nonly.\nOpenID Connect (OIDC) support is part of QuestDB's Identity and Access\nManagement. The database can be integrated with any OAuth2/OIDC Identity\nProvider (IdP).\nFor detailed information about OIDC, see the\nOpenID Connect (OIDC) integration guide\n.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| acl.oidc.enabled | false | No | Enables/disables OIDC authentication. When enabled, few other configuration options must also be set. |\n| acl.oidc.pkce.enabled | true | No | Enables/disables PKCE for the Authorization Code Flow. This should always be enabled in a production environment, the Web Console is not fully secure without it. |\n| acl.oidc.ropc.flow.enabled | false | No | Enables/disables Resource Owner Password Credentials flow. When enabled, this flow also has to be configured in the OIDC Provider. |\n| acl.oidc.configuration.url |  | No | URL where the OpenID Provider's configuration information cna be loaded in json format, should always end with/.well-known/openid-configuration. |\n| acl.oidc.host |  | No | OIDC provider hostname. Required when OIDC is enabled, unless the OIDC configuration URL is set. |\n| acl.oidc.port | 443 | No | OIDC provider port number. |\n| acl.oidc.tls.enabled | true | No | Whether the OIDC provider requires a secure connection or not. It is highly unlikely in a production environment, but if the OpenID Provider endpoints do not require a secure connection, this option can be set tofalse. |\n| acl.oidc.tls.validation.enabled | true | No | Enables/disables TLS certificate validation. If you are working with self-signed certificates that you would like QuestDB to trust, disable this option. Validation is strongly recommended in production environments. QuestDB will check that the certificate is valid, and that it is issued for the server to which it connects. |\n| acl.oidc.tls.keystore.path |  | No | Path to a keystore file that contains trusted Certificate Authorities. Will be used when validating the certificate of the OIDC provider. Not required if your OIDC provider's certificate is signed by a public CA. |\n| acl.oidc.tls.keystore.password |  | No | Keystore password, required if there is a keystore file and it is password protected. |\n| acl.oidc.http.timeout | 30000 | No | OIDC provider HTTP request timeout in milliseconds. |\n| acl.oidc.client.id |  | No | Client name assigned to QuestDB in the OIDC server, required when OIDC is enabled. |\n| acl.oidc.audience |  | No | OAuth2 audience as set on the tokens issued by the OIDC Provider, defaults to the client id. |\n| acl.oidc.redirect.uri |  | No | The redirect URI tells the OIDC server where to redirect the user after successful authentication. If not set, the Web Console defaults it to the location where it was loaded from (window.location.href). |\n| acl.oidc.scope | openid | No | The OIDC server should ask consent for the list of scopes provided in this property. The scopeopenidis mandatory, and always should be included. |\n| acl.oidc.public.keys.endpoint | /pf/JWKS | No | JSON Web Key Set (JWKS) Endpoint, the default value should work for the Ping Identity Platform. This endpoint provides the list of public keys can be used to decode and validate ID tokens issued by the OIDC Provider. |\n| acl.oidc.authorization.endpoint | /as/authorization.oauth2 | No | OIDC Authorization Endpoint, the default value should work for the Ping Identity Platform. |\n| acl.oidc.token.endpoint | /as/token.oauth2 | No | OIDC Token Endpoint, the default value should work for the Ping Identity Platform. |\n| acl.oidc.userinfo.endpoint | /idp/userinfo.openid | No | OIDC User Info Endpoint, the default value should work for the Ping Identity Platform. Used to retrieve additional user information which contains the user's group memberships. |\n| acl.oidc.groups.encoded.in.token | false | No | Should be set to false, if the OIDC Provider is configured to encode the group memberships of the user into the id token. When set to true, QuestDB will look for the groups in the token instead of calling the User Info endpoint. |\n| acl.oidc.sub.claim | sub | No | The name of the claim in the user information, which contains the name of the user. Could be a username, the user's full name or email. It will be displayed in the Web Console, and logged for audit purposes. |\n| acl.oidc.groups.claim | groups | No | The name of the custom claim in the user information, which contains the group memberships of the user. |\n| acl.oidc.cache.ttl | 30000 | No | User info cache entry TTL (time to live) in milliseconds, default value is 30 seconds. For improved performance QuestDB caches user info responses for each valid access token, this settings drives how often the access token should be validated and the user info updated. |\n| acl.oidc.pg.token.as.password.enabled | false | No | When enabled, the PGWire endpoint supports OIDC authentication. The OAuth2 token should be sent in the password field, while the username field should contain the string_sso, or left empty if that is an option. |\n\n\n### Config Validation‚Äã\n\nThe database startup phase checks for configuration issues, such as invalid or\ndeprecated settings. Issues may be classified as advisories or errors. Advisory\nissues are\nlogged\nwithout causing the database to stop its startup sequence: These are usually\nsetting deprecation warnings. Configuration errors can optionally cause the\ndatabase to fail its startup.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| config.validation.strict | false | No | When enabled, startup fails if there are configuration errors. |\n\nWe recommended enabling strict validation.\n\n### Telemetry‚Äã\n\nQuestDB sends anonymous telemetry data with information about usage which helps\nus improve the product over time. We do not collect any personally-identifying\ninformation, and we do not share any of this data with third parties.\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| telemetry.enabled | true | No | Enable or disable anonymous usage metrics collection. |\n| telemetry.hide.tables | false | No | Hides telemetry tables fromselect * from tables()output. As a result, telemetry tables will not be visible in the Web Console table view. |\n| telemetry.queue.capacity | 512 | No | Capacity of the internal telemetry queue, which is the gateway of all telemetry events. This queue capacity does not require tweaking. |\n\n\n## Materialized views‚Äã\n\ninfo\nMaterialized View support is now generally available (GA) and ready for production use.\nIf you are using versions earlier than\n8.3.1\n, we suggest you upgrade at your earliest convenience.\nThe following settings are available in\nserver.conf\n:\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| cairo.mat.view.enabled | true | No | Enables or disables SQL support and refresh job for materialized views. |\n| cairo.mat.view.parallel.sql.enabled | true | No | When disabled, SQL executed by the materialized view refresh job will always run single-threaded. |\n| mat.view.refresh.worker.count | 0 | No | Number of dedicated worker threads assigned to refresh materialized views. When0, the jobs will use the shared pool. |\n| mat.view.refresh.worker.affinity | Equal to the CPU core count | No | Comma separated list of numerical CPU core indexes. |\n| mat.view.refresh.worker.haltOnError | false | No | Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n\n\n## Logging & Metrics‚Äã\n\nThe following settings are available in\nserver.conf\n:\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| log.level.verbose | false | No | Converts short-hand log level indicators (E, C, I) into long-hand (ERROR, CRITICAL, INFO) |\n| log.timezone | UTC | No | Sets the timezone for log timestamps. Can be a timezone ID such as 'Antarctica/McMurdo', 'SystemDefault' to use system timezone, or the default UTC with 'Z' suffix |\n\nFurther settings are available in\nlog.conf\n. For more information, and details\nof our Prometheus metrics, please visit the\nLogging & Metrics\ndocumentation.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 11340,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-b4e71d175b93",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/rest-api",
    "title": "REST API | QuestDB",
    "text": "The QuestDB REST API is based on standard HTTP features and is understood by\noff-the-shelf HTTP clients. It provides a simple way to interact with QuestDB\nand is compatible with most programming languages. API functions are fully keyed\non the URL and they use query parameters as their arguments.\nThe\nWeb Console\nis the official Web client for QuestDB, that relies on the REST API.\nAvailable methods\n- /impfor importing data from.CSVfiles\n- /execto execute a SQL statement\n- /expto export data\n\n## Examples‚Äã\n\nQuestDB exposes a REST API for compatibility with a wide range of libraries and\ntools. The REST API is accessible on port\n9000\nand has the following\ninsert-capable entrypoints:\n\n| Entrypoint | HTTP Method | Description | API Docs |\n| --- | --- | --- | --- |\n| /imp | POST | Import CSV data | Reference |\n| /exec?query=.. | GET | Run SQL Query returning JSON result set | Reference |\n\nFor details such as content type, query parameters and more, refer to the\nREST API\ndocs.\n\n### /imp: Uploading Tabular Data‚Äã\n\nLet's assume you want to upload the following data via the\n/imp\nentrypoint:\n- CSV\n- Table\n\n```csv\ncol1,col2,col3a,10.5,Trueb,100,Falsec,,True\n```\n\n\n| col1 | col2 | col3 |\n| --- | --- | --- |\n| a | 10.5 | true |\n| b | 100 | false |\n| c | NULL | true |\n\nYou can do so via the command line using\ncURL\nor programmatically via HTTP\nAPIs in your scripts and applications.\nBy default, the response is designed to be human-readable. Use the\nfmt=json\nquery argument to obtain a response in JSON. You can also specify the schema\nexplicitly. See the second example in Python for these features.\n- cURL\n- Python\n- NodeJS\n- Go\nThis example imports a CSV file with automatic schema detection.\nBasic import with table name\n\n```shell\ncurl -F data=@data.csv http://localhost:9000/imp?name=table_name\n```\n\nThis example overwrites an existing table and specifies a timestamp format and a\ndesignated timestamp column. For more information on the optional parameters to\nspecify timestamp formats, partitioning and renaming tables, see the\nREST API documentation\n.\nProviding a user-defined schema\n\n```bash\ncurl \\-F schema='[{\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}]' \\-F data=@weather.csv 'http://localhost:9000/imp?overwrite=true&timestamp=ts'\n```\n\nThis first example shows uploading the\ndata.csv\nfile with automatic schema\ndetection.\n\n```python\nimport sysimport requestscsv = {'data': ('my_table', open('./data.csv', 'r'))}host = 'http://localhost:9000'try:    response = requests.post(host + '/imp', files=csv)    print(response.text)except requests.exceptions.RequestException as e:    print(f'Error: {e}', file=sys.stderr)\n```\n\nThe second example creates a CSV buffer from Python objects and uploads them\nwith a custom schema. Note UTF-8 encoding.\nThe\nfmt=json\nparameter allows us to obtain a parsable response, rather than a\ntabular response designed for human consumption.\n\n```python\nimport ioimport csvimport requestsimport pprintimport jsondef to_csv_str(table):    output = io.StringIO()    csv.writer(output, dialect='excel').writerows(table)    return output.getvalue().encode('utf-8')def main():    table_name = 'example_table2'    table = [        ['col1', 'col2', 'col3'],        ['a',    10.5,   True],        ['b',    100,    False],        ['c',    None,   True]]    table_csv = to_csv_str(table)    print(table_csv)    schema = json.dumps([        {'name': 'col1', 'type': 'SYMBOL'},        {'name': 'col2', 'type': 'DOUBLE'},        {'name': 'col3', 'type': 'BOOLEAN'}])    response = requests.post(        'http://localhost:9000/imp',        params={'fmt': 'json'},        files={            'schema': schema,            'data': (table_name, table_csv)}).json()    # You can parse the `status` field and `error` fields    # of individual columns. See Reference/API/REST docs for details.    pprint.pprint(response)if __name__ == '__main__':    main()\n```\n\n\n```javascript\nconst fetch = require(\"node-fetch\")const FormData = require(\"form-data\")const fs = require(\"fs\")const HOST = \"http://127.0.0.1:9000\"async function run() {  const form = new FormData()  form.append(\"data\", fs.readFileSync(__dirname + \"/data.csv\"), {    filename: \"data.csv\",    contentType: \"application/octet-stream\",  })  try {    const r = await fetch(`${HOST}/imp`, {      method: \"POST\",      body: form,      headers: form.getHeaders(),    })    console.log(r)  } catch (e) {    console.error(e)  }}run()\n```\n\n\n```go\npackage mainimport (  \"bytes\"  \"fmt\"  \"io\"  \"io/ioutil\"  \"log\"  \"mime/multipart\"  \"net/http\"  \"net/url\"  \"os\")func main() {  u, err := url.Parse(\"http://localhost:9000\")  checkErr(err)  u.Path += \"imp\"  url := fmt.Sprintf(\"%v\", u)  fileName := \"/path/to/data.csv\"  file, err := os.Open(fileName)  checkErr(err)  defer file.Close()  buf := new(bytes.Buffer)  writer := multipart.NewWriter(buf)  uploadFile, _ := writer.CreateFormFile(\"data\", \"data.csv\")  _, err = io.Copy(uploadFile, file)  checkErr(err)  writer.Close()  req, err := http.NewRequest(http.MethodPut, url, buf)  checkErr(err)  req.Header.Add(\"Content-Type\", writer.FormDataContentType())  client := &http.Client{}  res, err := client.Do(req)  checkErr(err)  defer res.Body.Close()  body, err := ioutil.ReadAll(res.Body)  checkErr(err)  log.Println(string(body))}func checkErr(err error) {  if err != nil {    panic(err)  }}\n```\n\n\n### /exec: SQLINSERTQuery‚Äã\n\nThe\n/exec\nentrypoint takes a SQL query and returns results as JSON.\nWe can use this for quick SQL inserts too, but note that there's no support for\nparameterized queries that are necessary to avoid SQL injection issues. Prefer\nInfluxDB Line Protocol\nif you\nneed high-performance inserts.\n- cURL\n- Python\n- NodeJS\n- Go\n\n```shell\n# Create Tablecurl -G \\  --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name STRING, value INT)\" \\  http://localhost:9000/exec# Insert a rowcurl -G \\  --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\  http://localhost:9000/exec\n```\n\n\n```python\nimport sysimport requestsimport jsonhost = 'http://localhost:9000'def run_query(sql_query):    query_params = {'query': sql_query, 'fmt' : 'json'}    try:        response = requests.get(host + '/exec', params=query_params)        json_response = json.loads(response.text)        print(json_response)    except requests.exceptions.RequestException as e:        print(f'Error: {e}', file=sys.stderr)# create tablerun_query(\"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\")# insert rowrun_query(\"INSERT INTO trades VALUES('abc', 123456)\")\n```\n\nThe\nnode-fetch\npackage can be installed using\nnpm i node-fetch\n.\n\n```javascript\nconst fetch = require(\"node-fetch\")const HOST = \"http://127.0.0.1:9000\"async function createTable() {  try {    const query = \"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\"    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    )    const json = await response.json()    console.log(json)  } catch (error) {    console.log(error)  }}async function insertData() {  try {    const query = \"INSERT INTO trades VALUES('abc', 123456)\"    const response = await fetch(      `${HOST}/exec?query=${encodeURIComponent(query)}`,    )    const json = await response.json()    console.log(json)  } catch (error) {    console.log(error)  }}createTable().then(insertData)\n```\n\n\n```go\npackage mainimport (  \"fmt\"  \"io/ioutil\"  \"log\"  \"net/http\"  \"net/url\")func main() {  u, err := url.Parse(\"http://localhost:9000\")  checkErr(err)  u.Path += \"exec\"  params := url.Values{}  params.Add(\"query\", `    CREATE TABLE IF NOT EXISTS      trades (name STRING, value INT);    INSERT INTO      trades    VALUES(      \"abc\",      123456    );  `)  u.RawQuery = params.Encode()  url := fmt.Sprintf(\"%v\", u)  res, err := http.Get(url)  checkErr(err)  defer res.Body.Close()  body, err := ioutil.ReadAll(res.Body)  checkErr(err)  log.Println(string(body))}func checkErr(err error) {  if err != nil {    panic(err)  }}\n```\n\n\n## /imp - Import data‚Äã\n\ntip\nFor a complete guide including text loader configuration and troubleshooting,\nsee\nCSV Import\n.\n/imp\nstreams tabular text data directly into a table. It supports CSV, TAB and\npipe (\n|\n) delimited inputs with optional headers. There are no restrictions on\ndata size. Data types and structures are detected automatically, without\nadditional configuration. In some cases, additional configuration can be\nprovided to improve the automatic detection as described in\nuser-defined schema\n.\nnote\nThe structure detection algorithm analyses the chunk in the beginning of the\nfile and relies on relative uniformity of data. When the first chunk is\nnon-representative of the rest of the data, automatic imports can yield errors.\nIf the data follows a uniform pattern, the number of lines which are analyzed\nfor schema detection can be reduced to improve performance during uploads using\nthe\nhttp.text.analysis.max.lines\nkey. Usage of this setting is described in\nthe\nHTTP server configuration\ndocumentation.\n\n### URL parameters‚Äã\n\n/imp\nis expecting an HTTP POST request using the\nmultipart/form-data\nContent-Type with following optional URL parameters which must be URL encoded:\n\n| Parameter | Required | Default | Description |\n| --- | --- | --- | --- |\n| atomicity | No | skipCol | abort,skipRoworskipCol. Behaviour when an error is detected in the data.abort: the entire file will be skipped.skipRow: the row is skipped.skipCol: the column is skipped. |\n| delimiter | No |  | URL encoded delimiter character. When set, import will try to detect the delimiter automatically. Since automatic delimiter detection requires at least two lines (rows) to be present in the file, this parameter may be used to allow single line file import. |\n| fmt | No | tabular | Can be set tojsonto get the response formatted as such. |\n| forceHeader | No | false | trueorfalse. Whenfalse, QuestDB will try to infer if the first line of the file is the header line. When set totrue, QuestDB will expect that line to be the header line. |\n| name | No | Name of the file | Name of the table to create,see below. |\n| overwrite | No | false | trueorfalse. When set to true, any existing data or structure will be overwritten. |\n| partitionBy | No | NONE | Seepartitions. |\n| o3MaxLag | No |  | Sets upper limit on the created table to be used for the in-memory out-of-order buffer. Can be also set globally via thecairo.o3.max.lagconfiguration property. |\n| maxUncommittedRows | No |  | Maximum number of uncommitted rows to be set for the created table. When the number of pending rows reaches this parameter on a table, a commit will be issued. Can be also set globally via thecairo.max.uncommitted.rowsconfiguration property. |\n| skipLev | No | false | trueorfalse. Skip ‚ÄúLine Extra Values‚Äù, when set to true, the parser will ignore those extra values rather than ignoring entire line. An extra value is something in addition to what is defined by the header. |\n| timestamp | No |  | Name of the column that will be used as adesignated timestamp. |\n| create | No | true | trueorfalse. When set tofalse, QuestDB will not automatically create a table 'name' if one does not exist, and will return an error instead. |\n\ntip\nIf you experience large latencies when importing big CSV files with out-of-order\ntimestamps, try increasing\nmaxUncommittedRows\nparameter from the default\n500,000\nvalue.\nExample usage\n\n```shell\ncurl -F data=@weather.csv \\'http://localhost:9000/imp?overwrite=true&name=new_table&timestamp=ts&partitionBy=MONTH'\n```\n\nFurther example queries with context on the source CSV file contents relative\nand the generated tables are provided in the\nexamples section\nbelow.\n\n### Names‚Äã\n\nTable and column names are subject to restrictions, the following list of\ncharacters are automatically removed:\n\n```plain\n[whitespace].?,:\\/\\\\\\0)(_+-*~%\n```\n\nWhen the header row is missing, column names are generated automatically.\n\n### Consistency guarantees‚Äã\n\n/imp\nbenefits from the properties of the QuestDB\nstorage engine\n,\nalthough Atomicity and Durability can be relaxed to meet convenience and\nperformance demands.\n\n#### Atomicity‚Äã\n\nQuestDB is fully insured against any connection problems. If the server detects\nclosed socket(s), the entire request is rolled back instantly and transparently\nfor any existing readers. The only time data can be partially imported is when\natomicity is in\nrelaxed\nmode and data cannot be converted to column type. In\nthis scenario, any \"defective\" row of data is discarded and\n/imp\ncontinues to\nstream request data into table.\n\n#### Consistency‚Äã\n\nThis property is guaranteed by consistency of append transactions against\nQuestDB storage engine.\n\n#### Isolation‚Äã\n\nData is committed to QuestDB storage engine at end of request. Uncommitted\ntransactions are not visible to readers.\n\n#### Durability‚Äã\n\n/imp\nstreams data from network socket buffer directly into memory mapped\nfiles. At this point data is handed over to the OS and is resilient against\nQuestDB internal errors and unlikely but hypothetically possible crashes. This\nis default method of appending data and it is chosen for its performance\ncharacteristics.\n\n### Examples‚Äã\n\n\n#### Automatic schema detection‚Äã\n\nThe following example uploads a file\nratings.csv\nwhich has the following\ncontents:\n\n| ts | visMiles | tempF | dewpF |\n| --- | --- | --- | --- |\n| 2010-01-01T00:00:00.000000Z | 8.8 | 34 | 30 |\n| 2010-01-01T00:51:00.000000Z | 9.100000000000 | 34 | 30 |\n| 2010-01-01T01:36:00.000000Z | 8.0 | 34 | 30 |\n| ... | ... | ... | ... |\n\nAn import can be performed with automatic schema detection with the following\nrequest:\n\n```shell\ncurl -F data=@weather.csv 'http://localhost:9000/imp'\n```\n\nA HTTP status code of\n200\nwill be returned and the response will be:\n\n```shell\n+-------------------------------------------------------------------------------+|      Location:  |     weather.csv  |        Pattern  | Locale  |      Errors  ||   Partition by  |            NONE  |                 |         |              ||      Timestamp  |            NONE  |                 |         |              |+-------------------------------------------------------------------------------+|   Rows handled  |           49976  |                 |         |              ||  Rows imported  |           49976  |                 |         |              |+-------------------------------------------------------------------------------+|              0  |              ts  |                TIMESTAMP  |           0  ||              1  |        visMiles  |                   DOUBLE  |           0  ||              2  |           tempF  |                      INT  |           0  ||              3  |           dewpF  |                      INT  |           0  |+-------------------------------------------------------------------------------+\n```\n\n\n#### User-defined schema‚Äã\n\nTo specify the schema of a table, a schema object can be provided:\n\n```shell\ncurl \\-F schema='[{\"name\":\"dewpF\", \"type\": \"STRING\"}]' \\-F data=@weather.csv 'http://localhost:9000/imp'\n```\n\nResponse\n\n```shell\n+------------------------------------------------------------------------------+|      Location:  |    weather.csv  |        Pattern  | Locale  |      Errors  ||   Partition by  |           NONE  |                 |         |              ||      Timestamp  |           NONE  |                 |         |              |+------------------------------------------------------------------------------+|   Rows handled  |          49976  |                 |         |              ||  Rows imported  |          49976  |                 |         |              |+------------------------------------------------------------------------------+|              0  |             ts  |                TIMESTAMP  |           0  ||              1  |       visMiles  |                   DOUBLE  |           0  ||              2  |          tempF  |                      INT  |           0  ||              3  |          dewpF  |                   STRING  |           0  |+------------------------------------------------------------------------------+\n```\n\nNon-standard timestamp formats\nGiven a file\nweather.csv\nwith the following contents which contains a\ntimestamp with a non-standard format:\n\n| ts | visMiles | tempF | dewpF |\n| --- | --- | --- | --- |\n| 2010-01-01 - 00:00:00 | 8.8 | 34 | 30 |\n| 2010-01-01 - 00:51:00 | 9.100000000000 | 34 | 30 |\n| 2010-01-01 - 01:36:00 | 8.0 | 34 | 30 |\n| ... | ... | ... | ... |\n\nThe file can be imported as usual with the following request:\nImporting CSV with non-standard timestamp\n\n```shell\ncurl -F data=@weather.csv 'http://localhost:9000/imp'\n```\n\nA HTTP status code of\n200\nwill be returned and the import will be successful,\nbut the timestamp column is detected as a\nVARCHAR\ntype:\nResponse with timestamp as VARCHAR type\n\n```shell\n+-------------------------------------------------------------------------------+|      Location:  |     weather.csv  |        Pattern  | Locale  |      Errors  ||   Partition by  |            NONE  |                 |         |              ||      Timestamp  |            NONE  |                 |         |              |+-------------------------------------------------------------------------------+|   Rows handled  |           49976  |                 |         |              ||  Rows imported  |           49976  |                 |         |              |+-------------------------------------------------------------------------------+|              0  |              ts  |                  VARCHAR  |           0  ||              1  |        visMiles  |                   DOUBLE  |           0  ||              2  |           tempF  |                      INT  |           0  ||              3  |           dewpF  |                      INT  |           0  |+-------------------------------------------------------------------------------+\n```\n\nTo amend the timestamp column type, this example curl can be used which has a\nschema\nJSON object to specify that the\nts\ncolumn is of\nTIMESTAMP\ntype with\nthe pattern\nyyyy-MM-dd - HH:mm:ss\nAdditionally, URL parameters are provided:\n- overwrite=trueto overwrite the existing table\n- timestamp=tsto specify that thetscolumn is the designated timestamp\ncolumn for this table\n- partitionBy=MONTHto set apartitioning strategyon the table byMONTH\nProviding a user-defined schema\n\n```shell\ncurl \\-F schema='[{\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}]' \\-F data=@weather.csv \\'http://localhost:9000/imp?overwrite=true&timestamp=ts&partitionBy=MONTH'\n```\n\nThe HTTP status code will be set to\n200\nand the response will show\n0\nerrors\nparsing the timestamp column:\n\n```shell\n+------------------------------------------------------------------------------+|      Location:  |    weather.csv  |        Pattern  | Locale  |      Errors  ||   Partition by  |          MONTH  |                 |         |              ||      Timestamp  |             ts  |                 |         |              |+------------------------------------------------------------------------------+|   Rows handled  |          49976  |                 |         |              ||  Rows imported  |          49976  |                 |         |              |+------------------------------------------------------------------------------+|              0  |             ts  |                TIMESTAMP  |           0  ||              1  |       visMiles  |                   DOUBLE  |           0  ||              2  |          tempF  |                      INT  |           0  ||              3  |          dewpF  |                      INT  |           0  |+------------------------------------------------------------------------------+\n```\n\n\n#### JSON response‚Äã\n\nIf you intend to upload CSV programmatically, it's easier to parse the response\nas JSON. Set\nfmt=json\nquery argument on the request.\nHere's an example of a successful response:\n\n```json\n{  \"status\": \"OK\",  \"location\": \"example_table\",  \"rowsRejected\": 0,  \"rowsImported\": 3,  \"header\": false,  \"columns\": [    { \"name\": \"col1\", \"type\": \"SYMBOL\", \"size\": 4, \"errors\": 0 },    { \"name\": \"col2\", \"type\": \"DOUBLE\", \"size\": 8, \"errors\": 0 },    { \"name\": \"col3\", \"type\": \"BOOLEAN\", \"size\": 1, \"errors\": 0 }  ]}\n```\n\nHere is an example with request-level errors:\n\n```json\n{  \"status\": \"not enough lines [table=example_table]\"}\n```\n\nHere is an example with column-level errors due to unsuccessful casts:\n\n```json\n{  \"status\": \"OK\",  \"location\": \"example_table2\",  \"rowsRejected\": 0,  \"rowsImported\": 3,  \"header\": false,  \"columns\": [    { \"name\": \"col1\", \"type\": \"DOUBLE\", \"size\": 8, \"errors\": 3 },    { \"name\": \"col2\", \"type\": \"SYMBOL\", \"size\": 4, \"errors\": 0 },    { \"name\": \"col3\", \"type\": \"BOOLEAN\", \"size\": 1, \"errors\": 0 }  ]}\n```\n\n\n## /exec - Execute queries‚Äã\n\n/exec\ncompiles and executes the SQL query supplied as a parameter and returns\na JSON response.\nnote\nThe query execution terminates automatically when the socket connection is\nclosed.\n\n### Overview‚Äã\n\n\n#### Parameters‚Äã\n\n/exec\nis expecting an HTTP GET request with following query parameters:\n\n| Parameter | Required | Default | Description |\n| --- | --- | --- | --- |\n| count | No | false | trueorfalse. Counts the number of rows and returns this value. |\n| limit | No |  | Allows limiting the number of rows to return.limit=10will return the first 10 rows (equivalent tolimit=1,10),limit=10,20will return row numbers 10 through to 20 inclusive. |\n| nm | No | false | trueorfalse. Skips the metadata section of the response when set totrue. |\n| query | Yes |  | URL encoded query text. It can be multi-line. |\n| timings | No | false | trueorfalse. When set totrue, QuestDB will also include atimingsproperty in the response which gives details about the execution times. |\n| explain | No | false | trueorfalse. When set totrue, QuestDB will also include anexplainproperty in the response which gives details about the execution plan. |\n| quoteLargeNum | No | false | trueorfalse. When set totrue, QuestDB will surroundLONGtype numbers with double quotation marks that will make them parsed as strings. |\n\nThe parameters must be URL encoded.\n\n#### Headers‚Äã\n\nSupported HTTP headers:\n\n| Header | Required | Description |\n| --- | --- | --- |\n| Statement-Timeout | No | Query timeout in milliseconds, overrides default timeout from server.conf |\n\n\n### Examples‚Äã\n\n\n#### SELECT query example:‚Äã\n\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT timestamp, price FROM trades LIMIT 2;\" \\  --data-urlencode \"count=true\" \\  http://localhost:9000/exec\n```\n\nA HTTP status code of\n200\nis returned with the following response body:\n\n```json\n{  \"query\": \"SELECT timestamp, price FROM trades LIMIT 2;\",  \"columns\": [    {      \"name\": \"timestamp\",      \"type\": \"TIMESTAMP\"    },    {      \"name\": \"price\",      \"type\": \"DOUBLE\"    }  ],  \"timestamp\": 0  \"dataset\": [    [\"2024-01-01T00:00:00.000000Z\", 142.50],    [\"2024-01-01T00:00:01.000000Z\", 142.75]  ],  \"count\": 2}\n```\n\nSELECT query returns response in the following format:\n\n```json\n{  \"query\": string,  \"columns\": Array<{ \"name\": string, \"type\": string }>  \"dataset\": Array<Array<Value for Column1, Value for Column2>>,  \"timestamp\": number,  \"count\": Optional<number>,  \"timings\": Optional<{ compiler: number, count: number, execute: number }>,  \"explain\": Optional<{ jitCompiled: boolean }>}\n```\n\nYou can find the exact list of column types in the\ndedicated page\n.\nThe\ntimestamp\nfield indicates which of the columns in the result set is the\ndesignated timestamp, or -1 if there isn't one.\n\n#### UPDATE query example:‚Äã\n\nThis request executes an update of table\nweather\nsetting 2 minutes query\ntimeout\n\n```shell\ncurl -G \\  -H \"Statement-Timeout: 120000\" \\  --data-urlencode \"query=UPDATE weather SET tempF = tempF + 0.12 WHERE tempF > 60\" \\  http://localhost:9000/exec\n```\n\nA HTTP status code of\n200\nis returned with the following response body:\n\n```json\n{  \"ddl\": \"OK\",  \"updated\": 34}\n```\n\n\n#### CREATE TABLE query example:‚Äã\n\nThis request creates a basic table, with a designated timestamp.\n\n```shell\ncurl -G \\  -H \"Statement-Timeout: 120000\" \\  --data-urlencode \"query=CREATE TABLE foo ( a INT, ts TIMESTAMP) timestamp(ts)\" \\  http://localhost:9000/exec\n```\n\nA HTTP status code of\n200\nis returned with the following response body:\n\n```json\n{  \"ddl\": \"OK\"}\n```\n\n\n## /exp - Export data‚Äã\n\nThis endpoint allows you to pass url-encoded queries but the request body is\nreturned in a tabular form to be saved and reused as opposed to JSON.\n\n### Overview‚Äã\n\n/exp\nis expecting an HTTP GET request with following parameters:\n\n| Parameter | Required | Description |\n| --- | --- | --- |\n| query | Yes | URL encoded query text. It can be multi-line. |\n| limit | No | Paging opp parameter. For example,limit=10,20will return row numbers 10 through to 20 inclusive andlimit=20will return first 20 rows, which is equivalent tolimit=0,20.limit=-20will return the last 20 rows. |\n| nm | No | trueorfalse. Skips the metadata section of the response when set totrue. |\n| fmt | No | Export format. Valid values:parquet,csv. When set toparquet, exports data in Parquet format instead of CSV. |\n\n\n#### Parquet Export Parameters‚Äã\n\nwarning\nParquet exports currently require writing interim data to disk, and therefore must be run on\nread-write instances only\n.\nThis limitation will be removed in future.\nWhen\nfmt=parquet\n, the following additional parameters are supported:\n\n| Parameter | Required | Default | Description |\n| --- | --- | --- | --- |\n| partition_by | No | NONE | Partition unit:NONE,HOUR,DAY,WEEK,MONTH, orYEAR. |\n| compression_codec | No | ZSTD | Compression algorithm:UNCOMPRESSED,SNAPPY,GZIP,LZ4,ZSTD,LZ4_RAW,BROTLI,LZO. |\n| compression_level | No | 9 | Compression level (codec-specific). Higher values = better compression but slower. |\n| row_group_size | No | 100000 | Number of rows per Parquet row group. |\n| data_page_size | No | 1048576 | Size of data pages in bytes (default 1MB). |\n| statistics_enabled | No | true | Enable Parquet column statistics:trueorfalse. |\n| parquet_version | No | 2 | Parquet format version:1(v1.0) or2(v2.0). |\n| raw_array_encoding | No | false | Use raw encoding for arrays:true(lighter-weight, less compatible) orfalse(heavier-weight, more compatible) |\n\nThe parameters must be URL encoded.\n\n### Examples‚Äã\n\n\n#### CSV Export (default)‚Äã\n\nConsidering the query:\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT AccidentIndex2, Date, Time FROM 'Accidents0514.csv'\" \\  --data-urlencode \"limit=5\" \\  http://localhost:9000/exp\n```\n\nA HTTP status code of\n200\nis returned with the following response body:\n\n```shell\n\"AccidentIndex\",\"Date\",\"Time\"200501BS00001,\"2005-01-04T00:00:00.000Z\",17:42200501BS00002,\"2005-01-05T00:00:00.000Z\",17:36200501BS00003,\"2005-01-06T00:00:00.000Z\",00:15200501BS00004,\"2005-01-07T00:00:00.000Z\",10:35200501BS00005,\"2005-01-10T00:00:00.000Z\",21:13\n```\n\n\n#### Parquet Export‚Äã\n\nExport query results to Parquet format:\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT * FROM trades WHERE timestamp IN today()\" \\  --data-urlencode \"fmt=parquet\" \\  http://localhost:9000/exp > trades_today.parquet\n```\n\n\n#### Parquet Export with Custom Options‚Äã\n\nExport with custom compression and partitioning:\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT * FROM trades\" \\  --data-urlencode \"fmt=parquet\" \\  --data-urlencode \"partition_by=DAY\" \\  --data-urlencode \"compression_codec=ZSTD\" \\  --data-urlencode \"compression_level=9\" \\  --data-urlencode \"row_group_size=1000000\" \\  http://localhost:9000/exp > trades.parquet\n```\n\n\n#### Parquet Export with LZ4 Compression‚Äã\n\nExport with LZ4_RAW compression for faster export:\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT symbol, price, amount FROM trades WHERE timestamp > dateadd('h', -1, now())\" \\  --data-urlencode \"fmt=parquet\" \\  --data-urlencode \"compression_codec=LZ4_RAW\" \\  http://localhost:9000/exp > recent_trades.parquet\n```\n\n\n## Error responses‚Äã\n\n\n### Malformed queries‚Äã\n\nA successful call to\n/exec\nor\n/exp\nwhich also contains a malformed query\nwill return response bodies with the following format:\n\n```json\n{  \"query\": string,  \"error\": string,  \"position\": number}\n```\n\nThe\nposition\nfield is the character number from the beginning of the string\nwhere the error was found.\nConsidering the query:\n\n```shell\ncurl -G \\  --data-urlencode \"query=SELECT * FROM table;\" \\  http://localhost:9000/exp\n```\n\nA HTTP status code of\n400\nis returned with the following response body:\n\n```json\n{  \"query\": \"SELECT * FROM table;\",  \"error\": \"function, literal or constant is expected\",  \"position\": 8}\n```\n\n\n## Authentication (RBAC)‚Äã\n\nnote\nRole-based Access Control (RBAC) is available in\nQuestDB Enterprise\n. See the next paragraph for authentication in\nQuestDB Open Source.\nREST API supports two authentication types:\n- HTTP basic authentication\n- Token-based authentication\nThe first authentication type is mainly supported by web browsers. But you can\nalso apply user credentials programmatically in a\nAuthorization: Basic\nheader.\nThis example\ncurl\ncommand that executes a\nSELECT 1;\nquery along with the\nAuthorization: Basic\nheader:\n\n```bash\ncurl -G --data-urlencode \"query=SELECT 1;\" \\    -u \"my_user:my_password\" \\    http://localhost:9000/exec\n```\n\nThe second authentication type requires a REST API token to be specified in a\nAuthorization: Bearer\nheader:\n\n```bash\ncurl -G --data-urlencode \"query=SELECT 1;\" \\    -H \"Authorization: Bearer qt1cNK6s2t79f76GmTBN9k7XTWm5wwOtF7C0UBxiHGPn44\" \\    http://localhost:9000/exec\n```\n\nRefer to the\nuser management\npage to\nlearn more on how to generate a REST API token.\n\n## Authentication in QuestDB open source‚Äã\n\nQuestDB Open Source supports HTTP basic authentication. To enable it, set the\nconfiguration options\nhttp.user\nand\nhttp.password\nin\nserver.conf\n.\nThe following example shows how to enable HTTP basic authentication in QuestDB\nopen source:\n\n```shell\nhttp.user=my_userhttp.password=my_password\n```\n\nThen this\ncurl\ncommand executes a\nSELECT 1;\nquery:\n\n```bash\ncurl -G --data-urlencode \"query=SELECT 1;\" \\    -u \"my_user:my_password\" \\    http://localhost:9000/exec\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3945,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-aa5113a317f4",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/select",
    "title": "SELECT keyword | QuestDB",
    "text": "SELECT\nallows you to specify a list of columns and expressions to be selected\nand evaluated from a table.\ntip\nLooking for SELECT best practices? Checkout our\nMaximize your SQL efficiency: SELECT best practices\nblog.\n\n## Syntax‚Äã\n\nNote:\ntable\ncan either a specified table in your database or passed forward as\nthe result of a sub-query.\n\n## Simple select‚Äã\n\n\n### All columns‚Äã\n\nQuestDB supports\nSELECT * FROM tablename\n. When selecting all, you can also\nomit most of the statement and pass the table name.\nThe two examples below are equivalent\nQuestDB dialect\nDemo this query\n\n```questdb-sql\ntrades;\n```\n\nTraditional SQL equivalent\nDemo this query\n\n```questdb-sql\nSELECT * FROM trades;\n```\n\n\n### Specific columns‚Äã\n\nTo select specific columns, replace * by the names of the columns you are\ninterested in.\nExample:\nSelect specific columns\nDemo this query\n\n```questdb-sql\nSELECT timestamp, symbol, side FROM trades;\n```\n\n\n### Aliases‚Äã\n\nUsing aliases allow you to give expressions or column names of your choice. You\ncan assign an alias to a column or an expression by writing the alias name you\nwant after that expression.\nnote\nAlias names and column names must be unique.\nColumn aliases\nDemo this query\n\n```questdb-sql\nSELECT timestamp, symbol,    price AS rate,    amount quantityFROM trades;\n```\n\nNotice how you can use or omit the\nAS\nkeyword.\n\n### Arithmetic expressions‚Äã\n\nSELECT\nis capable of evaluating multiple expressions and functions. You can\nmix comma separated lists of expressions with the column names you are\nselecting.\nArithmetic expressions\nDemo this query\n\n```questdb-sql\nSELECT timestamp, symbol,    price * 0.25 AS price25pct,    amount > 10 AS over10FROM trades;\n```\n\nThe result of\namount > 10\nis a boolean. The column will be named \"over10\" and\ntake values true or false.\n\n## Boolean expressions‚Äã\n\nSupports\nAND\n/\nOR\n,\nNOT\n&\nXOR\n.\n\n### AND and OR‚Äã\n\nAND returns true if both operands are true, and false otherwise.\nOR returns true if at least one of the operands is true.\n\n```questdb-sql\nSELECT    (true AND false) AS this_will_return_false,    (true OR false) AS this_will_return_true;\n```\n\n\n### NOT‚Äã\n\nNOT inverts the truth value of the operand.\n\n```questdb-sql\nSELECT    NOT (true AND false) AS this_will_return_true;\n```\n\n\n### XOR‚Äã\n\n^ is the bitwise XOR operator. It applies only to the Long data type.\nDepending on what you need, you might prefer to cast the input and\noutput to boolean values.\n\n```questdb-sql\nSELECT    (1 ^ 1) AS will_return_0,    (1 ^ 20) AS will_return_21,    (true::int ^ false::long)::boolean AS will_return_true,    (true::int ^ true::long)::boolean AS will_return_false;\n```\n\n\n## Aggregation‚Äã\n\nSupported aggregation functions are listed on the\naggregation reference\n.\n\n### Aggregation by group‚Äã\n\nQuestDB evaluates aggregation functions without need for traditional\nGROUP BY\nwhenever there is a mix of column names and aggregation functions\nin a\nSELECT\nclause. You can have any number of discrete value columns and\nany number of aggregation functions. The three statements below are equivalent.\nQuestDB dialect\nDemo this query\n\n```questdb-sql\nSELECT symbol, avg(price), count()FROM trades;\n```\n\nTraditional SQL equivalent\nDemo this query\n\n```questdb-sql\nSELECT symbol, avg(price), count()FROM tradesGROUP BY symbol;\n```\n\nTraditional SQL equivalent with positional argument\nDemo this query\n\n```questdb-sql\nSELECT symbol, avg(price), count()FROM tradesGROUP BY 1;\n```\n\n\n### Aggregation arithmetic‚Äã\n\nAggregation functions can be used in arithmetic expressions. The following\ncomputes\nmid\nof prices for every symbol.\nAggregation arithmetic\nDemo this query\n\n```questdb-sql\nSELECT symbol, (min(price) + max(price))/2 mid, count() countFROM trades;\n```\n\ntip\nWhenever possible, it is recommended to perform arithmetic\noutside\nof\naggregation functions as this can have a dramatic impact on performance. For\nexample,\nmin(price/2)\nis going to execute considerably more slowly than\nmin(price)/2\n, although both return the same result.\n\n## Supported clauses‚Äã\n\nQuestDB supports the following standard SQL clauses within SELECT statements.\n\n### CASE‚Äã\n\nConditional results based on expressions.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nCASE reference\n\n### CAST‚Äã\n\nConvert values and expression between types.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nCAST reference\n\n### DISTINCT‚Äã\n\nReturns distinct values of the specified column(s).\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nDISTINCT reference\n.\n\n### FILL‚Äã\n\nDefines filling strategy for missing data in aggregation queries. This function\ncomplements\nSAMPLE BY\nqueries.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nFILL reference\n.\n\n### JOIN‚Äã\n\nJoin tables based on a key or timestamp.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nJOIN reference\n\n### PIVOT‚Äã\n\nTransforms rows into columns by aggregating data across specified values.\nUseful for analytics, charting, and reshaping time-series data.\nFor more information, please refer to the\nPIVOT reference\n\n### LIMIT‚Äã\n\nSpecify the number and position of records returned by a query.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nLIMIT reference\n.\n\n### ORDER BY‚Äã\n\nOrders the results of a query by one or several columns.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nORDER BY reference\n\n### UNION, EXCEPT & INTERSECT‚Äã\n\nCombine the results of two or more select statements. Can include or ignore\nduplicates.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nUNION, EXCEPT & INTERSECT reference\n\n### WHERE‚Äã\n\nFilters query results\n\n#### Syntax‚Äã\n\nQuestDB supports complex WHERE clauses along with type-specific searches. For\nmore information, please refer to the\nWHERE reference\n. There are different syntaxes for\ntext\n,\nnumeric\n, or\ntimestamp\nfilters.\n\n## Additional time-series clauses‚Äã\n\nQuestDB augments SQL with the following clauses.\n\n### LATEST ON‚Äã\n\nRetrieves the latest entry by timestamp for a given key or combination of keys\nThis function requires a\ndesignated timestamp\n.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nLATEST ON reference\n.\n\n### SAMPLE BY‚Äã\n\nAggregates\ntime-series data\ninto homogeneous time chunks. For example daily\naverage, monthly maximum etc. This function requires a\ndesignated timestamp\n.\n\n#### Syntax‚Äã\n\nFor more information, please refer to the\nSAMPLE BY reference\n.\n\n### TIMESTAMP‚Äã\n\nDynamically creates a\ndesignated timestamp\non the output of a\nquery. This allows to perform timestamp operations like\nSAMPLE BY\nor\nLATEST ON\non tables which originally do not have a designated\ntimestamp.\ncaution\nThe output query must be ordered by time.\nTIMESTAMP()\ndoes not check for order\nand using timestamp functions on unordered data may produce unexpected results.\n\n#### Syntax‚Äã\n\nFor more information, refer to the\nTIMESTAMP reference",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1024,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-159d32b975a1",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/functions/aggregation",
    "title": "Aggregate functions | QuestDB",
    "text": "This page describes the available functions to assist with performing aggregate\ncalculations. Functions are organized by category below.\n\n## Function categories‚Äã\n\n\n### Basic aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| count | Count rows or non-NULL values |\n| sum | Sum of values |\n| avg | Arithmetic mean |\n| geomean | Geometric mean |\n| min | Minimum value |\n| max | Maximum value |\n\n\n### Positional aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| first | First value (by designated timestamp or insertion order) |\n| first_not_null | First non-NULL value |\n| last | Last value (by designated timestamp or insertion order) |\n| last_not_null | Last non-NULL value |\n| arg_min | Value at the row where another column is minimum |\n| arg_max | Value at the row where another column is maximum |\n\n\n### Statistical aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| stddev / stddev_samp | Sample standard deviation |\n| stddev_pop | Population standard deviation |\n| variance / var_samp | Sample variance |\n| var_pop | Population variance |\n| corr | Pearson correlation coefficient |\n| covar_pop | Population covariance |\n| covar_samp | Sample covariance |\n| mode | Most frequent value |\n\n\n### Approximate aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| approx_count_distinct | Estimated distinct count using HyperLogLog |\n| approx_percentile | Approximate percentile using HdrHistogram |\n| approx_median | Approximate median (50th percentile) |\n\n\n### String aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| string_agg | Concatenate values with delimiter |\n| string_distinct_agg | Concatenate distinct values with delimiter |\n\n\n### Boolean aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| bool_and | True if all values are true |\n| bool_or | True if any value is true |\n\n\n### Bitwise aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| bit_and | Bitwise AND of all non-NULL values |\n| bit_or | Bitwise OR of all non-NULL values |\n| bit_xor | Bitwise XOR of all non-NULL values |\n\n\n### Specialized aggregates‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| count_distinct | Exact count of distinct values |\n| ksum | Kahan compensated sum (for floating-point precision) |\n| nsum | Neumaier sum (for floating-point precision) |\n| haversine_dist_deg | Total traveled distance from lat/lon points |\n| weighted_avg | Weighted arithmetic mean |\n| weighted_stddev | Weighted standard deviation (reliability weights) |\n| weighted_stddev_freq | Weighted standard deviation (frequency weights) |\n| weighted_stddev_rel | Weighted standard deviation (reliability weights) |\n\nQuestDB supports implicit\nGROUP BY\n. When aggregate functions are used with\nnon-aggregated columns, QuestDB automatically groups by those columns. Examples\nin this documentation often omit\nGROUP BY\nfor brevity.\n\n## approx_count_distinct‚Äã\n\napprox_count_distinct(column_name, precision)\n- estimates the number of\ndistinct non-\nNULL\nvalues in\nIPv4\n,\nint\n, or\nlong\ncolumns using the\nHyperLogLog\ndata structure, which provides an\napproximation rather than an exact count.\nThe precision of HyperLogLog can be controlled via the optional\nprecision\nparameter, typically between 4 and 16. A higher precision leads to more accurate\nresults with increased memory usage. The default is 1.\nThis function is useful within\nhigh cardinality\ndatasets where an exact count is not required. Thus consider it the higher\ncardinality alternative to\ncount_distinct\n.\n\n#### Parameters‚Äã\n\n- column_name: The name of the column for which to estimate the count of\ndistinct values.\n- precision(optional): A number specifying the precision of theHyperLogLogalgorithm, which influences the\ntrade-off between accuracy and memory usage. A higher precision gives a more\naccurate estimate, but consumes more memory. Defaults to 1 (lower accuracy,\nhigh efficiency).\n\n#### Return value‚Äã\n\nReturn value type is\nlong\n.\n\n#### Examples‚Äã\n\nPlease note that exact example values will vary as they are approximations\nderived from the HyperLogLog algorithm.\nEstimate count of distinct symbols with precision 5\n\n```questdb-sql\nSELECT approx_count_distinct(symbol, 5) FROM trades;\n```\n\n\n| approx_count_distinct |\n| --- |\n| 1234567 |\n\nEstimate count of distinct user_id (int) values by date\n\n```questdb-sql\nSELECT date, approx_count_distinct(user_id) FROM sessions GROUP BY date;\n```\n\n\n| date | approx_count_distinct |\n| --- | --- |\n| 2023-01-01 | 2358 |\n| 2023-01-02 | 2491 |\n| ... | ... |\n\nEstimate count of distinct product_id values by region\n\n```questdb-sql\nSELECT region, approx_count_distinct(product_id) FROM sales GROUP BY region;\n```\n\n\n| region | approx_count_distinct |\n| --- | --- |\n| North | 1589 |\n| South | 1432 |\n| East | 1675 |\n| West | 1543 |\n\nEstimate count of distinct order_ids with precision 8\n\n```questdb-sql\nSELECT approx_count_distinct(order_id, 8) FROM orders;\n```\n\n\n| approx_count_distinct |\n| --- |\n| 3456789 |\n\nEstimate count of distinct transaction_ids by store_id\n\n```questdb-sql\nSELECT store_id, approx_count_distinct(transaction_id) FROM transactions GROUP BY store_id;\n```\n\n\n| store_id | approx_count_distinct |\n| --- | --- |\n| 1 | 56789 |\n| 2 | 67890 |\n| ... | ... |\n\n\n## approx_percentile‚Äã\n\napprox_percentile(value, percentile, precision)\ncalculates the approximate\nvalue for the given non-negative column and percentile using the\nHdrHistogram\nalgorithm.\n\n#### Parameters‚Äã\n\n- valueis any numeric non-negative value.\n- percentileis adoublevalue between 0.0 and 1.0, inclusive.\n- precisionis an optionalintvalue between 0 and 5, inclusive. This is the\nnumber of significant decimal digits to which the histogram will maintain\nvalue resolution and separation. For example, when the input column contains\ninteger values between 0 and 3,600,000,000 and the precision is set to 3,\nvalue quantization within the range will be no larger than 1/1,000th (or 0.1%)\nof any value. In this example, the function tracks and analyzes the counts of\nobserved response times ranging between 1 microsecond and 1 hour in magnitude,\nwhile maintaining a value resolution of 1 microsecond up to 1 millisecond, a\nresolution of 1 millisecond (or better) up to one second, and a resolution of\n1 second (or better) up to 1,000 seconds. At its maximum tracked value (1\nhour), it would still maintain a resolution of 3.6 seconds (or better).\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nApproximate percentile\n\n```questdb-sql\nSELECT approx_percentile(price, 0.99) FROM trades;\n```\n\n\n| approx_percentile |\n| --- |\n| 101.5 |\n\n\n#### See also‚Äã\n\n- approx_median- Shorthand for 50th percentile\n\n## approx_median‚Äã\n\napprox_median(value, precision)\ncalculates the approximate median (50th\npercentile) of a set of non-negative numeric values using the\nHdrHistogram\nalgorithm. This is equivalent to\ncalling\napprox_percentile(value, 0.5, precision)\n.\nThe function will throw an error if any negative values are encountered in the\ninput. All input values must be non-negative.\n\n#### Parameters‚Äã\n\n- valueis any non-negative numeric value.\n- precision(optional) is anintvalue between 0 and 5, inclusive. This is\nthe number of significant decimal digits to which the histogram will maintain\nvalue resolution and separation. Higher precision leads to more accurate\nresults with increased memory usage. Defaults to 1 (lower accuracy, high\nefficiency).\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nCalculate approximate median price by symbol\nDemo this query\n\n```questdb-sql\nSELECT symbol, approx_median(price) FROM tradesWHERE timestamp in today()GROUP BY symbol;\n```\n\n\n| symbol | approx_median |\n| --- | --- |\n| BTC-USD | 39265.31 |\n| ETH-USD | 2615.46 |\n\nCalculate approximate median with higher precision\nDemo this query\n\n```questdb-sql\nSELECT symbol, approx_median(price, 3) FROM tradesWHERE timestamp in today()GROUP BY symbol;\n```\n\n\n| symbol | approx_median |\n| --- | --- |\n| BTC-USD | 39265.312 |\n| ETH-USD | 2615.459 |\n\n\n#### See also‚Äã\n\n- approx_percentile- Approximate percentile for any quantile\n\n## arg_max‚Äã\n\narg_max(value, key)\nreturns the value of the first argument at the row where\nthe second argument reaches its maximum value. This function is useful for\nfinding values at extreme points in time-series and grouped data.\n\n#### Parameters‚Äã\n\n- valueis the column or expression whose value to return.\n- keyis the column or expression used to determine which row to select (the\nrow with the maximum key value).\n\n#### Return value‚Äã\n\nReturn value type matches the type of the\nvalue\nargument.\n\n#### Null handling‚Äã\n\n- Rows wherekeyisNULLare ignored during aggregation.\n- If the value at the maximum key row isNULL, the result isNULL.\n- If all keys in a group areNULL, the result isNULL.\n\n#### Supported type combinations‚Äã\n\nThe function supports the following type combinations for\nvalue\nand\nkey\n:\n\n| Value Type | Key Types |\n| --- | --- |\n| double | double, long, timestamp |\n| long | double, timestamp |\n| timestamp | double, long, uuid |\n| uuid | timestamp |\n\n\n#### Examples‚Äã\n\nFind the timestamp when the highest price occurred\n\n```questdb-sql\nSELECT arg_max(timestamp, price) AS peak_time FROM trades;\n```\n\n\n| peak_time |\n| --- |\n| 2024-03-14T09:32:15.000000Z |\n\nFind when each symbol hit its all-time high\n\n```questdb-sql\nSELECT symbol, arg_max(timestamp, price) AS ath_timeFROM tradesGROUP BY symbol;\n```\n\n\n| symbol | ath_time |\n| --- | --- |\n| BTC-USD | 2024-03-14T09:32:15.000000Z |\n| ETH-USD | 2024-03-12T14:05:22.000000Z |\n\nFind the order_id of the largest trade for each symbol\n\n```questdb-sql\nSELECT symbol, arg_max(order_id, amount) AS largest_orderFROM tradesGROUP BY symbol;\n```\n\n\n| symbol | largest_order |\n| --- | --- |\n| BTC-USD | 550e8400-e29b-41d4-a716-446655440000 |\n| ETH-USD | 6ba7b810-9dad-11d1-80b4-00c04fd430c8 |\n\n\n#### See also‚Äã\n\n- arg_min- Value at the row where another column is minimum\n- max- Returns the maximum value itself\n- last- Returns the last value by timestamp order\n\n## arg_min‚Äã\n\narg_min(value, key)\nreturns the value of the first argument at the row where\nthe second argument reaches its minimum value. This function is useful for\nfinding values at extreme points in time-series and grouped data.\n\n#### Parameters‚Äã\n\n- valueis the column or expression whose value to return.\n- keyis the column or expression used to determine which row to select (the\nrow with the minimum key value).\n\n#### Return value‚Äã\n\nReturn value type matches the type of the\nvalue\nargument.\n\n#### Null handling‚Äã\n\n- Rows wherekeyisNULLare ignored during aggregation.\n- If the value at the minimum key row isNULL, the result isNULL.\n- If all keys in a group areNULL, the result isNULL.\n\n#### Supported type combinations‚Äã\n\nThe function supports the following type combinations for\nvalue\nand\nkey\n:\n\n| Value Type | Key Types |\n| --- | --- |\n| double | double, long, timestamp |\n| long | double, timestamp |\n| timestamp | double, long, uuid |\n| uuid | timestamp |\n\n\n#### Examples‚Äã\n\nFind the timestamp when the lowest price occurred\n\n```questdb-sql\nSELECT arg_min(timestamp, price) AS bottom_time FROM trades;\n```\n\n\n| bottom_time |\n| --- |\n| 2024-01-15T04:23:00.000000Z |\n\nFind when each symbol hit its all-time low\n\n```questdb-sql\nSELECT symbol, arg_min(timestamp, price) AS atl_timeFROM tradesGROUP BY symbol;\n```\n\n\n| symbol | atl_time |\n| --- | --- |\n| BTC-USD | 2024-01-15T04:23:00.000000Z |\n| ETH-USD | 2024-01-22T08:15:33.000000Z |\n\nFind the sensor_id that recorded the coldest temperature\n\n```questdb-sql\nSELECT arg_min(sensor_id, temperature) AS coldest_sensorFROM weather_data;\n```\n\n\n| coldest_sensor |\n| --- |\n| 47 |\n\n\n#### See also‚Äã\n\n- arg_max- Value at the row where another column is maximum\n- min- Returns the minimum value itself\n- first- Returns the first value by timestamp order\n\n## avg‚Äã\n\navg(value)\ncalculates simple average of values ignoring missing data (e.g\nNULL\nvalues).\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nAverage transaction amount\n\n```questdb-sql\nSELECT avg(amount) FROM transactions;\n```\n\n\n| avg |\n| --- |\n| 22.4 |\n\nAverage transaction amount by payment_type\n\n```questdb-sql\nSELECT payment_type, avg(amount) FROM transactions;\n```\n\n\n| payment_type | avg |\n| --- | --- |\n| cash | 22.1 |\n| card | 27.4 |\n| NULL | 18.02 |\n\n\n#### See also‚Äã\n\n- sum- Sum of values\n- weighted_avg- Weighted arithmetic mean\n\n## bit_and‚Äã\n\nbit_and(value)\nreturns the bitwise AND of all non-NULL values in an integer\ncolumn.\n\n#### Parameters‚Äã\n\n- valueis abyte,short,int, orlongcolumn.\n\n#### Return value‚Äã\n\nReturn value type matches the type of the argument.\n\n#### Examples‚Äã\n\nBitwise AND of all status flags\n\n```questdb-sql\nSELECT bit_and(flags) FROM events;\n```\n\n\n| bit_and |\n| --- |\n| 4 |\n\nBitwise AND of status flags by category\n\n```questdb-sql\nSELECT category, bit_and(status_flags) FROM items;\n```\n\n\n| category | bit_and |\n| --- | --- |\n| A | 1 |\n| B | 0 |\n| C | 5 |\n\n\n#### See also‚Äã\n\n- bit_or- Bitwise OR of all non-NULL values\n- bit_xor- Bitwise XOR of all non-NULL values\n\n## bit_or‚Äã\n\nbit_or(value)\nreturns the bitwise OR of all non-NULL values in an integer\ncolumn.\n\n#### Parameters‚Äã\n\n- valueis abyte,short,int, orlongcolumn.\n\n#### Return value‚Äã\n\nReturn value type matches the type of the argument.\n\n#### Examples‚Äã\n\nBitwise OR of all permissions\n\n```questdb-sql\nSELECT bit_or(permissions) FROM users;\n```\n\n\n| bit_or |\n| --- |\n| 15 |\n\nBitwise OR of permissions by role\n\n```questdb-sql\nSELECT role, bit_or(permissions) FROM users;\n```\n\n\n| role | bit_or |\n| --- | --- |\n| admin | 255 |\n| editor | 31 |\n| viewer | 1 |\n\n\n#### See also‚Äã\n\n- bit_and- Bitwise AND of all non-NULL values\n- bit_xor- Bitwise XOR of all non-NULL values\n\n## bit_xor‚Äã\n\nbit_xor(value)\nreturns the bitwise XOR of all non-NULL values in an integer\ncolumn.\n\n#### Parameters‚Äã\n\n- valueis abyte,short,int, orlongcolumn.\n\n#### Return value‚Äã\n\nReturn value type matches the type of the argument.\n\n#### Examples‚Äã\n\nBitwise XOR of all checksums\n\n```questdb-sql\nSELECT bit_xor(checksum) FROM data;\n```\n\n\n| bit_xor |\n| --- |\n| 42 |\n\nBitwise XOR by partition\n\n```questdb-sql\nSELECT partition_id, bit_xor(value) FROM records;\n```\n\n\n| partition_id | bit_xor |\n| --- | --- |\n| 1 | 170 |\n| 2 | 85 |\n\n\n#### See also‚Äã\n\n- bit_and- Bitwise AND of all non-NULL values\n- bit_or- Bitwise OR of all non-NULL values\n\n## bool_and‚Äã\n\nbool_and(value)\nreturns\ntrue\nif all non-NULL values in the group are\ntrue\n,\notherwise returns\nfalse\n. This function is useful for checking if a condition\nholds across all rows in a group.\n\n#### Parameters‚Äã\n\n- valueis a boolean column or expression.\n\n#### Return value‚Äã\n\nReturn value type is\nboolean\n.\n\n#### Examples‚Äã\n\nCheck if all orders are fulfilled\n\n```questdb-sql\nSELECT bool_and(is_fulfilled) FROM orders;\n```\n\n\n| bool_and |\n| --- |\n| false |\n\nCheck if all items passed QA by batch\n\n```questdb-sql\nSELECT batch_id, bool_and(passed_qa) FROM items;\n```\n\n\n| batch_id | bool_and |\n| --- | --- |\n| 1 | true |\n| 2 | false |\n| 3 | true |\n\nCheck if all prices are above threshold\n\n```questdb-sql\nSELECT symbol, bool_and(price > 100) FROM trades;\n```\n\n\n| symbol | bool_and |\n| --- | --- |\n| BTC-USD | true |\n| ETH-USD | false |\n\n\n#### See also‚Äã\n\n- bool_or- True if any value is true\n\n## bool_or‚Äã\n\nbool_or(value)\nreturns\ntrue\nif any non-NULL value in the group is\ntrue\n,\notherwise returns\nfalse\n. This function is useful for checking if a condition\nholds for at least one row in a group.\n\n#### Parameters‚Äã\n\n- valueis a boolean column or expression.\n\n#### Return value‚Äã\n\nReturn value type is\nboolean\n.\n\n#### Examples‚Äã\n\nCheck if any order has errors\n\n```questdb-sql\nSELECT bool_or(has_error) FROM orders;\n```\n\n\n| bool_or |\n| --- |\n| true |\n\nCheck if any item failed QA by batch\n\n```questdb-sql\nSELECT batch_id, bool_or(failed_qa) FROM items;\n```\n\n\n| batch_id | bool_or |\n| --- | --- |\n| 1 | false |\n| 2 | true |\n| 3 | false |\n\nCheck if any trade exceeded volume threshold\n\n```questdb-sql\nSELECT symbol, bool_or(volume > 1000000) FROM trades;\n```\n\n\n| symbol | bool_or |\n| --- | --- |\n| BTC-USD | true |\n| ETH-USD | true |\n\n\n#### See also‚Äã\n\n- bool_and- True if all values are true\n\n## corr‚Äã\n\ncorr(arg0, arg1)\nis a function that measures how closely two sets of numbers\nmove in the same direction. It does this by comparing how much each number in\neach set differs from the average of its set. This calculation is based on\nWelford's Algorithm\n.\n- If the numbers in both sets tend to be above or below their average values at\nthe same time, the function will return a value close to 1.\n- If one set of numbers tends to be above its average value when the other set\nis below its average, the function will return a value close to -1.\n- If there's no clear pattern, the function will return a value close to 0.\n\n#### Parameters‚Äã\n\n- arg0is any numeric value representing the first variable\n- arg1is any numeric value representing the second variable\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nCorrelation between price and quantity\n\n```questdb-sql\nSELECT corr(price, quantity) FROM transactions;\n```\n\n\n| corr |\n| --- |\n| 0.89 |\n\nCorrelation between price and quantity grouped by payment type\n\n```questdb-sql\nSELECT payment_type, corr(price, quantity) FROM transactions GROUP BY payment_type;\n```\n\n\n| payment_type | corr |\n| --- | --- |\n| cash | 0.85 |\n| card | 0.92 |\n| NULL | 0.78 |\n\n\n## count‚Äã\n\n- count()orcount(*)- counts the number of rows irrespective of underlying\ndata.\n- count(column_name)- counts the number of non-NULL values in a given column.\n\n#### Parameters‚Äã\n\n- count()does not require arguments.\n- count(column_name)- supports the following data types:doublefloatintegercharactershortbytetimestampdatelonglong256geohashvarcharstringsymbol\n\n#### Return value‚Äã\n\nReturn value type is\nlong\n.\n\n#### Examples‚Äã\n\nCount of rows in the\ntransactions\ntable:\n\n```questdb-sql\nSELECT count() FROM transactions;\n```\n\n\n| count |\n| --- |\n| 100 |\n\nCount of rows in the\ntransactions\ntable aggregated by the\npayment_type\nvalue:\n\n```questdb-sql\nSELECT payment_type, count() FROM transactions;\n```\n\n\n| payment_type | count |\n| --- | --- |\n| cash | 25 |\n| card | 70 |\n| NULL | 5 |\n\nCount non-NULL transaction amounts:\n\n```questdb-sql\nSELECT count(amount) FROM transactions;\n```\n\n\n| count |\n| --- |\n| 95 |\n\nCount non-NULL transaction amounts by\npayment_type\n:\n\n```questdb-sql\nSELECT payment_type, count(amount) FROM transactions;\n```\n\n\n| payment_type | count |\n| --- | --- |\n| cash | 24 |\n| card | 67 |\n| NULL | 4 |\n\nnote\nNULL\nvalues are aggregated with\ncount()\n, but not with\ncount(column_name)\n\n#### See also‚Äã\n\n- count_distinct- Exact count of distinct values\n- approx_count_distinct- Estimated distinct count for large datasets\n\n## count_distinct‚Äã\n\ncount_distinct(column_name)\n- counts distinct non-\nNULL\nvalues in\nvarchar\n,\nsymbol\n,\nlong256\n,\nUUID\n,\nIPv4\n,\nlong\n,\nint\nor\nstring\ncolumns.\n\n#### Return value‚Äã\n\nReturn value type is\nlong\n.\n\n#### Examples‚Äã\n\n- Count of distinct sides in the transactions table. Side column can either beBUYorSELLorNULL.\n\n```questdb-sql\nSELECT count_distinct(side) FROM transactions;\n```\n\n\n| count_distinct |\n| --- |\n| 2 |\n\n- Count of distinct counterparties in the transactions table aggregated bypayment_typevalue.\n\n```questdb-sql\nSELECT payment_type, count_distinct(counterparty) FROM transactions;\n```\n\n\n| payment_type | count_distinct |\n| --- | --- |\n| cash | 3 |\n| card | 23 |\n| NULL | 5 |\n\n\n#### See also‚Äã\n\n- count- Count all rows or non-NULL values\n- approx_count_distinct- Estimated distinct count for large datasets\n\n## covar_pop‚Äã\n\ncovar_pop(arg0, arg1)\nis a function that measures how much two sets of numbers\nchange together. It does this by looking at how much each number in each set\ndiffers from the average of its set. It multiplies these differences together,\nadds them all up, and then divides by the total number of pairs. This gives a\nmeasure of the overall trend.\n- If the numbers in both sets tend to be above or below their average values at\nthe same time, the function will return a positive number.\n- If one set of numbers tends to be above its average value when the other set\nis below its average, the function will return a negative number.\n- The closer the result is to zero, the less relationship there is between the\ntwo sets of numbers.\n\n#### Parameters‚Äã\n\n- arg0is any numeric value representing the first variable\n- arg1is any numeric value representing the second variable.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nPopulation covariance between price and quantity\n\n```questdb-sql\nSELECT covar_pop(price, quantity) FROM transactions;\n```\n\n\n| covar_pop |\n| --- |\n| 15.2 |\n\nPopulation covariance between price and quantity grouped by payment type\n\n```questdb-sql\nSELECT payment_type, covar_pop(price, quantity) FROM transactions GROUP BY payment_type;\n```\n\n\n| payment_type | covar_pop |\n| --- | --- |\n| cash | 14.8 |\n| card | 16.2 |\n| NULL | 13.5 |\n\n\n## covar_samp‚Äã\n\ncovar_samp(arg0, arg1)\nis a function that finds the relationship between two\nsets of numbers. It does this by looking at how much the numbers vary from the\naverage in each set.\n- If the numbers in both sets tend to be above or below their average values at\nthe same time, the function will return a positive number.\n- If one set of numbers tends to be above its average value when the other set\nis below its average, the function will return a negative number.\n- The closer the result is to zero, the less relationship there is between the\ntwo sets of numbers.\n\n#### Parameters‚Äã\n\n- arg0is any numeric value representing the first variable.\n- arg1is any numeric value representing the second variable.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nSample covariance between price and quantity\n\n```questdb-sql\nSELECT covar_samp(price, quantity) FROM transactions;\n```\n\n\n| covar_samp |\n| --- |\n| 15.8 |\n\nSample covariance between price and quantity grouped by payment type\n\n```questdb-sql\nSELECT payment_type, covar_samp(price, quantity) FROM transactions GROUP BY payment_type;\n```\n\n\n| payment_type | covar_samp |\n| --- | --- |\n| cash | 15.4 |\n| card | 16.8 |\n| NULL | 14.1 |\n\n\n## first‚Äã\n\n- first(column_name)- returns the first value of a column.\nSupported column datatype:\ndouble\n,\nfloat\n,\ninteger\n,\nIPv4\n,\ncharacter\n,\nshort\n,\nbyte\n,\ntimestamp\n,\ndate\n,\nlong\n,\ngeohash\n,\nsymbol\n,\nvarchar\n,\nuuid\nand\narray\n.\nIf a table has a\ndesignated timestamp\n,\nthen the first row is always the row with the lowest timestamp (oldest). For a\ntable without a designated timestamp column,\nfirst\nreturns the first row\nregardless of any timestamp column.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nGiven a table\ntrades\n, which has a designated timestamp column:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| AAPL | 142 | 2021-06-02T14:33:19.970258Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n\nThe following query returns oldest value for the\nsymbol\ncolumn:\n\n```questdb-sql\nSELECT first(symbol) FROM trades;\n```\n\n\n| first |\n| --- |\n| AAPL |\n\nWithout selecting a designated timestamp column, the table may be unordered and\nthe query may return different result. Given an unordered table\ntrades_unordered\n:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| AAPL | 142 | 2021-06-02T14:33:19.970258Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n\nThe following query returns the first record for the\nsymbol\ncolumn:\n\n```questdb-sql\nSELECT first(symbol) FROM trades_unordered;\n```\n\n\n| first |\n| --- |\n| AAPL |\n\n\n#### See also‚Äã\n\n- first_not_null- First non-NULL value\n- last- Last value by timestamp order\n- arg_min- Value at the row where another column is minimum\n\n## first_not_null‚Äã\n\n- first_not_null(column_name)- returns the first non-NULL value of a column.\nSupported column datatype:\ndouble\n,\nfloat\n,\ninteger\n,\nIPv4\n,\nchar\n,\nshort\n,\nbyte\n,\ntimestamp\n,\ndate\n,\nlong\n,\ngeohash\n,\nsymbol\n,\nvarchar\n,\nuuid\nand\narray\n.\nIf a table has a designated timestamp, then the first non-NULL row is always the\nrow with the lowest timestamp (oldest). For a table without a designated\ntimestamp column,\nfirst_not_null\nreturns the first non-NULL row, regardless of\nany timestamp column.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nGiven a table\ntrades\n, which has a designated timestamp column:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| NULL | 142 | 2021-06-02T14:33:19.970258Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n\nThe following query returns oldest non-NULL value for the symbol column:\n\n```questdb-sql\nSELECT first_not_null(symbol) FROM trades;\n```\n\n\n| first_not_null |\n| --- |\n| GOOGL |\n\nWithout selecting a designated timestamp column, the table may be unordered and\nthe query may return different result. Given an unordered table\ntrades_unordered\n:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| NULL | 142 | 2021-06-02T14:33:19.970258Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n\nThe following query returns the first non-NULL record for the symbol column:\n\n```questdb-sql\nSELECT first_not_null(symbol) FROM trades_unordered;\n```\n\n\n| first_not_null |\n| --- |\n| MSFT |\n\n\n#### See also‚Äã\n\n- first- First value (may be NULL)\n- last_not_null- Last non-NULL value\n\n## geomean‚Äã\n\ngeomean(value)\ncalculates the geometric mean of a set of positive values. The\ngeometric mean is computed using the formula\nexp(avg(ln(x)))\n, which prevents\noverflow issues with large products by using logarithms.\nThe geometric mean is useful for calculating average growth rates, ratios, and\nother multiplicative quantities.\n\n#### Parameters‚Äã\n\n- valueis adoublecolumn or expression. Other numeric types are implicitly\nconverted todouble.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Null and edge case handling‚Äã\n\n\n| Input | Result | Reason |\n| --- | --- | --- |\n| Negative values | NULL | Geometric mean undefined |\n| Zero values | NULL | ln(0)is undefined |\n| NULL values | Skipped | Standard aggregate behavior |\n| Empty group | NULL | Standard aggregate behavior |\n\n\n#### Examples‚Äã\n\nGeometric mean of growth rates\n\n```questdb-sql\nSELECT geomean(growth_rate) FROM quarterly_data;\n```\n\n\n| geomean |\n| --- |\n| 1.12 |\n\nGeometric mean of returns by asset\n\n```questdb-sql\nSELECT asset, geomean(return_factor) FROM portfolio;\n```\n\n\n| asset | geomean |\n| --- | --- |\n| stocks | 1.08 |\n| bonds | 1.03 |\n| crypto | 1.25 |\n\nComparing arithmetic and geometric means\n\n```questdb-sql\nSELECT avg(return_factor) AS arithmetic_mean,       geomean(return_factor) AS geometric_meanFROM investments;\n```\n\n\n| arithmetic_mean | geometric_mean |\n| --- | --- |\n| 1.15 | 1.12 |\n\n\n#### See also‚Äã\n\n- avg- Arithmetic mean\n\n## haversine_dist_deg‚Äã\n\nhaversine_dist_deg(lat, lon, ts)\n- calculates the traveled distance for a\nseries of latitude and longitude points.\n\n#### Parameters‚Äã\n\n- latis the latitude expressed as degrees in decimal format (double)\n- lonis the longitude expressed as degrees in decimal format (double)\n- tsis thetimestampfor the data point\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nCalculate the aggregate traveled distance for each car_id\n\n```questdb-sql\nSELECT car_id, haversine_dist_deg(lat, lon, k)FROM rides;\n```\n\n\n## ksum‚Äã\n\nksum(value)\n- adds values ignoring missing data (e.g\nNULL\nvalues). Values\nare added using the\nKahan compensated sum algorithm\n.\nThis is only beneficial for floating-point values such as\nfloat\nor\ndouble\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT ksum(a)FROM (SELECT rnd_double() a FROM long_sequence(100));\n```\n\n\n| ksum |\n| --- |\n| 52.79143968514029 |\n\n\n## last‚Äã\n\n- last(column_name)- returns the last value of a column.\nSupported column datatype:\ndouble\n,\nfloat\n,\ninteger\n,\nIPv4\n,\ncharacter\n,\nshort\n,\nbyte\n,\ntimestamp\n,\ndate\n,\nlong\n,\ngeohash\n,\nsymbol\n,\nvarchar\n,\nuuid\nand\narray\n.\nIf a table has a\ndesignated timestamp\n,\nthe last row is always the one with the highest (latest) timestamp.\nFor a table without a designated timestamp column,\nlast\nreturns the last\ninserted row, regardless of any timestamp column.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nGiven a table\ntrades\n, which has a designated timestamp column:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| AAPL | 142 | 2021-06-02T14:33:19.970258Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n\nThe following query returns the latest value for the\nsymbol\ncolumn:\n\n```questdb-sql\nSELECT last(symbol) FROM trades;\n```\n\n\n| last |\n| --- |\n| MSFT |\n\nWithout selecting a designated timestamp column, the table may be unordered and\nthe query may return different result. Given an unordered table\ntrades_unordered\n:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| AAPL | 142 | 2021-06-02T14:33:19.970258Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n\nThe following query returns the last record for the\nsymbol\ncolumn:\n\n```questdb-sql\nSELECT last(symbol) FROM trades_unordered;\n```\n\n\n| last |\n| --- |\n| GOOGL |\n\n\n#### See also‚Äã\n\n- last_not_null- Last non-NULL value\n- first- First value by timestamp order\n- arg_max- Value at the row where another column is maximum\n\n## last_not_null‚Äã\n\n- last_not_null(column_name)- returns the last non-NULL value of a column.\nSupported column datatype:\ndouble\n,\nfloat\n,\ninteger\n,\nIPv4\n,\nchar\n,\nshort\n,\nbyte\n,\ntimestamp\n,\ndate\n,\nlong\n,\ngeohash\n,\nsymbol\n,\nvarchar\n,\nuuid\nand\narray\n.\nIf a table has a designated timestamp, then the last non-NULL row is always the\nrow with the highest timestamp (most recent). For a table without a designated\ntimestamp column,\nlast_not_null\nreturns the last non-NULL row, regardless of\nany timestamp column.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nGiven a table\ntrades\n, which has a designated timestamp column:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| NULL | 142 | 2021-06-02T14:33:19.970258Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n\nThe following query returns most recent non-NULL value for the symbol column:\n\n```questdb-sql\nSELECT last_not_null(symbol) FROM trades;\n```\n\n\n| last_not_null |\n| --- |\n| MSFT |\n\nWithout selecting a designated timestamp column, the table may be unordered and\nthe query may return different result. Given an unordered table\ntrades_unordered\n:\n\n| symbol | price | ts |\n| --- | --- | --- |\n| NULL | 142 | 2021-06-02T14:33:19.970258Z |\n| MSFT | 285 | 2021-06-02T14:33:23.707013Z |\n| GOOGL | 2750 | 2021-06-02T14:33:21.703934Z |\n\nThe following query returns the last non-NULL record for the\nsymbol\ncolumn:\n\n```questdb-sql\nSELECT last_not_null(symbol) FROM trades_unordered;\n```\n\n\n| last_not_null |\n| --- |\n| GOOGL |\n\n\n#### See also‚Äã\n\n- last- Last value (may be NULL)\n- first_not_null- First non-NULL value\n\n## max‚Äã\n\nmax(value)\n- returns the highest value ignoring missing data (e.g\nNULL\nvalues).\n\n#### Parameters‚Äã\n\n- valueis any numeric or string value\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nHighest transaction amount\n\n```questdb-sql\nSELECT max(amount) FROM transactions;\n```\n\n\n| max |\n| --- |\n| 55.3 |\n\nHighest transaction amount by payment_type\n\n```questdb-sql\nSELECT payment_type, max(amount) FROM transactions;\n```\n\n\n| payment_type | max |\n| --- | --- |\n| cash | 31.5 |\n| card | 55.3 |\n| NULL | 29.2 |\n\n\n#### See also‚Äã\n\n- min- Returns the minimum value\n- arg_max- Returns another column's value at the row where this column is maximum\n\n## min‚Äã\n\nmin(value)\n- returns the lowest value ignoring missing data (e.g\nNULL\nvalues).\n\n#### Parameters‚Äã\n\n- valueis any numeric or string value\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nLowest transaction amount\n\n```questdb-sql\nSELECT min(amount) FROM transactions;\n```\n\n\n| min |\n| --- |\n| 12.5 |\n\nLowest transaction amount, by payment_type\n\n```questdb-sql\nSELECT payment_type, min(amount) FROM transactions;\n```\n\n\n| payment_type | min |\n| --- | --- |\n| cash | 12.5 |\n| card | 15.3 |\n| NULL | 22.2 |\n\n\n#### See also‚Äã\n\n- max- Returns the maximum value\n- arg_min- Returns another column's value at the row where this column is minimum\n\n## mode‚Äã\n\nmode(value)\n- calculates the mode (most frequent) value out of a particular\ndataset.\nFor\nmode(B)\n, if there are an equal number of\ntrue\nand\nfalse\nvalues,\ntrue\nwill be returned as a tie-breaker.\nFor other modes, if there are equal mode values, the returned value will be\nwhichever the code identifies first.\nTo make the result deterministic, you must enforce an underlying sort order.\n\n#### Parameters‚Äã\n\n- value- one of (LONG, DOUBLE, BOOLEAN, STRING, VARCHAR, SYMBOL)\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the input\nvalue\n.\n\n#### Examples‚Äã\n\nWith this dataset:\n\n| symbol | value |\n| --- | --- |\n| A | alpha |\n| A | alpha |\n| A | alpha |\n| A | omega |\n| B | beta |\n| B | beta |\n| B | gamma |\n\n\n```questdb-sql\nSELECT symbol, mode(value) as mode FROM dataset;\n```\n\n\n| symbol | mode |\n| --- | --- |\n| A | alpha |\n| B | beta |\n\nOn demo:\nmode() on demo\nDemo this query\n\n```questdb-sql\nSELECT symbol, mode(side)FROM tradesWHERE timestamp IN today()ORDER BY symbol ASC;\n```\n\n\n| symbol | mode(side) |\n| --- | --- |\n| ADA-USD | buy |\n| ADA-USDT | buy |\n| AVAX-USD | sell |\n| AVAX-USDT | sell |\n| BTC-USD | sell |\n| BTC-USDT | sell |\n| ... | ... |\n\n\n## nsum‚Äã\n\nnsum(value)\n- adds values ignoring missing data (e.g\nNULL\nvalues). Values\nare added using the\nNeumaier sum algorithm\n.\nThis is only beneficial for floating-point values such as\nfloat\nor\ndouble\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT nsum(a)FROM (SELECT rnd_double() a FROM long_sequence(100));\n```\n\n\n| nsum |\n| --- |\n| 49.5442334742831 |\n\n\n## stddev / stddev_samp‚Äã\n\nstddev_samp(value)\n- Calculates the sample standard deviation of a set of\nvalues, ignoring missing data (e.g., NULL values). The sample standard deviation\nis a measure of the amount of variation or dispersion in a sample of a\npopulation. A low standard deviation indicates that the values tend to be close\nto the mean of the set, while a high standard deviation indicates that the\nvalues are spread out over a wider range.\nstddev\nis an alias for\nstddev_samp\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT stddev_samp(x)FROM (SELECT x FROM long_sequence(100));\n```\n\n\n| stddev_samp |\n| --- |\n| 29.011491975882 |\n\n\n## stddev_pop‚Äã\n\nstddev_pop(value)\n- Calculates the population standard deviation of a set of\nvalues. The population standard deviation is a measure of the amount of\nvariation or dispersion of a set of values. A low standard deviation indicates\nthat the values tend to be close to the mean of the set, while a high standard\ndeviation indicates that the values are spread out over a wider range.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT stddev_pop(x)FROM (SELECT x FROM long_sequence(100));\n```\n\n\n| stddev_pop |\n| --- |\n| 28.86607004772212 |\n\n\n## string_agg‚Äã\n\nstring_agg(value, delimiter)\n- Concatenates the given string values into a\nsingle string with the delimiter used as a value separator.\n\n#### Parameters‚Äã\n\n- valueis avarcharvalue.\n- delimiteris acharvalue.\n\n#### Return value‚Äã\n\nReturn value type is\nvarchar\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT string_agg(x::varchar, ',')FROM (SELECT x FROM long_sequence(5));\n```\n\n\n| string_agg |\n| --- |\n| 1,2,3,4,5 |\n\n\n## string_distinct_agg‚Äã\n\nstring_distinct_agg(value, delimiter)\n- concatenates distinct non-NULL string\nvalues into a single string, using the specified delimiter to separate the\nvalues.\n- string_distinct_aggignores NULL values and only concatenates non-NULL\ndistinct values.\n- Order is guaranteed.\n- Does not supportORDER BY.\n\n#### Parameters‚Äã\n\n- value: A varchar or string column containing the values to be aggregated.\n- delimiter: A char value used to separate the distinct values in the\nconcatenated string.\n\n#### Return value‚Äã\n\nReturn value type is\nstring\n.\n\n#### Examples‚Äã\n\nSuppose we want to find all the distinct symbols observed in the trades\ntable in our public demo:\nstring_distinct_agg example\nDemo this query\n\n```questdb-sql\nSELECT string_distinct_agg(symbol, ',') AS distinct_symbolsFROM tradesWHERE timestamp in today();\n```\n\nThis query will return a single string containing all the distinct symbol values\nseparated by commas. Even though the\nsymbol\ncolumn may have many rows with\nrepeated values,\nstring_distinct_agg\naggregates only the unique non-NULL\nvalues. The result is a comma-separated list of all distinct symbols observed.\nResult:\n\n| distinct_symbols |\n| --- |\n| BTC-USDT,BTC-USD,ETH-USDT,ETH-USD,SOL-USDT,SOL-USD,ADA-USDT,ADA-USD,XLM-USDT,XLM-USD,LTC-USDT,LTC-USD,UNI-USDT,UNI-USD,AVAX-USDT,AVAX-USD,DOT-USDT,DOT-USD,SOL-BTC,SOL-ETH,ETH-BTC,LTC-BTC,DAI-USDT,DAI-USD |\n\nYou can also group the aggregation by another column.\nTo find out which symbols are observed for each side:\nstring_distinct_agg example with GROUP BY\nDemo this query\n\n```questdb-sql\nSELECT side, string_distinct_agg(symbol, ',') AS distinct_symbolsFROM tradesWHERE timestamp in today();\n```\n\n\n| side | distinct_symbols |\n| --- | --- |\n| buy | BTC-USDT,BTC-USD,ETH-USDT,ETH-USD,ADA-USDT,ADA-USD,SOL-USDT,SOL-USD,LTC-USDT,LTC-USD,UNI-USDT,UNI-USD,DOT-USDT,DOT-USD,XLM-USDT,XLM-USD,SOL-BTC,AVAX-USDT,AVAX-USD,SOL-ETH,ETH-BTC,LTC-BTC,DAI-USDT,DAI-USD |\n| sell | ETH-USDT,ETH-USD,SOL-USDT,SOL-USD,XLM-USDT,XLM-USD,BTC-USDT,BTC-USD,LTC-USDT,LTC-USD,AVAX-USDT,AVAX-USD,DOT-USDT,DOT-USD,SOL-BTC,ADA-USDT,ADA-USD,SOL-ETH,ETH-BTC,UNI-USDT,UNI-USD,DAI-USDT,DAI-USD,LTC-BTC |\n\nNote we don't need to add\nGROUP BY side\nas it is implicit. But you can add it,\nif you prefer that syntax.\n\n## sum‚Äã\n\nsum(value)\n- adds values ignoring missing data (e.g\nNULL\nvalues).\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is the same as the type of the argument.\n\n#### Examples‚Äã\n\nSum all quantities in the transactions table\n\n```questdb-sql\nSELECT sum(quantity) FROM transactions;\n```\n\n\n| sum |\n| --- |\n| 100 |\n\nSum all quantities in the transactions table, aggregated by item\n\n```questdb-sql\nSELECT item, sum(quantity) FROM transactions;\n```\n\n\n| item | sum |\n| --- | --- |\n| apple | 53 |\n| orange | 47 |\n\n\n#### Overflow‚Äã\n\nsum\ndoes not perform overflow check. To avoid overflow, you can cast the\nargument to wider type.\nCast as long to avoid overflow\n\n```questdb-sql\nSELECT sum(cast(a AS LONG)) FROM my_table;\n```\n\n\n#### See also‚Äã\n\n- ksum- Kahan compensated sum for floating-point precision\n- nsum- Neumaier sum for floating-point precision\n- avg- Arithmetic mean\n\n## variance / var_samp‚Äã\n\nvar_samp(value)\n- Calculates the sample variance of a set of values. The\nsample variance is a measure of the amount of variation or dispersion of a set\nof values in a sample from a population. A low variance indicates that the\nvalues tend to be very close to the mean, while a high variance indicates that\nthe values are spread out over a wider range.\nvariance()\nis an alias for\nvar_samp\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT var_samp(x)FROM (SELECT x FROM long_sequence(100));\n```\n\n\n| var_samp |\n| --- |\n| 841.666666666666 |\n\n\n## var_pop‚Äã\n\nvar_pop(value)\n- Calculates the population variance of a set of values. The\npopulation variance is a measure of the amount of variation or dispersion of a\nset of values. A low variance indicates that the values tend to be very close to\nthe mean, while a high variance indicates that the values are spread out over a\nwider range.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\n\n```questdb-sql\nSELECT var_pop(x)FROM (SELECT x FROM long_sequence(100));\n```\n\n\n| var_pop |\n| --- |\n| 833.25 |\n\n\n## weighted_avg‚Äã\n\nweighted_avg(value, weight)\n- Calculates the weighted mean (average) of a set\nof observations (database rows). It calculates the equivalent of:\nxÀâw=‚àëwixi‚àëwi\\bar{x}_w = \\frac{\\sum w_i x_i}{\\sum w_i}xÀâw‚Äã=‚àëwi‚Äã‚àëwi‚Äãxi‚Äã‚Äã\nWhere:\n- xix_ixi‚Äãare the values\n- wiw_iwi‚Äãare the weights\nIf the value is\nNULL\n, that observation is ignored.\nIf the weight is\nNULL\nor zero, that observation is ignored.\nIf there are no observations, the result is\nNULL\n.\nIf the weights sum to zero, the result is\nNULL\n.\nWeights should be non-negative to make sense, but this isn't enforced.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n- weightis any numeric value.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nWeighted average of transaction prices\n\n```questdb-sql\nSELECT weighted_avg(price, quantity) FROM transactions;\n```\n\n\n| weighted_avg |\n| --- |\n| 25.3 |\n\n\n## weighted_stddev‚Äã\n\nweighted_stddev(value, weight)\n- Calculates the unbiased weighted standard\ndeviation of a set of observations using reliability weights.\nThis is an alias for\nweighted_stddev_rel\n.\n\n## weighted_stddev_freq‚Äã\n\nweighted_stddev_freq(value, weight)\n- Calculates the unbiased weighted\nstandard deviation of a set of observations using frequency weights.\nA\nfrequency weight\nrepresents the number of occurrences of each observation\nin the dataset. This variant uses the frequency-weighted estimator for the\npopulation variance. It calculates the equivalent of:\n‚àëwixi2‚àí(‚àëwixi)2‚àëwi‚àëwi‚àí1\\sqrt{\n  \\frac{\n    \\sum w_i x_i^2 - \\frac{(\\sum w_i x_i)^2}{\\sum w_i}\n  }{\n    \\sum w_i - 1\n  }\n}‚àëwi‚Äã‚àí1‚àëwi‚Äãxi2‚Äã‚àí‚àëwi‚Äã(‚àëwi‚Äãxi‚Äã)2‚Äã‚Äã‚Äã\nWhere:\n- xix_ixi‚Äãare the values\n- wiw_iwi‚Äãare the frequency weights\nIf the value is\nNULL\n, that observation is ignored.\nIf the weight is\nNULL\nor zero, that observation is ignored.\nIf there are fewer than two observations, the result is\nNULL\n.\nWeights should be positive integers to make sense, but this isn't enforced.\nWeights must not be normalized. If they sum to one, the result is\nNULL\n.\nIf the sum of weights is negative, the result is\nNULL\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n- weightis any numeric value representing the frequency weight (typically an\ninteger).\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nWeighted standard deviation of binned prices\n\n```questdb-sql\nSELECT weighted_stddev_freq(price_bucket, trade_count) FROM price_histogram;\n```\n\n\n| weighted_stddev_freq |\n| --- |\n| 3.42 |\n\nWeighted standard deviation of bucketed trade data by symbol\n\n```questdb-sql\nSELECT symbol, weighted_stddev_freq(price_bucket, trade_count)FROM trade_histogramGROUP BY symbol;\n```\n\n\n| symbol | weighted_stddev_freq |\n| --- | --- |\n| BTC-USD | 115.67 |\n| ETH-USD | 22.18 |\n\n\n## weighted_stddev_rel‚Äã\n\nweighted_stddev_rel(value, weight)\n- Calculates the unbiased weighted standard\ndeviation of a set of observations using reliability weights. You can also use\nthe shorthand name\nweighted_stddev\n.\nA\nreliability weight\nrepresents the \"importance\" or \"trustworthiness\" of\neach observation. This variant uses the reliability-weighted estimator for the\npopulation variance. It calculates the equivalent of:\n‚àëwixi2‚àí(‚àëwixi)2‚àëwi‚àëwi‚àí‚àëwi2‚àëwi\\sqrt{\n  \\frac{\n    \\sum w_i x_i^2 - \\frac{(\\sum w_i x_i)^2}{\\sum w_i}\n  }{\n    \\sum w_i - \\frac{\\sum w_i^2}{\\sum w_i}\n  }\n}‚àëwi‚Äã‚àí‚àëwi‚Äã‚àëwi2‚Äã‚Äã‚àëwi‚Äãxi2‚Äã‚àí‚àëwi‚Äã(‚àëwi‚Äãxi‚Äã)2‚Äã‚Äã‚Äã\nWhere:\n- xix_ixi‚Äãare the values\n- wiw_iwi‚Äãare the reliability weights\nIf the value is\nNULL\n, that observation is ignored.\nIf the weight is\nNULL\nor zero, that observation is ignored.\nIf there are fewer than two observations, the result is\nNULL\n.\nWeights should be positive to make sense, but this isn't enforced.\nIf the sum of weights is not positive, the result is\nNULL\n.\n\n#### Parameters‚Äã\n\n- valueis any numeric value.\n- weightis any numeric value representing the reliability weight.\n\n#### Return value‚Äã\n\nReturn value type is\ndouble\n.\n\n#### Examples‚Äã\n\nWeighted standard deviation of prices by trade volume\n\n```questdb-sql\nSELECT weighted_stddev(price, volume) FROM trades;\n```\n\n\n| weighted_stddev |\n| --- |\n| 2.45 |\n\nWeighted standard deviation grouped by symbol\n\n```questdb-sql\nSELECT symbol, weighted_stddev(price, volume)FROM tradesGROUP BY symbol;\n```\n\n\n| symbol | weighted_stddev |\n| --- | --- |\n| BTC-USD | 125.34 |\n| ETH-USD | 18.92 |\n\n\n## See also‚Äã\n\n- GROUP BY- Group rows for aggregation\n- SAMPLE BY- Time-series aggregation\n- PIVOT- Transform aggregation results from rows to columns",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 7184,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-22cfee456b14",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/sample-by",
    "title": "SAMPLE BY keyword | QuestDB",
    "text": "SAMPLE BY\nis used on\ntime-series data\nto summarize large datasets into\naggregates of homogeneous time chunks as part of a\nSELECT statement\n.\nTo use\nSAMPLE BY\n, a table column needs to be specified as a\ndesignated timestamp\n.\nUsers performing\nSAMPLE BY\nqueries on datasets\nwith missing data\nmay make\nuse of the\nFILL\nkeyword to specify a fill behavior.\n\n## Syntax‚Äã\n\n\n### SAMPLE BY keywords‚Äã\n\n\n### FROM-TO keywords‚Äã\n\n\n### FILL keywords‚Äã\n\n\n### ALIGN TO keywords‚Äã\n\n\n## Sample units‚Äã\n\nThe size of sampled groups are specified with the following syntax:\n\n```questdb-sql\nSAMPLE BY n{units}\n```\n\nWhere the unit for sampled groups may be one of the following:\n\n| unit | description |\n| --- | --- |\n| U | microsecond |\n| T | millisecond |\n| s | second |\n| m | minute |\n| h | hour |\n| d | day |\n| w | week |\n| M | month |\n| y | year |\n\nFor example, given a table\ntrades\n, the following query returns the number of\ntrades per hour:\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1h;\n```\n\n\n## FROM-TO‚Äã\n\nnote\nVersions prior to QuestDB 8.1.0 do not have access to this extension.\nPlease see the new blog for more information.\nWhen using\nSAMPLE BY\nwith\nFILL\n, you can fill missing rows within the result set with pre-determined values.\nHowever, this method will only fill rows between existing data in the data set and cannot fill rows outside of this range.\nTo fill outside the bounds of the existing data, you can specify a fill range using a\nFROM-TO\nclause. The boundary\ntimestamps are expected in UTC.\nNote that\nFROM-TO\nclause can be used only on non-keyed SAMPLE BY queries, i.e. queries that have no grouping columns\nother than the timestamp.\n\n#### Syntax‚Äã\n\nSpecify the shape of the query using\nFROM\nand\nTO\n:\nPre-filling trade data\nDemo this query\n\n```questdb-sql\nSELECT timestamp as ts, count()FROM tradesSAMPLE BY 1d FROM '2009-01-01' TO '2009-01-10' FILL(NULL);\n```\n\nIf no rows exist at the start of the range, QuestDB automatically fills in these rows.\nThis is distinct from the\nWHERE\nclause with a simple rule of thumb -\nWHERE\ncontrols what data flows in,\nFROM-TO\ncontrols what data flows out.\nUse both\nFROM\nand\nTO\nin isolation to pre-fill or post-fill data. If\nFROM\nis not provided, then the lower bound is the start of the dataset, aligned to calendar. The opposite is true omitting\nTO\n.\n\n#### WHEREclause optimisation‚Äã\n\nIf the user does not provide a\nWHERE\nclause, or the\nWHERE\nclause does not consider the designated timestamp,\nQuestDB will add one for you, matching the\nFROM-TO\ninterval.\nThis means that the query will run optimally, and avoid touching data not relevant to the result.\nTherefore, we compile the prior query into something similar to this:\nPre-filling trade data with WHERE optimisation\nDemo this query\n\n```questdb-sql\nSELECT timestamp as ts, count()FROM tradesWHERE timestamp >= '2009-01-01'  AND timestamp <  '2009-01-10'SAMPLE BY 1d FROM '2009-01-01' TO '2009-01-10' FILL(NULL);\n```\n\n\n#### Limitations‚Äã\n\nHere are the current limits to this feature.\n- This syntax is not compatible withFILL(PREV)orFILL(LINEAR).\n- This syntax is forALIGN TO CALENDARonly (default alignment).\n- Does not consider any specifiedOFFSET.\n- This syntax is for non-keyedSAMPLE BYi.e. only designated timestamp and aggregate columns.\n\n## Fill options‚Äã\n\nThe\nFILL\nkeyword is optional and expects one or more\nfillOption\nstrategies\nwhich will be applied to one or more aggregate columns. The following\nrestrictions apply:\n- Keywords denoting fill strategies may not be combined. Only one option fromNONE,NULL,PREV,LINEARand constants may be used.\n- LINEARstrategy is not supported for keyed queries, i.e. queries that\ncontain non-aggregated columns other than the timestamp in the SELECT clause.\n- TheFILLkeyword must precede alignment described in thesample calculation section, i.e.:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h FILL(LINEAR)ALIGN TO ...\n```\n\n\n| fillOption | Description |\n| --- | --- |\n| NONE | No fill applied. If there is no data, the time sample will be skipped in the results. A table could be missing intervals. |\n| NULL | Fills withNULLvalues. |\n| PREV | Fills using the previous value. |\n| LINEAR | Fills by linear interpolation of the 2 surrounding points. |\n| x | Fills with a constant value - wherexis the desired value, for exampleFILL(100.05). |\n\nConsider an example table named\nprices\nwhich has no records during the entire\nthird hour (\n2021-01-01T03\n):\n\n| ts | price |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | p1 |\n| 2021-01-01T02:00:00.000000Z | p2 |\n| 2021-01-01T04:00:00.000000Z | p4 |\n| 2021-01-01T05:00:00.000000Z | p5 |\n\nThe following query returns the maximum price per hour. As there are missing\nvalues, an aggregate cannot be calculated:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h;\n```\n\nA row is missing for the\n2021-01-01T03:00:00.000000Z\nsample:\n\n| ts | max |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\n\nA\nFILL\nstrategy can be employed which fills with the previous value using\nPREV\n:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h FILL(PREV);\n```\n\n\n| ts | max |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T03:00:00.000000Z | max2 |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\n\nLinear interpolation is done using the\nLINEAR\nfill option:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h FILL(LINEAR);\n```\n\n\n| ts | max |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T03:00:00.000000Z | (max2+max4)/2 |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\n\nA constant value can be used as a\nfillOption\n:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h FILL(100.5);\n```\n\n\n| ts | max |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T03:00:00.000000Z | 100.5 |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\n\nFinally,\nNULL\nmay be used as a\nfillOption\n:\n\n```questdb-sql\nSELECT ts, max(price) maxFROM pricesSAMPLE BY 1h FILL(NULL);\n```\n\n\n| ts | max |\n| --- | --- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T03:00:00.000000Z | null |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\n\n\n### Multiple fill values‚Äã\n\nFILL()\naccepts a list of values where each value corresponds to a single\naggregate column in the SELECT clause order:\n\n```questdb-sql\nSELECT min(price), max(price), avg(price), tsFROM pricesSAMPLE BY 1hFILL(NULL, 10, PREV);\n```\n\nIn the above query\nmin(price)\naggregate will get\nFILL(NULL)\nstrategy\napplied,\nmax(price)\nwill get\nFILL(10)\n, and\navg(price)\nwill get\nFILL(PREV)\n.\n\n## Sample calculation‚Äã\n\nThe default time calculation of sampled groups is an absolute value, in other\nwords, sampling by one day is a 24 hour range which is not bound to calendar\ndates. To align sampled groups to calendar dates, the\nALIGN TO\nkeywords can be\nused and are described in the\nALIGN TO CALENDAR\nsection\nbelow.\nnote\nSince QuestDB v7.4.0, the default behaviour for\nALIGN TO\nhas changed. If you do not specify\nan explicit alignment,\nSAMPLE BY\nexpressions will use\nALIGN TO CALENDAR\nbehaviour.\nThe prior default behaviour can be retained by specifying\nALIGN TO FIRST OBSERVATION\non a\nSAMPLE BY\nquery.\nAlternatively, one can set the\ncairo.sql.sampleby.default.alignment.calendar\noption to\nfalse\nin\nserver.conf\n.\n\n## ALIGN TO FIRST OBSERVATION‚Äã\n\nConsider a table\ntrades\nwith the following data spanning three calendar days:\n\n```questdb-sql\nCREATE TABLE trades (  ts TIMESTAMP,  price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY WAL;INSERT INTO trades (ts, price) VALUES  ('2021-05-31T23:10:00.000000Z', 100.5),  ('2021-06-01T01:10:00.000000Z', 101.2),  ('2021-06-01T07:20:00.000000Z', 100.8),  ('2021-06-01T13:20:00.000000Z', 101.0),  ('2021-06-01T19:20:00.000000Z', 102.1),  ('2021-06-02T01:10:00.000000Z', 101.5),  ('2021-06-02T07:20:00.000000Z', 100.9);\n```\n\nThe following query can be used to sample the table by day.\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1dALIGN TO FIRST OBSERVATION;\n```\n\nThis query will return two rows:\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T23:10:00.000000Z | 5 |\n| 2021-06-01T23:10:00.000000Z | 2 |\n\nThe timestamp value for the 24 hour groups start at the first-observed\ntimestamp, and continue in\n1d\nintervals.\n\n## ALIGN TO CALENDAR‚Äã\n\nThe default behaviour for SAMPLE BY, this option aligns data to calendar dates, with two optional parameters:\n- TIME ZONE\n- WITH OFFSET\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1d;\n```\n\nor:\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1dALIGN TO CALENDAR;\n```\n\nGives the following result:\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T00:00:00.000000Z | 1 |\n| 2021-06-01T00:00:00.000000Z | 4 |\n| 2021-06-02T00:00:00.000000Z | 2 |\n\nIn this case, the timestamps are floored to the nearest UTC day, and grouped. The counts correspond\nto the number of entries occurring within each UTC day.\nThis is particularly useful for summarising data for charting purposes; see the\ncandlestick chart\nfrom the example\ncrypto dashboard\n.\n\n### TIME ZONE‚Äã\n\nA time zone may be provided for sampling with calendar alignment. Details on the\noptions for specifying time zones with available formats are provided in the\nguide for\nworking with timestamps and time zones\n.\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1dALIGN TO CALENDAR TIME ZONE 'Europe/Berlin';\n```\n\nIn this case, the 24 hour samples begin at\n2021-05-31T22:00:00.000000Z\n:\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T22:00:00.000000Z | 5 |\n| 2021-06-01T22:00:00.000000Z | 2 |\n\nAdditionally, an offset may be applied when aligning sample calculation to\ncalendar\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1dALIGN TO CALENDAR TIME ZONE 'Europe/Berlin' WITH OFFSET '00:45';\n```\n\nIn this case, the 24 hour samples begin at\n2021-05-31T22:45:00.000000Z\n:\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T22:45:00.000000Z | 5 |\n| 2021-06-01T22:45:00.000000Z | 1 |\n\n\n#### Local timezone output‚Äã\n\nThe timestamp values output from\nSAMPLE BY\nqueries is in UTC. To have UTC\nvalues converted to specific timezones, the\nto_timezone() function\nshould\nbe used.\n\n```questdb-sql\nSELECT to_timezone(ts, 'PST') ts, countFROM (  SELECT ts, count()  FROM trades  SAMPLE BY 2h  ALIGN TO CALENDAR TIME ZONE 'PST');\n```\n\n\n#### Time zone transitions‚Äã\n\nCalendar dates may contain historical time zone transitions or may vary in the\ntotal number of hours due to daylight savings time. Considering the 31st October\n2021, in the\nEurope/London\ncalendar day which consists of 25 hours:\n- Sunday, 31 October 2021, 02:00:00 clocks are turned backward 1 hour to\n- Sunday, 31 October 2021, 01:00:00 local standard time\nWhen a\nSAMPLE BY\noperation crosses time zone transitions in cases such as\nthis, the first sampled group which spans a transition will include aggregates\nby full calendar range. Consider a table\ntrades\nwith one trade per hour\nspanning five calendar hours:\n\n| ts | price |\n| --- | --- |\n| 2021-10-31T00:10:00.000000Z | 100.5 |\n| 2021-10-31T01:10:00.000000Z | 101.2 |\n| 2021-10-31T02:10:00.000000Z | 100.8 |\n| 2021-10-31T03:10:00.000000Z | 101.5 |\n| 2021-10-31T04:10:00.000000Z | 102.0 |\n\nThe following query will sample by hour with the\nEurope/London\ntime zone and\nalign to calendar ranges:\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1hALIGN TO CALENDAR TIME ZONE 'Europe/London';\n```\n\nThe record count for the hour which encounters a time zone transition will\ncontain two records for both hours at the time zone transition:\n\n| ts | count |\n| --- | --- |\n| 2021-10-31T00:00:00.000000Z | 2 |\n| 2021-10-31T01:00:00.000000Z | 1 |\n| 2021-10-31T02:00:00.000000Z | 1 |\n| 2021-10-31T03:00:00.000000Z | 1 |\n\nSimilarly, given one data point per hour on this table, running\nSAMPLE BY 1d\nwill have a count of\n25\nfor this day when aligned to calendar time zone\nEurope/London\n.\n\n### WITH OFFSET‚Äã\n\nAligning sampling calculation can be provided an arbitrary offset in the format\n'+/-HH:mm'\n, for example:\n- '00:30'plus thirty minutes\n- '+00:30'plus thirty minutes\n- '-00:15'minus 15 minutes\nThe query uses the default offset '00:00' if the parameter is not set.\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1dALIGN TO CALENDAR WITH OFFSET '02:00';\n```\n\nIn this case, the 24 hour samples begin at\n2021-05-31T02:00:00.000000Z\n:\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T02:00:00.000000Z | 2 |\n| 2021-06-01T02:00:00.000000Z | 4 |\n| 2021-06-02T02:00:00.000000Z | 1 |\n\n\n### TIME ZONE WITH OFFSET‚Äã\n\nThe\nTIME ZONE\nand\nWITH OFFSET\noptions can be combined.\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1hALIGN TO CALENDAR TIME ZONE 'Europe/London' WITH OFFSET '02:00';\n```\n\nThe sample then begins from\nEurope/London\nat\n2021-10-31T02:00:00.000000Z\n:\n\n| ts | count |\n| --- | --- |\n| 2021-10-31T02:00:00.000000Z | 1 |\n| 2021-10-31T03:00:00.000000Z | 1 |\n| 2021-10-31T04:00:00.000000Z | 3 |\n| 2021-10-31T05:00:00.000000Z | 2 |\n\n\n## Examples‚Äã\n\nAssume the following table\ntrades\n:\n\n| ts | quantity | price |\n| --- | --- | --- |\n| 2021-05-31T23:45:10.000000Z | 10 | 100.05 |\n| 2021-06-01T00:01:33.000000Z | 5 | 100.05 |\n| 2021-06-01T00:15:14.000000Z | 200 | 100.15 |\n| 2021-06-01T00:30:40.000000Z | 300 | 100.15 |\n| 2021-06-01T00:45:20.000000Z | 10 | 100 |\n| 2021-06-01T01:00:50.000000Z | 50 | 100.15 |\n\nThis query will return the number of trades per hour:\nHourly interval\n\n```questdb-sql\nSELECT ts, count()FROM tradesSAMPLE BY 1h;\n```\n\n\n| ts | count |\n| --- | --- |\n| 2021-05-31T23:45:10.000000Z | 3 |\n| 2021-06-01T00:45:10.000000Z | 1 |\n| 2021-05-31T23:45:10.000000Z | 1 |\n| 2021-06-01T00:45:10.000000Z | 1 |\n\nThe following will return the trade volume in 30 minute intervals\n30 minute interval\n\n```questdb-sql\nSELECT ts, sum(quantity*price)FROM tradesSAMPLE BY 30m;\n```\n\n\n| ts | sum |\n| --- | --- |\n| 2021-05-31T23:45:10.000000Z | 1000.5 |\n| 2021-06-01T00:15:10.000000Z | 16024 |\n| 2021-06-01T00:45:10.000000Z | 8000 |\n| 2021-06-01T00:15:10.000000Z | 8012 |\n| 2021-06-01T00:45:10.000000Z | 8000 |\n\nThe following will return the average trade notional (where notional is = q *\np) by day:\nDaily interval\n\n```questdb-sql\nSELECT ts, avg(quantity*price)FROM tradesSAMPLE BY 1d;\n```\n\n\n| ts | avg |\n| --- | --- |\n| 2021-05-31T23:45:10.000000Z | 6839.416666666667 |\n\nTo make this sample align to calendar dates:\nCalendar alignment\n\n```questdb-sql\nSELECT ts, avg(quantity*price)FROM tradesSAMPLE BY 1dALIGN TO CALENDAR;\n```\n\n\n| ts | avg |\n| --- | --- |\n| 2021-05-31T00:00:00.000000Z | 1000.5 |\n| 2021-06-01T00:00:00.000000Z | 8007.2 |\n\n\n## Performance optimization‚Äã\n\nFor frequently executed\nSAMPLE BY\nqueries, consider using\nmaterialized views\nto pre-compute aggregates. This can significantly improve query performance, especially for complex sampling operations on large datasets.\n\n```questdb-sql\nCREATE MATERIALIZED VIEW hourly_metrics ASSELECT     timestamp_floor('h', ts) as hour,    symbol,    avg(price) as avg_price,    sum(volume) as total_volumeFROM tradesSAMPLE BY 1h;\n```\n\n\n## See also‚Äã\n\nThis section includes links to additional information such as tutorials:\n- PIVOT- Transform SAMPLE BY results from rows to columns\n- Materialized Views- Pre-compute SAMPLE BY queries for better performance\n- SQL Extensions for Time-Series Data in QuestDB\n- Three SQL Keywords for Finding Missing Data",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2439,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-dafb47dac13b",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/message-brokers/kafka",
    "title": "Ingestion from Kafka Overview | QuestDB",
    "text": "QuestDB provides a first-party Kafka Connect connector for streaming data from\nApache Kafka into QuestDB tables. The connector handles serialization, fault\ntolerance, and batching automatically, making it the recommended approach for\nmost use cases.\n\n## Choosing an integration strategy‚Äã\n\nThere are three ways to get data from Kafka into QuestDB:\n\n| Strategy | Recommended for | Complexity |\n| --- | --- | --- |\n| QuestDB Kafka connector | Most users | Low |\n| Stream processing (Flink) | Complex transformations | Medium |\n| Custom program | Special requirements | High |\n\nFor most users, the QuestDB Kafka connector is the best choice.\nIt provides\nexcellent performance (100,000+ rows/second), handles fault tolerance\nautomatically, and requires minimal configuration.\n\n## QuestDB Kafka connector‚Äã\n\nThe\nQuestDB Kafka connector\nis built on the\nKafka Connect framework\nand uses InfluxDB Line Protocol for high-performance data transfer. It works\nwith Kafka-compatible systems like\nRedpanda\n.\n\n### Quick start‚Äã\n\nThis guide walks through setting up the connector to read JSON data from Kafka\nand write it to QuestDB.\n\n#### Prerequisites‚Äã\n\n- Apache Kafka (or compatible system)\n- QuestDB instance with HTTP endpoint accessible\n- Java 17+ (JDK)\n\n#### Step 1: Install the connector‚Äã\n\nDownload and install the connector JAR files:\n\n```shell\ncurl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest |jq -r '.assets[]|select(.content_type == \"application/zip\")|.browser_download_url'|wget -qi -\n```\n\nExtract and copy to your Kafka installation:\n\n```shell\nunzip kafka-questdb-connector-*-bin.zipcd kafka-questdb-connectorcp ./*.jar /path/to/kafka_*.*-*.*.*/libs\n```\n\ninfo\nThe connector is also available from\nConfluent Hub\n.\nFor Confluent platform users, see the\nConfluent Docker images sample\n.\n\n#### Step 2: Configure the connector‚Äã\n\nCreate a configuration file at\n/path/to/kafka/config/questdb-connector.properties\n:\nquestdb-connector.properties\n\n```shell\nname=questdb-sinkconnector.class=io.questdb.kafka.QuestDBSinkConnector# QuestDB connectionclient.conf.string=http::addr=localhost:9000;# Kafka sourcetopics=example-topic# Target table (optional - defaults to topic name)table=example_table# Message formatkey.converter=org.apache.kafka.connect.storage.StringConvertervalue.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=falseinclude.key=false\n```\n\n\n#### Step 3: Start the services‚Äã\n\nRun these commands from your Kafka installation directory (single-node KRaft):\n\n```shell\n# Generate a unique cluster IDKAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"# Format storage directories (run once)bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties# Start Kafkabin/kafka-server-start.sh config/server.properties# Start the connector (from another terminal)bin/connect-standalone.sh config/connect-standalone.properties config/questdb-connector.properties\n```\n\n\n#### Step 4: Test the pipeline‚Äã\n\nPublish a test message:\n\n```shell\nbin/kafka-console-producer.sh --topic example-topic --bootstrap-server localhost:9092\n```\n\nEnter this JSON (as a single line):\n\n```json\n{\"symbol\": \"AAPL\", \"price\": 192.34, \"volume\": 1200}\n```\n\nVerify the data in QuestDB:\n\n```shell\ncurl -G --data-urlencode \"query=select * from 'example_table'\" http://localhost:9000/exp\n```\n\nExpected output:\n\n```csv\n\"symbol\",\"price\",\"volume\",\"timestamp\"\"AAPL\",192.34,1200,\"2026-02-03T15:10:00.000000Z\"\n```\n\nThe timestamp is assigned by QuestDB on ingestion, so the value you see will match your local ingest time.\n\n### Configuration reference‚Äã\n\nThe connector configuration has two parts:\n- Client configuration string: How the connector connects to QuestDB\n- Connector options: How the connector processes Kafka messages\n\n#### Connector options‚Äã\n\n\n| Name | Type | Example | Default | Description |\n| --- | --- | --- | --- | --- |\n| client.conf.string | string | http::addr=localhost:9000; | N/A | Client configuration string |\n| topics | string | orders,audit | N/A | Kafka topics to read from |\n| table | string | my_table | Topic name | Target table in QuestDB |\n| key.converter | string | org.apache.kafka.connect.storage.StringConverter | N/A | Converter for Kafka keys |\n| value.converter | string | org.apache.kafka.connect.json.JsonConverter | N/A | Converter for Kafka values |\n| include.key | boolean | false | true | Include message key in target table |\n| key.prefix | string | from_key | key | Prefix for key fields |\n| value.prefix | string | from_value | N/A | Prefix for value fields |\n| symbols | string | instrument,stock | N/A | Columns to create assymboltype |\n| doubles | string | volume,price | N/A | Columns to always send as double type |\n| timestamp.field.name | string | pickup_time | N/A | Designated timestamp field. Use comma-separated names forcomposed timestamps |\n| timestamp.units | string | micros | auto | Timestamp field units:nanos,micros,millis,seconds,auto |\n| timestamp.kafka.native | boolean | true | false | Use Kafka message timestamps as designated timestamps |\n| timestamp.string.fields | string | creation_time | N/A | String fields containing textual timestamps |\n| timestamp.string.format | string | yyyy-MM-dd HH:mm:ss.SSSUUU z | yyyy-MM-ddTHH:mm:ss.SSSUUUZ | Format for parsing string timestamps |\n| skip.unsupported.types | boolean | false | false | Skip unsupported types instead of failing |\n| allowed.lag | int | 250 | 1000 | Milliseconds to wait before flushing when no new events |\n\nThe connector uses Kafka Connect converters for deserialization and works with\nany format they support, including JSON, Avro, and Protobuf. When using Schema\nRegistry, configure the appropriate converter (e.g.,\nio.confluent.connect.avro.AvroConverter\n).\n\n#### Client configuration string‚Äã\n\nThe\nclient.conf.string\noption configures how the connector communicates with\nQuestDB. You can also set this via the\nQDB_CLIENT_CONF\nenvironment variable.\nFormat:\n\n```questdb-sql\n<protocol>::<key>=<value>;<key>=<value>;...;\n```\n\nNote the trailing semicolon.\nSupported protocols:\nhttp\n,\nhttps\nRequired keys:\n- addr- QuestDB hostname and port (port defaults to 9000)\nExamples:\n\n```properties\n# Minimal configurationclient.conf.string=http::addr=localhost:9000;# With HTTPS and retry timeoutclient.conf.string=https::addr=questdb.example.com:9000;retry_timeout=60000;# With authentication token from environment variableclient.conf.string=http::addr=localhost:9000;token=${QUESTDB_TOKEN};\n```\n\nSee the\nJava Client configuration guide\nfor all\navailable client options.\ndanger\nThe QuestDB client also supports TCP transport, but it is not recommended for\nKafka Connect because the TCP transport offers no delivery guarantees.\n\n##### Environment variable expansion‚Äã\n\nThe\nclient.conf.string\nsupports\n${VAR}\nsyntax for environment variable\nexpansion, useful for injecting secrets in Kubernetes environments:\n\n| Pattern | Result |\n| --- | --- |\n| ${VAR} | Replaced with environment variable value |\n| $$ | Escaped to literal$ |\n| $${VAR} | Escaped to literal${VAR}(not expanded) |\n| $VAR | Not expanded (braces required) |\n\nThe connector fails to start if:\n- A referenced environment variable is not defined\n- A variable reference is malformed (e.g., unclosed braces${VAR)\n- A variable name is empty (${}) or invalid (must start with letter or\nunderscore, followed by letters, digits, or underscores)\nwarning\nEnvironment variable values containing semicolons (\n;\n) will break the\nconfiguration string parsing.\n\n### How data is mapped‚Äã\n\nThe connector converts each Kafka message field to a QuestDB column. Nested\nstructures and maps are flattened with underscores.\nExample input:\n\n```json\n{  \"firstname\": \"John\",  \"lastname\": \"Doe\",  \"age\": 30,  \"address\": {    \"street\": \"Main Street\",    \"city\": \"New York\"  }}\n```\n\nResulting table:\n\n| firstname | lastname | age | address_street | address_city |\n| --- | --- | --- | --- | --- |\n| John | Doe | 30 | Main Street | New York |\n\n\n### Designated timestamps‚Äã\n\nThe connector supports four strategies for\ndesignated timestamps\n:\n\n| Strategy | Configuration | Use case |\n| --- | --- | --- |\n| Server-assigned | (default) | QuestDB assigns timestamp on receipt |\n| Message payload | timestamp.field.name=fieldname | Use a field from the message |\n| Kafka metadata | timestamp.kafka.native=true | Use Kafka's message timestamp |\n| Composed | timestamp.field.name=date,time | Combine multiple fields |\n\nThese strategies are mutually exclusive.\n\n#### Using a message field‚Äã\n\nIf your message contains a timestamp field:\n\n```properties\ntimestamp.field.name=event_timetimestamp.units=millis  # or: nanos, micros, seconds, auto\n```\n\nThe connector auto-detects units for timestamps after April 26, 1970.\n\n#### Using Kafka timestamps‚Äã\n\nTo use Kafka's built-in message timestamp:\n\n```properties\ntimestamp.kafka.native=true\n```\n\n\n#### Parsing string timestamps‚Äã\n\nFor timestamps stored as strings:\n\n```properties\ntimestamp.field.name=created_attimestamp.string.fields=updated_at,deleted_attimestamp.string.format=yyyy-MM-dd HH:mm:ss.SSSUUU z\n```\n\nThe\ntimestamp.field.name\nfield becomes the designated timestamp. Fields in\ntimestamp.string.fields\nare parsed as regular timestamp columns.\nSee\nQuestDB timestamp format\nfor format patterns.\n\n#### Composed timestamps‚Äã\n\nSome data sources split timestamps across multiple fields (common with KDB-style data):\n\n```json\n{  \"symbol\": \"BTC-USD\",  \"date\": \"20260202\",  \"time\": \"135010207\"}\n```\n\nConfigure the connector to concatenate and parse them:\n\n```properties\ntimestamp.field.name=date,timetimestamp.string.format=yyyyMMddHHmmssSSS\n```\n\nThe fields\ndate\nand\ntime\nare concatenated into\n20260202135010207\n, parsed\nto produce\n2026-02-02T13:50:10.207000Z\n. The source fields are consumed and do\nnot appear as columns in the output.\nAll listed fields must be present in each message.\n\n### Fault tolerance‚Äã\n\nThe connector automatically retries recoverable errors (network issues, server\nunavailability, timeouts). Non-recoverable errors (invalid data, authentication\nfailures) are not retried.\nConfigure retry behavior via the client configuration:\n\n```properties\n# Retry for up to 60 secondsclient.conf.string=http::addr=localhost:9000;retry_timeout=60000;\n```\n\nDefault retry timeout is 10,000 ms.\n\n#### Exactly-once delivery‚Äã\n\nRetries may cause duplicate rows. To ensure exactly-once delivery, enable\ndeduplication\non your target table.\nDeduplication requires a designated timestamp from the message payload or Kafka\nmetadata.\n\n#### Dead letter queue‚Äã\n\nFor messages that fail due to non-recoverable errors (invalid data, schema\nmismatches), configure a Dead Letter Queue to prevent the connector from\nstopping. These settings must be configured in the\nKafka Connect worker\nconfiguration\n(e.g.,\nconnect-standalone.properties\nor\nconnect-distributed.properties\n), not in the connector configuration:\n\n```properties\nerrors.tolerance=allerrors.deadletterqueue.topic.name=dlq-questdberrors.deadletterqueue.topic.replication.factor=1\n```\n\nFailed messages are sent to the DLQ topic for later inspection.\nSee the\nConfluent DLQ documentation\nfor details.\n\n### Performance tuning‚Äã\n\n\n#### Batch size‚Äã\n\nThe connector batches messages before sending. Default batch size is 75,000 rows.\nFor low-throughput scenarios, reduce this to lower latency:\n\n```properties\nclient.conf.string=http::addr=localhost:9000;auto_flush_rows=1000;\n```\n\n\n#### Flush interval‚Äã\n\nThe connector flushes data when:\n- Batch size is reached\n- No new events forallowed.lagmilliseconds (default: 1000)\n- Kafka Connect commits offsets\n\n```properties\n# Flush after 250ms of no new eventsallowed.lag=250\n```\n\nConfigure offset commit frequency in Kafka Connect via\noffset.flush.interval.ms\n.\nSee\nKafka Connect configuration\n.\n\n### Type handling‚Äã\n\n\n#### Symbol columns‚Äã\n\nUse the\nsymbols\noption to create columns as\nsymbol\ntype for better performance on\nrepeated string values:\n\n```properties\nsymbols=instrument,exchange,currency\n```\n\n\n#### Numeric type inference‚Äã\n\nWithout a schema, the connector infers types from values. This can cause issues\nwhen a field is sometimes an integer and sometimes a float:\n\n```json\n{\"volume\": 42}      // Inferred as long{\"volume\": 42.5}    // Error: column is long, value is double\n```\n\nSolutions:\n- Use thedoublesoption to force double type:doubles=volume,price\n- Pre-create the table with explicit column types usingCREATE TABLE\n\n### Target table options‚Äã\n\n\n#### Table naming‚Äã\n\nBy default, the table name matches the Kafka topic name. Override with:\n\n```properties\ntable=my_custom_table\n```\n\nThe\ntable\noption supports templating:\n\n```properties\ntable=kafka_${topic}_${partition}\n```\n\nAvailable variables:\n${topic}\n,\n${key}\n,\n${partition}\nIf\n${key}\nis used and the message has no key, it resolves to\nnull\n.\n\n#### Schema management‚Äã\n\nTables are created automatically when they don't exist. This is convenient for\ndevelopment but in production, pre-create tables using\nCREATE TABLE\nfor control over partitioning,\nindexes, and column types.\n\n### Transformations‚Äã\n\n\n#### OrderBookToArray‚Äã\n\nThe connector includes an\nOrderBookToArray\nSingle Message Transform (SMT)\nfor converting arrays of structs into arrays of arrays. This is useful for\norder book data or tabular data stored as rows that needs to be pivoted into\ncolumnar form.\nFor querying order book data stored as arrays, see\nOrder book analytics using arrays\n.\nInput:\n\n```json\n{  \"symbol\": \"BTC-USD\",  \"buy_entries\": [    { \"price\": 100.5, \"size\": 10.0 },    { \"price\": 99.8, \"size\": 25.0 }  ]}\n```\n\nOutput:\n\n```json\n{  \"symbol\": \"BTC-USD\",  \"bids\": [    [100.5, 99.8],    [10.0, 25.0]  ]}\n```\n\nConfiguration:\n\n```properties\ntransforms=orderbooktransforms.orderbook.type=io.questdb.kafka.OrderBookToArray$Valuetransforms.orderbook.mappings=buy_entries:bids:price,size;sell_entries:asks:price,size\n```\n\nThe\nmappings\nformat is\nsourceField:targetField:field1,field2;...\nBehavior:\n- All extracted values are converted todouble\n- Missing source fields are skipped (no error)\n- Empty source arrays are skipped\n- Null values inside structs cause an error\n- If the target field name already exists in the input, it is replaced\n- Works with both schema-based and schemaless messages\nnote\nQuestDB requires all inner arrays to have the same length. The OrderBookToArray\nSMT satisfies this naturally since each inner array comes from the same source\nentries.\n\n### Sample projects‚Äã\n\nAdditional examples are available on GitHub:\n- Sample projects\n- Debezium CDC integration\n\n## Stream processing‚Äã\n\nStream processing\nengines like\nApache Flink\nprovide rich APIs for data\ntransformation, enrichment, and filtering with built-in fault tolerance.\nQuestDB offers a\nFlink connector\nfor\nusers who need complex transformations while ingesting from Kafka.\nUse stream processing when you need:\n- Complex stateful transformations\n- Joining multiple data streams\n- Windowed aggregations before writing to QuestDB\n\n## Custom program‚Äã\n\nWriting a dedicated program to read from Kafka and write to QuestDB offers\nmaximum flexibility for arbitrary transformations and filtering.\nTrade-offs:\n- Full control over serialization, error handling, and batching\n- Highest implementation complexity\n- Must handle Kafka consumer groups, offset management, and retries\nThis approach is only recommended for advanced use cases where the Kafka\nconnector or stream processing cannot meet your requirements.\n\n## FAQ‚Äã\n\nDoes the connector work with Schema Registry?Yes. The connector relies on Kafka Connect converters for deserialization.\nConfigure converters usingkey.converterandvalue.converteroptions.\nIt works with Avro, JSON Schema, and other formats supported by Schema Registry.\nDoes the connector work with Debezium?Yes. QuestDB works well withDebeziumforchange data capture. Since QuestDB is\nappend-only, updates become new rows preserving history.Use Debezium'sExtractNewRecordStatetransformation to extract the new record\nstate. DELETE events are dropped by default.See theDebezium sample projectand the blog postChange Data Capture with QuestDB and Debezium.Typical pattern:Use a relational database for current state and QuestDB\nfor change history. For example, PostgreSQL holds current stock prices while\nQuestDB stores the complete price history for analytics.\nHow do I select which fields to include?Use Kafka Connect'sReplaceFieldtransformation:{\"transforms\":\"removeFields\",\"transforms.removeFields.type\":\"org.apache.kafka.connect.transforms.ReplaceField$Value\",\"transforms.removeFields.blacklist\":\"address,internal_id\"}SeeReplaceField documentation.\nI'm getting a JsonConverter schema errorIf you see:JsonConverter with schemas.enable requires 'schema' and 'payload' fieldsYour JSON data doesn't include a schema. Add to your configuration:value.converter.schemas.enable=falseOr for keys:key.converter.schemas.enable=false\n\n## See also‚Äã\n\n- Change Data Capture with QuestDB and Debezium\n- Realtime crypto tracker with QuestDB Kafka Connector",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2163,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-942c41364e74",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/join",
    "title": "JOIN keyword | QuestDB",
    "text": "QuestDB supports the type of joins you can frequently find in\nrelational databases\n:\nINNER\n,\nLEFT (OUTER)\n,\nCROSS\n. Additionally, it implements joins which are particularly useful for\ntime-series analytics:\nASOF\n,\nLT\n,\nSPLICE\n, and\nWINDOW\n.\nFULL\njoins are\nnot yet implemented and are on our roadmap.\nAll supported join types can be combined in a single SQL statement; QuestDB\nSQL's optimizer determines the best execution order and algorithms.\nThere are no known limitations on the size of tables or sub-queries used in\njoins and there are no limitations on the number of joins, either.\n\n## Syntax‚Äã\n\nHigh-level overview:\n- selectClause- seeSELECTfor more\ninformation.\n- whereClause- seeWHEREfor more information.\n- The specific syntax forjoinClausedepends on the type ofJOIN:INNERandLEFTJOINhas a mandatoryONclause allowing arbitraryJOINpredicates,operator:ASOF,LT, andSPLICEJOINhas optionalONclause allowing only the=predicate.ASOFandLTjoin additionally allows an optionalTOLERANCEclause:CROSS JOINdoes not allow anyONclause:\nColumns from joined tables are combined in a single row. Columns with the same\nname originating from different tables will be automatically aliased to create a\nunique column namespace of the resulting set.\nThough it is usually preferable to explicitly specify join conditions, QuestDB\nwill analyze\nWHERE\nclauses for implicit join conditions and will derive\ntransient join conditions where necessary.\n\n## Execution order‚Äã\n\nJoin operations are performed in order of their appearance in a SQL query. The\nfollowing query performs a join on a table with a very small table (just one row\nin this example) and a bigger table with 10 million rows:\n\n```questdb-sql\nWITH  Manytrades AS    (SELECT * FROM trades limit 10000000),  Lookup AS    (SELECT  'BTC-USD' AS Symbol, 'Bitcoin/USD Pair' AS Description)SELECT *FROM LookupINNER JOIN ManyTrades  ON Lookup.symbol = Manytrades.symbol;\n```\n\nThe performance of this query can be improved by rewriting the query as follows:\n\n```questdb-sql\nWITH  Manytrades AS    (SELECT * FROM trades limit 10000000),  Lookup AS    (SELECT  'BTC-USD' AS Symbol, 'Bitcoin/USD Pair' AS Description)SELECT *FROM ManyTradesINNER JOIN Lookup  ON Lookup.symbol = Manytrades.symbol;\n```\n\nAs a general rule, whenever you have a table significantly larger than the\nother, try to use the large one first. If you use\nEXPLAIN\nwith the queries\nabove, you should see the first version needs to Hash over 10 million rows,\nwhile the second version needs to Hash only over 1 row.\n\n## Implicit joins‚Äã\n\nIt is possible to join two tables using the following syntax:\n\n```questdb-sql\nSELECT *FROM a, bWHERE a.id = b.id;\n```\n\nThe type of join as well as the column are inferred from the\nWHERE\nclause, and\nmay be either an\nINNER\nor\nCROSS\njoin. For the example above, the equivalent\nexplicit statement would be:\n\n```questdb-sql\nSELECT *FROM aJOIN b ON (id);\n```\n\n\n## Using theONclause for theJOINpredicate‚Äã\n\nWhen tables are joined on a column that has the same name in both tables you can\nuse the\nON (column)\nshorthand.\nWhen the\nON\nclause is permitted (all except\nCROSS JOIN\n), it is possible to\njoin multiple columns.\nFor example, the following two tables contain identical column names\nsymbol\nand\nside\n:\nmayTrades\n:\n\n| symbol | side | total |\n| --- | --- | --- |\n| ADA-BTC | buy | 8079 |\n| ADA-BTC | sell | 7678 |\n| ADA-USD | buy | 308271 |\n| ADA-USD | sell | 279624 |\n\njuneTrades\n:\n\n| symbol | side | total |\n| --- | --- | --- |\n| ADA-BTC | buy | 10253 |\n| ADA-BTC | sell | 17460 |\n| ADA-USD | buy | 312359 |\n| ADA-USD | sell | 245066 |\n\nIt is possible to add multiple JOIN ON condition:\n\n```questdb-sql\nWITH  mayTrades AS (    SELECT symbol, side, COUNT(*) as total    FROM trades    WHERE timestamp in '2024-05'    ORDER BY Symbol    LIMIT 4  ),  juneTrades AS (    SELECT symbol, side, COUNT(*) as total    FROM trades    WHERE timestamp in '2024-06'    ORDER BY Symbol    LIMIT 4  )SELECT *FROM mayTradesJOIN JuneTrades  ON mayTrades.symbol = juneTrades.symbol    AND mayTrades.side = juneTrades.side;\n```\n\nThe query can be simplified further since the column names are identical:\n\n```questdb-sql\nWITH  mayTrades AS (    SELECT symbol, side, COUNT(*) as total    FROM trades    WHERE timestamp in '2024-05'    ORDER BY Symbol    LIMIT 4  ),  juneTrades AS (    SELECT symbol, side, COUNT(*) as total    FROM trades    WHERE timestamp in '2024-06'    ORDER BY Symbol    LIMIT 4  )SELECT *FROM mayTradesJOIN JuneTrades ON (symbol, side);\n```\n\nThe result of both queries is the following:\n\n| symbol | symbol1 | side | side1 | total | total1 |\n| --- | --- | --- | --- | --- | --- |\n| ADA-BTC | ADA-BTC | buy | buy | 8079 | 10253 |\n| ADA-BTC | ADA-BTC | sell | sell | 7678 | 17460 |\n| ADA-USD | ADA-USD | buy | buy | 308271 | 312359 |\n| ADA-USD | ADA-USD | sell | sell | 279624 | 245066 |\n\n\n## ASOF JOIN‚Äã\n\nASOF JOIN is a powerful time-series join extension.\nIt has its own page,\nASOF JOIN\n.\n\n## WINDOW JOIN‚Äã\n\nWINDOW JOIN aggregates data from a related table within a time-based window\naround each row. It is useful for calculating rolling statistics, moving\naverages, or aggregating readings within time windows.\nIt has its own page,\nWINDOW JOIN\n.\n\n## (INNER) JOIN‚Äã\n\n(INNER) JOIN\nreturns rows from two tables where the records on the compared\ncolumn have matching values in both tables.\nJOIN\nis interpreted as\nINNER JOIN\nby default, making the\nINNER\nkeyword implicit.\nThe query we just saw above is an example. It returns the\nsymbol\n,\nside\nand\ntotal\nfrom the\nmayTrades\nsubquery, and adds the\nsymbol\n,\nside\n, and\ntotal\nfrom the\njuneTrades\nsubquery. Both tables are matched based on the\nsymbol\nand\nside\n, as specified on the\nON\ncondition.\n\n## LEFT (OUTER) JOIN‚Äã\n\nLEFT OUTER JOIN\nor simply\nLEFT JOIN\nreturns\nall\nrecords from the left\ntable, and if matched, the records of the right table. When there is no match\nfor the right table, it returns\nNULL\nvalues in right table fields.\nThe general syntax is as follows:\nLEFT JOIN ON\n\n```questdb-sql\nWITH  Manytrades AS    (SELECT * FROM trades limit 100),  Lookup AS    (SELECT  'BTC-USD' AS Symbol, 'Bitcoin/USD Pair' AS Description)SELECT *FROM ManyTradesLEFT OUTER JOIN Lookup  ON Lookup.symbol = Manytrades.symbol;\n```\n\nIn this example, the result will have 100 rows, one for each row on the\nManyTrades\nsubquery. When there is no match with the\nLookup\nsubquery, the\ncolumns\nSymbol1\nand\nDescription\nwill be\nnull\n.\n\n```sql\n-- Omitting 'OUTER' makes no difference:WITH  Manytrades AS    (SELECT * FROM trades limit 100),  Lookup AS    (SELECT  'BTC-USD' AS Symbol, 'Bitcoin/USD Pair' AS Description)SELECT *FROM ManyTradesLEFT JOIN Lookup  ON Lookup.symbol = Manytrades.symbol;\n```\n\nA\nLEFT OUTER JOIN\nquery can also be used to select all rows in the left table\nthat do not exist in the right table.\n\n```questdb-sql\nWITH  Manytrades AS    (SELECT * FROM trades limit 100),  Lookup AS    (SELECT  'BTC-USD' AS Symbol, 'Bitcoin/USD Pair' AS Description)SELECT *FROM ManyTradesLEFT OUTER JOIN Lookup  ON Lookup.symbol = Manytrades.symbolWHERE Lookup.Symbol = NULL;\n```\n\nIn this case, the result has 71 rows out of the 100 in the larger table, and the\ncolumns corresponding to the\nLookup\ntable are all\nNULL\n.\n\n## CROSS JOIN‚Äã\n\nCROSS JOIN\nreturns the Cartesian product of the two tables being joined and\ncan be used to create a table with all possible combinations of columns.\nThe following query is joining a table (a subquery in this case) with itself, to\ncompare row by row if we have any rows with exactly the same values for all the\ncolumns except the timestamp, and if the timestamps are within 10 seconds from\neach other:\n\n```questdb-sql\n-- detect potential duplicates, with same values-- and within a 10 seconds rangeWITH t AS (  SELECT * FROM trades WHERE timestamp IN '2024-06-01')SELECT * from t CROSS JOIN t AS t2WHERE t.timestamp < t2.timestamp  AND datediff('s', t.timestamp , t2.timestamp ) < 10  AND t.symbol = t2.symbol  AND t.side = t2.side  AND t.price = t2.price  AND t.amount = t2.amount;\n```\n\nnote\nCROSS JOIN\ndoes not have an\nON\nclause.\n\n## LT JOIN‚Äã\n\nSimilar to\nASOF JOIN\n,\nLT JOIN\njoins two different time-series measured. For\neach row in the first time-series, the\nLT JOIN\ntakes from the second\ntime-series a timestamp that meets both of the following criteria:\n- The timestamp is the closest to the first timestamp.\n- The timestamp isstrictly prior tothe first timestamp.\nIn other words:\nLT JOIN\nwon't join records with equal timestamps.\n\n### Example‚Äã\n\nConsider the following tables:\nTable\ntradesA\n:\n\n| timestamp | price |\n| --- | --- |\n| 2022-03-08T18:03:57.710419Z | 39269.98 |\n| 2022-03-08T18:03:58.357448Z | 39265.31 |\n| 2022-03-08T18:03:58.357448Z | 39265.31 |\n\nTable\ntradesB\n:\n\n| timestamp | price |\n| --- | --- |\n| 2022-03-08T18:03:57.710419Z | 39269.98 |\n| 2022-03-08T18:03:58.357448Z | 39265.31 |\n| 2022-03-08T18:03:58.357448Z | 39265.31 |\n\nAn\nLT JOIN\ncan be built using the following query:\n\n```questdb-sql\nWITH miniTrades AS (  SELECT timestamp, price  FROM TRADES  WHERE symbol = 'BTC-USD'  LIMIT 3)SELECT tradesA.timestamp, tradesB.timestamp, tradesA.priceFROM miniTrades tradesALT JOIN miniTrades tradesB;\n```\n\nThe query above returns the following results:\n\n| timestamp | timestamp1 | price |\n| --- | --- | --- |\n| 2022-03-08T18:03:57.710419Z | NULL | 39269.98 |\n| 2022-03-08T18:03:58.357448Z | 2022-03-08T18:03:57.710419Z | 39265.31 |\n| 2022-03-08T18:03:58.357448Z | 2022-03-08T18:03:57.710419Z | 39265.31 |\n\nNotice how the first record in the\ntradesA\ntable is not joined with any record\nin the\ntradesB\ntable. This is because there is no record in the\ntradesB\ntable with a timestamp prior to the timestamp of the first record in the\ntradesA\ntable.\nSimilarly, the second record in the\ntradesB\ntable is joined with the first\nrecord in the\ntradesA\ntable because the timestamp of the first record in the\ntradesB\ntable is prior to the timestamp of the second record in the\ntradesA\ntable.\nnote\nAs seen on this example,\nLT\njoin is often useful to join a table to itself in\norder to get preceding values for every row.\nThe\nON\nclause can also be used in combination with\nLT JOIN\nto join both by\ntimestamp and column values.\n\n### TOLERANCE clause‚Äã\n\nThe\nTOLERANCE\nclause enhances LT JOIN by limiting how far back in time the join should look for a match in the right\ntable. The\nTOLERANCE\nparameter accepts a time interval value (e.g., 2s, 100ms, 1d).\nWhen specified, a record from the left table t1 at t1.ts will only be joined with a record from the right table t2 at\nt2.ts if both conditions are met:\nt2.ts < t1.ts\nand\nt1.ts - t2.ts <= tolerance_value\nThis ensures that the matched record from the right table is not only the latest one on or before t1.ts, but also within\nthe specified time window.\nLT JOIN with a TOLERANCE parameter\n\n```questdb-sql\nSELECT ...FROM table1LT JOIN table2 TOLERANCE 10s[WHERE ...]\n```\n\nThe interval_literal must be a valid QuestDB interval string, like '5s' (5 seconds), '100ms' (100 milliseconds),\n'2m' ( 2 minutes), '3h' (3 hours), or '1d' (1 day).\n\n#### Supported Units for interval_literal‚Äã\n\nThe\nTOLERANCE\ninterval literal supports the following time unit qualifiers:\n- n: Nanoseconds\n- U: Microseconds\n- T: Milliseconds\n- s: Seconds\n- m: Minutes\n- h: Hours\n- d: Days\n- w: Weeks\nFor example, '100U' is 100 microseconds, '50T' is 50 milliseconds, '2s' is 2 seconds, '30m' is 30 minutes,\n'1h' is 1 hour, '7d' is 7 days, and '2w' is 2 weeks. Please note that months (M) and years (Y) are not supported as\nunits for the\nTOLERANCE\nclause.\nThe effective precision of the\nTOLERANCE\nclause depends on the\ndesignated timestamp resolution\nof the tables involved. For example, if a table uses microsecond resolution, specifying nanosecond\ntolerance (e.g.,\n500n\n) will not provide nanosecond-level matching precision.\nSee\nASOF JOIN documentation\nfor more examples with the\nTOLERANCE\nclause.\n\n## SPLICE JOIN‚Äã\n\nSPLICE JOIN\nis a full\nASOF JOIN\n. It will return all the records from both\ntables. For each record from left table splice join will find prevailing record\nfrom right table and for each record from right table - prevailing record from\nleft table.\nConsidering the following tables:\nTable\nbuy\n(the left table):\n\n| timestamp | price |\n| --- | --- |\n| 2024-06-22T00:00:00.039906Z | 0.092014 |\n| 2024-06-22T00:00:00.343909Z | 9.805 |\n\nThe\nsell\ntable (the right table):\n\n| timestamp | price |\n| --- | --- |\n| 2024-06-22T00:00:00.222534Z | 64120.28 |\n| 2024-06-22T00:00:00.222534Z | 64120.28 |\n\nA\nSPLICE JOIN\ncan be built as follows:\n\n```questdb-sql\nWITHbuy AS (  -- select the first 5 buys in June 22  SELECT timestamp, price FROM trades  WHERE timestamp IN '2024-06-22' AND side = 'buy' LIMIT 2),sell AS ( -- select the first 5 sells in June 22  SELECT timestamp, price FROM trades  WHERE timestamp IN '2024-06-22' AND side = 'sell' LIMIT 2)SELECT  buy.timestamp, sell.timestamp, buy.price, sell.priceFROM buySPLICE JOIN sell;\n```\n\nThis query returns the following results:\n\n| timestamp | timestamp1 | price | price1 |\n| --- | --- | --- | --- |\n| 2024-06-22T00:00:00.039906Z | NULL | 0.092014 | NULL |\n| 2024-06-22T00:00:00.039906Z | 2024-06-22T00:00:00.222534Z | 0.092014 | 64120.28 |\n| 2024-06-22T00:00:00.039906Z | 2024-06-22T00:00:00.222534Z | 0.092014 | 64120.28 |\n| 2024-06-22T00:00:00.343909Z | 2024-06-22T00:00:00.222534Z | 9.805 | 64120.28 |\n\nNote that the above query does not use the optional\nON\nclause. In case you\nneed additional filtering on the two tables, the\nON\nclause can also be used.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2211,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-dfafaaa1a778",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/drop",
    "title": "DROP TABLE keyword | QuestDB",
    "text": "DROP TABLE\npermanently deletes a table and its contents.\nDROP ALL TABLES\npermanently deletes all tables, all materialized views, and their contents on disk.\nnote\nBackup your database\nto avoid unintended data loss.\n\n## Syntax‚Äã\n\n\n### IF EXISTS‚Äã\n\nAn optional\nIF EXISTS\nclause may be added directly after the\nDROP TABLE\nkeywords to indicate that the selected table should be dropped only if it exists.\nWithout\nIF EXISTS\n, QuestDB will throw an error if the table does not exist.\n\n## Description‚Äã\n\nThis command irremediably deletes the data in the target table. Unless the table\nwas created in a different volume than the standard, see\nCREATE TABLE IN VOLUME\n,\nin which case the table is only logically removed and data remains intact in its\nvolume. In doubt, make sure you have created\nbackups\nof your data.\nDisk space is reclaimed asynchronously after the table is dropped. Ongoing table\nreads might delay space reclamation.\n\n## Example‚Äã\n\n\n```questdb-sql\nDROP TABLE ratings;\n```\n\n\n```questdb-sql\nDROP ALL TABLES;\n```\n\n\n## See also‚Äã\n\nTo delete the data inside a table but keep the table and its structure, use\nTRUNCATE\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 186,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-f04e065fae0d",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/pgwire/overview",
    "title": "PostgreSQL Wire Protocol | QuestDB",
    "text": "QuestDB implements the PostgreSQL wire protocol (PGWire), allowing you to\nconnect using standard PostgreSQL client libraries. This is the recommended way\nto\nquery data\nfrom QuestDB, as you can use existing PostgreSQL clients and\ntools.\nPGWire also supports\nINSERT statements\nfor lower-volume data\ningestion. For high-throughput ingestion, use the\nQuestDB clients\ninstead.\n\n## Query examples‚Äã\n\n\n### .NET\n\nQuery QuestDB using Npgsql or other .NET PostgreSQL drivers.\nRead more\n\n### Go\n\nQuery QuestDB using pgx or other Go PostgreSQL drivers.\nRead more\n\n### Java\n\nQuery QuestDB using JDBC with any PostgreSQL-compatible driver.\nRead more\n\n### Node.js\n\nQuery QuestDB using pg or other Node.js PostgreSQL clients.\nRead more\n\n### Python\n\nQuery QuestDB using psycopg, asyncpg, or other Python drivers.\nRead more\n\n### Rust\n\nQuery QuestDB using tokio-postgres or other Rust PostgreSQL crates.\nRead more\n\n### PHP\n\nQuery QuestDB using PDO or other PHP PostgreSQL extensions.\nRead more\n\n### R\n\nQuery QuestDB using RPostgres or other R database packages.\nRead more\n\n## Compatibility‚Äã\n\n\n### Supported features‚Äã\n\n- Querying (all types exceptBLOB)\n- Prepared statements with bind parameters\n- INSERTstatements with bind parameters\n- UPDATEstatements with bind parameters\n- DDL execution\n- Batch inserts\n- Plain authentication\n\n### Unsupported features‚Äã\n\n- SSL\n- Remote file upload (COPYfromstdin)\n- DELETEstatements\n- BLOBtransfer\n\n### Connection properties‚Äã\n\n\n| Name | Example | Description |\n| --- | --- | --- |\n| database | qdb | Can be set to any value (e.g.,qdb). Database name is ignored; QuestDB does not have database instance names. |\n| user | admin | User name configured inpg.userorpg.readonly.userproperty inserver.conf. Default:admin |\n| password | quest | Password frompg.passwordorpg.readonly.passwordproperty inserver.conf. Default:quest |\n| options | -c statement_timeout=60000 | The only supported option isstatement_timeout, which specifies maximum execution time in milliseconds for SELECT or UPDATE statements. |\n\n\n## Important considerations‚Äã\n\n\n### Large result sets‚Äã\n\nWhen querying large datasets, most PostgreSQL drivers load the entire result\nset into memory before returning rows. This causes out-of-memory errors and\nslow performance.\nSolution:\nUse cursor-based fetching to retrieve rows in batches.\nSee\nHandling Large Result Sets\nfor\nper-language examples.\n\n### Timestamp handling‚Äã\n\nQuestDB stores all timestamps internally in\nUTC\n. However, when\ntransmitting timestamps over the PGWire protocol, QuestDB represents them as\nTIMESTAMP WITHOUT TIMEZONE\n. This can lead to client libraries interpreting\nthese timestamps in their local timezone by default, potentially causing\nconfusion or incorrect data representation.\nOur language-specific guides provide detailed examples on how to configure your\nclient to correctly interpret these timestamps as UTC.\nWe recommend setting the timezone in your client library to UTC to ensure\nconsistent handling of timestamps.\n\n### SQL dialect differences‚Äã\n\nWhile QuestDB supports the PGWire protocol for communication, its SQL dialect\nand feature set are not identical to PostgreSQL. QuestDB is a specialized\ntime-series database and does not support all SQL features, functions, or data\ntypes that a standard PostgreSQL server does.\nAlways refer to the\nQuestDB SQL documentation\nfor supported operations.\n\n### Forward-only cursors‚Äã\n\nQuestDB's cursors are forward-only, differing from PostgreSQL's support for\nscrollable cursors (which allow bidirectional navigation and arbitrary row\naccess). With QuestDB, you can iterate through query results sequentially from\nstart to finish, but you cannot move backward or jump to specific rows.\nExplicit\nDECLARE CURSOR\nstatements for scrollable types, or operations like\nfetching in reverse (e.g.,\nFETCH BACKWARD\n), are not supported.\nThis limitation can impact client libraries that rely on scrollable cursor\nfeatures. For example, Python's psycopg2 driver might encounter issues if\nattempting such operations. For optimal compatibility, choose drivers or\nconfigure existing ones to use forward-only cursors, such as Python's asyncpg\ndriver.\n\n### Protocol flavors and encoding‚Äã\n\nThe PostgreSQL wire protocol has different implementations and options. When\nyour client library allows:\n- Prefer theExtended Query Protocolover the Simple Query Protocol\n- Choose clients that supportBINARY encodingfor data transfer over TEXT\nencoding for optimal performance and type fidelity\nThe specifics of how to configure this will vary by client library.\n\n## Highly-available reads (Enterprise)‚Äã\n\nQuestDB Enterprise supports running\nmultiple replicas\nto serve queries. Many client\nlibraries allow specifying\nmultiple hosts\nin the connection string. This\nensures that initial connections succeed even if a node is unavailable. If the\nconnected node fails later, the application should catch the error, reconnect to\nanother host, and retry the read.\nFor background and code samples in multiple languages, see:\n- Blog:Highly-available reads with QuestDB\n- Examples:questdb/questdb-ha-reads\n\n## INSERT examples‚Äã\n\nPGWire supports INSERT statements for lower-volume ingestion use cases.\n- psql\n- Python\n- Java\n- NodeJS\n- Go\n- Rust\nCreate the table:\n\n```shell\npsql -h localhost -p 8812 -U admin -d qdb \\    -c \"CREATE TABLE IF NOT EXISTS t1 (name STRING, value INT);\"\n```\n\nInsert row:\n\n```shell\npsql -h localhost -p 8812 -U admin -d qdb -c \"INSERT INTO t1 VALUES('a', 42)\"\n```\n\nQuery back:\n\n```shell\npsql -h localhost -p 8812 -U admin -d qdb -c \"SELECT * FROM t1\"\n```\n\nNote that you can also run\npsql\nfrom Docker without installing the client\nlocally:\n\n```shell\ndocker run -it --rm --network=host -e PGPASSWORD=quest \\    postgres psql ....\n```\n\nThis example uses the\npsycopg3\nadapter.\nTo\ninstall\nthe\nclient library, use\npip\n:\n\n```shell\npython3 -m pip install \"psycopg[binary]\"\n```\n\n\n```python\nimport psycopg as pgimport time# Connect to an existing QuestDB instanceconn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb'with pg.connect(conn_str, autocommit=True) as connection:    # Open a cursor to perform database operations    with connection.cursor() as cur:        # Execute a command: this creates a new table        cur.execute('''          CREATE TABLE IF NOT EXISTS test_pg (              ts TIMESTAMP,              name STRING,              value INT          ) timestamp(ts);          ''')        print('Table created.')        # Insert data into the table.        for x in range(10):            # Converting datetime into millisecond for QuestDB            timestamp = time.time_ns() // 1000            cur.execute('''                INSERT INTO test_pg                    VALUES (%s, %s, %s);                ''',                (timestamp, 'python example', x))        print('Rows inserted.')        #Query the database and obtain data as Python objects.        cur.execute('SELECT * FROM test_pg;')        records = cur.fetchall()        for row in records:            print(row)# the connection is now closed\n```\n\n\n```java\npackage com.myco;import java.sql.*;import java.util.Properties;class App {  public static void main(String[] args) throws SQLException {    Properties properties = new Properties();    properties.setProperty(\"user\", \"admin\");    properties.setProperty(\"password\", \"quest\");    properties.setProperty(\"sslmode\", \"disable\");    final Connection connection = DriverManager.getConnection(      \"jdbc:postgresql://localhost:8812/qdb\", properties);    connection.setAutoCommit(false);    final PreparedStatement statement = connection.prepareStatement(      \"CREATE TABLE IF NOT EXISTS trades (\" +      \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +      \") timestamp(ts);\");    statement.execute();    try (PreparedStatement preparedStatement = connection.prepareStatement(        \"INSERT INTO TRADES  VALUES (?, ?, ?, ?)\")) {      preparedStatement.setTimestamp(        1,        new Timestamp(io.questdb.std.Os.currentTimeMicros()));      preparedStatement.setDate(2, new Date(System.currentTimeMillis()));      preparedStatement.setString(3, \"abc\");      preparedStatement.setInt(4, 123);      preparedStatement.execute();    }    System.out.println(\"Done\");    connection.close();  }}\n```\n\nThis example uses the\npgpackage\nwhich\nallows for quickly building queries using Postgres wire protocol. Details on the\nuse of this package can be found on the\nnode-postgres documentation\n.\nThis example uses naive\nDate.now() * 1000\ninserts for Timestamp types in\nmicrosecond resolution. For accurate microsecond timestamps, the\nprocess.hrtime.bigint()\ncall can be used.\n\n```javascript\n\"use strict\"const { Client } = require(\"pg\")const start = async () => {  const client = new Client({    database: \"qdb\",    host: \"127.0.0.1\",    password: \"quest\",    port: 8812,    user: \"admin\",  })  await client.connect()  const createTable = await client.query(    \"CREATE TABLE IF NOT EXISTS trades (\" +      \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +      \") timestamp(ts);\",  )  console.log(createTable)  let now = new Date().toISOString()  const insertData = await client.query(    \"INSERT INTO trades VALUES($1, $2, $3, $4);\",    [now, now, \"node pg example\", 123],  )  await client.query(\"COMMIT\")  console.log(insertData)  for (let rows = 0; rows < 10; rows++) {    // Providing a 'name' field allows for prepared statements / bind variables    now = new Date().toISOString()    const query = {      name: \"insert-values\",      text: \"INSERT INTO trades VALUES($1, $2, $3, $4);\",      values: [now, now, \"node pg prep statement\", rows],    }    await client.query(query)  }  await client.query(\"COMMIT\")  const readAll = await client.query(\"SELECT * FROM trades\")  console.log(readAll.rows)  await client.end()}start()  .then(() => console.log(\"Done\"))  .catch(console.error)\n```\n\nThis example uses the\npgx\ndriver and toolkit for\nPostgreSQL in Go. More details on the use of this toolkit can be found on the\nGitHub repository for pgx\n.\n\n```go\npackage mainimport (  \"context\"  \"fmt\"  \"log\"  \"time\"  \"github.com/jackc/pgx/v4\")var conn *pgx.Connvar err errorfunc main() {  ctx := context.Background()  conn, _ = pgx.Connect(ctx, \"postgresql://admin:quest@localhost:8812/qdb\")  defer conn.Close(ctx)  // text-based query  _, err := conn.Exec(ctx,    (\"CREATE TABLE IF NOT EXISTS trades (\" +     \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +     \") timestamp(ts);\"))  if err != nil {    log.Fatalln(err)  }  // Prepared statement given the name 'ps1'  _, err = conn.Prepare(ctx, \"ps1\", \"INSERT INTO trades VALUES($1,$2,$3,$4)\")  if err != nil {    log.Fatalln(err)  }  // Insert all rows in a single commit  tx, err := conn.Begin(ctx)  if err != nil {    log.Fatalln(err)  }  for i := 0; i < 10; i++ {    // Execute 'ps1' statement with a string and the loop iterator value    _, err = conn.Exec(      ctx,      \"ps1\",      time.Now(),      time.Now().Round(time.Millisecond),      \"go prepared statement\",      i + 1)    if err != nil {      log.Fatalln(err)    }  }  // Commit the transaction  err = tx.Commit(ctx)  if err != nil {    log.Fatalln(err)  }  // Read all rows from table  rows, err := conn.Query(ctx, \"SELECT * FROM trades\")  fmt.Println(\"Reading from trades table:\")  for rows.Next() {    var name string    var value int64    var ts time.Time    var date time.Time    err = rows.Scan(&ts, &date, &name, &value)    fmt.Println(ts, date, name, value)  }  err = conn.Close(ctx)}\n```\n\nThe following example shows how to use parameterized queries and prepared\nstatements using the\nrust-postgres\nclient.\n\n```rust\nuse postgres::{Client, NoTls, Error};use chrono::{Utc};use std::time::SystemTime;fn main() -> Result<(), Error> {    let mut client = Client::connect(\"postgresql://admin:quest@localhost:8812/qdb\", NoTls)?;    // Basic query    client.batch_execute(      \"CREATE TABLE IF NOT EXISTS trades ( \\          ts TIMESTAMP, date DATE, name STRING, value INT \\      ) timestamp(ts);\")?;    // Parameterized query    let name: &str = \"rust example\";    let val: i32 = 123;    let utc = Utc::now();    let sys_time = SystemTime::now();    client.execute(        \"INSERT INTO trades VALUES($1,$2,$3,$4)\",        &[&utc.naive_local(), &sys_time, &name, &val],    )?;    // Prepared statement    let mut txn = client.transaction()?;    let statement = txn.prepare(\"INSERT INTO trades VALUES ($1,$2,$3,$4)\")?;    for value in 0..10 {        let utc = Utc::now();        let sys_time = SystemTime::now();        txn.execute(&statement, &[&utc.naive_local(), &sys_time, &name, &value])?;    }    txn.commit()?;    println!(\"import finished\");    Ok(())}\n```\n\n\n### Decimal values‚Äã\n\nTo insert\ndecimal\nvalues via PGWire, you must either use the\nm\nsuffix to\nindicate that the value is a decimal literal or cast the value to\ndecimal\n:\n\n```questdb-sql\nINSERT INTO my_table (decimal_column) VALUES (123.45m);                        -- Using 'm' suffixINSERT INTO my_table (decimal_column) VALUES (CAST($1 AS DECIMAL(18, 3)));     -- Using CAST over bind parameter\n```\n\nIn the text format, PostgreSQL clients send decimal values as strings.\nCurrently, QuestDB parses these strings as\ndouble\nvalues and doesn't\nimplicitly convert them to\ndecimal\nto avoid unintended precision loss. You\nmust explicitly cast\ndouble\nvalues to\ndecimal\nin your SQL queries when\ninserting into\ndecimal\ncolumns.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1752,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-eedef5115f66",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/getting-started/web-console/overview",
    "title": "Web Console Overview | QuestDB",
    "text": "Web Console is a client that allows you to interact with QuestDB. It\nprovides UI tools to query and explore the data, visualize the results in a table or plot.\n\n### Accessing the Web Console‚Äã\n\nWeb Console will be available at\nhttp://[server-address]:9000\n. When\nrunning locally, this will be\nhttp://localhost:9000\n.\n\n### Layout‚Äã\n\nThe Web Console is organized into the following main sections that work together to provide a complete workflow:\n\n### Code Editor‚Äã\n\nThe\nCode Editor\nis where you write and execute SQL queries with features like syntax highlighting, auto-completion, and error tracing. It supports executing queries by selection, multiple query execution, and query planning.\nLearn more about Code Editor ‚Üí\n\n### AI Assistant‚Äã\n\nThe\nAI Assistant\nprovides intelligent query assistance directly in the Web Console using AI-powered explanations and suggestions. It helps you write, understand, and fix SQL queries while maintaining complete control over your data and API keys through a Bring Your Own Key (BYOK) model.\nLearn more about AI Assistant ‚Üí\n\n### Metrics View‚Äã\n\nThe\nMetrics View\nprovides real-time monitoring and telemetry capabilities for your QuestDB instance. It displays interactive charts and widgets to track database performance, WAL operations, and table-specific metrics.\nLearn more about Metrics View ‚Üí\n\n### Schema Explorer‚Äã\n\nThe\nSchema Explorer\nis the navigation hub for exploring tables and materialized views. It provides detailed information about each database object including columns with data types, storage configuration (partitioning and WAL status), and for materialized views, their base tables.\nLearn more about Schema Explorer ‚Üí\n\n### Result Grid‚Äã\n\nThe\nResult Grid\ndisplays your query results in an interactive table format with features for data navigation, export, and visualization.\nLearn more about Result Grid ‚Üí\n\n### Query Log‚Äã\n\nThe\nQuery Log\nmonitors query execution status and performance metrics, providing real-time feedback and maintaining a history of recent operations. It shows execution times, row counts, and detailed error information to help optimize your queries.\nLearn more about Query Log ‚Üí\n\n### Import CSV‚Äã\n\nThe\nImport CSV\ninterface allows you to upload and import CSV files into QuestDB with automatic schema detection, flexible configuration options, and detailed progress tracking. You can create new tables or append to existing ones with full control over the import process.\nLearn more about Import CSV ‚Üí\n\n### Right Sidebar‚Äã\n\nThe\nRight Sidebar\nprovides quick access to essential tools and information:\n- Help: Access quick links and contact options through a convenient help menu\n- QuestDB News: Stay up-to-date with the latest QuestDB announcements and updates\n- Create Table: Build new tables visually using an intuitive interface. Define table structure, configure partitioning, enable WAL, and add columns with their data types‚Äîall without writing SQL code.Learn more about Create Table ‚Üí\n\n### Instance Naming‚Äã\n\nWeb Console allows you to set the instance name, type, and color. This functionality is particularly useful for production users who manage multiple deployments and frequently navigate between them. This feature makes it easier to keep track of instance information and label instances with meaningful names for their users.\nThe instance name, instance type, and description are displayed when hovering over the icon in the instance information badge.\nInstance information can be modified through the dialog that opens when clicking the edit icon:\ninfo\nIf\nhttp.settings.readonly\nconfiguration is set to true, instance information is not editable.\ninfo\nWhen using QuestDB Enterprise with Role-Based Access Control (RBAC), only the users with\nSETTINGS\nor\nDATABASE ADMIN\npermission can edit the instance information. See\nDatabase Permissions\nfor more details.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 576,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-ae18a26af34b",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/rust",
    "title": "Rust Client Documentation | QuestDB",
    "text": "QuestDB offers a Rust client designed for high-performance data ingestion. These\nare some of the highlights:\n- Creates tables automatically: no need to define your schema up-front\n- Concurrent schema changes: seamlessly handle multiple data streams that\nmodify the table schema on the fly\n- Optimized batching: buffer the data and send many rows in one go\n- Health checks and feedback: built-in health monitoring ensures the health\nof your system\nView full docs\nView source code\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized\nfor\nwriting\ndata to QuestDB. For retrieving data, we recommend using a\nPostgreSQL-compatible Rust library\nor our\nHTTP query endpoint\n.\nIf you don't have a QuestDB server yet, follow the\nQuick Start\nsection to set it up.\n\n## Add the client crate to your project‚Äã\n\nQuestDB clients requires Rust 1.40 or later. Add its crate to your project using\nthe command line:\n\n```bash\ncargo add questdb-rs\n```\n\n\n## Authenticate‚Äã\n\nThis is how you authenticate using the HTTP Basic authentication:\n\n```rust\nlet mut sender = Sender::from_conf(    \"https::addr=localhost:9000;username=admin;password=quest;\")?;\n```\n\nYou can also pass the connection configuration via the\nQDB_CLIENT_CONF\nenvironment variable:\n\n```bash\nexport QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n```\n\nThen you use it like this:\n\n```rust\nlet mut sender = Sender::from_env()?;\n```\n\nWhen using QuestDB Enterprise, you can authenticate via a REST token. Please\ncheck the\nRBAC docs\nfor more info.\n\n## Insert data‚Äã\n\nThis snippet connects to QuestDB and inserts one row of data:\n\n```rust\nuse questdb::{    Result,    ingress::{        Sender,        Buffer,        TimestampNanos}};fn main() -> Result<()> {   let mut sender = Sender::from_conf(\"http::addr=localhost:9000;\")?;   let mut buffer = Buffer::new();   buffer       .table(\"trades\")?       .symbol(\"symbol\", \"ETH-USD\")?       .symbol(\"side\", \"sell\")?       .column_f64(\"price\", 2615.54)?       .column_f64(\"amount\", 0.00044)?       .at(TimestampNanos::now())?;   sender.flush(&mut buffer)?;   Ok(())}\n```\n\nThese are the main steps it takes:\n- UseSender::from_conf()to get thesenderobject\n- Populate aBufferwith one or more rows of data\n- Send the buffer usingsender.flush()(Sender::flush)\nIn this case, the designated timestamp will be the one at execution time.\nLet's see now an example with timestamps using Chrono, custom timeout, and basic\nauth.\nYou need to enable the\nchrono_timestamp\nfeature to the QuestDB crate and add\nthe Chrono crate.\n\n```bash\ncargo add questdb-rs --features chrono_timestampcargo add chrono\n```\n\n\n```rust\nuse questdb::{    Result,    ingress::{        Sender,        Buffer,        TimestampNanos    },};use chrono::Utc;fn main() -> Result<()> {    let mut sender = Sender::from_conf(      \"http::addr=localhost:9000;username=admin;password=quest;retry_timeout=20000;\"      )?;    let mut buffer = Buffer::new();    let current_datetime = Utc::now();    buffer        .table(\"trades\")?        .symbol(\"symbol\", \"ETH-USD\")?        .symbol(\"side\", \"sell\")?        .column_f64(\"price\", 2615.54)?        .column_f64(\"amount\", 0.00044)?        .at(TimestampNanos::from_datetime(current_datetime)?)?;    sender.flush(&mut buffer)?;    Ok(())}\n```\n\nnote\nAvoid using\nat_now()\ninstead of\nat(some_timestamp)\n. This removes the ability\nto deduplicate rows, which is\nimportant for exactly-once processing\n.\n\n## Ingest arrays‚Äã\n\nThe\nSender::column_arr\nmethod supports efficient ingestion of N-dimensional\narrays using several convenient types:\n- native Rust arrays and slices (up to 3-dimensional)\n- native Rust vectors (up to 3-dimensional)\n- arrays from thendarraycrate, or other types that\nsupport thequestdb::ingress::NdArrayViewtrait.\nnote\nArrays are supported from QuestDB version 9.0.0, and require updated\nclient libraries.\nIn this example, we insert some FX order book data.\n- bidsandasks: 2D arrays of L2 order book depth. Each level contains price and volume.\n- bids_exec_probsandasks_exec_probs: 1D arrays of calculated execution probabilities for the next minute.\nnote\nYou must use protocol version 2 to ingest arrays. HTTP transport will\nautomatically enable it as long as you're connecting to an up-to-date QuestDB\nserver (version 9.0.0 or later), but with TCP you must explicitly specify it in\nthe configuration string:\nprotocol_version=2;\nSee\nbelow\nfor more details on protocol versions.\n\n```rust\nuse questdb::{Result, ingress::{SenderBuilder, TimestampNanos}};use ndarray::arr2;fn main() -> Result<()> {    // or `tcp::addr=127.0.0.1:9009;protocol_version=2;`    let mut sender = SenderBuilder::from_conf(\"http::addr=127.0.0.1:9000;\")?        .build()?;    let mut buffer = sender.new_buffer();    buffer        .table(\"fx_order_book\")?         .symbol(\"symbol\", \"EUR/USD\")?        .column_arr(\"bids\", &vec![            vec![1.0850, 600000.0],            vec![1.0849, 300000.0],            vec![1.0848, 150000.0]])?        .column_arr(\"asks\", &arr2(&[            [1.0853, 500000.0],            [1.0854, 250000.0],            [1.0855, 125000.0]]).view())?        .column_arr(\"bids_exec_probs\",            &[0.85, 0.50, 0.25])?        .column_arr(\"asks_exec_probs\",            &vec![0.90, 0.55, 0.20])?        .at(TimestampNanos::now())?;    sender.flush(&mut buffer)?;    Ok(())}\n```\n\n\n## Configuration options‚Äã\n\nThe easiest way to configure the line sender is the configuration string. The\ngeneral structure is:\n\n```plain\n<transport>::addr=host:port;param1=val1;param2=val2;...\n```\n\ntransport\ncan be\nhttp\n,\nhttps\n,\ntcp\n, or\ntcps\n. Go to the client's\ncrate documentation\nfor the\nfull details on configuration.\nAlternatively, for breakdown of available params, see the\nConfiguration string\npage.\n\n## Don't forget to flush‚Äã\n\nThe sender and buffer objects are entirely decoupled. This means that the sender\nwon't get access to the data in the buffer until you explicitly call\nsender.flush(&mut buffer)\nor a variant. This may lead to a pitfall where you\ndrop a buffer that still has some data in it, resulting in permanent data loss.\nA common technique is to flush periodically on a timer and/or once the buffer\nexceeds a certain size. You can check the buffer's size by calling\nbuffer.len()\n.\nThe default\nflush()\nmethod clears the buffer after sending its data. If you\nwant to preserve its contents (for example, to send the same data to multiple\nQuestDB instances), call\nsender.flush_and_keep(&mut buffer)\ninstead.\n\n## Transactional flush‚Äã\n\nAs described in\nILP overview\n, the\nHTTP transport has some support for transactions.\nIn order to ensure in advance that a flush will not affect more than one table,\ncall\nsender.flush_and_keep_with_flags(&mut buffer, true)\n. This call will\nrefuse to flush a buffer if the flush wouldn't be data-transactional.\n\n## Error handling‚Äã\n\nThe two supported transport modes, HTTP and TCP, handle errors very differently.\nIn a nutshell, HTTP is much better at error handling.\n\n### HTTP‚Äã\n\nHTTP distinguishes between recoverable and non-recoverable errors. For\nrecoverable ones, it enters a retry loop with exponential backoff, and reports\nthe error to the caller only after it has exhausted the retry time budget\n(configuration parameter:\nretry_timeout\n).\nsender.flush()\nand variant methods communicate the error in the\nResult\nreturn value. The category of the error is signalled through the\nErrorCode\nenum, and it's accompanied with an error message.\nAfter the sender has signalled an error, it remains usable. You can handle the\nerror as appropriate and continue using it.\n\n### TCP‚Äã\n\nTCP doesn't report errors at all to the sender; instead, the server quietly\ndisconnects and you'll have to inspect the server logs to get more information\non the reason. When this has happened, the sender transitions into an error\nstate, and it is permanently unusable. You must drop it and create a new sender.\nYou can inspect the sender's error state by calling\nsender.must_close()\n.\nFor more details about the HTTP and TCP transports, please refer to the\nILP overview\n.\n\n## Protocol Version‚Äã\n\nTo enhance data ingestion performance, QuestDB introduced an upgrade to the\ntext-based InfluxDB Line Protocol which encodes arrays and\nf64\nvalues in\nbinary form. Arrays are supported only in this upgraded protocol version.\nYou can select the protocol version with the\nprotocol_version\nsetting in the\nconfiguration string.\nHTTP transport automatically negotiates the protocol version by default. In order\nto avoid the slight latency cost at connection time, you can explicitly configure\nthe protocol version by setting\nprotocol_version=2|1;\n.\nTCP transport does not negotiate the protocol version and uses version 1 by\ndefault. You must explicitly set\nprotocol_version=2;\nin order to ingest\narrays, as in this example:\n\n```text\ntcp::addr=localhost:9009;protocol_version=2;\n```\n\nProtocol Version 2 along with its support for arrays is available from QuestDB\nversion 9.0.0.\n\n## Crate features‚Äã\n\nThe QuestDB client crate supports some optional features, mostly related to\nadditional library dependencies.\n\n### Default-enabled features‚Äã\n\n- tls-webpki-certs: supports using thewebpki-rootscrate for TLS\ncertificate verification.\n\n### Optional features‚Äã\n\nThese features are opt-in:\n- ilp-over-http: Enables ILP/HTTP support using theureqcrate.\n- chrono_timestamp: Allows specifying timestamps aschrono::Datetimeobjects.\n- tls-native-certs: Supports validating TLS certificates against the OS's\ncertificates store.\n- insecure-skip-verify: Allows skipping server certificate validation in TLS\n(this compromises security).\n- ndarray: Enables ingestion of arrays from thendarraycrate.\n\n## Next steps‚Äã\n\nPlease refer to the\nILP overview\nfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\nExplore the full capabilities of the Rust client via the\nCrate API page\n.\nWith data flowing into QuestDB, now it's time for analysis.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nAlone? Stuck? Want help? Visit us in our\nCommunity Forum\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1328,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-0b660d670efd",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/visualization/grafana",
    "title": "Grafana | QuestDB",
    "text": "Grafana\nis a popular observability and monitoring\napplication used to visualize data and enable\ntime-series data analysis\n.\nQuestDB is available within Grafana via the\nofficial QuestDB plugin\n.\nwarning\nQuestDB can also be used with the PostgreSQL Grafana plugin, but the configuration options are different in that case. The QuestDB official plugin is strongly recommended instead.\nFor a walk-through style guide, see our\nblog post\n.\n\n## Prerequisites‚Äã\n\n- Dockerto run both Grafana and QuestDBWe will use the--add-hostparameter for both Grafana and QuestDB.\n\n## Start Grafana‚Äã\n\nStart Grafana using\ndocker run\n:\n\n```shell\ndocker run --add-host=host.docker.internal:host-gateway \\-p 3000:3000 --name=grafana \\-v grafana-storage:/var/lib/grafana \\grafana/grafana-oss\n```\n\nOnce the Grafana server has started, you can access it via port 3000\n(\nhttp://localhost:3000\n). The default login credentials\nare as follows:\n\n```shell\nuser:adminpassword:admin\n```\n\n\n## Start QuestDB‚Äã\n\nThe Docker version runs on port\n8812\nfor the database connection and port\n9000\nfor the\nWeb Console\nand REST interface:\n\n```shell\ndocker run --add-host=host.docker.internal:host-gateway \\-p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\-v \"$(pwd):/var/lib/questdb\" \\-e QDB_PG_READONLY_USER_ENABLED=true \\questdb/questdb:latest\n```\n\n\n## Add a data source‚Äã\n\n- Open Grafana's UI (by default available athttp://localhost:3000)\n- Navigate to the bottom of the page and clickFind more data source\nplugins.\n- Search for QuestDB and clickInstall.\n- Once the QuestDB data source for Grafana is finished installing, click on the\nblueAdd new data sourcebutton where theInstallbutton used to be.\n- Enter the connection settings.Notice thatServer Addressis the host address without the port. Some common values arehost.docker.internalwhen using Docker on the same host,localhostwhen running standalone Grafana on the same host, or the QuestDB instance IP address when running Grafana remotely.The port, which defaults to8812is passed as a separate parameter.For QuestDB Open Source, TLS/SSL mode should bedisable. This can be left empty for QuestDB Enterprise.\n\n```questdb-sql\nServer address: host.docker.internalServer port: 8812Username: userPassword: questTLS/SSL mode: disable\n```\n\n- Toggle theQuery BuildertoSQL Editorby clicking the button.\n- Write SQL queries!\n\n## Real-time refresh rates‚Äã\n\nBy default, Grafana limits the maximum refresh rate of your dashboards. The\nmaximum default rate is to refresh every 5 seconds. This is to provide relief to\nthe database under-the-hood. However, with QuestBD's significant performance\noptimizations, we can lower this rate for greater fluidity.\nTo learn how, see our\nblog post\n.\n\n## Global variables‚Äã\n\nUse\nglobal variables\nto simplify queries with dynamic elements such as date range filters.\n\n### $__timeFilter(timestamp)‚Äã\n\nThis variable allows filtering results by sending a start-time and end-time to\nQuestDB. This expression evaluates to:\n\n```questdb-sql\ntimestamp BETWEEN    '2018-02-01T00:00:00Z' AND '2018-02-28T23:59:59Z'\n```\n\n\n### $__interval‚Äã\n\nThis variable calculates a dynamic interval based on the time range applied to\nthe dashboard. By using this function, the sampling interval changes\nautomatically as the user zooms in and out of the panel.\nAn example of $__interval\n\n```questdb-sql\nSELECT  timestamp AS time,  avg(price) AS avg_priceFROM tradesWHERE $__timeFilter(timestamp)SAMPLE BY $__interval;\n```\n\n\n## See also‚Äã\n\n- QuestDB + Grafana walkthrough\n- QuestDB Grafana blogs\n- Official QuestDB plugin",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 483,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-1060d3140b0a",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/ilp/overview",
    "title": "InfluxDB Line Protocol Overview | QuestDB",
    "text": "QuestDB implements the InfluxDB Line Protocol to ingest data.\nThe InfluxDB Line Protocol is for\ndata ingestion only\n.\nFor building queries, see the\nQuery & SQL Overview\n.\nEach ILP client library also has its own language-specific documentation set.\nThis supporting document thus provides an overview to aid in client selection\nand initial configuration:\n- Client libraries\n- Server-Side configuration\n- Transport selection\n- Client-Side configuration\n- Error handling\n- Authentication\n- Table and column auto-creation\n- Timestamp column name\n- HTTP Transaction semantics\n- Exactly-once delivery\n- Multiple URLs for High Availability\n- Health Check\n\n## Client libraries‚Äã\n\nThe quickest way to get started is to select your library of choice.\nFrom there, its documentation will carry you through to implementation.\nClient libraries are available for several languages:\n\n### C & C++\n\nHigh-performance client for systems programming and embedded applications.\nRead more\n\n### .NET\n\nCross-platform client for building applications with .NET technologies.\nRead more\n\n### Go\n\nAn open-source programming language supported by Google with built-in concurrency.\nRead more\n\n### Java\n\nPlatform-independent client for enterprise applications and Android development.\nRead more\n\n### Node.js\n\nNode.js¬Æ is an open-source, cross-platform JavaScript runtime environment.\nRead more\n\n### Python\n\nPython is a programming language that lets you work quickly and integrate systems more effectively.\nRead more\n\n### Rust\n\nSystems programming language focused on safety, speed, and concurrency.\nRead more\nIf you'd like more context on ILP overall, please continue reading.\n\n## Enable or disable ILP‚Äã\n\nIf going over HTTP, ILP will use shared HTTP port\n9000\n(default) if the\nfollowing is set in\nserver.conf\n:\n\n```conf\nline.http.enabled=true\n```\n\n\n## Server-Side Configuration‚Äã\n\nThe HTTP receiver configuration can be completely customized using\nQuestDB configuration keys for ILP\n.\nConfigure the thread pools, buffer and queue sizes, receiver IP address and\nport, load balancing, and more.\nFor more guidance in how to tune QuestDB, see\ncapacity planning\n.\n\n## Transport selection‚Äã\n\nThe ILP protocol in QuestDB supports the following transport options:\n- HTTP (default port 9000)\n- TCP (default port 9009)\nOn QuestDB Enterprise HTTPS and TCPS are also available.\nThe HTTP(s) transport is recommended for most use cases. It provides feedback on\nerrors, automatically retries failed requests, and is easier to configure. The\nTCP(s) transport is kept for compatibility with older QuestDB versions. It has\nlimited error feedback, no automatic retries, and requires manual handling of\nconnection failures. However, while HTTP is recommended, TCP has slightly lower\noverhead than HTTP and may be useful in high-throughput scenarios in\nhigh-latency networks.\n\n## Client-Side Configuration‚Äã\n\nClients connect to a QuestDB using ILP via a configuration string. Configuration\nstrings combine a set of key/value pairs.\nThe standard configuration string pattern is:\n\n```text\nschema::key1=value1;key2=value2;key3=value3;\n```\n\nIt is made up of the following parts:\n- Schema: One of the specified schemas in thecore parameterssection\nbelow\n- Key=Value: Each key-value pair sets a specific parameter for the client\n- Terminating semicolon: A semicolon must follow the last key-value pair\nBasic example:\n\n```text\nhttp::addr=localhost:9000;\n```\n\n\n### Client parameters‚Äã\n\nBelow is a list of common parameters that ILP clients will accept.\nThese params facilitate connection to QuestDB's ILP server and define\nclient-specific behaviors.\nSome are shared across all clients, while some are client specific. Refer to the\nclients documentation for details.\nwarning\nAny parameters tagged as\nSENSITIVE\nmust be handled with care.\nExposing these values may expose your database to bad actors.\n\n#### Core parameters‚Äã\n\n- schema: Specifies the transport method, with support for:http,https,tcp&tcps\n- addr: The address and port of the QuestDB server, as inlocalhost:9000.\n- protocol_version: QuestDB has evolved its protocol beyond the basic ILP.\nHTTP client autodetects the highest protocol version it can use, but TCP\ndoesn't and defaults to 1. This is how you specify protocol version 2 (latest):\n\n```text\ntcp::addr=localhost:9009;protocol_version=2\n```\n\nYou need protocol version 2 in order to ingest n-dimensional arrays.\nIt is available from QuestDB version 9.0.0.\n\n#### HTTP Parameters‚Äã\n\n- password(SENSITIVE): Password for HTTP Basic Authentication.\n- request_min_throughput: Expected throughput for network send to the\ndatabase server, in bytes.Defaults to 100 KiB/sUsed to calculate a dynamic timeout for the request, so that larger requests\ndo not prematurely timeout.\n- request_timeout: Base timeout for HTTP requests to the database, in\nmilliseconds.Defaults to 10 seconds.\n- retry_timeout: Maximum allowed time for client to attempt retries, in\nmilliseconds.Defaults to 10 seconds.Not all errors are retriable.\n- token(SENSITIVE): Bearer token for HTTP Token authentication.Open source HTTP users are unable to generate tokens. For TCP token auth,\nsee the below section.\n- username: Username for HTTP Basic Authentication.\n\n#### TCP Parameters‚Äã\n\nnote\nThese parameters are only useful when using ILP over TCP with authentication\nenabled. Most users should use ILP over HTTP. These parameters are listed for\ncompleteness and for users who have specific requirements.\nSee theAuthenticationsection below for configuration.\n- auth_timeout: Timeout for TCP authentication with QuestDB server, in\nmilliseconds.Default 15 seconds.\n- token(SENSITIVE): TCP Authenticationdparameter.token_x(SENSITIVE): TCP Authenticationxparameter.Used in C/C++/Rust/Python clients.token_y(SENSITIVE): TCP Authenticationyparameter.Used in C/C++/Rust/Python clients.\n- username: Username for TCP authentication.\n\n#### Auto-flushing behavior‚Äã\n\n- auto_flush: Enable or disable automatic flushing (on/off).Default is ‚Äúon‚Äù for clients that support auto-flushing (all except C, C++ &\nRust).\n- auto_flush_bytesAuto-flushing is triggered above this buffer size.Disabled by default.\n- auto_flush_interval: Auto-flushing is triggered after this time period has\nelapsed since the last flush, in milliseconds.Defaults to 1 secondThis is not a periodic timer - it will only be checked on the next row\ncreation.\n- auto_flush_rows: Auto-flushing is triggered above this row count.Defaults to75,000for HTTP, and600for TCP.If set, this implies ‚Äúauto_flush=on‚Äù.\n\n#### Buffer configuration‚Äã\n\n- init_buf_size: Set the initial (but growable) size of the buffer in bytes.Defaults to64 KiB.\n- max_buf_size: Sets the growth limit of the buffer in bytes.Defaults to100 MiB.Clients will error if this is exceeded.\n- max_name_len: The maximum alloable number of UTF-8 bytes in the table or\ncolumn names.Defaults to127.Related to length limits for filenames on the user's host OS.\n\n#### TLS configuration‚Äã\n\nQuestDB Enterprise only.\n- tls_verify: Toggle verification of TLS certificates. Default ison.\n- tls_roots: Specify the source of bundled TLS certificates.The defaults and possible param values are client-specific.In Rust and Python this might be ‚Äúwebpki‚Äù, ‚Äúos-certs‚Äù or a path to a ‚Äúpem‚Äù\nfile.In Java this might be a path to a ‚Äújks‚Äù trust store.tls_roots_passwordPassword to a configured tls_roots if any.Passwords are sensitive! Manage appropriately.\n- tls_ca: Path to single certificate authourity, not supported on all\nclients.Java for instance would applytls_roots=/path/to/Java/key/store\n\n#### Network configuration‚Äã\n\n- bind_interface: Optionally, specify the local network interface for\noutbound connections. Useful if you have multiple interfaces or an accelerated\nnetwork interface (e.g. Solarflare)Not to be confused with the QuestDB port in theaddrparam.\n\n## Error handling‚Äã\n\nThe HTTP transport supports automatic retries for failed requests deemed\nrecoverable. Recoverable errors include network errors, some server errors, and\ntimeouts, while non-recoverable errors encompass invalid data, authentication\nerrors, and other client-side errors.\nRetrying is particularly beneficial during network issues or when the server is\ntemporarily unavailable. The retrying behavior can be configured through the\nretry_timeout\nconfiguration option or, in some clients, via their API. The\nclient continues to retry recoverable errors until they either succeed or the\nspecified timeout is reached.\nThe TCP transport lacks support for error propagation from the server. In such\ncases, the server merely closes the connection upon encountering an error.\nConsequently, the client receives no additional error information from the\nserver. This limitation significantly contributes to the preference for HTTP\ntransport over TCP transport.\n\n## Authentication‚Äã\n\nnote\nUsing\nQuestDB Enterprise\n?\nSkip to\nadvanced security features\ninstead, which\nprovides holistic security out-of-the-box.\nInfluxDB Line Protocol supports authentication via HTTP Basic Authentication,\nusing\nthe HTTP Parameters\n,\nor via token when using the TCP transport, using\nthe TCP Parameters\n.\nA similar pattern is used across all client libraries. If you want to use a TCP\ntoken, you need to configure your QuestDB server. This document will break down\nand demonstrate the configuration keys and core configuration options.\nOnce a client has been selected and configured, resume from your language client\ndocumentation.\n\n### TCP token authentication setup‚Äã\n\nCreate\nd\n,\nx\n&\ny\ntokens for client usage.\n\n#### Prerequisites‚Äã\n\n- jose: C-language implementation of Javascript Object Signing and Encryption.\nGenerates tokens.\n- jq: For pretty JSON output.\n- macOS\n- Debian\n- Ubuntu\n\n```bash\nbrew install josebrew install jq\n```\n\n\n```bash\nyum install joseyum install jq\n```\n\n\n```bash\napt install joseapt install jq\n```\n\n\n#### Server configuration‚Äã\n\nNext, create an authentication file.\nOnly elliptic curve (P-256) are supported (key type\nec-p-256-sha256\n):\n\n```bash\ntestUser1 ec-p-256-sha256 fLKYEaoEb9lrn3nkwLDA-M_xnuFOdSt9y0Z7_vWSHLU Dt5tbS1dEDMSYfym3fgMv0B99szno-dFc1rYF9t0aac# [key/user id] [key type] {keyX keyY}\n```\n\nGenerate an authentication file using the\njose\nutility:\n\n```bash\njose jwk gen -i '{\"alg\":\"ES256\", \"kid\": \"testUser1\"}' -o /var/lib/questdb/conf/full_auth.jsonKID=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.kid')X=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.x')Y=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.y')echo \"$KID ec-p-256-sha256 $X $Y\" | tee /var/lib/questdb/conf/auth.txt\n```\n\nOnce created, reference it in the server\nconfiguration\n:\n/path/to/server.conf\n\n```ini\nline.tcp.auth.db.path=conf/auth.txt\n```\n\n\n#### Client keys‚Äã\n\nFor the server configuration above, the corresponding JSON Web Key must be\nstored on the clients' side.\nWhen sending a fully-composed JWK, it will have the following keys:\n\n```json\n{  \"kty\": \"EC\",  \"d\": \"5UjEMuA0Pj5pjK8a-fa24dyIf-Es5mYny3oE_Wmus48\",  \"crv\": \"P-256\",  \"kid\": \"testUser1\",  \"x\": \"fLKYEaoEb9lrn3nkwLDA-M_xnuFOdSt9y0Z7_vWSHLU\",  \"y\": \"Dt5tbS1dEDMSYfym3fgMv0B99szno-dFc1rYF9t0aac\"}\n```\n\nThe\nd\n,\nx\nand\ny\nparameters generate the public key.\nFor example, the Python client would be configured as outlined in the\nPython docs\n.\n\n## Table and column auto-creation‚Äã\n\nWhen sending data to a table that does not exist, the server will create the\ntable automatically. This also applies to columns that do not exist. The server\nwill use the first row of data to determine the column types.  Please note that table\nand column names must follow the QuestDB\nnaming rules\n.\nIf the table already exists, the server will validate that the columns match the\nexisting table. If the columns do not match, the server will return a\nnon-recoverable error which, when using the HTTP/HTTPS transport, is propagated\nto the client.\nYou can avoid table and/or column auto-creation by setting the\nline.auto.create.new.columns\nand\nline.auto.create.new.tables\nconfiguration\nparameters to false.\nIf you're using QuestDB Enterprise, you must grant further permissions to the\nauthenticated user:\n\n```sql\nCREATE SERVICE ACCOUNT ingest_user; -- creates a service account to be used by a clientGRANT ilp, create table TO ingest_user; -- grants permissions to ingest data and create tablesGRANT add column, insert ON all tables TO ingest_user; -- grants permissions to add columns and insert data to all tables--  ORGRANT add column, insert ON table1, table2 TO ingest_user; -- grants permissions to add columns and insert data to specific tables\n```\n\nRead more setup details in the\nEnterprise quickstart\nand the\nrole-based access control\nguides.\n\n## Timestamp Column Name‚Äã\n\nQuestDB's ILP protocol sends timestamps without a column name.\nAuto-created tables always usetimestampas the designated timestamp column name.\nThere is no client-side option to change this. If you need a different column name,\nyou must pre-create the table before sending data.\nIf the table already exists, the designated timestamp column is used regardless\nof its name.\nTo pre-create a table with a custom timestamp column name, use\nCREATE TABLE\n:\nCreating a timestamp named my_ts\n\n```questdb-sql\nCREATE TABLE IF NOT EXISTS 'trades' (  symbol SYMBOL capacity 256 CACHE,  side SYMBOL capacity 256 CACHE,  price DOUBLE,  amount DOUBLE,  my_ts TIMESTAMP) timestamp (my_ts) PARTITION BY DAY WAL;\n```\n\nYou can use the\nCREATE TABLE IF NOT EXISTS\nconstruct to make sure the table is\ncreated, but without raising an error if the table already exists.\n\n## HTTP transaction semantics‚Äã\n\nThe TCP endpoint does not support transactions. The HTTP ILP endpoint treats\nevery requests as an individual transaction, so long as it contains rows for a\nsingle table.\nAs of writing, the HTTP endpoint does not provide full transactionality in all\ncases.\nSpecifically:\n- If an HTTP request contains data for two tables and the final commit fails for\nthe second table, the data for the first table will still be committed. This\nis a deviation from full transactionality, where a failure in any part of the\ntransaction would result in the entire transaction being rolled back. If data\ntransactionality is important for you, the best practice is to make sure you\nflush data to the server in batches that contain rows for a single table.\n- Even when you are sending data to a single table, when dynamically adding new\ncolumns to a table, an implicit commit occurs each time a new column is added.\nIf the request is aborted or has parse errors, no data will be inserted into\nthe corresponding table, but the new column will be added and will not be\nrolled back.\n- Some clients have built-in support for controlling transactions. These APIs\nhelp to comply with the single-table-per-request pre-requisite for HTTP\ntransactions, but they don't control if new columns are being added.\n- As of writing, if you want to make sure you have data transactionality and\nschema/metadata transactionality, you should disableline.auto.create.new.columnsandline.auto.create.new.tableson your\nconfiguration. Be aware that if you do this, you will not have dynamic schema\ncapabilities and you will need to create each table and column before you try\nto ingest data, viaCREATE TABLEand/orALTER TABLE ADD COLUMNSQL\nstatements.\n\n## Exactly-once delivery vs at-least-once delivery‚Äã\n\nThe retrying behavior of the HTTP transport can lead to some data being sent to\nthe server more than once.\nExample\n: Client sends a batch to the server, the server receives the batch,\nprocesses it, but fails to send a response back to the client due to a network\nerror. The client will retry sending the batch to the server. This means the\nserver will receive the batch again and process it again. This can lead to\nduplicated rows in the server.\nThe are two ways to mitigate this issue:\n- UseQuestDB deduplication featureto remove\nduplicated rows. QuestDB server can detect and remove duplicated rows\nautomatically, resulting in exactly-once processing. This is recommended when\nusing the HTTP transport with retrying enabled.\n- Disable retrying by settingretry_timeoutto 0. This will make the client\nsend the batch only once, failed requests will not be retried and the client\nwill receive an error. This effectively turns the client into an at-most-once\ndelivery.\n\n## Multiple URLs for High Availability‚Äã\n\nThe ILP client can be configured with multiple possible endpoints to send your data to.\nOnly one will be sent to at any one time.\nnote\nThis feature requires QuestDB OSS 9.1.0+ or Enterprise 3.0.4+. OSS users are discouraged of using\nthis feature, as once data is sent to another primary, there is no way to reconcilliate the\ndiverging instances. QuestDB Enterprise users can leverage this feature to transparently\nhandle replication failover.\nTo configure this feature, simply provide multiple addr entries. For example, when using Java:\n\n```java\ntry (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;addr=localhost:9999;\")) {   // ...}\n```\n\ntip\nAt the moment of writing this guide, only some of the QuestDB clients support multi-url configuration. Please\nrefer to the documentation of your client to make sure it is available.\nOn initialisation, if\nprotocol_version=auto\n, the sender will identify the first instance that is writeable. Then it\nwill stick to this instance and write any subsequent data to it.\nIn the event that the instance becomes unavailable for writes, the client will retry the other possible endpoints. As long\nas one instance becomes writable before the maximum retry timeout is reached, it will stick to it instead. This unvailability is characterised by failures to connect or locate the instance, or the instance returning an error code due to it being read-only.\nBy configuring multiple addresses, you can continue capturing data if your primary instance fails, without having to reconfigure the clients, as they will automatically failover to the new primary once available.\nEnterprise users can use multiple URLs to handle replication failover, without the need to introduce a load-balancer or reconfigure clients.\n\n## Health Check‚Äã\n\nTo monitor your active connection, there is a\nping\nendpoint:\n\n```shell\ncurl -I http://localhost:9000/ping\n```\n\nReturns (pong!):\n\n```shell\nHTTP/1.1 204 OKServer: questDB/1.0Date: Fri, 2 Feb 2024 17:09:38 GMTTransfer-Encoding: chunkedContent-Type: text/plain; charset=utf-8X-Influxdb-Version: v2.7.4\n```\n\nDetermine whether an instance is active and confirm the version of InfluxDB Line\nProtocol with which you are interacting.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2669,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-9bb2b0730050",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/nodejs",
    "title": "Node.js Client Documentation | QuestDB",
    "text": "QuestDB offers Node.js developers a dedicated client designed for efficient and\nhigh-performance data ingestion.\nThe Node.js client has solid benefits:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\nThis quick start guide introduces the basic functionalities of the Node.js\nclient, including setting up a connection, inserting data, and flushing data to\nQuestDB.\nView full docs\nView source code\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized for\nwriting\ndata to QuestDB.\nFor retrieving data, we recommend using a\nPostgreSQL-compatible Node.js library\nor our\nHTTP query endpoint\n.\n\n## Requirements‚Äã\n\n- Node.js v16 or newer.\n- Assumes QuestDB is running. If it's not, refer tothe general quick start.\n\n## Client installation‚Äã\n\nInstall the QuestDB Node.js client via npm:\n\n```shell\nnpm i -s @questdb/nodejs-client\n```\n\n\n## Authentication‚Äã\n\nPassing in a configuration string with basic auth:\n\n```javascript\nconst { Sender } = require(\"@questdb/nodejs-client\");const conf = \"http::addr=localhost:9000;username=admin;password=quest;\"const sender = Sender.fromConfig(conf);    ...\n```\n\nPassing via the\nQDB_CLIENT_CONF\nenv var:\n\n```bash\nexport QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n```\n\n\n```javascript\nconst { Sender } = require(\"@questdb/nodejs-client\");const sender = Sender.fromEnv();    ...\n```\n\nWhen using QuestDB Enterprise, authentication can also be done via REST token.\nPlease check the\nRBAC docs\nfor more\ninfo.\n\n## Basic insert‚Äã\n\nExample: inserting executed trades for cryptocurrencies.\nWithout authentication and using the current timestamp.\n\n```javascript\nconst { Sender } = require(\"@questdb/nodejs-client\")async function run() {  // create a sender using HTTP protocol  const sender = Sender.fromConfig(\"http::addr=localhost:9000\")  // add rows to the buffer of the sender  await sender    .table(\"trades\")    .symbol(\"symbol\", \"ETH-USD\")    .symbol(\"side\", \"sell\")    .floatColumn(\"price\", 2615.54)    .floatColumn(\"amount\", 0.00044)    .atNow()  // flush the buffer of the sender, sending the data to QuestDB  // the buffer is cleared after the data is sent, and the sender is ready to accept new data  await sender.flush()  // close the connection after all rows ingested  // unflushed data will be lost  await sender.close()}run().then(console.log).catch(console.error)\n```\n\nIn this case, the designated timestamp will be the one at execution time. Let's\nsee now an example with an explicit timestamp, custom auto-flushing, and basic\nauth.\n\n```javascript\nconst { Sender } = require(\"@questdb/nodejs-client\")async function run() {  // create a sender using HTTP protocol  const sender = Sender.fromConfig(    \"http::addr=localhost:9000;username=admin;password=quest;auto_flush_rows=100;auto_flush_interval=1000;\",  )  // Calculate the current timestamp. You could also parse a date from your source data.  const timestamp = Date.now()  // add rows to the buffer of the sender  await sender    .table(\"trades\")    .symbol(\"symbol\", \"ETH-USD\")    .symbol(\"side\", \"sell\")    .floatColumn(\"price\", 2615.54)    .floatColumn(\"amount\", 0.00044)    .at(timestamp, \"ms\")  // add rows to the buffer of the sender  await sender    .table(\"trades\")    .symbol(\"symbol\", \"BTC-USD\")    .symbol(\"side\", \"sell\")    .floatColumn(\"price\", 39269.98)    .floatColumn(\"amount\", 0.001)    .at(timestamp, \"ms\")  // flush the buffer of the sender, sending the data to QuestDB  // the buffer is cleared after the data is sent, and the sender is ready to accept new data  await sender.flush()  // close the connection after all rows ingested  // unflushed data will be lost  await sender.close()}run().then(console.log).catch(console.error)\n```\n\nAs you can see, both events now are using the same timestamp. We recommended to\nuse the original event timestamps when ingesting data into QuestDB. Using the\ncurrent timestamp hinder the ability to deduplicate rows which is\nimportant for exactly-once processing\n.\n\n## Decimal insertion‚Äã\n\nnote\nDecimal columns are available with ILP protocol version 3 (QuestDB v9.2.0+ and NodeJS client v4.2.0+).\nHTTP/HTTPS connections negotiate this automatically (\nprotocol_version=auto\n), while TCP/TCPS connections must opt in explicitly (for example\ntcp::...;protocol_version=3\n). Once on v3, you can choose between the textual helper and the binary helper.\ncaution\nQuestDB does not auto-create decimal columns. Define them ahead of ingestion with\nDECIMAL(precision, scale)\nso the server knows how many digits to store, as explained in the\ndecimal data type\nguide.\n\n### Text literal (easy to use)‚Äã\n\n\n```typescript\nimport { Sender } from \"@questdb/nodejs-client\";async function runDecimalsText() {  const sender = await Sender.fromConfig(    \"tcp::addr=localhost:9009;protocol_version=3\",  );  await sender    .table(\"fx\")    .symbol(\"pair\", \"EURUSD\")    .decimalColumnText(\"mid\", \"1.234500\") // keeps trailing zeros    .atNow();  await sender.flush();  await sender.close();}\n```\n\ndecimalColumnText\naccepts strings or numbers. String literals go through\nvalidateDecimalText\nand are written verbatim with the\nd\nsuffix, so every digit (including trailing zeros or exponent form) is preserved. Passing a number is convenient, but JavaScript‚Äôs default formatting will drop insignificant zeros.\n\n### Binary form (high throughput)‚Äã\n\n\n```typescript\nconst sender = await Sender.fromConfig(  \"tcp::addr=localhost:9009;protocol_version=3\",);const scale = 4;const notional = 12345678901234567890n; // represents 1_234_567_890_123_456.7890await sender  .table(\"positions\")  .symbol(\"desk\", \"ny\")  .decimalColumnUnscaled(\"notional\", notional, scale)  .atNow();await sender.flush();await sender.close();\n```\n\ndecimalColumnUnscaled\nconverts\nBigInt\ninputs into the ILP v3 binary payload. You can also pass an\nInt8Array\nif you already have a two‚Äôs-complement, big-endian byte\narray. The scale must stay between 0 and 76, and payloads wider than 32 bytes are rejected up front. This binary path keeps rows compact, making it the preferred option for high-performance feeds.\n\n## Configuration options‚Äã\n\nThe minimal configuration string needs to have the protocol, host, and port, as\nin:\n\n```questdb-sql\nhttp::addr=localhost:9000;\n```\n\nFor all the extra options you can use, please check\nthe client docs\nAlternatively, for a breakdown of Configuration string options available across\nall clients, see the\nConfiguration string\npage.\n\n## Next Steps‚Äã\n\nPlease refer to the\nILP overview\nfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\nDive deeper into the Node.js client capabilities, including TypeScript and\nWorker Threads examples, by exploring the\nGitHub repository\n.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nShould you encounter any issues or have questions, the\nCommunity Forum\nis a vibrant platform for\ndiscussions.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 938,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-a04ef250d382",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/java",
    "title": "Java Client Documentation | QuestDB",
    "text": "note\nThis is the reference for the QuestDB Java Client when QuestDB is used as a\nserver.\nFor embedded QuestDB, please check our\nJava Embedded Guide\n.\nThe QuestDB Java client is distributed as a separate Maven artifact\n(\norg.questdb:questdb-client\n).\nThe client provides the following benefits:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized\nfor\nwriting\ndata to QuestDB. For retrieving data, we recommend using a\nPostgreSQL-compatible Java library\nor our\nHTTP query endpoint\n.\n\n## Quick start‚Äã\n\nAdd the QuestDB Java client as a dependency in your project's build configuration file.\n- Maven\n- Gradle\n\n```xml\n<dependency><groupId>org.questdb</groupId><artifactId>questdb-client</artifactId><version>1.0.1</version></dependency>\n```\n\n\n```text\nimplementation 'org.questdb:questdb-client:1.0.1'\n```\n\nThe code below creates a client instance configured to use HTTP transport to\nconnect to a QuestDB server running on localhost, port 9000. It then sends two\nrows, each containing one symbol and two floating-point values. The client asks\nthe server to assign a timestamp to each row based on the server's wall-clock\ntime.\n\n```java\npackage com.example.sender;import io.questdb.client.Sender;public class HttpExample {    public static void main(String[] args) {        try (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;\")) {            sender.table(\"trades\")                    .symbol(\"symbol\", \"ETH-USD\")                    .symbol(\"side\", \"sell\")                    .doubleColumn(\"price\", 2615.54)                    .doubleColumn(\"amount\", 0.00044)                    .atNow();            sender.table(\"trades\")                    .symbol(\"symbol\", \"TC-USD\")                    .symbol(\"side\", \"sell\")                    .doubleColumn(\"price\", 39269.98)                    .doubleColumn(\"amount\", 0.001)                    .atNow();        }    }}\n```\n\nThe client is configured using a configuration string. See\nWays to create the client\nfor all configuration\nmethods, and\nConfiguration options\nfor available\nsettings.\n\n## Authenticate and encrypt‚Äã\n\nThis sample configures the client to use HTTP transport with TLS enabled for a\nconnection to a QuestDB server. It also instructs the client to authenticate\nusing HTTP Basic Authentication.\nWhen using QuestDB Enterprise, you can authenticate using a REST bearer token as\nwell. Please check the\nRBAC docs\nfor\nmore info.\n\n```java\npackage com.example.sender;import io.questdb.client.Sender;public class HttpsAuthExample {    public static void main(String[] args) {        try (Sender sender = Sender.fromConfig(\"https::addr=localhost:9000;username=admin;password=quest;\")) {            sender.table(\"trades\")                    .symbol(\"symbol\", \"ETH-USD\")                    .symbol(\"side\", \"sell\")                    .doubleColumn(\"price\", 2615.54)                    .doubleColumn(\"amount\", 0.00044)                    .atNow();            sender.table(\"trades\")                    .symbol(\"symbol\", \"TC-USD\")                    .symbol(\"side\", \"sell\")                    .doubleColumn(\"price\", 39269.98)                    .doubleColumn(\"amount\", 0.001)                    .atNow();        }    }}\n```\n\n\n## Ways to create the client‚Äã\n\nThere are three ways to create a client instance:\n- From a configuration string.This is the most common way to create a\nclient instance. It describes the entire client configuration in a single\nstring, and allows sharing the same configuration across clients in different\nlanguages. The general format is:<protocol>::<key>=<value>;<key>=<value>;...;Transport protocolcan be one of these:http‚Äî ILP/HTTPhttps‚Äî ILP/HTTP with TLS encryptiontcp‚Äî ILP/TCPtcps‚Äî ILP/TCP with TLS encryptionThe keyaddrsets the hostname and port of the QuestDB server. Port\ndefaults to 9000 for HTTP(S) and 9009 for TCP(S). The minimum configuration\nincludes the transport and the address.try (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;auto_flush_rows=5000;retry_timeout=10000;\")) {// ...}For all available options, seeConfiguration options.\n- From an environment variable.TheQDB_CLIENT_CONFenvironment variable\nis used to set the configuration string. Moving configuration parameters to\nan environment variable allows you to avoid hard-coding sensitive information\nsuch as tokens and passwords in your code.export QDB_CLIENT_CONF=\"http::addr=localhost:9000;auto_flush_rows=5000;retry_timeout=10000;\"try (Sender sender = Sender.fromEnv()) {// ...}\n- Using the Java builder API.This provides type-safe configuration.try (Sender sender = Sender.builder(Sender.Transport.HTTP).address(\"localhost:9000\").autoFlushRows(5000).retryTimeoutMillis(10000).build()) {// ...}\n\n## Configuring multiple URLs‚Äã\n\nnote\nThis feature requires QuestDB OSS 9.1.0+ or Enterprise 3.0.4+.\nThe ILP client can be configured with multiple\npossible\nendpoints to send your data to. Only one endpoint is used at\na time.\nTo configure this feature, simply provide multiple\naddr\nentries. For example:\n\n```java\ntry (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;addr=localhost:9999;\")) {   // ...}\n```\n\nOn initialisation, if\nprotocol_version=auto\n, the sender will identify the first instance that is writeable. Then it will\nstick\nto this instance and write\nany subsequent data to it.\nIn the event that the instance becomes unavailable for writes, the client will retry the other possible endpoints, and when it finds\na new writeable instance, will\nstick\nto it instead. This unavailability is characterised by failures to connect or locate the instance,\nor the instance returning an error code due to it being read-only.\nBy configuring multiple addresses, you can continue to capture data if your primary instance\nfails, without having to reconfigure the clients. This backup instance can be hot or cold, and so long as it is assigned a known address, it will be written to as soon as it is started.\nEnterprise users can leverage this feature to transparently handle replication failover, without the need to introduce a load-balancer or\nreconfigure clients.\ntip\nYou may wish to increase the value of\nretry_timeout\nif you expect your backup instance to take a large amount of time to become writeable.\nFor example, when performing a primary migration (Enterprise replication), with default settings, you might want to increase this\nto\n30s\nor higher.\n\n## General usage pattern‚Äã\n\n- Create a client instance viaSender.fromConfig().\n- Usetable(CharSequence)to select a table for inserting a new row.\n- Usesymbol(CharSequence, CharSequence)to add all symbols. You must add\nsymbols before adding other column types.\n- Use the following options to add all the remaining columns:stringColumn(CharSequence, CharSequence)longColumn(CharSequence, long)doubleColumn(CharSequence, double)boolColumn(CharSequence, boolean)arrayColumn()-- several variants, see belowtimestampColumn(CharSequence, Instant), ortimestampColumn(CharSequence, long, ChronoUnit)decimalColumn(CharSequence, Decimal256)ordecimalColumn(CharSequence, CharSequence)(string literal)\ncaution\nDecimal values require QuestDB version 9.2.0 or later.\nCreate decimal columns ahead of time with\nDECIMAL(precision, scale)\nso QuestDB can ingest the values\nwith the expected precision. See the\ndecimal data type\npage for a refresher on\nprecision and scale.\n- Useat(Instant)orat(long timestamp, ChronoUnit unit)oratNow()to\nset a designated timestamp.\n- Optionally: You can useflush()to send locally buffered data into a\nserver.\n- Repeat from step 2 to start a new row.\n- Useclose()to dispose the Sender after you no longer need it.\n\n## Ingest arrays‚Äã\n\nTo ingest a 1D or 2D array, simply construct a Java array of the appropriate\ntype (\ndouble[]\n,\ndouble[][]\n) and supply it to the\narrayColumn()\nmethod. In\norder to avoid GC overheads, create the array instance once, and then populate\nit with the data of each row.\nFor arrays of higher dimensionality, use the\nDoubleArray\nclass. Here's a basic\nexample for a 3D array:\n\n```java\n// or \"tcp::addr=localhost:9009;protocol_version=2;\"try (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;\");     DoubleArray ary = new DoubleArray(3, 3, 3);) {    for (int i = 0; i < ROW_COUNT; i++) {        for (int value = 0; value < 3 * 3 * 3; value++) {            ary.append(value);        }        sender.table(\"tango\")              .doubleArray(\"array\", ary)              .at(getTimestamp(), ChronoUnit.MICROS);    }}\n```\n\nThe\nary.append(value)\nmethod allows you to populate the array in the row-major\norder, without having to compute every coordinate individually. You can also use\nary.set(value, coords...)\nto set a value at specific coordinates.\nnote\nArrays are supported from QuestDB version 9.0.0, and require updated\nclient libraries.\n\n## Flush the buffer‚Äã\n\nThe client accumulates the data into an internal buffer and doesn't immediately\nsend it to the server. It can flush the buffer to the server either\nautomatically or on explicit request.\n\n### Flush explicitly‚Äã\n\nYou can configure the client to not use automatic flushing, and issue explicit\nflush requests by calling\nsender.flush()\n:\n\n```java\ntry (Sender sender = Sender.fromConfig(\"http::addr=localhost:9000;auto_flush=off\")) {    sender.table(\"trades\")          .symbol(\"symbol\", \"ETH-USD\")          .symbol(\"side\", \"sell\")          .doubleColumn(\"price\", 2615.54)          .doubleColumn(\"amount\", 0.00044)          .atNow();    sender.table(\"trades\")          .symbol(\"symbol\", \"BTC-USD\")          .symbol(\"side\", \"sell\")          .doubleColumn(\"price\", 39269.98)          .doubleColumn(\"amount\", 0.001)          .atNow();    sender.flush();}\n```\n\nnote\nCalling\nsender.flush()\nwill flush the buffer even with auto-flushing enabled,\nbut this isn't a typical way to use the client.\n\n### Flush automatically‚Äã\n\nBy default, the client automatically flushes the buffer according to a simple\npolicy. With HTTP, it will automatically flush at the time you append a new\nrow, if either of these has become true:\n- reached 75,000 rows\n- hasn't been flushed for 1 second\nBoth parameters can be customized in order to achieve a good tradeoff between\nthroughput (large batches) and latency (small batches).\nThis configuration string will cause the client to auto-flush every 10 rows or\nevery 10 seconds, whichever comes first:\nhttp::addr=localhost:9000;auto_flush_rows=10;auto_flush_interval=10000;\nWith TCP, the client flushes its internal buffer whenever it gets full.\nThe client will also flush automatically when it is being closed and there's\nstill some data in the buffer. However,\nif the network operation fails at this\ntime, the client won't retry it.\nAlways explicitly flush the buffer before\nclosing the client.\n\n## Error handling‚Äã\n\nHTTP automatically retries failed, recoverable requests: network errors, some\nserver errors, and timeouts. Non-recoverable errors include invalid data,\nauthentication errors, and other client-side errors.\nnote\nIf you have configured multiple addresses, retries will be run against different instances.\nRetrying is especially useful during transient network issues or when the server\ngoes offline for a short period. Configure the retrying behavior through the\nretry_timeout\nconfiguration option or via the builder API with\nretryTimeoutMillis(long timeoutMillis)\n. The client continues to retry after\nrecoverable errors until it either succeeds or the specified timeout expires. If\nit hits the timeout without success, the client throws a\nLineSenderException\n.\nThe client won't retry requests while it's being closed and attempting to flush\nthe data left over in the buffer.\nThe TCP transport has no mechanism to notify the client it encountered an\nerror; instead it just disconnects. When the client detects this, it throws a\nLineSenderException\nand becomes unusable.\n\n## Recover after a client-side error‚Äã\n\nWith HTTP transport, the client always prepares a full row in RAM before trying\nto send it. It also remains usable after an exception has occurred. This allows\nyou to cancel sending a row, for example due to a validation error, and go on\nwith the next row.\nWith TCP transport, you don't have this option. If you get an exception, you\ncan't continue with the same client instance, and don't have insight into which\nrows were accepted by the server.\ncaution\nError handling behaviour changed with the release of QuestDB 9.1.0.\nPreviously, failing all retries would cause an exception and release the buffered data.\nNow the buffer will not be released. If you wish to re-use the same sender with fresh data, you must call the\nnew\nreset()\nfunction.\n\n## Designated timestamp considerations‚Äã\n\nThe concept of\ndesignated timestamp\nis\nimportant when ingesting data into QuestDB.\nThere are two ways to assign a designated timestamp to a row:\n- User-assigned timestamp: the client assigns a specific timestamp to the row.java.time.Instant timestamp = Instant.now(); // or any other timestampsender.table(\"trades\").symbol(\"symbol\", \"ETH-USD\").symbol(\"side\", \"sell\").doubleColumn(\"price\", 2615.54).doubleColumn(\"amount\", 0.00044).at(timestamp);TheInstantclass is part of thejava.timepackage and is used to\nrepresent a specific moment in time. Thesender.at()method can accept a\nlong timestamp representing the elapsed time since the beginning of theUnix epoch, as well as aChronoUnitto specify the time unit. This approach is useful in\nhigh-throughput scenarios where instantiating anInstantobject for each\nrow is not feasible due to performance considerations.\n- Server-assigned timestamp: the server automatically assigns a timestamp to\nthe row based on the server's wall-clock time at the time of ingesting the\nrow. Example:sender.table(\"trades\").symbol(\"symbol\", \"ETH-USD\").symbol(\"side\", \"sell\").doubleColumn(\"price\", 2615.54).doubleColumn(\"amount\", 0.00044).atNow();\nWe recommend using the event's original timestamp when ingesting data into\nQuestDB. Using ingestion-time timestamps precludes the ability to deduplicate\nrows, which is\nimportant for exactly-once processing\n.\nnote\nQuestDB works best when you send data in chronological order (sorted by\ntimestamp).\n\n## Protocol Version‚Äã\n\nTo enhance data ingestion performance, QuestDB\nversion 9.0.0\nintroduced an\nupgraded version \"2\" to the text-based InfluxDB Line Protocol which encodes\narrays and f64 values in binary form. Arrays are supported only in this upgraded\nprotocol version.\nYou can select the protocol version with the\nprotocol_version\nsetting in the\nconfiguration string.\nHTTP transport automatically negotiates the protocol version by default.\nIn order to avoid the slight latency cost at connection time, you can explicitly\nconfigure the protocol version by setting\nprotocol_version=2|1;\n.\nTCP transport does not negotiate the protocol version and uses version 1 by\ndefault. You must explicitly set\nprotocol_version=2;\nin order to ingest\narrays, as in this example:\n\n```text\ntcp::addr=localhost:9009;protocol_version=2;\n```\n\n\n## Configuration options‚Äã\n\nClient can be configured either by using a configuration string as shown in the\nexamples above, or by using the builder API.\nThe builder API is available via the\nSender.builder(Transport transport)\nmethod.\nFor a breakdown of available options, see the\nConfiguration string\npage.\n\n## Compatible JDKs‚Äã\n\nThe client relies on some JDK internal libraries, which certain specialised JDK\nofferings may not support.\nHere is a list of known incompatible JDKs:\n- Azul Zing 17A fix is in progress. You can use Azul Zulu 17 in the meantime.\n\n## Other considerations‚Äã\n\n- Refer to theILP overviewfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\n- The methodflush()can be called to force sending the internal buffer to a\nserver, even when the buffer is not full yet.\n- The Sender is not thread-safe. For multiple threads to send data to QuestDB,\neach thread should have its own Sender instance. An object pool can also be\nused to re-use Sender instances.\n- The Sender instance has to be closed after it is no longer in use. The Sender\nimplements thejava.lang.AutoCloseableinterface, and therefore thetry-with-resourcepattern can be used to ensure that the Sender is closed.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2160,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-627f70f08320",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/go",
    "title": "Go Client Documentation | QuestDB",
    "text": "QuestDB supports the Go ecosystem, offering a Go client designed for\nhigh-performance data ingestion, tailored specifically for insert-only\noperations. This combination of QuestDB and its Go client provides exceptional\ntime series data ingestion and analytical capabilities.\nThe Go client introduces several advantages:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\nThis quick start guide will help you get up and running with the basic\nfunctionalities of the Go client, covering connection setup, authentication, and\nsome common insert patterns.\nView full docs\nView source code\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized for\nwriting\ndata to QuestDB.\nFor retrieving data, we recommend using a\nPostgreSQL-compatible Go library\nor our\nHTTP query endpoint\n.\n\n## Requirements‚Äã\n\n- Requires Go 1.19 or later.\n- Assumes QuestDB is running. If it's not, refer tothe general quick start.\n\n## Client Installation‚Äã\n\nTo add the QuestDB client to your Go project:\n\n```toml\ngo get github.com/questdb/go-questdb-client/\n```\n\n\n## Authentication‚Äã\n\nPassing in a configuration string with HTTP basic authentication:\n\n```go\npackage mainimport (\t\"context\"\t\"github.com/questdb/go-questdb-client/v4\")func main() {\tctx := context.TODO()\tclient, err := questdb.LineSenderFromConf(ctx, \"http::addr=localhost:9000;username=admin;password=quest;\")\tif err != nil {\t\tpanic(\"Failed to create client\")\t}\t// Utilize the client for your operations...}\n```\n\nOr, set the QDB_CLIENT_CONF environment variable and call\nquestdb.LineSenderFromEnv()\n.\n- Export the configuration string as an environment variable:export QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n- Then in your Go code:client,err:=questdb.LineSenderFromEnv(context.TODO())\nAlternatively, you can use the built-in Go API to specify the connection\noptions.\n\n```go\npackage mainimport (       \"context\"       qdb \"github.com/questdb/go-questdb-client/v4\")func main() {       ctx := context.TODO()       client, err := qdb.NewLineSender(context.TODO(), qdb.WithHttp(), qdb.WithAddress(\"localhost:9000\"), qdb.WithBasicAuth(\"admin\", \"quest\"))\n```\n\nWhen using QuestDB Enterprise, authentication can also be done via REST token.\nPlease check the\nRBAC docs\nfor more\ninfo.\n\n## Basic Insert‚Äã\n\nExample: inserting executed trades for cryptocurrencies.\nWithout authentication and using the current timestamp:\n\n```go\npackage mainimport (\t\"context\"\t\"github.com/questdb/go-questdb-client/v4\")func main() {\tctx := context.TODO()\tclient, err := questdb.LineSenderFromConf(ctx, \"http::addr=localhost:9000;\")\tif err != nil {\t\tpanic(\"Failed to create client\")\t}\terr = client.Table(\"trades\").\t\tSymbol(\"symbol\", \"ETH-USD\").\t\tSymbol(\"side\", \"sell\").\t\tFloat64Column(\"price\", 2615.54).\t\tFloat64Column(\"amount\", 0.00044).\t\tAtNow(ctx)\tif err != nil {\t\tpanic(\"Failed to insert data\")\t}\terr = client.Flush(ctx)\tif err != nil {\t\tpanic(\"Failed to flush data\")\t}}\n```\n\nIn this case, the designated timestamp will be the one at execution time. Let's\nsee now an example with an explicit timestamp, custom auto-flushing, and basic\nauth.\n\n```go\npackage mainimport (\t\"context\"\t\"github.com/questdb/go-questdb-client/v4\"\t\"time\")func main() {\tctx := context.TODO()\tclient, err := questdb.LineSenderFromConf(ctx, \"http::addr=localhost:9000;username=admin;password=quest;auto_flush_rows=100;auto_flush_interval=1000;\")\tif err != nil {\t\tpanic(\"Failed to create client\")\t}\ttimestamp := time.Now()\terr = client.Table(\"trades\").\t\tSymbol(\"symbol\", \"ETH-USD\").\t\tSymbol(\"side\", \"sell\").\t\tFloat64Column(\"price\", 2615.54).\t\tFloat64Column(\"amount\", 0.00044).\t\tAt(ctx, timestamp)\tif err != nil {\t\tpanic(\"Failed to insert data\")\t}\terr = client.Flush(ctx)\t// You can flush manually at any point.\t// If you don't flush manually, the client will flush automatically\t// when a row is added and either:\t//   * The buffer contains 75000 rows (if HTTP) or 600 rows (if TCP)\t//   * The last flush was more than 1000ms ago.\t// Auto-flushing can be customized via the `auto_flush_..` params.\tif err != nil {\t\tpanic(\"Failed to flush data\")\t}}\n```\n\nWe recommended to use User-assigned timestamps when ingesting data into QuestDB.\nUsing the current timestamp hinder the ability to deduplicate rows which is\nimportant for exactly-once processing\n.\n\n## Configuration options‚Äã\n\nThe minimal configuration string needs to have the protocol, host, and port, as\nin:\n\n```questdb-sql\nhttp::addr=localhost:9000;\n```\n\nIn the Go client, you can set the configuration options via the standard config\nstring, which is the same across all clients, or using\nthe built-in API\n.\nFor all the extra options you can use, please check\nthe client docs\nAlternatively, for a breakdown of Configuration string options available across\nall clients, see the\nConfiguration string\npage.\n\n## Next Steps‚Äã\n\nPlease refer to the\nILP overview\nfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\nExplore the full capabilities of the Go client via\nGo.dev\n.\nWith data flowing into QuestDB, now it's time to for analysis.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nAlone? Stuck? Want help? Visit us in our\nCommunity Forum\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 741,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-819ca3cbd288",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/python",
    "title": "Python Client Documentation | QuestDB",
    "text": "QuestDB supports the Python ecosystem.\nThe QuestDB Python client provides ingestion high performance and is insert\nonly.\nThe client, in combination with QuestDB, offers peak performance time-series\ningestion and analysis.\nApart from blazing fast ingestion, our clients provide these key benefits:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\nThis quick start will help you get started.\nIt covers basic connection, authentication and some insert patterns.\nView full docs\nView source code\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized for\nwriting\ndata to QuestDB.\nFor retrieving data, we recommend using a\nPostgreSQL-compatible Python library\nor our\nHTTP query endpoint\n.\n\n## Requirements‚Äã\n\nRequires Python >= 3.8 Assumes QuestDB is running. Not running? See the\ngeneral quick start\n.\n\n## Client installation‚Äã\n\nTo install the client (or update it) globally:\n\n```bash\npython3 -m pip install -U questdb\n```\n\nOr, from from within a virtual environment:\n\n```bash\npip install -U questdb\n```\n\nIf you‚Äôre using poetry, you can add questdb as a dependency:\n\n```bash\npoetry add questdb\n```\n\nOr to update the dependency:\n\n```bash\npoetry update questdb\n```\n\nUsing dataframes?\nAdd following dependencies:\n- pandas\n- pyarrow\n- numpy\n\n## Authentication‚Äã\n\nPassing in a configuration string with basic auth:\n\n```python\nfrom questdb.ingress import Senderconf = \"http::addr=localhost:9000;username=admin;password=quest;\"with Sender.from_conf(conf) as sender:    ...\n```\n\nPassing via the\nQDB_CLIENT_CONF\nenv var:\n\n```bash\nexport QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n```\n\n\n```python\nfrom questdb.ingress import Senderwith Sender.from_env() as sender:    ...\n```\n\n\n```python\nfrom questdb.ingress import Sender, Protocolwith Sender(Protocol.Http, 'localhost', 9000, username='admin', password='quest') as sender:\n```\n\nWhen using QuestDB Enterprise, authentication can also be done via REST token.\nPlease check the\nRBAC docs\nfor more\ninfo.\n\n## Basic insert‚Äã\n\nBasic insertion (no-auth):\n\n```python\nfrom questdb.ingress import Sender, TimestampNanosconf = f'http::addr=localhost:9000;'with Sender.from_conf(conf) as sender:    sender.row(        'trades',        symbols={'symbol': 'ETH-USD', 'side': 'sell'},        columns={'price': 2615.54, 'amount': 0.00044},        at=TimestampNanos.now())    sender.row(        'trades',        symbols={'symbol': 'BTC-USD', 'side': 'sell'},        columns={'price': 39269.98, 'amount': 0.001},        at=TimestampNanos.now())    sender.flush()\n```\n\nIn this case, the designated timestamp will be the one at execution time. Let's\nsee now an example with timestamps, custom auto-flushing, basic auth, and error\nreporting.\n\n```python\nfrom questdb.ingress import Sender, IngressError, TimestampNanosimport sysimport datetimedef example():    try:        conf = (            'http::addr=localhost:9000;'            'username=admin;password=quest;'            'auto_flush_rows=100;auto_flush_interval=1000;')        with Sender.from_conf(conf) as sender:            # Record with provided designated timestamp (using the 'at' param)            # Notice the designated timestamp is expected in Nanoseconds,            # but timestamps in other columns are expected in Microseconds.            # You can use the TimestampNanos or TimestampMicros classes,            # or you can just pass a datetime object            sender.row(                'trades',                symbols={                    'symbol': 'ETH-USD',                    'side': 'sell'},                columns={                    'price': 2615.54,                    'amount': 0.00044},                at=datetime.datetime(                    2022, 3, 8, 18, 53, 57, 609765,                    tzinfo=datetime.timezone.utc))            # You can call `sender.row` multiple times inside the same `with`            # block. The client will buffer the rows and send them in batches.            # You can flush manually at any point.            sender.flush()            # If you don't flush manually, the client will flush automatically            # when a row is added and either:            #   * The buffer contains 75000 rows (if HTTP) or 600 rows (if TCP)            #   * The last flush was more than 1000ms ago.            # Auto-flushing can be customized via the `auto_flush_..` params.        # Any remaining pending rows will be sent when the `with` block ends.    except IngressError as e:        sys.stderr.write(f'Got error: {e}\\n')if __name__ == '__main__':    example()\n```\n\nWe recommended\nUser\n-assigned timestamps when ingesting data into QuestDB.\nUsing\nServer\n-assigned timestamps hinders the ability to deduplicate rows which\nis\nimportant for exactly-once processing\n.\nThe same\ntrades\ninsert, but via a Pandas dataframe:\n\n```python\nimport pandas as pdfrom questdb.ingress import Senderdf = pd.DataFrame({    'symbol': pd.Categorical(['ETH-USD', 'BTC-USD']),    'side': pd.Categorical(['sell', 'sell']),    'price': [2615.54, 39269.98],    'amount': [0.00044, 0.001],    'timestamp': pd.to_datetime(['2022-03-08T18:03:57.609765Z', '2022-03-08T18:03:57.710419Z'])})conf = f'http::addr=localhost:9000;'with Sender.from_conf(conf) as sender:    sender.dataframe(df, table_name='trades', at=TimestampNanos.now())\n```\n\nNote that you can also add a column of your dataframe with your timestamps and\nreference that column in the\nat\nparameter:\n\n```python\nimport pandas as pdfrom questdb.ingress import Senderdf = pd.DataFrame({    'symbol': pd.Categorical(['ETH-USD', 'BTC-USD']),    'side': pd.Categorical(['sell', 'sell']),    'price': [2615.54, 39269.98],    'amount': [0.00044, 0.001],    'timestamp': pd.to_datetime(['2022-03-08T18:03:57.609765Z', '2022-03-08T18:03:57.710419Z'])})conf = f'http::addr=localhost:9000;'with Sender.from_conf(conf) as sender:    sender.dataframe(df, table_name='trades', at='timestamp')\n```\n\n\n## Insert numpy.ndarray‚Äã\n\nNumPy arrays of\ndtype=numpy.float64\nmay be inserted either row-by-row or as objects inside a dataframe.\nnote\nArrays are supported from QuestDB version 9.0.0, and require updated\nclient libraries.\nnote\nOther types such as\nlist\n,\narray.array\n,\ntorch.Tensor\nand other objects\naren't supported directly and must first be converted to NumPy arrays.\nIn the two examples below, we insert some FX order\nbook data.\n- bidsandasks: 2D arrays of L2 order book depth. Each level contains price and volume.\n- bids_exec_probsandasks_exec_probs: 1D arrays of calculated execution probabilities for the next minute.\n\n### Direct Array Insertion‚Äã\n\n\n```python\nfrom questdb.ingress import Sender, TimestampNanosimport numpy as npconf = f'http::addr=localhost:9000;'with Sender.from_conf(conf) as sender:    sender.row(        'fx_order_book',        symbols={            'symbol': 'EUR/USD'        },        columns={            'bids': np.array(                [                    [1.0850, 600000],                    [1.0849, 300000],                    [1.0848, 150000]                ],                dtype=np.float64            ),            'asks': np.array(                [                    [1.0853, 500000],                    [1.0854, 250000],                    [1.0855, 125000]                ],                dtype=np.float64            )        },        at=TimestampNanos.now())    sender.flush()\n```\n\n\n### DataFrame Insertion‚Äã\n\n\n```python\nimport pandas as pdfrom questdb.ingress import Senderimport numpy as npdf = pd.DataFrame({    'symbol': [        'EUR/USD',        'GBP/USD'    ]    'bids': [        np.array(            [                [1.0850, 600000],                [1.0849, 300000],                [1.0848, 150000]            ],            dtype=np.float64        ),        np.array(            [                [1.3200, 550000],                [1.3198, 275000],                [1.3196, 130000]            ],            dtype=np.float64        )    ],    'asks': [        np.array(            [                [1.0853, 500000],                [1.0854, 250000],                [1.0855, 125000]            ],            dtype=np.float64        ),        np.array(            [                [1.3203, 480000],                [1.3205, 240000],                [1.3207, 120000]            ],            dtype=np.float64        )    ],    'timestamp': pd.to_datetime([        '2022-03-08T18:03:57.609765Z',        '2022-03-08T18:03:57.710419Z'    ])})# or 'tcp::addr=localhost:9009;protocol_version=2;'conf = 'http::addr=localhost:9000;'with Sender.from_conf(conf) as sender:    sender.dataframe(        df,        table_name='fx_order_book',        at='timestamp')\n```\n\nnote\nThe example above uses ILP/HTTP. If instead you're using ILP/TCP you'll need\nto explicity opt into the newer protocol version 2 that supports sending arrays.\n\n```questdb-sql\ntcp::addr=127.0.0.1:9009;protocol_version=2;\n```\n\nProtocol Version 2 along with its support for arrays is available from QuestDB\nversion 9.0.0.\n\n## Configuration options‚Äã\n\nThe minimal configuration string needs to have the protocol, host, and port, as in:\n\n```questdb-sql\nhttp::addr=localhost:9000;\n```\n\nIn the Python client, you can set the configuration options via the standard\nconfig string, which is the same across all clients, or using\nthe built-in API\n.\nFor all the extra options you can use, please check\nthe client docs\nAlternatively, for a breakdown of Configuration string options available across\nall clients, see the\nConfiguration string\npage.\n\n## Transactional flush‚Äã\n\nAs described at the\nILP overview\n, the\nHTTP transport has some support for transactions.\nThe python client exposes\nan API\nto make working with transactions more convenient\n\n## Next steps‚Äã\n\nPlease refer to the\nILP overview\nfor general\ndetails about transactions, error control, delivery guarantees, health check, or\ntable and column auto-creation. The\nPython client docs\nexplain how to apply those concepts using the built-in API.\nFor full docs, checkout\nReadTheDocs\n.\nWith data flowing into QuestDB, now it's time to for analysis.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nAlone? Stuck? Want help? Visit us in our\nCommunity Forum\n.\n\n## Additional resources‚Äã\n\n- QuestDB Python clients guide\n- Integration with Polars\n- Integration with Pandas",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1198,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-0721e478a67d",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/operations/logging-metrics",
    "title": "Logging and metrics | QuestDB",
    "text": "This page outlines logging in QuestDB. It covers how to configure logs via\nlog.conf\nand expose metrics via Prometheus.\n- Logging\n- Metrics\n\n## Log location‚Äã\n\nQuestDB creates the following file structure in its\nroot_directory\n:\n\n```filestructure\nquestdb‚îú‚îÄ‚îÄ conf‚îú‚îÄ‚îÄ db‚îú‚îÄ‚îÄ log‚îú‚îÄ‚îÄ public‚îî‚îÄ‚îÄ snapshot (optional)\n```\n\nLog files are stored in the\nlog\nfolder:\n\n```filestructure\n‚îú‚îÄ‚îÄ log‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stdout-2020-04-15T11-59-59.txt‚îÇ¬†¬† ‚îî‚îÄ‚îÄ stdout-2020-04-12T13-31-22.txt\n```\n\n\n## Understanding log levels‚Äã\n\nQuestDB provides the following types of log information:\n\n| Type | Marker | Details | Default |\n| --- | --- | --- | --- |\n| Advisory | A | Startup information such as hosts, listening ports, etc. Rarely used after startup | Enabled |\n| Critical | C | Internal database errors. Serious issues. Things that should not happen in general operation. | Enabled |\n| Error | E | An error, usually (but not always) caused by a user action such as inserting asymbolinto atimestampcolumn. For context on how this error happened, check for Info-level messages logged before the error. | Enabled |\n| Info | I | Logs for activities. Info-level messages often provide context for an error if one is logged later. | Enabled |\n| Debug | D | Finer details on what is happening. Useful to debug issues. | Disabled |\n\nFor more information, see the\nQuestDB source code\n.\n\n### Example log messages‚Äã\n\nAdvisory:\n\n```questdb-sql\n2023-02-24T14:59:45.076113Z A server-main Config:2023-02-24T14:59:45.076130Z A server-main  - http.enabled : true2023-02-24T14:59:45.076144Z A server-main  - tcp.enabled  : true2023-02-24T14:59:45.076159Z A server-main  - pg.enabled   : true\n```\n\nCritical:\n\n```questdb-sql\n2022-08-08T11:15:13.040767Z C i.q.c.p.WriterPool could not open [table=`sys.text_import_log`, thread=1, ex=could not open read-write [file=/opt/homebrew/var/questdb/db/sys.text_import_log/_todo_], errno=13]\n```\n\nError:\n\n```questdb-sql\n2023-02-24T14:59:45.059012Z I i.q.c.t.t.InputFormatConfiguration loading input format config [resource=/text_loader.json]2023-03-20T08:38:17.076744Z E i.q.c.l.u.AbstractLineProtoUdpReceiver could not set receive buffer size [fd=140, size=8388608, errno=55]\n```\n\nInfo:\n\n```questdb-sql\n2020-04-15T16:42:32.879970Z I i.q.c.TableReader new transaction [txn=2, transientRowCount=1, fixedRowCount=1, maxTimestamp=1585755801000000, attempts=0]2020-04-15T16:42:32.880051Z I i.q.g.FunctionParser call to_timestamp('2020-05-01:15:43:21','yyyy-MM-dd:HH:mm:ss') -> to_timestamp(Ss)\n```\n\nDebug:\n\n```questdb-sql\n2023-03-31T11:47:05.723715Z D i.q.g.FunctionParser call cast(investmentMill,INT) -> cast(Li)2023-03-31T11:47:05.723729Z D i.q.g.FunctionParser call rnd_symbol(4,4,4,2) -> rnd_symbol(iiii)\n```\n\n\n## Logging‚Äã\n\nThe logging behavior of QuestDB may be set in dedicated configuration files or\nby environment variables.\nThis section describes how to configure logging using these methods.\n\n### Enable debug log‚Äã\n\nQuestDB\nDEBUG\nlogging can be set globally.\n- Provide the java option-Debugon startup\n- Setting theQDB_DEBUG=trueas an environment variable\n\n### Configure log.conf‚Äã\n\nLogs may be configured via a dedicated configuration file\nlog.conf\n.\nQuestDB will look for\n/log.conf\nfirst in\nconf/\ndirectory and then on the\nclasspath, unless this name is overridden via a command line property:\n-Dout=/something_else.conf\n.\nQuestDB will create\nconf/log.conf\nusing default values if\n-Dout\nis not set\nand file doesn't exist .\nOn Windows log messages go to depending on run mode :\n- interactive session - console and$dataDir\\log\\stdout-%Y-%m-%dT%H-%M-%S.txt(default is.\\log\\stdout-%Y-%m-%dT%H-%M-%S.txt)\n- service -$dataDir\\log\\service-%Y-%m-%dT%H-%M-%S.txt(default isC:\\Windows\\System32\\qdbroot\\log\\service-%Y-%m-%dT%H-%M-%S.txt)\nThe possible values to enable within the\nlog.conf\nappear as such:\nlog.conf\n\n```shell\n# list of configured writerswriters=file,stdout,http.min# rolling file writerw.file.class=io.questdb.log.LogRollingFileWriterw.file.location=${log.dir}/questdb-rolling.log.${date:yyyyMMdd}w.file.level=INFO,ERRORw.file.rollEvery=dayw.file.rollSize=1g# Optionally, use a single log# w.file.class=io.questdb.log.LogFileWriter# w.file.location=questdb-docker.log# w.file.level=INFO,ERROR,DEBUG# stdoutw.stdout.class=io.questdb.log.LogConsoleWriterw.stdout.level=INFO# min http server, used for error monitoringw.http.min.class=io.questdb.log.LogConsoleWriterw.http.min.level=ERROR## Scope provides specific context for targeted log parsingw.http.min.scope=http-min-server\n```\n\n\n#### Log writer types‚Äã\n\nThere are four types of writer.\nWhich one you need depends on your use case.\n\n| Available writers | Description |\n| --- | --- |\n| file | Select from one of the two above patterns. Write to a single log that will grow indefinitely, or write a rolling log. Rolling logs can be split intominute,hour,day,monthoryear. |\n| stdout | Writes logs to standard output. |\n| http.min | Enabled at port9003by default. For more information, see the next section:minimal HTTP server. |\n\n\n### Minimal HTTP server‚Äã\n\nTo provide a dedicated health check feature that would have no performance knock\non other system components, QuestDB decouples health checks from the REST\nendpoints used for querying and ingesting data. For this purpose, a\nmin\nHTTP\nserver runs embedded in a QuestDB instance and has a separate log and thread\npool configuration.\nThe\nmin\nserver is enabled by default and will reply to any\nHTTP GET\nrequest\nto port\n9003\n:\nGET health status of local instance\n\n```shell\ncurl -v http://127.0.0.1:9003\n```\n\nThe server will respond with an HTTP status code of\n200\n, indicating that the\nsystem is operational:\n200 'OK' response\n\n```shell\n*   Trying 127.0.0.1...* TCP_NODELAY set* Connected to 127.0.0.1 (127.0.0.1) port 9003 (#0)> GET / HTTP/1.1> Host: 127.0.0.1:9003> User-Agent: curl/7.64.1> Accept: */*>< HTTP/1.1 200 OK< Server: questDB/1.0< Date: Tue, 26 Jan 2021 12:31:03 GMT< Transfer-Encoding: chunked< Content-Type: text/plain<* Connection #0 to host 127.0.0.1 left intact\n```\n\nPath segments are ignored which means that optional paths may be used in the URL\nand the server will respond with identical results, e.g.:\nGET health status with arbitrary path\n\n```shell\ncurl -v http://127.0.0.1:9003/status\n```\n\nThe following configuration options can be set in your\nserver.conf\n:\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| http.min.enabled | true | No | Enable or disable Minimal HTTP server. |\n| http.min.bind.to | 0.0.0.0:9003 | No | IPv4 address and port of the server.0means it will bind to all network interfaces, otherwise the IP address must be one of the existing network adapters. |\n| http.min.net.connection.limit | 4 | No | Active connection limit. |\n| http.min.net.connection.timeout | 300000 | No | Idle connection timeout in milliseconds. |\n| http.min.net.connection.hint | false | No | Windows specific flag to overcome OS limitations on TCP backlog size. |\n| http.min.worker.count |  | No | By default, minimal HTTP server uses shared thread pool for CPU core count 16 and below. It will use dedicated thread for core count above 16. When0, the server will use the shared pool. Do not set pool size to more than1. |\n| http.min.worker.affinity |  | No | Core number to pin thread to. |\n| http.min.worker.haltOnError | false | No | Flag that indicates if the worker thread must stop when an unexpected error occurs. |\n\nwarning\nOn systems with\n8 Cores and less\n, contention\nfor threads might increase the latency of health check service responses. If you\nuse a load balancer, and it thinks the QuestDB service is dead with nothing\napparent in the QuestDB logs, you may need to configure a dedicated thread pool\nfor the health check service. To do so, increase\nhttp.min.worker.count\nto\n1\n.\n\n### Environment variables‚Äã\n\nValues in the log configuration file can be overridden with environment\nvariables. All configuration keys must be formatted as described in the\nenvironment variables\nsection above.\nFor example, to set logging on\nERROR\nlevel only:\nSetting log level to ERROR in log-stdout.conf\n\n```shell\nw.stdout.level=ERROR\n```\n\nThis can be passed as an environment variable as follows:\nSetting log level to ERROR via environment variable\n\n```shell\nexport QDB_LOG_W_STDOUT_LEVEL=ERROR\n```\n\n\n### Docker logging‚Äã\n\nWhen mounting a volume to a Docker container, a logging configuration file may\nbe provided in the container located at\n./conf/log.conf\n. For example, a file\nwith the following contents can be created:\n./conf/log.conf\n\n```shell\n# list of configured writerswriters=file,stdout,http.min# file writerw.file.class=io.questdb.log.LogFileWriterw.file.location=questdb-docker.logw.file.level=INFO,ERROR,DEBUG# stdoutw.stdout.class=io.questdb.log.LogConsoleWriterw.stdout.level=INFO# min http server, used for monitoringw.http.min.class=io.questdb.log.LogConsoleWriterw.http.min.level=ERROR## Scope provides specific context for targeted log parsingw.http.min.scope=http-min-server\n```\n\nThe current directory can be mounted:\nMount the current directory to a QuestDB container\n\n```shell\ndocker run -p 9000:9000 -v \"$(pwd):/var/lib/questdb/\" questdb/questdb\n```\n\nThe container logs will be written to disk using the logging level and file name\nprovided in the\n./conf/log.conf\nfile, in this case in\n./questdb-docker.log\n.\n\n### Windows log locations‚Äã\n\nWhen running QuestDB as Windows service you can check status in both:\n- Windows Event Viewer: Look for events with \"QuestDB\" source inWindows Logs | Application\n- The service log file:$dataDir\\log\\service-%Y-%m-%dT%H-%M-%S.txtDefault:C:\\Windows\\System32\\qdbroot\\log\\service-%Y-%m-%dT%H-%M-%S.txt\n\n## Metrics‚Äã\n\nQuestDB exposes a\n/metrics\nendpoint on port\n9003\nfor internal system metrics\nin the Prometheus format. To use this functionality and get started with an\nexample configuration, enable it in within your\nserver.conf\n:\n\n| Property | Default | Description |\n| --- | --- | --- |\n| metrics.enabled | false | Enable or disable metrics endpoint. |\n\nFor an example on how to setup Prometheus, see the\nQuestDB and Prometheus documentation\n.\n\n### Prometheus Alertmanager‚Äã\n\nQuestDB includes a log writer that sends any message logged at critical level\n(logger.critical(\"may-day\")) to Prometheus Alertmanager over a TCP/IP socket. To\nconfigure this writer, add it to the\nwriters\nconfig alongside other log\nwriters:\nlog.conf\n\n```ini\n# Which writers to enablewriters=stdout,alert# stdoutw.stdout.class=io.questdb.log.LogConsoleWriterw.stdout.level=INFO# Prometheus Alertingw.alert.class=io.questdb.log.LogAlertSocketWriterw.alert.level=CRITICALw.alert.location=/alert-manager-tpt.jsonw.alert.alertTargets=localhost:9093,localhost:9096,otherhost:9093w.alert.defaultAlertHost=localhostw.alert.defaultAlertPort=9093# The `inBufferSize` and `outBufferSize` properties are the size in bytes for the# socket write buffers.w.alert.inBufferSize=2mw.alert.outBufferSize=4m# Delay in milliseconds between two consecutive attempts to alert when# there is only one target configuredw.alert.reconnectDelay=250\n```\n\nOf all properties, only\nw.alert.class\nand\nw.alert.level\nare required, the\nrest assume default values as stated above (except for\nw.alert.alertTargets\nwhich is empty by default).\nAlert targets are specified using\nw.alert.alertTargets\nas a comma-separated\nlist of up to 12\nhost:port\nTCP/IP addresses. Specifying a port is optional and\ndefaults to the value of\ndefaultAlertHost\n. One of these alert managers is\npicked at random when QuestDB starts, and a connection is created.\nAll alerts will be sent to the chosen server unless it becomes unavailable. If\nit is unavailable, the next server is chosen. If there is only one server\nconfigured and a fail-over cannot occur, a delay of 250 milliseconds is added\nbetween send attempts.\nThe\nw.alert.location\nproperty refers to the path (absolute, otherwise relative\nto\n-d database-root\n) of a template file. By default, it is a resource file\nwhich contains:\n/alert-manager-tpt.json\n\n```json\n[  {    \"Status\": \"firing\",    \"Labels\": {      \"alertname\": \"QuestDbInstanceLogs\",      \"service\": \"QuestDB\",      \"category\": \"application-logs\",      \"severity\": \"critical\",      \"version\": \"${QDB_VERSION}\",      \"cluster\": \"${CLUSTER_NAME}\",      \"orgid\": \"${ORGID}\",      \"namespace\": \"${NAMESPACE}\",      \"instance\": \"${INSTANCE_NAME}\",      \"alertTimestamp\": \"${date: yyyy/MM/ddTHH:mm:ss.SSS}\"    },    \"Annotations\": {      \"description\": \"ERROR/cl:${CLUSTER_NAME}/org:${ORGID}/ns:${NAMESPACE}/db:${INSTANCE_NAME}\",      \"message\": \"${ALERT_MESSAGE}\"    }  }]\n```\n\nFour environment variables can be defined, and referred to with the\n${VAR_NAME}\nsyntax:\n- ORGID\n- NAMESPACE\n- CLUSTER_NAME\n- INSTANCE_NAME\nTheir default value is\nGLOBAL\n, they mean nothing outside a cloud environment.\nIn addition,\nALERT_MESSAGE\nis a placeholder for the actual\ncritical\nmessage\nbeing sent, and\nQDB_VERSION\nis the runtime version of the QuestDB instance\nsending the alert. The\n${date: <format>}\nsyntax can be used to produce a\ntimestamp at the time of sending the alert.\n\n### Unhandled error detection‚Äã\n\nWhen the metrics subsystem is enabled, the health endpoint may be configured to\ncheck the occurrences of any unhandled errors since the database started. For\nany errors detected, it returns the HTTP 500 status code. The check is based on\nthe\nquestdb_unhandled_errors_total\nmetric.\nTo enable this setting, set the following in\nserver.conf\n:\nserver.conf to enable critical error checks in the health check endpoint\n\n```ini\nmetrics.enabled=truehttp.pessimistic.health.check.enabled=true\n```\n\nWhen the metrics subsystem is disabled, the health check endpoint always returns\nthe HTTP 200 status code.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1765,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-a7547aef653c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/dotnet",
    "title": ".NET Client Documentation | QuestDB",
    "text": "QuestDB supports the .NET ecosystem with its dedicated .NET client, engineered\nfor high-throughput data ingestion, focusing on insert-only operations.\nApart from blazing fast ingestion, our clients provide these key benefits:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\nThis quick start guide aims to familiarize you with the fundamental features of\nthe .NET client, including how to establish a connection, authenticate, and\nperform basic insert operations.\nView source code\ninfo\nThis page focuses on our high-performance ingestion client, which is optimized\nfor\nwriting\ndata to QuestDB. For retrieving data, we recommend using a\nPostgreSQL-compatible .NET library\nor our\nHTTP query endpoint\n.\n\n## Requirements‚Äã\n\n- .NET 6.0 or higher is required.\n- QuestDB must be running. If not, seethe general quick start guide.\n\n## Client installation‚Äã\n\nThe latest version of the library is\n2.1.0\n(\nchangelog\n)\nThe NuGet package can be installed using the dotnet CLI:\n\n```shell\ndotnet add package net-questdb-client\n```\n\nnote\nSender\nis single-threaded, and uses a single connection to the database.\nIf you want to send in parallel, you can use multiple senders and standard async\ntasks.\n\n## Authentication‚Äã\n\n\n### HTTP‚Äã\n\nThe HTTP protocol supports authentication via\nBasic Authentication\n, and\nToken Authentication\n.\nBasic Authentication\nConfigure Basic Authentication with the\nusername\nand\npassword\nparameters:\n\n```csharp\nusing QuestDB; ...using var sender = Sender.New(\"http::addr=localhost:9000;username=admin;password=quest;\"); ...\n```\n\nToken Authentication\nQuestDB Enterprise Only\nConfigure Token Authentication with the\nusername\nand\ntoken\nparameters:\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;username=admin;token=<token>\");\n```\n\n\n### TCP‚Äã\n\nTCP authentication can be configured using JWK tokens:\n\n```csharp\nusing var sender = Sender.New(\"tcp::addr=localhost:9009;username=admin;token=<token>\");\n```\n\nThe connection options can also be built programatically. See\nWays to create the client\nfor details.\n\n## Basic insert‚Äã\n\nBasic insertion (no-auth):\n\n```csharp\nusing System;using QuestDB;using var sender =  Sender.New(\"http::addr=localhost:9000;\");await sender.Table(\"trades\")    .Symbol(\"symbol\", \"ETH-USD\")    .Symbol(\"side\", \"sell\")    .Column(\"price\", 2615.54)    .Column(\"amount\", 0.00044)    .AtNowAsync();await sender.Table(\"trades\")    .Symbol(\"symbol\", \"BTC-USD\")    .Symbol(\"side\", \"sell\")    .Column(\"price\", 39269.98)    .Column(\"amount\", 0.001)    .AtNowAsync();await sender.SendAsync();\n```\n\nIn this case, we asked the server to assign the timestamp to each row. Let's see\nnow an example with timestamps, custom auto-flushing, basic auth, and error\nreporting.\n\n```csharp\nusing QuestDB;using System;using System.Threading.Tasks;class Program{    static async Task Main(string[] args)    {        using var sender = Sender.New(          \"http::addr=localhost:9000;username=admin;password=quest;auto_flush_rows=100;auto_flush_interval=1000;\"        );        var now = DateTime.UtcNow;        try        {            await sender.Table(\"trades\")                        .Symbol(\"symbol\", \"ETH-USD\")                        .Symbol(\"side\", \"sell\")                        .Column(\"price\", 2615.54)                        .Column(\"amount\", 0.00044)                        .AtAsync(now);            await sender.Table(\"trades\")                        .Symbol(\"symbol\", \"BTC-USD\")                        .Symbol(\"side\", \"sell\")                        .Column(\"price\", 39269.98)                        .Column(\"amount\", 0.001)                        .AtAsync(now);            await sender.SendAsync();            Console.WriteLine(\"Data flushed successfully.\");        }        catch (Exception ex)        {            Console.Error.WriteLine($\"Error: {ex.Message}\");        }    }}\n```\n\nNow, both events use the same timestamp. We recommend using the event's\noriginal timestamp when ingesting data into QuestDB. Using ingestion-time\ntimestamps precludes the ability to deduplicate rows, which is\nimportant for exactly-once processing\n.\n\n## Ways to create the client‚Äã\n\nThere are three ways to create a client instance:\n- From a configuration string.This is the most common way to create a\nclient instance. It describes the entire client configuration in a single\nstring. SeeConfiguration optionsfor all available\noptions. It allows sharing the same configuration across clients in different\nlanguages.using var sender = Sender.New(\"http::addr=localhost:9000;\");\n- From an environment variable.TheQDB_CLIENT_CONFenvironment variable\nis used to set the configuration string. Moving configuration parameters to\nan environment variable allows you to avoid hard-coding sensitive information\nsuch as tokens and password in your code.If you want to initialise some properties programmatically after the initial\nconfig string, you can useConfigureandBuild.export QDB_CLIENT_CONF=\"http::addr=localhost:9000;auto_flush_rows=5000;retry_timeout=10000;\"(Sender.Configure(\"http::addr=localhost:9000;\") with { auto_flush = AutoFlushType.off }).Build()\n- From SenderOptions.await using var sender = Sender.New(new SenderOptions());This way you can bind options from configuration:{\"QuestDB\":{\"addr\":\"localhost:9000\",\"tls_verify\":\"unsafe_off;\"}}var options = new ConfigurationBuilder().AddJsonFile(\"config.json\").Build().GetSection(\"QuestDB\").Get<SenderOptions>();\n\n## Configuration options‚Äã\n\nThe easiest way to configure the\nSender\nis the configuration string. The\ngeneral structure is:\n\n```plain\n<transport>::addr=host:port;param1=val1;param2=val2;...\n```\n\ntransport\ncan be\nhttp\n,\nhttps\n,\ntcp\n, or\ntcps\n. Go to the client's\ncrate documentation\nfor the\nfull details on configuration.\nAlternatively, for breakdown of available params, see the\nConfiguration string\npage.\n\n## Preparing Data‚Äã\n\nThe Sender uses an internal buffer to convert input values into an\nILP-compatible UTF-8 byte-string.\nYou can control buffer sizing with the\ninit_buf_size\nand\nmax_buf_size\nparameters.\nHere is how to build a buffer of rows ready to be sent to QuestDB.\nwarning\nThe senders are\nnot\nthread safe, since they manage an internal buffer. If\nyou wish to send data in parallel, construct multiple senders and use\nnon-blocking I/O to submit to QuestDB.\nThe API follows the following overall flow:\n\n### Specify the table‚Äã\n\nAn ILP row starts with a table name, using\nTable\n.\n\n```csharp\nsender.Table(\"table_name\");\n```\n\nThe table name must always be called before other builder functions.\n\n### Add symbols‚Äã\n\nA\nsymbol\nis a dictionary-encoded string, used to\nefficiently store commonly repeated data. We recommend using this type for\nidentifiers, because you can create a\nsecondary index\nfor a symbol column.\nAdd symbols by calling\nSymbol()\n, which expects a symbol column\nname, and a string value.\n\n```csharp\nsender.Symbol(\"foo\", \"bah\");\n```\n\nYou must specify all symbol columns first, before any other columns.\n\n### Add other columns‚Äã\n\nThere are several data types you can send to QuestDB via ILP, including string /\nlong / double / DateTime / DateTimeOffset.\nProvide these by calling\nColumn()\n.\n\n```csharp\nsender.Column(\"baz\", 102);\n```\n\n\n### Finish the row‚Äã\n\nCompleted a row by specifying the designated timestamp:\n\n```csharp\nsender.At(DateTime.UtcNow);\n```\n\nYou can also let the server assign the timestamp, by calling\nAtNow()\ninstead.\ncaution\nWe recommend using the event's original timestamp when ingesting data into\nQuestDB. Using ingestion-time timestamps precludes the ability to deduplicate\nrows, which is\nimportant for exactly-once processing\n.\n\n## Flushing‚Äã\n\nOnce the buffer is filled with data ready to be sent, it can be flushed to the\ndatabase automatically, or manually.\n\n### Auto-flushing‚Äã\n\nWhen you call one of the\nAt\nfunctions, the row is complete. The sender checks\nthe auto-flushing parameters to see if it should flush the buffer to the server.\n\n```csharp\nsender.At(new DateTime(0,0,1));\n```\n\nTo avoid blocking the calling thread, use the Async overloads of the\nAt\n, such\nas\nAtAsync\n.\n\n```csharp\nawait sender.AtNowAsync();\n```\n\nAuto-flushing can be enabled or disabled:\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;auto_flush=off;\"); // or `on`, defaults to `on`\n```\n\n\n#### Flush by rows‚Äã\n\nYou can specify the number of rows that will trigger an auto-flush, creating a\nbatch insert operation of that size.\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;auto_flush=on;auto_flush_rows=5000;\");\n```\n\nBy default, the HTTP sender auto-flushes after 75,000 rows, and TCP after 600\nrows.\ntip\nauto_flush_rows\nand\nauto_flush_interval\nare both enabled by default. If you\nwish to only auto-flush based on one of these properties, disable the other\nusing\noff\nor\n-1\n.\n\n#### Flush by interval‚Äã\n\nYou can specify the time interval between auto-flushes. The sender checks it\nevery time you call an\nAt\nfunction.\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;auto_flush=on;auto_flush_interval=5000;\");\n```\n\nBy default,\nauto_flush_interval\nis 1000 ms.\n\n#### Flush by bytes‚Äã\n\nAs an additional option, disabled by default, you can specify the batch size in\nterms of bytes instead of rows. You should ensure that\ninit_buf_size\n<\\lt<\nauto_flush_bytes\n‚â§\\leq‚â§\nmax_buf_size\n.\nThis can be useful if you have large variation in row sizes and want to limit\nthe request sizes. By default, this is disabled, but set to\n100 KiB\n.\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;auto_flush=on;auto_flush_bytes=65536;\");\n```\n\n\n### Explicit flushing‚Äã\n\nYou can also manually flush the buffer at any time by calling\nSend\nor\nSendAsync\n. This will send any outstanding data to the QuestDB server.\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;auto_flush=off;\");sender.Table(\"foo\").Symbol(\"bah\", \"baz\").Column(\"num\", 123).At(DateTime.UtcNow);await sender.SendAsync(); // send non-blocking// ORsender.Send(); // send synchronously\n```\n\nYou should always perform an explicit flush before closing the sender. The HTTP\nsender normally retries the requests in case of errors, but won't do that while\nauto-flushing before closing. Flushing explicitly ensures that the client\napplies the same effort to send all the remaining data.\n\n## Transactions‚Äã\n\nThe HTTP transport provides transactionality for requests. Each request in a\nflush sends a batch of rows, which will be committed at once, or not at all.\nServer-side transactions are only for a single table. Therefore, a request\ncontaining multiple tables will be split into a single transaction per table. If\na transaction fails for one table, other transactions may still complete.\nFor data transactionality, one can use the transaction feature to enforce a\nbatch only for a single table.\ncaution\nAs described in the\nILP overview\n, the\nHTTP transport has some limitations for transactions when adding new columns.\nTransactions follow this flow:\nOne way to use this route effectively is to assign a single\nSender\nper table,\nand then use transactions for each sender. This minimises server-side overhead\nby reducing how many tables are submitted to from different connections.\nIt is still recommended to enable deduplication keys on your tables. This is\nbecause an early request timeout, or failure to read the response stream, could\ncause an error in the client, even though the server was returning a success\nresponse. Therefore, making the table idempotent is best to allow for safe\nretries. With TCP, this is a much greater risk.\n\n### Opening a transaction‚Äã\n\nTo start a\nTransaction\n, and pass the name of the table.\n\n```csharp\nsender.Transaction(\"foo\");\n```\n\nThe sender will return errors if you try to specify an alternate table whilst a\ntransaction is open.\n\n### Adding data‚Äã\n\nAdd data to a transaction in the usual way, but without calling\nTable\nbetween\nrows.\n\n```csharp\n// add a symbol, integer column, and end with current timestampsender.Symbol(\"bah\", \"baz\").Column(\"num\", 123).At(DateTime.UtcNow);\n```\n\n\n### Closing a transaction‚Äã\n\nCommit transactions and flush using\nCommit\nor\nCommitAsync\n. This will flush\ndata to the database, and remove the transactional state.\n\n```csharp\nawait sender.CommitAsync();\n```\n\nAlternatively, if you wish to discard the transaction, you can use\nRollback\n.\nThis will clear the buffer and transactional state, without sending data to the\nserver.\n\n```csharp\nsender.Rollback();\n```\n\n\n## Misc‚Äã\n\n\n### Cancelling rows‚Äã\n\nCancel the current line using\nCancelRow\n.\nThis must be called before the row is complete, as otherwise it may have been\nsent already.\n\n```csharp\nsender.Table(\"foo\").Symbol(\"bah\", \"baz\").CancelRow(); // cancels the current rowsender.Table(\"foo\").Symbol(\"bah\", \"baz\").At(DateTime.UtcNow); // invalid - no row to cancel\n```\n\nThis can be useful if a row is being built step-by-step, and an error is thrown.\nThe user can cancel the row and preserve the rest of the buffer that was built\ncorrectly.\n\n### Trimming the buffer‚Äã\n\nSet properties in the configuration string to control the buffer size.\nIt may be that the case that the buffer¬†needs to grow earlier and shrink later.\nIn this scenario, the user can call\nTruncate\n. This will trim the internal\nbuffer, removing extra pages (each of which is the size of\ninit_buf_size\n),\nreducing overall memory consumption:\n\n```csharp\nusing var sender = Sender.New(\"http::addr=localhost:9000;init_buf_size=1024;\");for (int i = 0; i < 100_000; i++) {    sender.Table(\"foo\").Column(\"num\", i).At(DateTime.UtcNow);}await sender.SendAsync(); // buffer is now flushed and emptysender.Truncate(); // buffer is trimmed back to `init_buf_size`\n```\n\n\n### Clearing the buffer‚Äã\n\nKeep the sender, but clear the internal buffer.\nThis can be performed using\nClear\n.\n\n```csharp\nsender.Clear(); // empties the internal buffer\n```\n\n\n## Security‚Äã\n\nQuestDB Enterprise offers native TLS support\n\n### TLS‚Äã\n\nEnable TLS via the\nhttps\nor\ntcps\nprotocol, along with other associated\nconfiguration.\nTLS is supported only by\nQuestDB Enterprise\nversion of\nQuestDB.\nFor development purposes, the verification of TLS certificates can be disabled:\n\n```csharp\nusing var sender = Sender.New(\"https::addr=localhost:9000;tls_verify=unsafe_off;\");\n```\n\n\n### HTTP TLS with Basic Authentication‚Äã\n\n\n```csharp\n// Runs against QuestDB Enterprise, demonstrating HTTPS and Basic Authentication support.using var sender =    Sender.New(\"https::addr=localhost:9000;tls_verify=unsafe_off;username=admin;password=quest;\");\n```\n\n\n### TCP TLS with JWK Authentication‚Äã\n\n\n```csharp\n//    Demonstrates TCPS connection against QuestDB Enterpriseusing var sender =    Sender.New(        \"tcps::addr=localhost:9009;tls_verify=unsafe_off;username=admin;token=NgdiOWDoQNUP18WOnb1xkkEG5TzPYMda5SiUOvT1K0U=;\");// See: /docs/ingestion/ilp/authenticate\n```\n\n\n## Next Steps‚Äã\n\nPlease refer to the\nILP overview\nfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\nDive deeper into the .NET client capabilities by exploring more examples\nprovided in the\nGitHub repository\n.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nShould you encounter any issues or have questions, the\nCommunity Forum\nis a vibrant platform for\ndiscussions.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2023,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-2fe44ab4df1d",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/deployment/docker",
    "title": "Using Docker with QuestDB | QuestDB",
    "text": "QuestDB has images for both Linux/macOS and Windows on\nDocker Hub\n.\n\n## Install Docker‚Äã\n\nTo begin, install Docker. You can find guides for your platform on the\nofficial documentation\n.\n\n## Run QuestDB image‚Äã\n\nOnce Docker is installed, you will need to pull QuestDB's image from\nDocker Hub\nand create a container.\nThis can be done with a single command using:\n\n```shell\ndocker run \\-p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\questdb/questdb:9.3.2\n```\n\nThis command starts a Docker container from\nquestdb/questdb\nimage. In\naddition, it exposes some ports, allowing you to explore QuestDB.\nIn order to configure QuestDB, it is recommended to mount a\nvolume\nto allow data persistance. This can be\ndone by adding a\n-v\nflag to the above command:\n\n```questdb-sql\n-v \"/host/volume/location:/var/lib/questdb\"\n```\n\nBelow each parameter is described in detail.\n\n### -pparameter to expose ports‚Äã\n\nThis parameter will expose a port to the host. You can specify:\n- -p 9000:9000-REST APIandWeb Console\n- -p 9009:9009-InfluxDB line protocol\n- -p 8812:8812-Postgres wire protocol\n- -p 9003:9003-Min health server\nAll ports are optional, you can pick only the ones you need. For example, it is\nenough to expose\n8812\nif you only plan to use\nPostgres wire protocol\n.\n\n### -vparameter to mount storage‚Äã\n\nThis parameter will make a local directory available to QuestDB Docker\ncontainer. It will have all data ingested to QuestDB, server logs and\nconfiguration.\nThe QuestDB\nroot_directory\nis located\nat the\n/var/lib/questdb\npath in the container.\n\n### Docker image version‚Äã\n\nBy default,\nquestdb/questdb\npoints to the latest QuestDB version available on\nDocker. However, it is recommended to define the version used.\n\n```shell\nquestdb/questdb:9.3.2\n```\n\n\n## Environment variables‚Äã\n\nServer configuration can be passed to QuestDB running in Docker by using the\n-e\nflag to pass an environment variable to a container:\n\n```bash\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb\n```\n\nFor a list of configuration options, see\nConfiguration\n.\n\n## Container status‚Äã\n\nYou can check the status of your container with\ndocker ps\n.\nIt also lists the exposed ports, container name, uptime and more:\nFinding container status with docker ps\n\n```shell\ndocker ps\n```\n\nResult of docker ps\n\n```shell\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMESdd363939f261        questdb/questdb     \"/app/bin/java -m io‚Ä¶\"   3 seconds ago       Up 2 seconds        8812/tcp, 9000/tcp   frosty_gauss\n```\n\nThis container:\n- has an id ofdd363939f261\n- uses ports8812&9000, for Postgres wire protocol and HTTP respectively\n- is using aquestdb/questdbimage\n- ran java to start the binary\n- is 3 seconds old\n- has been up for 2 seconds\n- has the unfortunate name offrosty_gauss\nFor full container status information, see the\ndocker psmanual\n.\n\n### Debugging container logs‚Äã\n\nDocker may generate a runtime error.\nThe error may not be accurate, as the true culprit is often indicated higher up\nin the logs.\nTo see the full log, retrieve the UUID - also known as the\nCONTAINER ID\n-\nusing\ndocker ps\n:\nFinding the CONTAINER ID\n\n```shell\nCONTAINER ID        IMAGE               ...dd363939f261        questdb/questdb     ...\n```\n\nNow pass the\nCONTAINER ID\n- or\ndd363939f261\n- to the\ndocker logs\ncommand:\nGenerating a docker log from a CONTAINER ID\n\n```shell\n$ docker logs dd363939f261No arguments found, start with default argumentsRunning as questdb userLog configuration loaded from: /var/lib/questdb/conf/log.conf......\n```\n\nNote that the log will pull from\n/var/lib/questdb/conf/log.conf\nby default.\nSharing this log when seeking support for Docker deployments will help us find\nthe root cause.\n\n## Importing data and sending queries‚Äã\n\nWhen QuestDB is running, you can start interacting with it:\n- Port9000is for REST. More info is available on theREST documentation page.\n- Port8812is used for Postgres. Check ourPostgres reference page.\n- Port9009is dedicated to InfluxDB Line Protocol. Consult ourInfluxDB protocol page.\n\n## Data persistence‚Äã\n\n\n### Mounting a volume‚Äã\n\nVolumes can be mounted to the QuestDB Docker container so that data may be\npersisted or server configuration settings may be passed to an instance. The\nfollowing example demonstrated how to mount the current directory to a QuestDB\ncontainer using the\n-v\nflag in a Docker\nrun\ncommand:\nMounting a volume\n\n```shell\ndocker run -p 9000:9000 \\-p 9009:9009 \\-p 8812:8812 \\-p 9003:9003 \\-v \"$(pwd):/var/lib/questdb\" \\questdb/questdb:9.3.2\n```\n\nThe current directory will then have data persisted to disk for convenient\nmigration or backups:\nCurrent directory contents\n\n```bash\n‚îú‚îÄ‚îÄ conf‚îÇ   ‚îî‚îÄ‚îÄ server.conf‚îú‚îÄ‚îÄ db‚îú‚îÄ‚îÄ log‚îú‚îÄ‚îÄ public‚îî‚îÄ‚îÄ snapshot (optional)\n```\n\nA server configuration file can also be provided by mounting a local directory\nin a QuestDB container. Given the following configuration file which overrides\nthe default HTTP bind property:\n./server.conf\n\n```shell\nhttp.bind.to=0.0.0.0:4000\n```\n\nRunning the container with the\n-v\nflag allows for mounting the current\ndirectory to QuestDB's\nconf\ndirectory in the container. With the server\nconfiguration above, HTTP ports for the\nWeb Console\nand REST API will be\navailable on\nhttp://localhost:4000\n:\n\n```bash\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n```\n\nnote\nIf you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses.\nPlease see the\ndocker documentation\nfor more information.\n\n### Upgrade QuestDB version‚Äã\n\nIt is possible to upgrade your QuestDB instance on Docker when a volume is\nmounted to maintain data persistence.\nnote\n- Check therelease notesand\nensure that necessarybackupis completed.\n- Upgrading an instance is possible only when the original instance has a volume\nmounted. Without mounting a volume for the original instance, the following\nsteps create a new instance and data in the old instance cannot be retrieved.\n- Rundocker psto copy the container name or ID:\nContainer status\n\n```shell\n# The existing QuestDB version is 6.5.2:CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                NAMESdd363939f261        questdb/questdb:6.5.2     \"/app/bin/java -m io‚Ä¶\"   3 seconds ago       Up 2 seconds        8812/tcp, 9000/tcp   frosty_gauss\n```\n\n- Stop the instance and then remove the container:\n\n```shell\ndocker stop dd363939f261docker rm dd363939f261\n```\n\n- Download the latest QuestDB image:\n\n```shell\ndocker pull questdb/questdb:9.3.2\n```\n\n- Start a new container with the new version and the same volume mounted:\n\n```shell\ndocker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.3.2\n```\n\n\n### Writing logs to disk‚Äã\n\nWhen mounting a volume to a Docker container, a logging configuration file may\nbe provided in the container located at\n/conf/log.conf\n:\nCurrent directory contents\n\n```bash\n‚îî‚îÄ‚îÄ conf    ‚îú‚îÄ‚îÄ log.conf    ‚îî‚îÄ‚îÄ server.conf\n```\n\nFor example, a file with the following contents can be created:\n./conf/log.conf\n\n```shell\n# list of configured writerswriters=file,stdout,http.min# file writerw.file.class=io.questdb.log.LogFileWriterw.file.location=questdb-docker.logw.file.level=INFO,ERROR,DEBUG# stdoutw.stdout.class=io.questdb.log.LogConsoleWriterw.stdout.level=INFO# min http server, used monitoringw.http.min.class=io.questdb.log.LogConsoleWriterw.http.min.level=ERRORw.http.min.scope=http-min-server\n```\n\nThe current directory can be mounted:\nMounting the current directory to a QuestDB container\n\n```shell\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb\n```\n\nThe container logs will be written to disk using the logging level and file name\nprovided in the\nconf/log.conf\nfile, in this case in\n./questdb-docker.log\n:\nCurrent directory tree\n\n```shell\n‚îú‚îÄ‚îÄ conf‚îÇ  ‚îú‚îÄ‚îÄ log.conf‚îÇ  ‚îî‚îÄ‚îÄ server.conf‚îú‚îÄ‚îÄ db‚îÇ  ‚îú‚îÄ‚îÄ table1‚îÇ  ‚îî‚îÄ‚îÄ table2‚îú‚îÄ‚îÄ public‚îÇ  ‚îú‚îÄ‚îÄ ui / assets‚îÇ  ‚îú‚îÄ‚îÄ ...‚îÇ  ‚îî‚îÄ‚îÄ version.txt‚îî‚îÄ‚îÄ questdb-docker.log\n```\n\nFor more information on logging, see the\nconfiguration reference documentation\n.\n\n### Restart an existing container‚Äã\n\nRunning the following command will create a new container for the QuestDB image:\n\n```shell\ndocker run -p 9000:9000 \\  -p 9009:9009 \\  -p 8812:8812 \\  -p 9003:9003 \\  questdb/questdb\n```\n\nBy giving the container a name with\n--name container_name\n, we have an easy way\nto refer to the container created by run later on:\n\n```shell\ndocker run -p 9000:9000 \\  -p 9009:9009 \\  -p 8812:8812 \\  -p 9003:9003 \\  --name docker_questdb \\  questdb/questdb\n```\n\nIf we want to re-use this container and its data after it has been stopped, we\ncan use the following commands:\n\n```shell\n# bring the container updocker start docker_questdb# shut the container downdocker stop docker_questdb\n```\n\nAlternatively, restart it using the\nCONTAINER ID\n:\nStarting a container by CONTAINER ID\n\n```shell\ndocker start dd363939f261\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1287,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-0bee8b9fba8f",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/clients/c-and-cpp",
    "title": "C & C++ Client Documentation | QuestDB",
    "text": "QuestDB supports the C & C++ programming languages, providing a high-performance\ningestion client tailored for insert-only operations. This integration ensures\npeak efficiency in time series data ingestion and analysis, perfectly suited for\nsystems which require top performance and minimal latency.\nKey features of the QuestDB C & C++ client include:\n- Automatic table creation: No need to define your schema upfront.\n- Concurrent schema changes: Seamlessly handle multiple data streams with\non-the-fly schema modifications\n- Optimized batching: Use strong defaults or curate the size of your batches\n- Health checks and feedback: Ensure your system's integrity with built-in\nhealth monitoring\n- Automatic write retries: Reuse connections and retry after interruptions\n\n### Requirements‚Äã\n\n- Requires a C/C++ compiler and standard libraries.\n- Assumes QuestDB is running. If it's not, refer tothe general quick start.\n\n### Client Installation‚Äã\n\nYou need to add the client as a dependency to your project. Depending on your\nenvironment, you can do this in different ways. Please check the documentation\nat the\nclient's repository\n.\n\n## C++‚Äã\n\nnote\nThis section is for the QuestDB C++ client.\nFor the QuestDB C Client, see the below seciton.\nView full docs\nView source code\nExplore the full capabilities of the C++ client via the\nC++ README\n.\n\n## Authentication‚Äã\n\nThe QuestDB C++ client supports basic connection and authentication\nconfigurations.\nHere is an example of how to configure and use the client for data ingestion:\n\n```c\n#include <questdb/ingress/line_sender.hpp>...auto sender = questdb::ingress::line_sender::from_conf(    \"http::addr=localhost:9000;\");\n```\n\nYou can also pass the connection configuration via the\nQDB_CLIENT_CONF\nenvironment variable:\n\n```bash\nexport QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n```\n\nThen you use it like this:\n\n```cpp\nauto sender = questdb::ingress::line_sender::from_env();\n```\n\nWhen using QuestDB Enterprise, authentication can also be done via REST token.\nPlease check the\nRBAC docs\nfor more\ninfo.\n\n### Basic data insertion‚Äã\n\nBasic insertion (no-auth):\n\n```cpp\n// main.cpp#include <questdb/ingress/line_sender.hpp>int main(){    auto sender = questdb::ingress::line_sender::from_conf(        \"http::addr=localhost:9000;\");    questdb::ingress::line_sender_buffer buffer;    buffer        .table(\"trades\")        .symbol(\"symbol\",\"ETH-USD\")        .symbol(\"side\",\"sell\")        .column(\"price\", 2615.54)        .column(\"amount\", 0.00044)        .at(questdb::ingress::timestamp_nanos::now());    // To insert more records, call `buffer.table(..)...` again.    sender.flush(buffer);    return 0;}\n```\n\nThese are the main steps it takes:\n- Usequestdb::ingress::line_sender::from_confto get thesenderobject\n- Populate aBufferwith one or more rows of data\n- Send the buffer usingsender.flush()(Sender::flush)\nIn this case, we call\nat()\n, with the current timestamp.\nLet's see now an example with explicit timestamps, custom timeout, basic auth,\nand error control.\n\n```cpp\n#include <questdb/ingress/line_sender.hpp>#include <iostream>#include <chrono>int main(){    try    {        // Create a sender using HTTP protocol        auto sender = questdb::ingress::line_sender::from_conf(            \"http::addr=localhost:9000;username=admin;password=quest;retry_timeout=20000;\");        // Get the current time as a timestamp        auto now = std::chrono::system_clock::now();        auto duration = now.time_since_epoch();        auto nanos = std::chrono::duration_cast<std::chrono::nanoseconds>(duration).count();        // Add rows to the buffer of the sender with the same timestamp        questdb::ingress::line_sender_buffer buffer;        buffer            .table(\"trades\")            .symbol(\"symbol\", \"ETH-USD\")            .symbol(\"side\", \"sell\")            .column(\"price\", 2615.54)            .column(\"amount\", 0.00044)            .at(questdb::ingress::timestamp_nanos(nanos));        buffer            .table(\"trades\")            .symbol(\"symbol\", \"BTC-USD\")            .symbol(\"side\", \"sell\")            .column(\"price\", 39269.98)            .column(\"amount\", 0.001)            .at(questdb::ingress::timestamp_nanos(nanos));        // Transactionality check        if (!buffer.transactional()) {            std::cerr << \"Buffer is not transactional\" << std::endl;            sender.close();            return 1;        }        // Flush and clear the buffer, sending the data to QuestDB        sender.flush(buffer);        // Close the connection after all rows ingested        sender.close();        return 0;    }    catch (const questdb::ingress::line_sender_error& err)    {        std::cerr << \"Error running example: \" << err.what() << std::endl;        return 1;    }}\n```\n\nNow, both events use the same timestamp. We recommend using the event's\noriginal timestamp when ingesting data into QuestDB. Using ingestion-time\ntimestamps precludes the ability to deduplicate rows, which is\nimportant for exactly-once processing\n.\n\n### Array Insertion‚Äã\n\nQuestDB can accept N-dimensional arrays. For now these are limited to the\ndouble\nelement type. The easiest way is to insert an\nstd::array\n, but the\ndatabase can also support\nstd::vector\n,\nstd::span\n(C++20) and additional\ncustom array types via a\ncustomization point\n.\nThe customization point can be used to integrate your own (or third party)\nn-dimensional array types by providing\nshape\nand, optionally if not row-major,\nstrides\n.\nPlease refer to the\nConcepts section on n-dimensional arrays\n,\nwhere this is explained in more detail.\nnote\nArrays are supported from QuestDB version 9.0.0, and require updated\nclient libraries.\nIn this example, we insert some FX order book data.\n- bidsandasks: 2D arrays of L2 order book depth. Each level contains price and volume.\n- bids_exec_probsandasks_exec_probs: 1D arrays of calculated execution probabilities for the next minute.\n\n```cpp\n#include <questdb/ingress/line_sender.hpp>#include <iostream>#include <vector>#include <array>using namespace std::literals::string_view_literals;using namespace questdb::ingress::literals;struct tensor {    std::vector<double> data;    std::vector<uintptr_t> shape;};// Customization point for the QuestDB array API (discovered via ADL lookup)inline auto to_array_view_state_impl(const tensor& t){    return questdb::ingress::array::row_major_view<double>{        t.shape.size(), // rank        t.shape.data(), // shape        t.data.data(), t.data.size() // array data    };}int main(){    try    {        auto sender = questdb::ingress::line_sender::from_conf(            \"http::addr=127.0.0.1:9000;\");        questdb::ingress::line_sender_buffer buffer = sender.new_buffer();        buffer            .table(\"fx_order_book\"_tn)            .symbol(\"symbol\"_cn, \"EUR/USD\"_utf8)            .column(\"bids\"_cn, tensor{                {                    1.0850, 600000,                    1.0849, 300000,                    1.0848, 150000                },                {3, 2}            })            .column(\"asks\"_cn, tensor{                {                    1.0853, 500000,                    1.0854, 250000,                    1.0855, 125000                },                {3, 2}            })            .column(\"bids_exec_probs\"_cn, std::array<double, 3>{                0.85, 0.50, 0.25})            .column(\"asks_exec_probs\"_cn, std::vector<double>{                0.90, 0.55, 0.20})            .at(questdb::ingress::timestamp_nanos::now());        sender.flush(buffer);        return true;    }    catch (const questdb::ingress::line_sender_error& err)    {        std::cerr << \"[ERROR] \" << err.what() << std::endl;        return false;    }}\n```\n\nIf your type also supports strides, use the\nquestdb::ingress::array::strided_view\ninstead.\nnote\nThe example above uses ILP/HTTP. If instead you're using ILP/TCP you'll need\nto explicity opt into the newer protocol version 2 that supports sending arrays.\n\n```questdb-sql\ntcp::addr=127.0.0.1:9009;protocol_version=2;\n```\n\nProtocol Version 2 along with its support for arrays is available from QuestDB\nversion 9.0.0.\n\n## C‚Äã\n\nnote\nThis section is for the QuestDB C client.\nSkip to the bottom of this page for information relating to both the C and C++\nclients.\nView full docs\nView source code\nExplore the full capabilities of the C client via the\nC README\n.\n\n### Connection‚Äã\n\nThe QuestDB C client supports basic connection and authentication\nconfigurations. Here is an example of how to configure and use the client for\ndata ingestion:\n\n```c\n#include <questdb/ingress/line_sender.h>...line_sender_utf8 conf = QDB_UTF8_LITERAL(    \"http::addr=localhost:9000;\");line_sender_error *error = NULL;line_sender *sender = line_sender_from_conf(    line_sender_utf8, &error);if (!sender) {    /* ... handle error ... */}\n```\n\nYou can also pass the connection configuration via the\nQDB_CLIENT_CONF\nenvironment variable:\n\n```bash\nexport QDB_CLIENT_CONF=\"http::addr=localhost:9000;username=admin;password=quest;\"\n```\n\nThen you use it like this:\n\n```c\n#include <questdb/ingress/line_sender.h>...line_sender *sender = line_sender_from_env(&error);\n```\n\n\n### Basic data insertion‚Äã\n\n\n```c\n// line_sender_trades_example.c#include <questdb/ingress/line_sender.h>#include <stdio.h>#include <stdint.h>int main() {    // Initialize line sender    line_sender_error *error = NULL;    line_sender *sender = line_sender_from_conf(        QDB_UTF8_LITERAL(\"http::addr=localhost:9000;username=admin;password=quest;\"), &error);    if (error != NULL) {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Failed to create line sender: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        return 1;    }    // Print success message    printf(\"Line sender created successfully\\n\");    // Initialize line sender buffer    line_sender_buffer *buffer = line_sender_buffer_new();    if (buffer == NULL) {        fprintf(stderr, \"Failed to create line sender buffer\\n\");        line_sender_close(sender);        return 1;    }    // Add data to buffer for ETH-USD trade    if (!line_sender_buffer_table(buffer,        QDB_TABLE_NAME_LITERAL(\"trades\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"symbol\"), QDB_UTF8_LITERAL(\"ETH-USD\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"side\"), QDB_UTF8_LITERAL(\"sell\"), &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"price\"), 2615.54, &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"amount\"), 0.00044, &error)) goto error;    if (!line_sender_buffer_at_nanos(buffer, line_sender_now_nanos(), &err))        goto on_error;    // Flush the buffer to QuestDB    if (!line_sender_flush(sender, buffer, &error)) {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Failed to flush data: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        line_sender_buffer_free(buffer);        line_sender_close(sender);        return 1;    }    // Print success message    printf(\"Data flushed successfully\\n\");    // Free resources    line_sender_buffer_free(buffer);    line_sender_close(sender);    return 0;error:    {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Error: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        line_sender_buffer_free(buffer);        line_sender_close(sender);        return 1;    }}\n```\n\nIn this case, we call\nline_sender_buffer_at_nanos()\nand pass the current\ntimestamp. The value returned by\nline_sender_now_nanos()\nis nanoseconds\nfrom unix epoch (UTC).\nLet's see now an example with timestamps, custom timeout, basic auth, error\ncontrol, and transactional awareness.\n\n```c\n// line_sender_trades_example.c#include <questdb/ingress/line_sender.h>#include <stdio.h>#include <time.h>#include <stdint.h>int main() {    // Initialize line sender    line_sender_error *error = NULL;    line_sender *sender = line_sender_from_conf(        QDB_UTF8_LITERAL(          \"http::addr=localhost:9000;username=admin;password=quest;retry_timeout=20000;\"          ), &error);    if (error != NULL) {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Failed to create line sender: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        return 1;    }    // Print success message    printf(\"Line sender created successfully\\n\");    // Initialize line sender buffer    line_sender_buffer *buffer = line_sender_buffer_new();    if (buffer == NULL) {        fprintf(stderr, \"Failed to create line sender buffer\\n\");        line_sender_close(sender);        return 1;    }    // Get current time in nanoseconds    int64_t nanos = line_sender_now_nanos();    // Add data to buffer for ETH-USD trade    if (!line_sender_buffer_table(buffer,        QDB_TABLE_NAME_LITERAL(\"trades\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"symbol\"), QDB_UTF8_LITERAL(\"ETH-USD\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"side\"), QDB_UTF8_LITERAL(\"sell\"), &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"price\"), 2615.54, &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"amount\"), 0.00044, &error))        goto error;    if (!line_sender_buffer_at_nanos(buffer, nanos, &error))        goto error;    // Add data to buffer for BTC-USD trade    if (!line_sender_buffer_table(buffer,        QDB_TABLE_NAME_LITERAL(\"trades\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"symbol\"),        QDB_UTF8_LITERAL(\"BTC-USD\"), &error))        goto error;    if (!line_sender_buffer_symbol(buffer,        QDB_COLUMN_NAME_LITERAL(\"side\"), QDB_UTF8_LITERAL(\"sell\"), &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"price\"), 39269.98, &error))        goto error;    if (!line_sender_buffer_column_f64(buffer,        QDB_COLUMN_NAME_LITERAL(\"amount\"), 0.001, &error))        goto error;    if (!line_sender_buffer_at_nanos(buffer, nanos, &error))        goto error;    // If we detect multiple tables within the same buffer, we abort to avoid potential    // inconsistency issues. Read below in this page for transaction details    if (!line_sender_buffer_transactional(buffer)) {        fprintf(stderr, \"Buffer is not transactional\\n\");        line_sender_buffer_free(buffer);        line_sender_close(sender);        return 1;    }    // Flush the buffer to QuestDB    if (!line_sender_flush(sender, buffer, &error)) {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Failed to flush data: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        line_sender_buffer_free(buffer);        line_sender_close(sender);        return 1;    }    // Print success message    printf(\"Data flushed successfully\\n\");    // Free resources    line_sender_buffer_free(buffer);    line_sender_close(sender);    return 0;error:    {        size_t len;        const char *msg = line_sender_error_msg(error, &len);        fprintf(stderr, \"Error: %.*s\\n\", (int)len, msg);        line_sender_error_free(error);        line_sender_buffer_free(buffer);        line_sender_close(sender);        return 1;    }}\n```\n\nNow, both events use the same timestamp. We recommend using the event's\noriginal timestamp when ingesting data into QuestDB. Using ingestion-time\ntimestamps precludes the ability to deduplicate rows, which is\nimportant for exactly-once processing\n.\n\n### Array Insertion‚Äã\n\nThe sender uses a plain 1-dimensional C array to insert an array of any\ndimensionality. It contains the elements laid out flat in row-major order.\nThe shape describes the rank and dimensions of the array.\nnote\nArrays are supported from QuestDB version 9.0.0, and require updated\nclient libraries.\nIn this example,  we insert arrays of\ndouble\nvalues for some FX order book data.\n- bidsandasks: 2D arrays of L2 order book depth. Each level contains price and volume.\n- bids_exec_probsandasks_exec_probs: 1D arrays of calculated execution probabilities for the next minute.\n\n```c\n#include <stdio.h>#include <stdlib.h>#include <string.h>#include <questdb/ingress/line_sender.h>int main(){    line_sender_error* err = NULL;    line_sender* sender = NULL;    line_sender_buffer* buffer = NULL;    // or \"tcp::addr=127.0.0.1:9009;protocol_version=2;\"    const char* conf_str = \"http::addr=127.0.0.1:9000;\";    line_sender_utf8 conf_str_utf8 = {0, NULL};    if (!line_sender_utf8_init(            &conf_str_utf8, strlen(conf_str), conf_str, &err))        goto on_error;    sender = line_sender_from_conf(conf_str_utf8, &err);    if (!sender)        goto on_error;    buffer = line_sender_buffer_new_for_sender(sender);    line_sender_buffer_reserve(buffer, 64 * 1024);    line_sender_table_name table_name = QDB_TABLE_NAME_LITERAL(\"fx_order_book\");    line_sender_column_name symbol_col = QDB_COLUMN_NAME_LITERAL(\"symbol\");    line_sender_column_name bids_col = QDB_COLUMN_NAME_LITERAL(\"bids\");    line_sender_column_name asks_col = QDB_COLUMN_NAME_LITERAL(\"asks\");    if (!line_sender_buffer_table(buffer, table_name, &err))        goto on_error;    line_sender_utf8 symbol_val = QDB_UTF8_LITERAL(\"EUR/USD\");    if (!line_sender_buffer_symbol(buffer, symbol_col, symbol_val, &err))        goto on_error;    // bids: 3 rows (levels), 2 columns (price, volume)    uintptr_t bids_rank = 2;    uintptr_t bids_shape[] = {3, 2};    double bids_data[] = {        1.0850, 600000,        1.0849, 300000,        1.0848, 150000    };    if (!line_sender_buffer_column_f64_arr_c_major(            buffer,            bids_col,            bids_rank,            bids_shape,            (const uint8_t*)bids_data,            sizeof(bids_data),            &err))        goto on_error;    // asks: 3 rows (levels), 2 columns (price, volume)    uintptr_t asks_rank = 2;    uintptr_t asks_shape[] = {3, 2};    double asks_data[] = {        1.0853, 500000,        1.0854, 250000,        1.0855, 125000    };    if (!line_sender_buffer_column_f64_arr_c_major(            buffer,            asks_col,            asks_rank,            asks_shape,            (const uint8_t*)asks_data,            sizeof(asks_data),            &err))        goto on_error;    // Timestamp, leave as-is (similar to your example)    if (!line_sender_buffer_at_nanos(buffer, line_sender_now_nanos(), &err))        goto on_error;    if (!line_sender_flush(sender, buffer, &err))        goto on_error;    line_sender_close(sender);    return 0;on_error:;    size_t err_len = 0;    const char* err_msg = line_sender_error_msg(err, &err_len);    fprintf(stderr, \"Error: %.*s\\n\", (int)err_len, err_msg);    line_sender_error_free(err);    line_sender_buffer_free(buffer);    line_sender_close(sender);    return 1;}\n```\n\nIf you need to specify strides, you can do this via either the\nline_sender_buffer_column_f64_arr_byte_strides\nor the\nline_sender_buffer_column_f64_arr_elem_strides\nfunctions.\nPlease refer to the\nConcepts section on n-dimensional arrays\n, where this is\nexplained in more detail.\n\n## Other Considerations for both C and C++‚Äã\n\n\n### Configuration options‚Äã\n\nThe easiest way to configure the line sender is the configuration string. The\ngeneral structure is:\n\n```plain\n<transport>::addr=host:port;param1=val1;param2=val2;...\n```\n\ntransport\ncan be\nhttp\n,\nhttps\n,\ntcp\n, or\ntcps\n. The C/C++ and Rust clients\nshare the same codebase. Please refer to the\nRust client's documentation\nfor the full details on configuration.\nAlternatively, for a breakdown of Configuration string options available across\nall clients, see the\nConfiguration string\npage.\n\n### Don't forget to flush‚Äã\n\nThe sender and buffer objects are entirely decoupled. This means that the sender\nwon't get access to the data in the buffer until you explicitly call\nsender.flush\nor\nline_sender_flush\n. This may lead to a pitfall where you drop\na buffer that still has some data in it, resulting in permanent data loss.\nA common technique is to flush periodically on a timer and/or once the buffer\nexceeds a certain size. You can check the buffer's size by calling\nbuffer.size()\nor\nline_sender_buffer_size(...)\n.\nThe default\nflush()\nmethod clears the buffer after sending its data. If you\nwant to preserve its contents (for example, to send the same data to multiple\nQuestDB instances), call\nsender.flush_and_keep(&buffer)\nor\nline_sender_flush_and_keep(...)\ninstead.\n\n### Transactional flush‚Äã\n\nAs described in\nILP overview\n, the\nHTTP transport has some support for transactions.\nTo ensure in advance that a flush will not affect more than one table, call\nbuffer.transactional()\nor\nline_sender_buffer_transactional(buffer)\n, as shown\nin the examples above. This call will return false if the flush wouldn't be\ndata-transactional.\n\n### Protocol Version‚Äã\n\nTo enhance data ingestion performance, QuestDB introduced an upgrade to the\ntext-based InfluxDB Line Protocol which encodes arrays and\ndouble\nvalues in\nbinary form. Arrays are supported only in this upgraded protocol version.\nYou can select the protocol version with the\nprotocol_version\nsetting in the\nconfiguration string.\nHTTP transport automatically negotiates the protocol version by default. In order\nto avoid the slight latency cost at connection time, you can explicitly configure\nthe protocol version by setting\nprotocol_version=2|1;\n.\nTCP transport does not negotiate the protocol version and uses version 1 by\ndefault. You must explicitly set\nprotocol_version=2;\nin order to ingest\narrays, as in this example:\n\n```text\ntcp::addr=localhost:9009;protocol_version=2;\n```\n\nProtocol Version 2 along with its support for arrays is available from QuestDB\nversion 9.0.0.\n\n## Next Steps‚Äã\n\nPlease refer to the\nILP overview\nfor details\nabout transactions, error control, delivery guarantees, health check, or table\nand column auto-creation.\nWith data flowing into QuestDB, now it's time for analysis.\nTo learn\nThe Way\nof QuestDB SQL, see the\nQuery & SQL Overview\n.\nAlone? Stuck? Want help? Visit us in our\nCommunity Forum\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2350,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-4ec0cfdb5259",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/configuration/command-line-options",
    "title": "Command-line options | QuestDB",
    "text": "QuestDB may be started, stopped and passed configuration options from the\ncommand line. On Windows, the QuestDB server can also start an\ninteractive session\n.\n\n## Options‚Äã\n\nThe following sections describe the options that may be passed to QuestDB when\nstarting the server from the command line.\n- Linux\n- macOS (Homebrew)\n- Windows\n\n```shell\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n```\n\n\n```shell\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n```\n\n\n```shell\nquestdb.exe [start|stop|status|install|remove] \\  [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n```\n\n\n### Start‚Äã\n\nstart\n- starts QuestDB as a service.\n\n| Option | Description |\n| --- | --- |\n| -d | Expects adirdirectory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see thedefault rootsection below. |\n| -t | Expects atagstring value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will bequestdb. |\n| -f | Force re-deploying theWeb Console. Without this option, theWeb Consoleis cached and deployed only when missing. |\n| -n | Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. |\n| -j | Windows only!This option allows to specify a path toJAVA_HOME. |\n\nnote\n- When running multiple QuestDB services, a tag must be used to disambiguate\nbetween services forstartandstopcommands. There will be conflicting\nports and root directories if only the tag flag is specified when starting\nmultiple services. Each new service should have its own config file or should\nbe started with separate port and root directory options.\n- When running QuestDB as Windows service you can check status in both:Windows Event Viewer - look for events with \"QuestDB\" source in Windows Logs\n| Application .service log file -$dataDir\\log\\service-%Y-%m-%dT%H-%M-%S.txt(default isC:\\Windows\\System32\\qdbroot\\log\\service-%Y-%m-%dT%H-%M-%S.txt)\n- Linux\n- macOS (Homebrew)\n- Windows\n\n```shell\n./questdb.sh start [-d dir] [-f] [-n] [-t tag]\n```\n\n\n```shell\nquestdb start [-d dir] [-f] [-n] [-t tag]\n```\n\n\n```shell\nquestdb.exe start [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n```\n\n\n#### Default root directory‚Äã\n\nBy default, QuestDB's\nroot directory\nwill be the following:\n- Linux\n- macOS (Homebrew)\n- Windows\n\n```shell\n$HOME/.questdb\n```\n\nPath on Macs with Apple Silicon (M1 or M2) chip:\n\n```shell\n/opt/homebrew/var/questdb\n```\n\nPath on Macs with Intel chip:\n\n```shell\n/usr/local/var/questdb\n```\n\n\n```shell\nC:\\Windows\\System32\\qdbroot\n```\n\n\n### Stop‚Äã\n\nstop\n- stops a service.\n\n| Option | Description |\n| --- | --- |\n| -t | Expects atagstring value which to stop a service by tag. If this is omitted, the default tag will bequestdb |\n\n- Linux\n- macOS (Homebrew)\n- Windows\n\n```shell\n./questdb.sh stop\n```\n\n\n```shell\nquestdb stop\n```\n\n\n```shell\nquestdb.exe stop\n```\n\n\n### Status‚Äã\n\nstatus\n- shows the status for a service.\n\n| Option | Description |\n| --- | --- |\n| -t | Expects atagstring value which to stop a service by tag. If this is omitted, the default will bequestdb |\n\n- Linux\n- macOS (Homebrew)\n- Windows\n\n```shell\n./questdb.sh status\n```\n\n\n```shell\nquestdb status\n```\n\n\n```shell\nquestdb.exe status\n```\n\n\n### Install (Windows)‚Äã\n\ninstall\n- installs the Windows QuestDB service. The service will start\nautomatically at startup.\n\n```shell\nquestdb.exe install\n```\n\n\n### Remove (Windows)‚Äã\n\nremove\n- removes the Windows QuestDB service. It will no longer start at\nstartup.\n\n```shell\nquestdb.exe remove\n```\n\n\n## Interactive session (Windows)‚Äã\n\nYou can start QuestDB interactively by running\nquestdb.exe\n. This will launch\nQuestDB interactively in the active\nShell\nwindow. QuestDB will be stopped when\nthe Shell is closed.\n\n### Default root directory‚Äã\n\nWhen started interactively, QuestDB's root directory defaults to the\ncurrent\ndirectory.\n\n### Stop‚Äã\n\nTo stop, press\nCtrl\n+\nC\nin the terminal or close it\ndirectly.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 629,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-1cb2153ec895",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/operators/tick",
    "title": "TICK interval syntax | QuestDB",
    "text": "TICK (Temporal Interval Calendar Kit) is a syntax for expressing complex\ntemporal intervals in a single string. Use it with the\nIN\noperator to query\nmultiple time ranges, schedules, and patterns efficiently.\n\n```questdb-sql\n-- NYSE trading hours on workdays for JanuarySELECT * FROM tradesWHERE ts IN '2025-01-[02..8,10..19,21..31]T09:30@America/New_York#workday;6h30m';\n```\n\nThis single expression generates interval scans for every weekday except\nholidays in January, each starting at 9:30 AM New York time and lasting 6 hours\n30 minutes.\nEnterprise\n‚Äî\nWith\nexchange calendars\n, TICK\ndirectly understands exchange schedules including holidays, early closes, and\nlunch breaks. Here's an expression equivalent to the one above (XNYS is the\nISO 10383 MIC code of NYSE):\n\n```questdb-sql\n-- NYSE trading hours for January, holidays excluded automaticallySELECT * FROM tradesWHERE ts IN '2025-01-[01..31]#XNYS';\n```\n\nLearn more\nKey Points\n- TICK = declarative syntax for complex time intervals inWHERE ts IN '...'\n- Syntax order:date [T time] @timezone #dayFilter ;duration\n- Each generated interval uses optimizedinterval scan(binary search)\n- Use[a,b,c]for values,[a..b]for ranges,#workdayfor day filters\n- Overlapping intervals are automatically merged\n\n## Grammar summary‚Äã\n\n\n```text\nTICK_EXPR     = DATE_PART [TIME] [TIMEZONE] [FILTER] [DURATION]DATE_PART     = literal_date                    -- '2024-01-15'              | date_list                       -- '[2024-01-15, 2024-03-20]'              | date_variable                   -- '$today', '$now - 2h'              | bracket_expansion               -- '2024-01-[10..15]'              | iso_week                        -- '2024-W01-1'TIME          = 'T' time_value                  -- 'T09:30'              | 'T' bracket_expansion           -- 'T[09:00,14:30]'TIMEZONE      = '@' iana_name                   -- '@America/New_York'              | '@' offset                      -- '@+02:00', '@UTC'FILTER        = '#workday' | '#weekend'         -- business day filters              | '#' day_list                    -- '#Mon,Wed,Fri'              | '#' exchange_code              -- '#XNYS' (exchange calendar, Enterprise)DURATION      = ';' duration_value              -- ';6h30m'-- Bracket expansion: generates multiple values from a single fieldbracket_expansion = '[' expansion_item (',' expansion_item)* ']'expansion_item    = value                       -- single: [10]                  | value '..' value            -- range:  [10..15]                  -- mixed example: [5,10..12,20] = 5, 10, 11, 12, 20-- Date list: multiple complete dates (can nest bracket expansions)date_list    = '[' date_entry (',' date_entry)* ']'date_entry   = literal_date                     -- '2024-01-15'             | date_variable                    -- '$today'             | literal_date with brackets       -- '2024-01-[01..05]'-- Date variable with optional arithmetic and rangesdate_variable = '$today' | '$yesterday' | '$tomorrow' | '$now'              | date_variable ('+' | '-') amount unit              | date_variable '..' date_variable   -- '$now - 2h..$now'unit          = 'y' | 'M' | 'w' | 'd' | 'bd' | 'h' | 'm' | 's' | 'T' | 'u' | 'n'              --  ‚Üë    ‚Üë         ‚Üë    ‚Üë              -- 'bd' (business days) valid only in date arithmetic, not duration\n```\n\nThe\nexchange_code\nfilter uses an ISO 10383 MIC code (e.g.,\n#XNYS\n) to apply\nreal exchange trading schedules. See\nexchange calendars\nfor details.\n\n## Why TICK‚Äã\n\nTraditional approaches to complex time queries require:\n- MultipleUNION ALLstatements\n- Application-side date generation\n- ComplexBETWEENlogic with manyORclauses\nTICK replaces all of these with a declarative syntax that generates multiple\noptimized interval scans from a single expression.\nUse TICK when:\n- Querying relative time windows ($now - 1h..$now,$today)\n- Building rolling windows with business day calculations\n- Working with schedules (workdays, weekends, specific days)\n- Needing timezone-aware time windows with DST handling\n- Querying multiple non-contiguous dates or time windows\nUse simpleINorBETWEENwhen:\n- Single continuous time range with absolute dates (WHERE ts IN '2024-01-15')\n- Simple date/time literals without patterns or variables\n\n## Quick start‚Äã\n\nCommon patterns to get started:\n\n```questdb-sql\n-- Last hour of dataWHERE ts IN '$now - 1h..$now'-- Last 30 minutesWHERE ts IN '$now - 30m..$now'-- Today's data (full day)WHERE ts IN '$today'-- Last 5 business daysWHERE ts IN '$today - 5bd..$today - 1bd'-- Workdays only with time windowWHERE ts IN '2024-01-[01..31]T09:00#workday;8h'-- Multiple times on one dayWHERE ts IN '2024-01-15T[09:00,12:00,18:00];1h'-- With timezoneWHERE ts IN '2024-01-15T09:30@America/New_York;6h30m'\n```\n\n\n## Syntax order‚Äã\n\nComponents must appear in this order:\n\n```text\ndate [T time] @ timezone # dayFilter ; duration ‚îÇ       ‚îÇ         ‚îÇ          ‚îÇ           ‚îÇ ‚îÇ       ‚îÇ         ‚îÇ          ‚îÇ           ‚îî‚îÄ interval length (e.g., ;6h30m) ‚îÇ       ‚îÇ         ‚îÇ          ‚îî‚îÄ day filter (e.g., #workday) ‚îÇ       ‚îÇ         ‚îî‚îÄ timezone (e.g., @America/New_York) ‚îÇ       ‚îî‚îÄ time component (e.g., T09:30) ‚îî‚îÄ date with optional brackets (e.g., 2024-01-[01..31])\n```\n\nExamples showing the order:\n\n| Expression | Components used |\n| --- | --- |\n| '2024-01-15' | date only |\n| '2024-01-15T09:30' | date + time |\n| '2024-01-15T09:30@UTC' | date + time + timezone |\n| '2024-01-15T09:30#workday' | date + time + filter |\n| '2024-01-15T09:30;1h' | date + time + duration |\n| '2024-01-15T09:30@America/New_York#workday;6h30m' | all components |\n\n\n## Quick reference‚Äã\n\n\n| Feature | Syntax | Example |\n| --- | --- | --- |\n| Bracket expansion | [a,b,c] | '2024-01-[10,15,20]' |\n| Range expansion | [a..b] | '2024-01-[10..15]' |\n| Date list | [date1,date2] | '[2024-01-15,2024-03-20]' |\n| Time list | T[time1,time2] | '2024-01-15T[09:00,14:30]' |\n| Timezone | @timezone | 'T09:00@America/New_York' |\n| Day filter | #filter | '#workday','#Mon,Wed,Fri' |\n| Duration | ;duration | ';6h30m',';1h' |\n| ISO week | YYYY-Www-D | '2024-W01-1' |\n| Date variable | $var | '$today','$now - 2h' |\n| Date arithmetic | $var ¬± Nu | '$today+5bd','$now-30m','$today+1M' |\n| Variable range | $start..$end | '$now-2h..$now','$today..$today+5d' |\n\n\n## Interval behavior‚Äã\n\n\n### Whitespace‚Äã\n\nWhitespace is flexible in TICK expressions:\n\n```questdb-sql\n-- Inside brackets - spaces are ignored:'2024-01-[10,15,20]''2024-01-[ 10 , 15 , 20 ]'-- Around arithmetic operators - spaces are optional:'$now - 2h..$now''$now-2h..$now'\n```\n\n\n### Interval merging‚Äã\n\nWhen expanded intervals overlap, they are automatically merged:\n\n```questdb-sql\n'2024-01-15T[09:00,10:30];2h'-- 09:00-11:00 overlaps with 10:30-12:30-- Result: single merged interval 09:00-12:30\n```\n\nThis ensures efficient query execution without duplicate scans.\n\n### Optional brackets for date variables‚Äã\n\nSingle date variables can omit brackets, even with suffixes:\n\n```questdb-sql\n-- Single variable - brackets optional:WHERE ts IN '$today'WHERE ts IN '$now;1h'WHERE ts IN '$todayT09:30'WHERE ts IN '$today@Europe/London'\n```\n\nRanges can also omit brackets when used alone:\n\n```questdb-sql\n-- Range without suffixes - brackets optional:WHERE ts IN '$now - 2h..$now'WHERE ts IN '$today..$today + 5d'\n```\n\nBrackets are\nrequired\nfor:\n\n```questdb-sql\n-- Ranges with suffixes - brackets required:WHERE ts IN '[$now - 2h..$now]@America/New_York'WHERE ts IN '[$today..$today + 5d]#workday;8h'-- Lists - brackets required:WHERE ts IN '[$today, $yesterday, 2024-01-15]'\n```\n\n\n## Date variables‚Äã\n\nUse dynamic date references that resolve at query time:\n\n| Variable | Description | Interval type | Example value (Jan 22, 2026 at 14:35:22) |\n| --- | --- | --- | --- |\n| $today | Current day | Full day | 2026-01-22T00:00:00to2026-01-22T23:59:59.999999 |\n| $yesterday | Previous day | Full day | 2026-01-21T00:00:00to2026-01-21T23:59:59.999999 |\n| $tomorrow | Next day | Full day | 2026-01-23T00:00:00to2026-01-23T23:59:59.999999 |\n| $now | Current timestamp | Point-in-time | 2026-01-22T14:35:22.123456(exact moment) |\n\nInterval vs point-in-time\n- $today,$yesterday,$tomorrowproducefull day intervals(midnight to midnight)\n- $nowproduces apoint-in-time(exact moment with microsecond\nprecision)\nWithout a duration suffix,\n$now\nmatches only the exact microsecond. Add a\nduration or use a range to create a useful window:\n\n```questdb-sql\n-- Point-in-time: matches only the exact microsecond (rarely useful alone)WHERE ts IN '$now'-- 1-hour window starting at current moment (extends forward)WHERE ts IN '$now;1h'-- Last 2 hours (from 2h ago until now)WHERE ts IN '$now - 2h..$now'\n```\n\nVariables are case-insensitive:\n$TODAY\n,\n$Today\n, and\n$today\nare equivalent.\n\n### Date arithmetic‚Äã\n\nAdd or subtract time from date variables using any\ntime unit\n. All\nunits except\nbd\n(business days) work in both duration and arithmetic contexts.\n\n```questdb-sql\n-- Calendar day arithmetic'$today + 5d'      -- 5 days from today'$today - 3d'      -- 3 days ago-- Business day arithmetic (skips weekends) - arithmetic only'$today + 1bd'     -- next business day'$today - 5bd'     -- 5 business days ago-- Hour/minute/second arithmetic (typically with $now)'$now - 2h'        -- 2 hours ago'$now - 30m'       -- 30 minutes ago'$now - 90s'       -- 90 seconds ago-- Sub-second precision'$now - 500T'      -- 500 milliseconds ago'$now - 100u'      -- 100 microseconds ago-- Calendar-aware units (handle varying month lengths, leap years)'$today + 1M'      -- same day next month'$today + 1y'      -- same day next year'$today + 2w'      -- 2 weeks from today\n```\n\n\n### Date variable ranges‚Äã\n\nGenerate multiple intervals from start to end:\n\n```questdb-sql\n-- Next 5 calendar days'$today..$today + 5d'-- Next 5 business days (weekdays only)'$today..$today + 5bd'-- Last work week'$today - 5bd..$today - 1bd'-- Last 2 hours'$now - 2h..$now'-- Last 30 minutes'$now - 30m..$now'-- Next 3 months'$today..$today + 3M'\n```\n\nRanges vs durations\nRanges\n(\n$start..$end\n) create a single continuous interval from start to\nend:\n\n```questdb-sql\n-- Single interval: from 2 hours ago until now'$now - 2h..$now'-- Single interval: from 3 days ago until today (end of day)'$today - 3d..$today'\n```\n\nDurations\n(\n;Nh\n) extend from a point by the specified amount:\n\n```questdb-sql\n-- Single interval: starting at $now, lasting 2 hours forward'$now;2h'\n```\n\nFor multiple discrete intervals, use a list with duration:\n\n```questdb-sql\n-- Three separate 1-hour intervals'[$now - 3h, $now - 2h, $now - 1h];1h'\n```\n\n\n### Mixed date lists‚Äã\n\nCombine variables with static dates (brackets required for lists):\n\n```questdb-sql\n-- Today, yesterday, and a specific dateSELECT * FROM trades WHERE ts IN '[$today, $yesterday, 2024-01-15]';-- Compare today vs same day last weekSELECT * FROM trades WHERE ts IN '[$today, $today - 7d]T09:30;6h30m';-- Hourly windows starting 4 hours agoSELECT * FROM trades WHERE ts IN '[$now - 4h, $now - 3h, $now - 2h, $now - 1h, $now]';\n```\n\n\n## Bracket expansion‚Äã\n\nBrackets expand a single field into multiple values:\n\n```questdb-sql\n-- Days 10, 15, and 20 of JanuarySELECT * FROM trades WHERE ts IN '2024-01-[10,15,20]';-- Days 10 through 15 (inclusive range)SELECT * FROM trades WHERE ts IN '2024-01-[10..15]';-- Mixed: specific values and rangesSELECT * FROM trades WHERE ts IN '2024-01-[5,10..12,20]';\n```\n\n\n### Multiple brackets (Cartesian product)‚Äã\n\nMultiple bracket groups produce all combinations:\n\n```questdb-sql\n-- January and June, 10th and 15th = 4 intervalsSELECT * FROM trades WHERE ts IN '2024-[01,06]-[10,15]';-- Expands to: 2024-01-10, 2024-01-15, 2024-06-10, 2024-06-15\n```\n\n\n### Bracket positions‚Äã\n\nBrackets work in any numeric field:\n\n| Field | Example | Result |\n| --- | --- | --- |\n| Month | '2024-[01,06]-15' | Jan 15, Jun 15 |\n| Day | '2024-01-[10,15]' | 10th, 15th |\n| Hour | '2024-01-10T[09,14]:30' | 09:30, 14:30 |\n| Minute | '2024-01-10T10:[00,30]' | 10:00, 10:30 |\n\n\n## Date lists‚Äã\n\nStart with\n[\nfor non-contiguous dates:\n\n```questdb-sql\n-- Specific datesSELECT * FROM trades WHERE ts IN '[2024-01-15,2024-03-20,2024-06-01]';-- With nested bracket expansionSELECT * FROM trades WHERE ts IN '[2024-12-31,2025-01-[01..05]]';-- Expands to: Dec 31, Jan 1, Jan 2, Jan 3, Jan 4, Jan 5\n```\n\n\n### Date lists with time suffix‚Äã\n\n\n```questdb-sql\n-- 09:30 on specific datesSELECT * FROM trades WHERE ts IN '[2024-01-15,2024-01-20]T09:30';-- Trading hours on specific datesSELECT * FROM trades WHERE ts IN '[2024-01-15,2024-01-20]T09:30;6h30m';\n```\n\n\n## Time lists‚Äã\n\nSpecify multiple complete times with colons inside brackets:\n\n```questdb-sql\n-- Morning and evening sessionsSELECT * FROM trades WHERE ts IN '2024-01-15T[09:00,18:00];1h';-- Three daily check-insSELECT * FROM metrics WHERE ts IN '2024-01-15T[08:00,12:00,18:00];30m';\n```\n\nTime list vs numeric expansion\nThe presence of\n:\ninside the bracket determines the mode:\n\n| Syntax | Mode | Expands to |\n| --- | --- | --- |\n| T[09,14]:30 | Numeric expansion (hour field) | 09:30 and 14:30 |\n| T[09:00,14:30] | Time list (complete times) | 09:00 and 14:30 |\n\nUse\nnumeric expansion\nwhen times share the same minutes (e.g., both at :30).\nUse\ntime lists\nwhen times differ completely (e.g., 09:00 and 14:30).\n\n## Timezone support‚Äã\n\nAdd\n@timezone\nafter the time component:\n\n```questdb-sql\n-- 09:30 in New York time (automatically handles DST)SELECT * FROM trades WHERE ts IN '2024-01-15T09:30@America/New_York';-- Numeric offsetSELECT * FROM trades WHERE ts IN '2024-01-15T09:30@+02:00';-- UTCSELECT * FROM trades WHERE ts IN '2024-01-15T09:30@UTC';\n```\n\n\n### Supported timezone formats‚Äã\n\n\n| Format | Example |\n| --- | --- |\n| IANA name | @America/New_York,@Europe/London |\n| Offset | @+03:00,@-05:00 |\n| Compact offset | @+0300,@-0500 |\n| Hour-only | @+03,@-05 |\n| UTC/GMT | @UTC,@GMT,@Z |\n\n\n### Per-element timezones‚Äã\n\nEach date or time can have its own timezone:\n\n```questdb-sql\n-- Market opens in different citiesSELECT * FROM tradesWHERE ts IN '2024-01-15T[09:30@America/New_York,08:00@Europe/London,09:00@Asia/Tokyo];6h';-- Per-date timezone (comparing same local time in winter vs summer)SELECT * FROM tradesWHERE ts IN '[2024-01-15@Europe/London,2024-07-15@Europe/London]T08:00';\n```\n\n\n## Day-of-week filter‚Äã\n\nAdd\n#filter\nto include only specific days:\n\n```questdb-sql\n-- Workdays only (Monday-Friday)SELECT * FROM trades WHERE ts IN '2024-01-[01..31]#workday';-- Weekends onlySELECT * FROM logs WHERE ts IN '2024-01-[01..31]T02:00#weekend;4h';-- Specific daysSELECT * FROM attendance WHERE ts IN '2024-01-[01..31]#Mon,Wed,Fri';\n```\n\n\n### Available filters‚Äã\n\n\n| Filter | Days included |\n| --- | --- |\n| #workdayor#wd | Monday - Friday |\n| #weekend | Saturday, Sunday |\n| #Mon,#Tue, etc. | Specific day |\n| #Mon,Wed,Fri | Multiple days |\n\nDay names are case-insensitive. Both\n#Mon\nand\n#Monday\nwork.\n\n### Filter with timezone‚Äã\n\nThe filter applies to\nlocal time\nbefore timezone conversion:\n\n```questdb-sql\n-- 09:30 New York time, workdays only-- \"Monday\" means Monday in New York, not Monday in UTCSELECT * FROM tradesWHERE ts IN '2024-01-[01..31]T09:30@America/New_York#workday;6h30m';\n```\n\n\n## Duration suffix‚Äã\n\nAdd\n;duration\nto specify interval length:\n\n```questdb-sql\n-- 1-hour intervalsSELECT * FROM trades WHERE ts IN '2024-01-15T09:00;1h';-- 6 hours 30 minutes (NYSE trading day)SELECT * FROM trades WHERE ts IN '2024-01-15T09:30;6h30m';-- Precise sub-second durationSELECT * FROM hft_data WHERE ts IN '2024-01-15T09:30:00;1s500T';\n```\n\n\n### Time units‚Äã\n\n\n| Unit | Name | Description | Duration | Arithmetic |\n| --- | --- | --- | --- | --- |\n| y | Years | Calendar years (handles leap years) | Yes | Yes |\n| M | Months | Calendar months (handles varying lengths) | Yes | Yes |\n| w | Weeks | 7 days | Yes | Yes |\n| d | Days | 24 hours | Yes | Yes |\n| bd | Business days | Weekdays only (skips Sat/Sun) | No | Yes |\n| h | Hours | 60 minutes | Yes | Yes |\n| m | Minutes | 60 seconds | Yes | Yes |\n| s | Seconds | 1,000 milliseconds | Yes | Yes |\n| T | Milliseconds | 1,000 microseconds | Yes | Yes |\n| u | Microseconds | 1,000 nanoseconds | Yes | Yes |\n| n | Nanoseconds | Base unit | Yes | Yes |\n\nUnits are case-sensitive:\nM\n= months,\nm\n= minutes,\nT\n= milliseconds. The\nd\nunit also accepts uppercase\nD\nfor backward compatibility.\n\n### Multi-unit durations‚Äã\n\nCombine units for precise specifications:\n\n```questdb-sql\n-- 2 hours, 15 minutes, 30 seconds';2h15m30s'-- 500 milliseconds + 250 microseconds';500T250u'-- NYSE trading hours';6h30m'\n```\n\n\n## ISO week dates‚Äã\n\nUse ISO 8601 week format for weekly schedules:\n\n```questdb-sql\n-- Week 1 of 2024 (entire week)SELECT * FROM trades WHERE ts IN '2024-W01';-- Monday of week 1 (day 1 = Monday)SELECT * FROM trades WHERE ts IN '2024-W01-1';-- Friday of week 1 at 09:00SELECT * FROM trades WHERE ts IN '2024-W01-5T09:00';\n```\n\n\n### Week bracket expansion‚Äã\n\n\n```questdb-sql\n-- First 4 weeks of the yearSELECT * FROM trades WHERE ts IN '2024-W[01..04]';-- Weekdays (Mon-Fri) of week 1SELECT * FROM trades WHERE ts IN '2024-W01-[1..5]';-- Every Monday and Friday of weeks 1-4SELECT * FROM trades WHERE ts IN '2024-W[01..04]-[1,5]';\n```\n\n\n### Day-of-week values‚Äã\n\n\n| Value | Day |\n| --- | --- |\n| 1 | Monday |\n| 2 | Tuesday |\n| 3 | Wednesday |\n| 4 | Thursday |\n| 5 | Friday |\n| 6 | Saturday |\n| 7 | Sunday |\n\n\n## Complete examples‚Äã\n\n\n### Trading hours‚Äã\n\n\n```questdb-sql\n-- NYSE trading hours for January workdaysSELECT * FROM nyse_tradesWHERE ts IN '2024-01-[01..31]T09:30@America/New_York#workday;6h30m';-- Compare trading sessions across marketsSELECT * FROM global_tradesWHERE ts IN '2024-01-15T[09:30@America/New_York,08:00@Europe/London,09:00@Asia/Tokyo];6h';\n```\n\n\n### Scheduled reports‚Äã\n\n\n```questdb-sql\n-- Weekly Monday standup (52 weeks)SELECT * FROM standup_notesWHERE ts IN '2024-W[01..52]-1T09:00;1h';-- Bi-weekly Friday reportsSELECT * FROM reportsWHERE ts IN '2024-W[02,04,06,08,10,12]-5T14:00;2h';\n```\n\n\n### Rolling windows‚Äã\n\n\n```questdb-sql\n-- Last 5 trading days at market openSELECT * FROM pricesWHERE ts IN '[$today - 5bd..$today - 1bd]T09:30@America/New_York;1m';-- Same hour comparison across recent daysSELECT * FROM metricsWHERE ts IN '[$today - 2d, $yesterday, $today]T14:00;1h';\n```\n\n\n### Real-time monitoring‚Äã\n\n\n```questdb-sql\n-- Last 2 hours of dataSELECT * FROM sensor_dataWHERE ts IN '$now - 2h..$now';-- Last 30 minutesSELECT * FROM metricsWHERE ts IN '$now - 30m..$now';-- Last 90 seconds (useful for dashboards)SELECT * FROM logsWHERE ts IN '$now - 90s..$now';-- Sub-second precision for high-frequency dataSELECT * FROM hft_dataWHERE ts IN '$now - 500T..$now';-- Hourly snapshots from last 4 hoursSELECT * FROM tradesWHERE ts IN '[$now - 4h, $now - 3h, $now - 2h, $now - 1h, $now];5m';\n```\n\n\n### Maintenance windows‚Äã\n\n\n```questdb-sql\n-- Weekend maintenance (every Sat/Sun at 02:00)SELECT * FROM system_logsWHERE ts IN '2024-01-[01..31]T02:00#weekend;4h';-- Quarterly maintenance (first Sunday of each quarter)SELECT * FROM maintenanceWHERE ts IN '2024-[01,04,07,10]-[01..07]T02:00#Sun;6h';\n```\n\n\n## Performance‚Äã\n\nTICK expressions are fully optimized by QuestDB's query engine:\n- Interval scan‚Äî Each generated interval uses binary search on thedesignated timestamp\n- Partition pruning‚Äî Partitions outside all intervals are skipped entirely\n- Parallel expansion‚Äî Complex expressions generate multiple efficient\ninterval scans\nA TICK expression like\n'2024-01-[01..31]T09:00#workday;8h'\n(22 workdays)\nperforms comparably to 22 separate simple queries, but with a single parse.\nUse\nEXPLAIN\nto see the generated intervals:\n\n```questdb-sql\nEXPLAIN SELECT * FROM tradesWHERE ts IN '2024-01-[15,16,17]T09:00;1h';\n```\n\n\n## Error messages‚Äã\n\n\n| Error | Cause |\n| --- | --- |\n| Unclosed '[' in interval | Missing closing bracket |\n| Empty bracket expansion | Nothing inside brackets |\n| Range must be ascending: 15..10 | End before start in range |\n| Invalid timezone: xyz | Unknown timezone |\n| Unknown date variable: $invalid | Unrecognized variable |\n| Invalid day name: xyz | Unknown day in filter |\n\n\n## See also‚Äã\n\n- Designated timestamp‚Äî Required for\ninterval scan optimization\n- Interval scan‚Äî How QuestDB\noptimizes time queries\n- WHERE clause‚Äî Full WHERE syntax reference\n- Date/time operators‚Äî Additional timestamp\noperators",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2882,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-c54756f7ee4b",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/deep-dive/interval-scan",
    "title": "Interval Scan | QuestDB",
    "text": "An\ninterval scan\nis QuestDB's optimized method for querying time ranges.\nInstead of scanning all rows, QuestDB uses binary search on the\ndesignated timestamp\ncolumn to jump\ndirectly to relevant data.\nFor how interval scans work and their performance impact, see\nDesignated timestamp: Performance impact\n.\nFor complex multi-interval patterns, see\nTICK interval syntax\n.\n\n## How it looks‚Äã\n\nThe query engine:\n- Prunes partitions outside the time range\n- Binary searches within relevant partitions to find exact row boundaries\n- Reads only rows within those boundaries\n\n## Verifying interval scan with EXPLAIN‚Äã\n\nUse\nEXPLAIN\nto confirm a query uses interval scan:\nCheck for interval scan\nDemo this query\n\n```questdb-sql\nEXPLAIN SELECT * FROM tradesWHERE timestamp IN '2024-01-20';\n```\n\nGood\n- Interval scan is being used:\n\n```questdb-sql\n| QUERY PLAN                                                    ||---------------------------------------------------------------|| DataFrame                                                     ||     Row forward scan                                          ||     Interval forward scan on: trades                          ||       intervals: [(\"2024-01-20T00:00:00.000000Z\",             ||                    \"2024-01-20T23:59:59.999999Z\")]            |\n```\n\nNot optimal\n- Full scan with async filter:\n\n```questdb-sql\n| QUERY PLAN                                                    ||---------------------------------------------------------------|| Async Filter                                                  ||     workers: 4                                                ||     filter: timestamp IN '2024-01-20'                         ||     DataFrame                                                 ||         Full scan on: trades                                  |\n```\n\nIf you see\nAsync Filter\nor\nFull scan\ninstead of\nInterval forward scan\n,\nthe query is not using the designated timestamp optimization.\n\n## Equivalent query forms‚Äã\n\nThese queries all produce the same interval scan plan:\nUsing IN\n\n```questdb-sql\nSELECT * FROM trades WHERE timestamp IN '2024-01-20';\n```\n\nUsing BETWEEN\n\n```questdb-sql\nSELECT * FROM tradesWHERE timestamp BETWEEN '2024-01-20T00:00:00.000000Z'                    AND '2024-01-20T23:59:59.999999Z';\n```\n\nUsing comparison operators\n\n```questdb-sql\nSELECT * FROM tradesWHERE timestamp >= '2024-01-20T00:00:00.000000Z'  AND timestamp <= '2024-01-20T23:59:59.999999Z';\n```\n\nAll three produce:\n\n```questdb-sql\nInterval forward scan on: trades  intervals: [(\"2024-01-20T00:00:00.000000Z\",\"2024-01-20T23:59:59.999999Z\")]\n```\n\nUse whichever form is most readable for your use case.\nIN\nwith partial\ntimestamps is typically the most concise.\n\n## Multiple intervals‚Äã\n\nFor multiple time ranges, use\nTICK syntax\n:\n\n```questdb-sql\nEXPLAIN SELECT * FROM tradesWHERE timestamp IN '2024-01-[15,16,17]';\n```\n\n\n```questdb-sql\nInterval forward scan on: trades  intervals: [(\"2024-01-15T00:00:00.000000Z\",\"2024-01-15T23:59:59.999999Z\"),              (\"2024-01-16T00:00:00.000000Z\",\"2024-01-16T23:59:59.999999Z\"),              (\"2024-01-17T00:00:00.000000Z\",\"2024-01-17T23:59:59.999999Z\")]\n```\n\nEach interval uses binary search independently‚Äîcomplex patterns perform as\nfast as simple queries.\n\n## Edge cases‚Äã\n\n\n### Tables without designated timestamp‚Äã\n\nTables without a designated timestamp cannot use interval scan. Queries fall\nback to full table scan with async filter.\nTo enable interval scan, recreate the table with a designated timestamp:\n\n```questdb-sql\nCREATE TABLE trades_new (    ts TIMESTAMP,    symbol SYMBOL,    price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY;INSERT INTO trades_new SELECT * FROM trades_old ORDER BY ts;\n```\n\n\n### Declaring timestamp on query results‚Äã\n\nFor subqueries or tables without a designated timestamp, you can declare one\nusing\nTIMESTAMP(columnName)\n:\n\n```questdb-sql\nEXPLAIN SELECT * FROM trades_nodts TIMESTAMP(ts)WHERE ts IN '2024-01-20';\n```\n\nThis enables interval scan on the result.\nwarning\nTIMESTAMP(columnName)\nonly works if the data is\nactually ordered\nby that\ncolumn. If the data is not in timestamp order, query results will be incorrect.\nFor unordered data, add\nORDER BY\nfirst:\n\n```questdb-sql\nSELECT * FROM (SELECT * FROM unordered_table ORDER BY ts) TIMESTAMP(ts)WHERE ts IN '2024-01-20';\n```\n\n\n### Subqueries lose designated timestamp‚Äã\n\nSubquery results don't inherit the designated timestamp from the source table:\n\n```questdb-sql\n-- This does NOT use interval scan on the subquery result:SELECT * FROM (SELECT * FROM trades WHERE symbol = 'BTC-USD')WHERE timestamp IN '2024-01-20';\n```\n\nTo restore interval scan, explicitly declare the timestamp:\n\n```questdb-sql\n-- This uses interval scan:SELECT * FROM (SELECT * FROM trades WHERE symbol = 'BTC-USD') TIMESTAMP(timestamp)WHERE timestamp IN '2024-01-20';\n```\n\nSee\nDesignated timestamp: Troubleshooting\nfor more scenarios where designated timestamp is lost.\n\n## See also‚Äã\n\n- Designated timestamp‚Äî Why interval scan works\n- TICK intervals‚Äî Complex multi-interval patterns\n- EXPLAIN‚Äî Query plan analysis",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 589,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-820522b0a925",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/designated-timestamp",
    "title": "Designated timestamp | QuestDB",
    "text": "Every table in QuestDB should have a designated timestamp. This column defines\nthe time axis for your data and unlocks QuestDB's core time-series capabilities\nincluding partitioning, time-series joins, and optimized interval scans.\nWithout a designated timestamp, a table behaves like a generic append-only\nstore‚Äîyou lose partitioning, efficient time-range queries, and most time-series\nSQL features.\nKey Points\n- The designated timestamp column defines your table's time axis\n- Data is physically sorted by this column, enabling sub-millisecond time-range queries\n- Enables: partitioning, SAMPLE BY, LATEST ON, ASOF JOIN, TTL, deduplication, replication\n- Constraints: cannot be NULL, cannot be changed after creation, cannot be updated\n- Without it: no partitioning, time queries must load all data into RAM\n\n## Why designated timestamp exists‚Äã\n\nTraditional databases store rows in insertion order or by primary key. When you\nquery \"show me the last 5 minutes of data,\" the database must scan the entire\ntable to find matching rows‚Äîeven if that's 0.001% of your data.\nFor time-series workloads, this is catastrophically inefficient. Consider a\ntable with 1 billion rows spanning 30 days. A query for \"last hour\" should read\n~1.4 million rows, not 1 billion.\nQuestDB solves this with the designated timestamp:\n\n| Problem | Solution |\n| --- | --- |\n| Data scattered across disk | Data storedphysically sortedby timestamp |\n| Must scan entire table for time queries | Binary searchjumps directly to relevant rows |\n| Can't skip irrelevant data | Partition pruningskips entire time ranges |\n| Time-series operations require sorting | Data ispre-sorted, no runtime cost |\n\nThe designated timestamp is not just metadata‚Äîit fundamentally changes how\nQuestDB stores and queries your data.\n\n## Performance impact‚Äã\n\nQuestDB's query engine leverages the designated timestamp aggressively:\n- Timestamp predicates execute first‚Äî Before any other filters\n- Partition pruning‚Äî Entire partitions outside the time range are skipped,\nreducing I/O\n- Binary search within partitions‚Äî Finds exact row boundaries without\nscanning\n- Targeted column reads‚Äî Only the relevant data frames from other columns\nare read from disk\nThe result: most queries with timestamp predicates complete in\nsub-millisecond\ntime, regardless of total table size. A query for \"last hour\" on a table with\nbillions of rows performs the same as on a table with thousands‚Äîonly the\nmatching rows are touched.\n\n### Advanced: TICK interval syntax‚Äã\n\nFor complex temporal patterns, use\nTICK\nsyntax\nto generate multiple optimized interval scans from a single expression:\n\n```questdb-sql\n-- NYSE trading hours on workdays for January (22 intervals, one query)SELECT * FROM tradesWHERE ts IN '2024-01-[01..31]T09:30@America/New_York#workday;6h30m';-- Last 5 business days at market openSELECT * FROM tradesWHERE ts IN '[$today-5bd..$today-1bd]T09:30;1h';\n```\n\nEach generated interval uses the same binary search optimization‚Äîcomplex\nschedules perform as fast as simple time-range queries.\n\n## How it works‚Äã\n\n\n### Physical storage order‚Äã\n\nWhen you designate a timestamp column, QuestDB stores all rows sorted by that\ncolumn's values. New data appends efficiently when it arrives in chronological\norder. When data arrives out of order, QuestDB\nrearranges it\nto maintain timestamp order.\n\n```questdb-sql\nWithout designated timestamp:     With designated timestamp:(stored in insertion order)       (stored sorted by time)‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ Row 1: 10:05:00         ‚îÇ      ‚îÇ Row 1: 10:00:00         ‚îÇ‚îÇ Row 2: 10:00:00         ‚îÇ      ‚îÇ Row 2: 10:01:15         ‚îÇ‚îÇ Row 3: 10:02:30         ‚îÇ      ‚îÇ Row 3: 10:02:30         ‚îÇ‚îÇ Row 4: 10:01:15         ‚îÇ      ‚îÇ Row 4: 10:05:00         ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚Üì                                  ‚Üì  Query for 10:01-10:03            Query for 10:01-10:03  must scan ALL rows               jumps directly to rows 2-3\n```\n\nThis physical ordering enables all downstream optimizations.\n\n### Partition assignment‚Äã\n\nThe designated timestamp determines which\npartition\nstores each row. QuestDB uses the\ntimestamp value to route rows to time-based directories (hourly, daily, weekly,\nmonthly, or yearly).\nFor example, with daily partitioning:\n- A row with timestamp2024-01-15T10:30:00Zgoes to the2024-01-15partition\n- A row with timestamp2024-01-16T08:00:00Zgoes to the2024-01-16partition\nThis physical separation allows QuestDB to skip entire partitions during queries.\n\n### Interval scan optimization‚Äã\n\nWhen you query with a time filter on the designated timestamp, QuestDB performs\nan\ninterval scan\ninstead of a full table scan:\n- Partition pruning: Skip partitions entirely outside the time range\n- Binary search: Within relevant partitions, use binary search to find\nthe exact start and end positions\n- Sequential read: Read only the rows within the boundaries\n\n```questdb-sql\n-- This query on a 1-year table with daily partitions:SELECT * FROM tradesWHERE timestamp > '2024-01-15' AND timestamp < '2024-01-16';-- Skips 364 partitions, binary searches within 1 partition-- Reads only matching rows, not the entire table\n```\n\nUse\nEXPLAIN\nto verify interval scans:\n\n```questdb-sql\nEXPLAIN SELECT * FROM trades WHERE timestamp IN '2024-01-15';\n```\n\n\n```questdb-sql\n| QUERY PLAN                                                    ||---------------------------------------------------------------|| DataFrame                                                     ||     Row forward scan                                          ||     Interval forward scan on: trades                          |  ‚Üê Interval scan!|       intervals: [(\"2024-01-15T00:00:00.000000Z\",             ||                    \"2024-01-15T23:59:59.999999Z\")]            |\n```\n\nIf you see\nAsync Filter\nor\nTable scan\ninstead of\nInterval forward scan\n,\nthe query is not using the designated timestamp optimization.\n\n## What it enables‚Äã\n\nThe designated timestamp unlocks these features:\nQuery features:\n\n| Feature | Why it needs designated timestamp |\n| --- | --- |\n| SAMPLE BY | Aggregates by time buckets on sorted data |\n| LATEST ON | Finds most recent rows using sorted order |\n| ASOF JOIN | Matches rows by nearest timestamp |\n| WINDOW JOIN | Time-windowed joins between tables |\n| Interval scan | Binary search on sorted data for time-range queries |\n\nStorage and lifecycle:\n\n| Feature | Why it needs designated timestamp |\n| --- | --- |\n| Partitioning | Routes rows to time-based partitions |\n| TTL | Drops partitions by age (requires partitioning) |\n| Deduplication | Leverages sorted order to find overlapping timestamps for efficient upsert |\n| Materialized views | SAMPLE BY-based views inherit the requirement |\n| Replication | Requires WAL, which requires partitioning |\n\n\n### Without a designated timestamp‚Äã\n\nTables without a designated timestamp lose all of the above. They are\nappropriate only for temporary tables during data manipulation.\n\n| Capability | Without designated timestamp |\n| --- | --- |\n| Time-range queries | Must load entire projection into RAM |\n| Partitioning | Not available ‚Äî single partition |\n| Tiered storage | Not available |\n| Replication | Not available |\n| ILP ingestion | HTTP ILP protocol cannot be used |\n\nnote\nException\n: Static lookup tables (country codes, currency mappings) with no\ntime dimension don't need a designated timestamp.\n\n## How to set it‚Äã\n\n\n### At table creation (recommended)‚Äã\n\nUse the\nTIMESTAMP(columnName)\nclause:\n\n```questdb-sql\nCREATE TABLE trades (    ts TIMESTAMP,    symbol SYMBOL,    price DOUBLE,    amount DOUBLE) TIMESTAMP(ts) PARTITION BY DAY;\n```\n\nThe designated timestamp column must be defined in the column list before being\nreferenced in the\nTIMESTAMP()\nclause.\n\n### Via InfluxDB Line Protocol‚Äã\n\nTables created automatically via ILP include a\ntimestamp\ncolumn as the\ndesignated timestamp, partitioned by day by default:\n\n```questdb-sql\ntrades,symbol=BTC-USD price=50000,amount=1.5 1234567890000000000        ‚îî‚îÄ‚îÄ Creates table with designated timestamp automatically\n```\n\n\n### On query results (dynamic timestamp)‚Äã\n\nFor queries that lose the designated timestamp (see\nTroubleshooting\n), use the\nTIMESTAMP()\nkeyword to restore it:\n\n```questdb-sql\nSELECT * FROM (    SELECT ts, symbol, price FROM trades    UNION ALL    SELECT ts, symbol, price FROM trades_archive    ORDER BY ts) TIMESTAMP(ts);\n```\n\nwarning\nDynamic\nTIMESTAMP()\nonly works if the data is actually sorted by that column.\nIf the data is not in order, query results will be incorrect. Always include\nORDER BY\nbefore applying\nTIMESTAMP()\non potentially unordered data.\n\n## Properties‚Äã\n\n\n| Property | Value |\n| --- | --- |\n| Eligible column types | TIMESTAMP(microseconds) orTIMESTAMP_NS(nanoseconds) |\n| Columns per table | Exactly one (or none) |\n| NULL values | Not allowed |\n| Mutability | Cannot be changed after table creation |\n| Updatability | Cannot be modified with UPDATE |\n\n\n### Timestamp resolution‚Äã\n\nQuestDB supports two timestamp resolutions:\n\n| Type | Resolution | Precision | Use case |\n| --- | --- | --- | --- |\n| TIMESTAMP | microseconds | 10‚Åª‚Å∂ s | Most applications |\n| TIMESTAMP_NS | nanoseconds | 10‚Åª‚Åπ s | High-frequency trading, scientific data |\n\nUseTIMESTAMPunless you specifically need nanosecond precision.\nBoth\ntypes work identically with all time-series features.\nFor more on timestamp handling, see\nTimestamps and time zones\n.\n\n## Limitations‚Äã\n\n\n### Cannot be changed after table creation‚Äã\n\nThe designated timestamp is set at\nCREATE TABLE\nand cannot be altered. To use\na different column:\n\n```questdb-sql\n-- 1. Create new table with correct designated timestampCREATE TABLE trades_new (    event_time TIMESTAMP,  -- new designated timestamp    ingest_time TIMESTAMP,    symbol SYMBOL,    price DOUBLE) TIMESTAMP(event_time) PARTITION BY DAY;-- 2. Copy data (will be reordered by new designated timestamp)INSERT INTO trades_newSELECT event_time, ingest_time, symbol, priceFROM tradesORDER BY event_time;-- 3. Swap tablesDROP TABLE trades;RENAME TABLE trades_new TO trades;\n```\n\nFor large tables (billions of rows), this migration can take significant time\nand disk space. Plan for:\n- Sufficient disk space for both tables temporarily\n- Application downtime or dual-write period\n- Data validation after migration\n\n### Cannot be NULL‚Äã\n\nEvery row must have a valid timestamp value. The designated timestamp column\ncannot contain NULL.\nIf your source data has missing timestamps:\n- Filter out NULL rows before inserting\n- Use a default/sentinel value (e.g.,'1970-01-01T00:00:00Z')\n- Use a different column as designated timestamp\n\n### Cannot be updated‚Äã\n\nThe designated timestamp column cannot be modified with\nUPDATE\n:\n\n```questdb-sql\n-- This will fail:UPDATE trades SET ts = '2024-01-15T12:00:00Z' WHERE symbol = 'BTC-USD';-- Error: Designated timestamp column cannot be updated\n```\n\nWhy?\nUpdating the timestamp would require reordering rows within the\npartition and potentially moving rows between partitions. This would break\nQuestDB's append-optimized storage model.\nWorkaround\n: Copy data to a temp table, modify it, and re-insert:\n\n```questdb-sql\n-- 1. Create temp table WITHOUT designated timestamp--    Copy the partition(s) containing rows you need to modifyCREATE TABLE trades_temp AS (    SELECT * FROM trades    WHERE ts IN '2024-01-15');-- 2. Drop the partition from the source tableALTER TABLE trades DROP PARTITION LIST '2024-01-15';-- 3. Update timestamps freely in the temp table (no designated timestamp)UPDATE trades_tempSET ts = dateadd('h', 1, ts)WHERE symbol = 'BTC-USD';-- 4. Re-insert into main table (data will be sorted automatically)INSERT INTO trades SELECT * FROM trades_temp;-- 5. Clean upDROP TABLE trades_temp;\n```\n\nFor ongoing correction workflows where you expect duplicate keys, consider\nusing\ndeduplication\nwith UPSERT KEYS instead.\n\n### Only one designated timestamp per table‚Äã\n\nA table can have multiple\nTIMESTAMP\ncolumns, but only one can be the\ndesignated timestamp:\n\n```questdb-sql\nCREATE TABLE orders (    exchange_ts TIMESTAMP,     -- designated timestamp (when exchange received)    gateway_ts TIMESTAMP,      -- when our gateway received    ack_ts TIMESTAMP,          -- when exchange acknowledged    symbol SYMBOL,    side SYMBOL,    qty DOUBLE) TIMESTAMP(exchange_ts) PARTITION BY DAY;\n```\n\nChoose the column you'll filter by most often in WHERE clauses.\n\n## Best practices‚Äã\n\n\n### Choosing the right column‚Äã\n\nIf your data has multiple timestamp columns:\n\n| Column type | Example | Recommended? |\n| --- | --- | --- |\n| Event time | When the trade executed | ‚úÖ Best choice |\n| Ingestion time | When QuestDB received it | ‚ö†Ô∏è Only if event time unavailable |\n| Processing time | When downstream system handled it | ‚ùå Rarely appropriate |\n\nRule of thumb\n: Choose the timestamp that:\n- You'll filter by most often in queries\n- Represents the actual time of the event\n- Has the most uniform distribution\n\n### Common concerns‚Äã\n\nDuplicate timestamps\n: Duplicate timestamp values are allowed. Multiple rows\ncan have the same designated timestamp. If you need uniqueness, enable\ndeduplication\nwith UPSERT KEYS.\nFuture timestamps and TTL\n: If you use\nTTL\nfor\nautomatic data retention, be careful with future timestamps. By default, TTL\nuses wall-clock time as the reference to prevent accidental data loss from\nfar-future timestamps. See the\nTTL documentation\nfor\ndetails.\nTimezones\n: All timestamps are stored in UTC internally. When you query with\na timezone (e.g.,\nSAMPLE BY 1d ALIGN TO CALENDAR TIME ZONE 'Europe/London'\n),\nQuestDB converts from the specified timezone to UTC for the search, then\nconverts results back. Your source data should ideally be in UTC; if not,\nuse\nto_utc()\nduring ingestion.\n\n### Multiple timestamp columns‚Äã\n\nKeep additional timestamps as regular columns:\n\n```questdb-sql\nCREATE TABLE quotes (    exchange_ts TIMESTAMP,     -- when exchange published (designated)    received_ts TIMESTAMP,     -- when we received it    symbol SYMBOL,    bid DOUBLE,    ask DOUBLE) TIMESTAMP(exchange_ts) PARTITION BY DAY;-- Query by exchange time (uses interval scan):SELECT * FROM quotesWHERE exchange_ts > dateadd('h', -1, now());-- Query by received time (full scan, but still works):SELECT * FROM quotesWHERE received_ts > dateadd('h', -1, now());\n```\n\n\n### Out-of-order data‚Äã\n\nQuestDB handles out-of-order data automatically‚Äîno special configuration\nneeded. Data arriving out of order is merged into the correct position.\nHowever, excessive out-of-order data increases write amplification. If most\nof your data arrives significantly out of order:\n- Consider using ingestion time as designated timestamp\n- Store event time as a separate indexed column\n- Use appropriate partition sizing (smaller partitions = less rewrite per\nout-of-order event)\n\n### Partition size alignment‚Äã\n\nMatch your partition interval to your designated timestamp's data distribution:\n\n| Data volume | Partition interval |\n| --- | --- |\n| < 100K rows/day | MONTHorYEAR |\n| 100K - 10M rows/day | DAY |\n| 10M - 100M rows/day | HOUR |\n| > 100M rows/day | HOUR |\n\nSee\nPartitions\nfor detailed guidance.\n\n## Troubleshooting‚Äã\n\nCertain SQL operations produce results without a designated timestamp. This\nbreaks time-series features like SAMPLE BY on the result set.\n\n### Operations that lose designated timestamp‚Äã\n\n\n| Operation | Why | Solution |\n| --- | --- | --- |\n| UNION/UNION ALL | Combined results aren't guaranteed ordered | ORDER BYthenTIMESTAMP() |\n| Subqueries | Derived tables lose table metadata | ApplyTIMESTAMP()to subquery |\n| read_parquet() | External files have no QuestDB metadata | ORDER BYthenTIMESTAMP() |\n| Type casting | ts::STRING::TIMESTAMPloses designation | Avoid round-trip casting |\n| Some expressions | Computed timestamps aren't designated | UseTIMESTAMP()on result |\n\n\n### How to restore it‚Äã\n\nUse the\nTIMESTAMP()\nkeyword on ordered data:\n\n```questdb-sql\n-- UNION loses designated timestamp-- Solution: ORDER BY, then apply TIMESTAMP()SELECT * FROM (    SELECT ts, symbol, price FROM trades_2023    UNION ALL    SELECT ts, symbol, price FROM trades_2024    ORDER BY ts) TIMESTAMP(ts)SAMPLE BY 1h;\n```\n\n\n```questdb-sql\n-- Parquet files have no designated timestamp-- Solution: ORDER BY, then apply TIMESTAMP()SELECT timestamp, avg(price)FROM (    (SELECT * FROM read_parquet('trades.parquet') ORDER BY timestamp)    TIMESTAMP(timestamp))SAMPLE BY 1m;\n```\n\n\n```questdb-sql\n-- Subquery loses designated timestamp-- Solution: Apply TIMESTAMP() to the subquery resultWITH recent AS (    (SELECT * FROM trades WHERE timestamp > dateadd('d', -7, now()))    TIMESTAMP(timestamp))SELECT * FROM recent SAMPLE BY 1h;\n```\n\n\n### Verifying designated timestamp‚Äã\n\nCheck if a table has a designated timestamp:\n\n```questdb-sql\nSELECT table_name, designatedTimestampFROM tables()WHERE table_name = 'trades';\n```\n\n\n| table_name | designatedTimestamp |\n| --- | --- |\n| trades | ts |\n\nCheck column details:\n\n```questdb-sql\nSELECT \"column\", type, designatedFROM table_columns('trades');\n```\n\n\n| column | type | designated |\n| --- | --- | --- |\n| ts | TIMESTAMP | true |\n| symbol | SYMBOL | false |\n| price | DOUBLE | false |\n\nCheck if a query uses interval scan optimization:\n\n```questdb-sql\nEXPLAIN SELECT * FROM trades WHERE timestamp IN '2024-01-15';\n```\n\nLook for\nInterval forward scan\n‚Äîif you see\nAsync Filter\ninstead, the\ndesignated timestamp optimization isn't being used.\n\n## FAQ‚Äã\n\nCan I add a designated timestamp to an existing table?\nNo. The designated timestamp must be defined at table creation. To add one,\ncreate a new table with the designated timestamp and migrate your data.\nWhat happens if I insert data with NULL timestamp?\nThe insert fails. The designated timestamp column cannot contain NULL values.\nCan I have two designated timestamps?\nNo. Each table can have at most one designated timestamp. Use additional\nTIMESTAMP\ncolumns for other time values.\nDoes out-of-order data break anything?\nNo. QuestDB handles out-of-order data automatically by merging it into the\ncorrect sorted position. However, excessive out-of-order data increases write\namplification.\nIs designated timestamp the same as a primary key?\nNo. The designated timestamp:\n- Doesn't enforce uniqueness (usededuplicationfor that)\n- Determines physical storage order\n- Cannot be updated\n- Is optional (though strongly recommended)\nWhy can't I UPDATE the designated timestamp?\nUpdating the timestamp would require reordering rows and potentially moving\nthem between partitions, breaking QuestDB's append-optimized storage model.\nDelete and re-insert instead, or use deduplication for correction workflows.\n\n## See also‚Äã\n\n- CREATE TABLE‚Äî Full syntax for table creation\n- Partitions‚Äî Time-based data organization\n- Interval scan‚Äî Query optimization details\n- TICK intervals‚Äî Complex temporal patterns in a single expression\n- SAMPLE BY‚Äî Time-based aggregation\n- LATEST ON‚Äî Finding most recent records\n- Timestamps and time zones‚Äî Working with time values",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2750,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-486ce208afe2",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/functions/date-time",
    "title": "Timestamp, date and time functions | QuestDB",
    "text": "This page describes the available functions to assist with performing time-based\ncalculations using timestamps.\n\n## Timestamp and date types‚Äã\n\nQuestDB has three temporal types with different precision:\n\n| Type | Precision | Approximate Range |\n| --- | --- | --- |\n| DATE | milliseconds | ¬±2.9 million years |\n| TIMESTAMP | microseconds | ¬±290,000 years |\n| TIMESTAMP_NS | nanoseconds | ¬±2,920 years |\n\nAll three are stored as signed 64-bit integers representing offsets from the\nUnix epoch.\nTIMESTAMP\nis recommended for most use cases as it offers the\nbest balance of precision and function support.\nFor details on all data types, see the\ndata types overview\n.\nDesignated timestamp restriction\nWhen used as a\ndesignated timestamp\n,\nTIMESTAMP\nand\nTIMESTAMP_NS\nvalues cannot be before the Unix epoch\n(\n1970-01-01T00:00:00.000000Z\n).\n\n### Converting between types‚Äã\n\nUse\nCAST\nto convert between temporal types:\n\n```questdb-sql\n-- Reduce precisionSELECT CAST(ts_column AS DATE) FROM my_table;-- Increase precisionSELECT CAST(date_column AS TIMESTAMP) FROM my_table;SELECT CAST(ts_column AS TIMESTAMP_NS) FROM my_table;\n```\n\n\n### Converting from programming languages‚Äã\n\nTo convert language-specific datetime objects (Python\ndatetime\n, Java\nInstant\n, etc.) into QuestDB timestamps, see the\nDate to Timestamp conversion\nreference for Python, Go, Java, JavaScript, C/C++, Rust, and C#/.NET.\n\n## Function categories‚Äã\n\n\n### Current time‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| now | Current timestamp (stable within query) |\n| now_ns | Current timestamp with nanosecond precision (stable within query) |\n| systimestamp | Current timestamp (changes per row) |\n| systimestamp_ns | Current timestamp with nanosecond precision (changes per row) |\n| sysdate | Current date with millisecond precision |\n| today | Interval for current day |\n| tomorrow | Interval for next day |\n| yesterday | Interval for previous day |\n\n\n### Extraction‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| extract | Extract any time unit from timestamp |\n| year | Extract year from timestamp |\n| month | Extract month (1-12) |\n| day | Extract day of month (1-31) |\n| hour | Extract hour (0-23) |\n| minute | Extract minute (0-59) |\n| second | Extract second (0-59) |\n| millis | Extract milliseconds (0-999) |\n| micros | Extract microseconds (0-999) |\n| nanos | Extract nanoseconds (0-999) |\n| day_of_week | Day number (1=Monday to 7=Sunday) |\n| day_of_week_sunday_first | Day number (1=Sunday to 7=Saturday) |\n| days_in_month | Number of days in the month |\n| week_of_year | Week number in year |\n| is_leap_year | Check if year is a leap year |\n\n\n### Arithmetic‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| dateadd | Add time period to timestamp |\n| datediff | Difference between timestamps |\n| date_trunc | Truncate timestamp to specified precision |\n| timestamp_ceil | Round timestamp up to unit boundary |\n| timestamp_floor | Round timestamp down to unit/interval boundary |\n\n\n### Conversion‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| to_timestamp | Parse string to timestamp (microsecond) |\n| to_timestamp_ns | Parse string to timestamp (nanosecond) |\n| to_date | Parse string to date |\n| to_str | Format timestamp as string |\n| to_timezone | Convert timestamp to timezone |\n| to_utc | Convert timestamp to UTC |\n\n\n### Interval construction‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| interval | Create interval from two timestamps |\n| interval_start | Extract interval lower bound |\n| interval_end | Extract interval upper bound |\n\n\n### Utilities‚Äã\n\n\n| Function | Description |\n| --- | --- |\n| timestamp_shuffle | Generate random timestamp in range |\n| pg_postmaster_start_time | Server start time (PostgreSQL compatibility) |\n\nFiltering vs projection\nFor filtering (WHERE clause)\n: Use\nTICK syntax\nfor optimized interval scans:\n\n```questdb-sql\nSELECT * FROM trades WHERE ts IN '$today'SELECT * FROM trades WHERE ts IN '$now - 1h..$now'\n```\n\nFor projection (SELECT clause)\n: Use these functions for computed values:\n\n```questdb-sql\nSELECT dateadd('h', 2, ts) as shifted_time FROM tradesSELECT year(ts), month(ts) FROM trades\n```\n\nTICK syntax leverages\ninterval scans\nfor efficient filtering. Functions are for transformations and calculations.\n\n## date_trunc‚Äã\n\ndate_trunc(unit, timestamp)\n- returns a timestamp truncated to the specified\nprecision.\nArguments:\n- unitis one of the following:millenniumdecadecenturyyearquartermonthweekdayhourminutesecondmillisecondmicrosecondnanosecond\n- timestampis anytimestamp,timestamp_ns, or ISO-8601 string value.\nReturn value:\nReturn value defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is\nof type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n\n```questdb-sql\nSELECT date_trunc('hour', '2022-03-11T22:00:30.555555Z') hour,date_trunc('month', '2022-03-11T22:00:30.555555Z') month,date_trunc('year','2022-03-11T22:00:30.555555Z') year,date_trunc('year','2022-03-11T22:00:30.555555555Z') year2;\n```\n\n\n| hour (timestamp_ns) | month (timestamp_ns) | year (timestamp) | year2 (timestamp_ns) |\n| --- | --- | --- | --- |\n| 2022-03-11T22:00:00.000000Z | 2022-03-01T00:00:00.000000Z | 2022-01-01T00:00:00.000000Z | 2022-01-01T00:00:00.000000000Z |\n\n\n## dateadd‚Äã\n\ndateadd(period, n, startDate[, timezone])\n- adds\nn\nperiod\nto\nstartDate\n,\noptionally respecting timezone DST transitions.\nUse in projections (SELECT clause) to shift timestamps. For filtering relative\ntime windows in WHERE clauses, prefer\nTICK syntax\n(e.g.,\n$now - 1h..$now\n) for optimized interval scans.\ntip\nWhen a timezone is specified, the function handles daylight savings time\ntransitions correctly. This is particularly important when adding periods that\ncould cross DST boundaries (like weeks, months, or years).\nWithout the timezone parameter, the function performs simple UTC arithmetic\nwhich may lead to incorrect results when crossing DST boundaries. For\ntimezone-aware calculations, use the timezone parameter.\nArguments:\n- periodis achar. Period to be added. Available periods are:n: nanosecondsu: microsecondsT: millisecondss: secondm: minuteh: hourd: dayw: weekM: monthy: year\n- nis anintindicating the number of periods to add.\n- startDateis a timestamp, timestamp_ns, or date indicating the timestamp to add the period\nto.\n- timezone(optional) is a string specifying the timezone to use for DST-aware\ncalculations - for example, 'Europe/London'.\nReturn value:\nReturn value type defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the\nstartDate\nargument is a\ntimestamp_ns\n.\nExamples:\nAdding hours\n\n```questdb-sql\nSELECT systimestamp(), dateadd('h', 2, systimestamp())FROM long_sequence(1);\n```\n\n\n| systimestamp | dateadd |\n| --- | --- |\n| 2020-04-17T00:30:51.380499Z | 2020-04-17T02:30:51.380499Z |\n\nAdding days\n\n```questdb-sql\nSELECT systimestamp(), dateadd('d', 2, systimestamp())FROM long_sequence(1);\n```\n\n\n| systimestamp | dateadd |\n| --- | --- |\n| 2020-04-17T00:30:51.380499Z | 2020-04-19T00:30:51.380499Z |\n\nAdding weeks with timezone\n\n```questdb-sql\nSELECT    '2024-10-21T10:00:00Z',    dateadd('w', 1, '2024-10-21T10:00:00Z', 'Europe/Bratislava') as with_tz,    dateadd('w', 1, '2024-10-21T10:00:00Z') as without_tzFROM long_sequence(1);\n```\n\n\n| timestamp | with_tz | without_tz |\n| --- | --- | --- |\n| 2024-10-21T10:00:00.000Z | 2024-10-28T10:00:00.000Z | 2024-10-28T09:00:00.000Z |\n\nNote how the timezone-aware calculation correctly handles the DST transition in\nEurope/Bratislava\n.\nAdding months\n\n```questdb-sql\nSELECT systimestamp(), dateadd('M', 2, systimestamp())FROM long_sequence(1);\n```\n\n\n| systimestamp | dateadd |\n| --- | --- |\n| 2020-04-17T00:30:51.380499Z | 2020-06-17T00:30:51.380499Z |\n\n\n#### See also‚Äã\n\n- datediff- Difference between timestamps\n- TICK syntax- For filtering with optimized interval scans\n\n## datediff‚Äã\n\ndatediff(period, date1, date2)\n- returns the absolute number of\nperiod\nbetween\ndate1\nand\ndate2\n.\nArguments:\n- periodis a char. Period to be added. Available periods are:n: nanosecondsu: microsecondsT: millisecondss: secondm: minuteh: hourd: dayw: weekM: monthy: year\n- date1anddate2aretimestamp,timestamp_ns,date, or date literal strings defining the dates to compare.\nReturn value:\nReturn value type is\nlong\nExamples:\nDifference in days\n\n```questdb-sql\nSELECT datediff('d', '2020-01-23', '2020-01-27');\n```\n\n\n| datediff |\n| --- |\n| 4 |\n\nDifference in months\n\n```questdb-sql\nSELECT datediff('M', '2020-01-23', '2020-02-27');\n```\n\n\n| datediff |\n| --- |\n| 1 |\n\n\n## day‚Äã\n\nday(value)\n- returns the\nday\nof month for a given timestamp from\n1\nto\n31\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nDay of the month\nDemo this query\n\n```questdb-sql\nSELECT day(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM tradesLIMIT -1;\n```\n\n\n| day |\n| --- |\n| 01 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT day(ts), count() FROM transactions;\n```\n\n\n| day | count |\n| --- | --- |\n| 1 | 2323 |\n| 2 | 6548 |\n| ... | ... |\n| 30 | 9876 |\n| 31 | 2567 |\n\n\n## day_of_week‚Äã\n\nday_of_week(value)\n- returns the day number in a week from\n1\n(Monday) to\n7\n(Sunday).\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\n\n```questdb-sql\nSELECT to_str(ts,'EE'),day_of_week(ts) FROM myTable;\n```\n\n\n| day | day_of_week |\n| --- | --- |\n| Monday | 1 |\n| Tuesday | 2 |\n| Wednesday | 3 |\n| Thursday | 4 |\n| Friday | 5 |\n| Saturday | 6 |\n| Sunday | 7 |\n\n\n## day_of_week_sunday_first‚Äã\n\nday_of_week_sunday_first(value)\n- returns the day number in a week from\n1\n(Sunday) to\n7\n(Saturday).\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\n\n```questdb-sql\nSELECT to_str(ts,'EE'),day_of_week_sunday_first(ts) FROM myTable;\n```\n\n\n| day | day_of_week_sunday_first |\n| --- | --- |\n| Monday | 2 |\n| Tuesday | 3 |\n| Wednesday | 4 |\n| Thursday | 5 |\n| Friday | 6 |\n| Saturday | 7 |\n| Sunday | 1 |\n\n\n## days_in_month‚Äã\n\ndays_in_month(value)\n- returns the number of days in a month from a given\ntimestamp or date.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\n\n```questdb-sql\nSELECT month(ts), days_in_month(ts) FROM myTable;\n```\n\n\n| month | days_in_month |\n| --- | --- |\n| 4 | 30 |\n| 5 | 31 |\n| 6 | 30 |\n| 7 | 31 |\n| 8 | 31 |\n\n\n## extract‚Äã\n\nextract(unit, timestamp)\n- returns the selected time unit from the input\ntimestamp.\nArguments:\n- unitis one of the following:millenniumepochdecadecenturyyearisoyeardoy(day of year)quartermonthweekdow(day of week)isodowdayhourminutesecondmicrosecondsmillisecondsnanoseconds\n- timestampis anytimestamp,timestamp_ns,date, or date literal string value.\nReturn value:\nReturn value type is\ninteger\n.\nExamples\n\n```questdb-sql\nSELECT extract(millennium from '2023-03-11T22:00:30.555555Z') millennium,extract(year from '2023-03-11T22:00:30.555555Z') year,extract(month from '2023-03-11T22:00:30.555555Z') month,extract(week from '2023-03-11T22:00:30.555555Z') week,extract(hour from '2023-03-11T22:00:30.555555Z') hour,extract(second from '2023-03-11T22:00:30.555555Z') second;\n```\n\n\n| millennium | year | month | week | hour | second |\n| --- | --- | --- | --- | --- | --- |\n| 3 | 2023 | 3 | 10 | 22 | 30 |\n\n\n#### See also‚Äã\n\n- year,month,day,hour,minute,second- Individual extraction functions\n\n## hour‚Äã\n\nhour(timestamp)\n- returns the\nhour\nof day for a given timestamp from\n0\nto\n23\n.\nArguments:\n- timestampis anytimestamp,timestamp_ns,date, or date literal string value.\nReturn value:\nReturn value type is\nint\nExamples:\nHour of the day\n\n```questdb-sql\nSELECT hour(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM long_sequence(1);\n```\n\n\n| hour |\n| --- |\n| 15 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT hour(ts), count() FROM transactions;\n```\n\n\n| hour | count |\n| --- | --- |\n| 0 | 2323 |\n| 1 | 6548 |\n| ... | ... |\n| 22 | 9876 |\n| 23 | 2567 |\n\n\n## interval‚Äã\n\ninterval(start_timestamp, end_timestamp)\n- creates a time interval from two\ntimestamps.\nIntervals are\nruntime-only values\nthat cannot be stored in tables. Use this\nfunction for:\n- Checking if a timestamp falls within a range:ts IN interval(start, end)\n- Extracting bounds withinterval_start()andinterval_end()\n- Working with intervals returned bytoday(),tomorrow(),yesterday()\nFor filtering in WHERE clauses, prefer\nTICK syntax\n(e.g.,\n$today\n,\n$now - 1h..$now\n) which enables\ninterval scan\noptimization.\nArguments:\n- start_timestampis a timestamp.\n- end_timestampis a timestamp not earlier than thestart_timestamp.\nReturn value:\nReturn value type is\ninterval\n.\nExamples:\nConstruct an interval\nDemo this query\n\n```questdb-sql\nSELECT interval('2024-10-08T11:09:47.573Z', '2024-10-09T11:09:47.573Z')\n```\n\n\n| interval |\n| --- |\n| ('2024-10-08T11:09:47.573Z', '2024-10-09T11:09:47.573Z') |\n\n\n#### See also‚Äã\n\n- interval_start- Extract interval lower bound\n- interval_end- Extract interval upper bound\n- TICK syntax- For filtering with optimized interval scans\n\n## interval_start‚Äã\n\ninterval_start(interval)\n- extracts the lower bound of the interval.\nUse to extract bounds from intervals returned by functions or stored in columns.\nArguments:\n- intervalis aninterval.\nReturn value:\nReturn value type is\ntimestamp\nor\ntimestamp_ns\n, depending on the type of values in the interval.\nExamples:\nExtract an interval lower bound\nDemo this query\n\n```questdb-sql\nSELECT  interval_start(    interval('2024-10-08T11:09:47.573Z', '2024-10-09T11:09:47.573Z')  )\n```\n\n\n| interval_start |\n| --- |\n| 2024-10-08T11:09:47.573000Z |\n\n\n## interval_end‚Äã\n\ninterval_end(interval)\n- extracts the upper bound of the interval.\nUse to extract bounds from intervals returned by functions or stored in columns.\nArguments:\n- intervalis aninterval.\nReturn value:\nReturn value type is\ntimestamp\nor\ntimestamp_ns\n, depending on the type of values in the interval.\nExamples:\nExtract an interval upper bound\nDemo this query\n\n```questdb-sql\nSELECT  interval_end(    interval('2024-10-08T11:09:47.573Z', '2024-10-09T11:09:47.573Z')  )\n```\n\n\n| interval_end |\n| --- |\n| 2024-10-09T11:09:47.573000Z |\n\n\n## is_leap_year‚Äã\n\nis_leap_year(value)\n- returns\ntrue\nif the\nyear\nof\nvalue\nis a leap year,\nfalse\notherwise.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nboolean\nExamples:\nSimple example\nDemo this query\n\n```questdb-sql\nSELECT year(timestamp), is_leap_year(timestamp)FROM tradeslimit -1;\n```\n\n\n| year | is_leap_year |\n| --- | --- |\n| 2020 | true |\n| 2021 | false |\n| 2022 | false |\n| 2023 | false |\n| 2024 | true |\n| 2025 | false |\n\n\n## micros‚Äã\n\nmicros(value)\n- returns the\nmicros\nof the millisecond for a given date or\ntimestamp from\n0\nto\n999\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nMicros of the second\n\n```questdb-sql\nSELECT micros(to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSUUU'))FROM long_sequence(1);\n```\n\n\n| micros |\n| --- |\n| 456 |\n\nParsing 3 digits when no unit is added after U\n\n```questdb-sql\nSELECT micros(to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSU'))FROM long_sequence(1);\n```\n\n\n| micros |\n| --- |\n| 456 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT micros(ts), count() FROM transactions;\n```\n\n\n| micros | count |\n| --- | --- |\n| 0 | 2323 |\n| 1 | 6548 |\n| ... | ... |\n| 998 | 9876 |\n| 999 | 2567 |\n\n\n## millis‚Äã\n\nmillis(value)\n- returns the\nmillis\nof the second for a given date or\ntimestamp from\n0\nto\n999\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nMillis of the second\n\n```questdb-sql\nSELECT millis(    to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSUUU'))FROM long_sequence(1);\n```\n\n\n| millis |\n| --- |\n| 123 |\n\nParsing 3 digits when no unit is added after S\n\n```questdb-sql\nSELECT millis(to_timestamp('2020-03-01:15:43:21.123', 'yyyy-MM-dd:HH:mm:ss.S'))FROM long_sequence(1);\n```\n\n\n| millis |\n| --- |\n| 123 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT millis(ts), count() FROM transactions;\n```\n\n\n| millis | count |\n| --- | --- |\n| 0 | 2323 |\n| 1 | 6548 |\n| ... | ... |\n| 998 | 9876 |\n| 999 | 2567 |\n\n\n## minute‚Äã\n\nminute(value)\n- returns the\nminute\nof the hour for a given timestamp from\n0\nto\n59\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nMinute of the hour\nDemo this query\n\n```questdb-sql\nSELECT minute(to_timestamp('2022-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM tradesLIMIT -1;\n```\n\n\n| minute |\n| --- |\n| 43 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT minute(ts), count() FROM transactions;\n```\n\n\n| minute | count |\n| --- | --- |\n| 0 | 2323 |\n| 1 | 6548 |\n| ... | ... |\n| 58 | 9876 |\n| 59 | 2567 |\n\n\n## month‚Äã\n\nmonth(value)\n- returns the\nmonth\nof year for a given date from\n1\nto\n12\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nMonth of the year\n\n```questdb-sql\nSELECT month(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM long_sequence(1);\n```\n\n\n| month |\n| --- |\n| 03 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT month(ts), count() FROM transactions;\n```\n\n\n| month | count |\n| --- | --- |\n| 1 | 2323 |\n| 2 | 6548 |\n| ... | ... |\n| 11 | 9876 |\n| 12 | 2567 |\n\n\n## nanos‚Äã\n\nnanos(value)\n- returns the\nnanos\nof the second for a given date or\ntimestamp from\n0\nto\n999\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nNanos of the second\n\n```questdb-sql\nSELECT nanos(    to_timestamp_ns('2020-03-01:15:43:21.123456789', 'yyyy-MM-dd:HH:mm:ss.SSSUUUNNN')) as nanosFROM long_sequence(1);\n```\n\n\n| nanos |\n| --- |\n| 789 |\n\n\n## now‚Äã\n\nnow()\n- offset from UTC Epoch in microseconds.\nCalculates\nUTC timestamp\nusing system's real time clock. Unlike\nsystimestamp()\n, it does not change within the query execution timeframe and\nshould be used in WHERE clause to filter designated timestamp column relative to\ncurrent time, i.e.:\n- SELECT now() FROM long_sequence(200)will return the same timestamp for all\nrows\n- SELECT systimestamp() FROM long_sequence(200)will have new timestamp values\nfor each row\nArguments:\n- now()does not accept arguments.\nReturn value:\nReturn value type is\ntimestamp\n.\nExamples:\nFilter records to created within last day\n\n```questdb-sql\nSELECT created, origin FROM telemetryWHERE created > dateadd('d', -1, now());\n```\n\n\n| created | origin |\n| --- | --- |\n| 2021-02-01T21:51:34.443726Z | 1 |\n\nQuery returns same timestamp in every row\n\n```questdb-sql\nSELECT now() FROM long_sequence(3)\n```\n\n\n| now |\n| --- |\n| 2021-02-01T21:51:34.443726Z |\n| 2021-02-01T21:51:34.443726Z |\n| 2021-02-01T21:51:34.443726Z |\n\nQuery based on last minute\n\n```questdb-sql\nSELECT * FROM tradesWHERE timestamp > now() - 60000000L;\n```\n\n\n## now_ns‚Äã\n\nnow_ns()\n- offset from UTC Epoch in nanoseconds.\nCalculates\nUTC timestamp\nusing system's real time clock with nanosecond\nprecision. Like\nnow()\n, it does not change within the query execution timeframe.\nArguments:\n- now_ns()does not accept arguments.\nReturn value:\nReturn value type is\ntimestamp_ns\n.\nExamples:\nQuery returns same timestamp in every row\n\n```questdb-sql\nSELECT now_ns() FROM long_sequence(3)\n```\n\n\n| now_ns |\n| --- |\n| 2021-02-01T21:51:34.443726123Z |\n| 2021-02-01T21:51:34.443726123Z |\n| 2021-02-01T21:51:34.443726123Z |\n\n\n#### See also‚Äã\n\n- now- Current timestamp with microsecond precision\n- systimestamp_ns- Current timestamp with nanosecond precision (changes per row)\n\n## pg_postmaster_start_time‚Äã\n\npg_postmaster_start_time()\n- returns the time when the server started.\nArguments\n- pg_postmaster_start_time()does not accept arguments.\nReturn value:\nReturn value type is\ntimestamp\nExamples\n\n```questdb-sql\nSELECT pg_postmaster_start_time();\n```\n\n\n| pg_postmaster_start_time |\n| --- |\n| 2023-03-30T16:20:29.763961Z |\n\n\n## second‚Äã\n\nsecond(value)\n- returns the\nsecond\nof the minute for a given date or\ntimestamp from\n0\nto\n59\n.\nArguments:\n- valueis anytimestamp,timestamp_ns, ordate\nReturn value:\nReturn value type is\nint\nExamples:\nSecond of the minute\n\n```questdb-sql\nSELECT second(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM long_sequence(1);\n```\n\n\n| second |\n| --- |\n| 21 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT second(ts), count() FROM transactions;\n```\n\n\n| second | count |\n| --- | --- |\n| 0 | 2323 |\n| 1 | 6548 |\n| ... | ... |\n| 58 | 9876 |\n| 59 | 2567 |\n\n\n## today, tomorrow, yesterday‚Äã\n\n- today()- returns an interval representing the current day.\n- tomorrow()- returns an interval representing the next day.\n- yesterday()- returns an interval representing the previous day.\nInterval is in the UTC/GMT+0 timezone.\nThese functions return intervals for use in projections or comparisons. For\nfiltering in WHERE clauses, prefer\nTICK syntax\n(\n$today\n,\n$tomorrow\n,\n$yesterday\n) which enables\ninterval scan\noptimization.\nArguments:\nNo arguments taken.\nReturn value:\nReturn value is of type\ninterval\n.\nExamples:\nUsing today\n\n```questdb-sql\nSELECT true as in_today FROM long_sequence(1)WHERE now() IN today();\n```\n\n\n## today, tomorrow, yesterday with timezone‚Äã\n\n- today(timezone)- returns an interval representing the current day with\ntimezone adjustment.\n- tomorrow(timezone)- returns an interval representing the next day timezone\nadjustment.\n- yesterday(timezone)- returns an interval representing the previous day\ntimezone adjustment.\nArguments:\ntimezone\nis a\nstring\nmatching a timezone.\nReturn value:\nReturn value is of type\ninterval\n.\nExamples:\nUsing today\nDemo this query\n\n```questdb-sql\nSELECT today() as today, today('CEST') as adjusted;\n```\n\n\n| today | adjusted |\n| --- | --- |\n| ('2024-10-08T00:00:00.000Z', '2024-10-08T23:59:59.999Z') | ('2024-10-07T22:00:00.000Z', '2024-10-08T21:59:59.999Z') |\n\nThis function allows the user to specify their local timezone and receive a UTC\ninterval that corresponds to their 'day'.\nIn this example,\nCEST\nis a +2h offset, so the\nCEST\nday started at\n10:00 PM\nUTC\nthe day before.\n\n#### See also‚Äã\n\n- TICK syntax- Use$today,$tomorrow,$yesterdayfor optimized filtering\n\n## sysdate‚Äã\n\nsysdate()\n- returns the timestamp of the host system as a\ndate\nwith\nmillisecond\nprecision.\nCalculates\nUTC date\nwith millisecond precision using system's real time clock.\nThe value is affected by discontinuous jumps in the system time (e.g., if the\nsystem administrator manually changes the system time).\nsysdate()\nvalue can change within the query execution timeframe and should\nNOT\nbe used in WHERE clause to filter designated timestamp column.\ntip\nUse\nnow()\nwith WHERE clause filter.\nArguments:\n- sysdate()does not accept arguments.\nReturn value:\nReturn value type is\ndate\n.\nExamples:\nInsert current system date along with a value\n\n```questdb-sql\nINSERT INTO readingsVALUES(sysdate(), 123.5);\n```\n\n\n| sysdate | reading |\n| --- | --- |\n| 2020-01-02T19:28:48.727516Z | 123.5 |\n\nQuery based on last minute\n\n```questdb-sql\nSELECT * FROM tradesWHERE timestamp > sysdate() - 60000000L;\n```\n\n\n## systimestamp‚Äã\n\nsystimestamp()\n- offset from UTC Epoch in microseconds. Calculates\nUTC timestamp\nusing system's real time clock. The value is affected by\ndiscontinuous jumps in the system time (e.g., if the system administrator\nmanually changes the system time).\nsystimestamp()\nvalue can change within the query execution timeframe and\nshould\nNOT\nbe used in WHERE clause to filter designated timestamp column.\ntip\nUse now() with WHERE clause filter.\nArguments:\n- systimestamp()does not accept arguments.\nReturn value:\nReturn value type is\ntimestamp\n.\nExamples:\nInsert current system timestamp\n\n```questdb-sql\nINSERT INTO readingsVALUES(systimestamp(), 123.5);\n```\n\n\n| ts | reading |\n| --- | --- |\n| 2020-01-02T19:28:48.727516Z | 123.5 |\n\n\n## systimestamp_ns‚Äã\n\nsystimestamp_ns()\n- offset from UTC Epoch in nanoseconds. Calculates\nUTC timestamp\nusing system's real time clock. The value is affected by\ndiscontinuous jumps in the system time (e.g., if the system administrator\nmanually changes the system time).\nsystimestamp_ns()\nvalue can change within the query execution timeframe and\nshould\nNOT\nbe used in WHERE clause to filter designated timestamp column.\ntip\nUse now() with WHERE clause filter.\nArguments:\n- systimestamp_ns()does not accept arguments.\nReturn value:\nReturn value type is\ntimestamp_ns\n.\nExamples:\nInsert current system timestamp_ns\n\n```questdb-sql\nINSERT INTO readingsVALUES(systimestamp_ns(), 123.5);\n```\n\n\n| ts | reading |\n| --- | --- |\n| 2020-01-02T19:28:48.727516132Z | 123.5 |\n\n\n## timestamp_ceil‚Äã\n\ntimestamp_ceil(unit, timestamp)\n- performs a ceiling calculation on a\ntimestamp by given unit.\nA unit must be provided to specify which granularity to perform rounding.\nArguments:\ntimestamp_ceil(unit, timestamp)\nhas the following arguments:\nunit\n- may be one of the following:\n- nnanoseconds\n- Umicroseconds\n- Tmilliseconds\n- sseconds\n- mminutes\n- hhours\n- ddays\n- wweeks\n- Mmonths\n- yyear\ntimestamp\n- any\ntimestamp\n,\ntimestamp_ns\n,\ndate\n, or date literal string value.\nReturn value:\nReturn value type defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is of type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n\n```questdb-sql\nWITH t AS (SELECT cast('2016-02-10T16:18:22.862145333Z' AS timestamp_ns) ts)SELECT  ts,  timestamp_ceil('n', ts) c_nano,  timestamp_ceil('U', ts) c_micro,  timestamp_ceil('T', ts) c_milli,  timestamp_ceil('s', ts) c_second,  timestamp_ceil('m', ts) c_minute,  timestamp_ceil('h', ts) c_hour,  timestamp_ceil('d', ts) c_day,  timestamp_ceil('M', ts) c_month,  timestamp_ceil('y', ts) c_year  FROM t\n```\n\n\n| ts | c_nano | c_micro | c_milli | c_second | c_minute | c_hour | c_day | c_month | c_year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2016-02-10T16:18:22.862145333Z | 2016-02-10T16:18:22.862145333Z | 2016-02-10T16:18:22.862146000Z | 2016-02-10T16:18:22.863000000Z | 2016-02-10T16:18:23.000000000Z | 2016-02-10T16:19:00.000000000Z | 2016-02-10T17:00:00.000000000Z | 2016-02-11T00:00:00.000000000Z | 2016-03-01T00:00:00.000000000Z | 2017-01-01T00:00:00.000000000Z |\n\n\n## timestamp_floor‚Äã\n\ntimestamp_floor(interval, timestamp)\n- performs a floor calculation on a\ntimestamp by given interval expression.\nUse for custom time bucketing in projections. For time-series aggregation,\nconsider\nSAMPLE BY\nwhich provides optimized\ngrouping with fill options.\nAn interval expression must be provided to specify which granularity to perform\nrounding for.\nArguments:\ntimestamp_floor(interval, timestamp)\nhas the following arguments:\nunit\n- is a time interval expression that may use one of the following\nsuffices:\n- nnanoseconds\n- Umicroseconds\n- Tmilliseconds\n- sseconds\n- mminutes\n- hhours\n- ddays\n- wweeks\n- Mmonths\n- yyear\ntimestamp\n- any\ntimestamp\n,\ntimestamp_ns\n,\ndate\n, or date literal string value.\nReturn value:\nReturn value type defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is of type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n\n```questdb-sql\nSELECT timestamp_floor('5d', '2018-01-01')\n```\n\nGives:\n\n| timestamp_floor |\n| --- |\n| 2017-12-30T00:00:00.000000Z |\n\nThe number part of the expression is optional:\n\n```questdb-sql\nWITH t AS (SELECT cast('2016-02-10T16:18:22.862145333Z' AS timestamp_ns) ts)SELECT  ts,  timestamp_floor('n', ts) c_nano,  timestamp_floor('U', ts) c_micro,  timestamp_floor('T', ts) c_milli,  timestamp_floor('s', ts) c_second,  timestamp_floor('m', ts) c_minute,  timestamp_floor('h', ts) c_hour,  timestamp_floor('d', ts) c_day,  timestamp_floor('M', ts) c_month,  timestamp_floor('y', ts) c_year  FROM t\n```\n\nGives:\n\n| ts | c_nano | c_micro | c_milli | c_second | c_minute | c_hour | c_day | c_month | c_year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2016-02-10T16:18:22.862145333Z | 2016-02-10T16:18:22.862145333Z | 2016-02-10T16:18:22.862145000Z | 2016-02-10T16:18:22.862000000Z | 2016-02-10T16:18:22.000000000Z | 2016-02-10T16:18:00.000000000Z | 2016-02-10T16:00:00.000000000Z | 2016-02-10T00:00:00.000000000Z | 2016-02-01T00:00:00.000000000Z | 2016-01-01T00:00:00.000000000Z |\n\n\n#### timestamp_floor with offset‚Äã\n\nWhen timestamps are floored by\ntimestamp_floor(interval, timestamp)\n, they are\nbased on a root timestamp of\n0\n. This means that some floorings with a stride\ncan be confusing, since they are based on a modulo from\n1970-01-01\n.\nFor example:\n\n```questdb-sql\nSELECT timestamp_floor('5d', '2018-01-01')\n```\n\nGives:\n\n| timestamp_floor |\n| --- |\n| 2017-12-30T00:00:00.000000Z |\n\nIf you wish to calculate bins from an offset other than\n1970-01-01\n, you can\nadd a third parameter:\ntimestamp_floor(interval, timestamp, offset)\n. The\noffset acts as a baseline from which further values are calculated.\n\n```questdb-sql\nSELECT timestamp_floor('5d', '2018-01-01', '2018-01-01')\n```\n\nGives:\n\n| timestamp_floor |\n| --- |\n| 2018-01-01T00:00:00.000000Z |\n\nYou can test this on the QuestDB Demo:\n\n```questdb-sql\nSELECT timestamp_floor('5d', timestamp, '2018') t, countFROM tradesWHERE timestamp in '2018'ORDER BY 1;\n```\n\nGives:\n\n| t | count |\n| --- | --- |\n| 2018-01-01T00:00:00.000000Z | 1226531 |\n| 2018-01-06T00:00:00.000000Z | 1468302 |\n| 2018-01-11T00:00:00.000000Z | 1604016 |\n| 2018-01-16T00:00:00.000000Z | 1677303 |\n| ... | ... |\n\n\n## timestamp_shuffle‚Äã\n\ntimestamp_shuffle(timestamp_1, timestamp_2)\n- generates a random timestamp\ninclusively between the two input timestamps.\nArguments:\n- timestamp_1- anytimestamp,timestamp_ns,date, or date literal string value.\n- timestamp_2- a timestamp value that is not equal totimestamp_1\nReturn value:\nReturn value type defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is of type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n\n```questdb-sql\nSELECT timestamp_shuffle('2023-03-31T22:00:30.555998Z', '2023-04-01T22:00:30.555998Z');\n```\n\n\n| timestamp_shuffle |\n| --- |\n| 2023-04-01T11:44:41.893394Z |\n\n\n## to_date‚Äã\n\nnote\nWhile the\ndate\ndata type is available, we highly recommend applying the\ntimestamp\ndata type in its place.\nThe only material advantage of date is a wider time range; timestamp however is\nadequate in virtually all cases.\nDate supports fewer functions and uses milliseconds instead of microseconds.\nto_date(string, format)\n- converts string to\ndate\nby using the supplied\nformat\nto extract the value.\nWill convert a\nstring\nto\ndate\nusing the format definition passed as an\nargument. When the\nformat\ndefinition does not match the\nstring\ninput, the\nresult will be\nnull\n.\nFor more information about recognized timestamp formats, see the\ntimestamp format section\n.\nArguments:\n- stringis any string that represents a date and/or time.\n- formatis a string that describes thedate formatin whichstringis\nexpressed.\nReturn value:\nReturn value type is\ndate\nExamples:\nstring matches format\nDemo this query\n\n```questdb-sql\nSELECT to_date('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss')FROM trades;\n```\n\n\n| to_date |\n| --- |\n| 2020-03-01T15:43:21.000Z |\n\nstring does not match format\n\n```questdb-sql\nSELECT to_date('2020-03-01:15:43:21', 'yyyy')FROM long_sequence(1);\n```\n\n\n| to_date |\n| --- |\n| null |\n\nUsing with INSERT\n\n```questdb-sql\nINSERT INTO measurementsvalues(to_date('2019-12-12T12:15', 'yyyy-MM-ddTHH:mm'), 123.5);\n```\n\n\n| date | value |\n| --- | --- |\n| 2019-12-12T12:15:00.000Z | 123.5 |\n\n\n## to_str‚Äã\n\nto_str(value, format)\n- converts timestamp value to a string in the specified\nformat.\nWill convert a timestamp value to a string using the format definition passed as\nan argument. When elements in the\nformat\ndefinition are unrecognized, they\nwill be passed-through as string.\nFor more information about recognized timestamp formats, see the\ntimestamp format section\n.\nArguments:\n- valueis anydate,timestamp, ortimestamp_nsvalue\n- formatis a timestamp format.\nReturn value:\nReturn value type is\nstring\nExamples:\n- Basic example\n\n```questdb-sql\nSELECT to_str(systimestamp(), 'yyyy-MM-dd') FROM long_sequence(1);\n```\n\n\n| to_str |\n| --- |\n| 2020-03-04 |\n\n- With unrecognized timestamp definition\n\n```questdb-sql\nSELECT to_str(systimestamp(), 'yyyy-MM-dd gooD DAY 123') FROM long_sequence(1);\n```\n\n\n| to_str |\n| --- |\n| 2020-03-04 gooD DAY 123 |\n\n\n## to_timestamp‚Äã\n\nto_timestamp(string, format)\n- converts\nstring\nto\ntimestamp\nby using the\nsupplied\nformat\nto extract the value with microsecond precision.\nWhen the\nformat\ndefinition does not match the\nstring\ninput, the result will\nbe\nnull\n.\nFor more information about recognized timestamp formats, see the\ntimestamp format section\n.\nArguments:\n- stringis any string that represents a date and/or time.\n- formatis a string that describes the timestamp format in whichstringis\nexpressed.\nReturn value:\nReturn value type is\ntimestamp\n. QuestDB provides\ntimestamp\nwith microsecond\nresolution. Input strings with nanosecond precision will be parsed but lose the\nprecision. Use\nto_timestamp_ns\nif nanosecond precision is required.\nExamples:\nPattern matching with microsecond precision\n\n```questdb-sql\nSELECT to_timestamp('2020-03-01:15:43:21.127329', 'yyyy-MM-dd:HH:mm:ss.SSSUUU')FROM long_sequence(1);\n```\n\n\n| to_timestamp |\n| --- |\n| 2020-03-01T15:43:21.127329Z |\n\nPrecision loss when pattern matching with nanosecond precision\n\n```questdb-sql\nSELECT to_timestamp('2020-03-01:15:43:00.000000001Z', 'yyyy-MM-dd:HH:mm:ss.SSSUUUNNNZ')FROM long_sequence(1);\n```\n\n\n| to_timestamp |\n| --- |\n| 2020-03-01T15:43:00.000000Z |\n\nString does not match format\n\n```questdb-sql\nSELECT to_timestamp('2020-03-01:15:43:21', 'yyyy')FROM long_sequence(1);\n```\n\n\n| to_timestamp |\n| --- |\n| null |\n\nUsing with INSERT\n\n```questdb-sql\nINSERT INTO measurementsvalues(to_timestamp('2019-12-12T12:15', 'yyyy-MM-ddTHH:mm'), 123.5);\n```\n\n\n| timestamp | value |\n| --- | --- |\n| 2019-12-12T12:15:00.000000Z | 123.5 |\n\nNote that conversion of ISO timestamp format is optional. QuestDB automatically\nconverts\nstring\nto\ntimestamp\nif it is a partial or full form of\nyyyy-MM-ddTHH:mm:ss.SSSUUU\nor\nyyyy-MM-dd HH:mm:ss.SSSUUU\nwith a valid time\noffset,\n+01:00\nor\nZ\n. See more examples in\nNative timestamp\n\n#### See also‚Äã\n\n- to_timestamp_ns- Parse string to timestamp with nanosecond precision\n- to_str- Format timestamp as string\n- Timestamp format- Format pattern reference\n\n## to_timestamp_ns‚Äã\n\nto_timestamp_ns(string, format)\n- converts\nstring\nto\ntimestamp_ns\nby using the\nsupplied\nformat\nto extract the value with nanosecond precision.\nWhen the\nformat\ndefinition does not match the\nstring\ninput, the result will\nbe\nnull\n.\nFor more information about recognized timestamp formats, see the\ntimestamp format section\n.\nArguments:\n- stringis any string that represents a date and/or time.\n- formatis a string that describes the timestamp format in whichstringis\nexpressed.\nReturn value:\nReturn value type is\ntimestamp_ns\n. If nanoseconds are not needed, you can use\nto_timestamp\ninstead.\nExamples:\nPattern matching with nanosecond precision\n\n```questdb-sql\nSELECT to_timestamp_ns('2020-03-01:15:43:21.127329512', 'yyyy-MM-dd:HH:mm:ss.SSSUUUNNN') as timestamp_nsFROM long_sequence(1);\n```\n\n\n| timestamp_ns |\n| --- |\n| 2020-03-01T15:43:21.127329512Z |\n\n\n## to_timezone‚Äã\n\nto_timezone(timestamp, timezone)\n- converts a timestamp value to a specified\ntimezone. For more information on the time zone database used for this function,\nsee the\nQuestDB time zone database documentation\n.\nArguments:\n- timestampis anytimestamp,timestamp_ns, microsecond Epoch, or string equivalent\n- timezonemay beCountry/Citytz database name, time zone abbreviation such\nasPSTor in UTC offset in string format.\nReturn value:\nReturn value defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is\nof type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n- Unix UTC timestamp in microseconds toEurope/Berlin\n\n```questdb-sql\nSELECT to_timezone(1623167145000000, 'Europe/Berlin')\n```\n\n\n| to_timezone |\n| --- |\n| 2021-06-08T17:45:45.000000Z |\n\n- Unix UTC timestamp in microseconds to PST by UTC offset\n\n```questdb-sql\nSELECT to_timezone(1623167145000000, '-08:00')\n```\n\n\n| to_timezone |\n| --- |\n| 2021-06-08T07:45:45.000000Z |\n\n- Timestamp as string toPST\n\n```questdb-sql\nSELECT to_timezone('2021-06-08T13:45:45.000000Z', 'PST')\n```\n\n\n| to_timezone |\n| --- |\n| 2021-06-08T06:45:45.000000Z |\n\n\n## to_utc‚Äã\n\nto_utc(timestamp, timezone)\n- converts a timestamp by specified timezone to\nUTC. May be provided a timezone in string format or a UTC offset in hours and\nminutes. For more information on the time zone database used for this function,\nsee the\nQuestDB time zone database documentation\n.\nArguments:\n- timestampis anytimestamp,timestamp_ns, microsecond Epoch, or string equivalent\n- timezonemay beCountry/Citytz database name, time zone abbreviation such\nasPSTor in UTC offset in string format.\nReturn value:\nReturn value defaults to\ntimestamp\n, but it will return a\ntimestamp_ns\nif the timestamp argument is\nof type\ntimestamp_ns\nor if the date passed as a string contains nanoseconds resolution.\nExamples:\n- Convert a Unix timestamp in microseconds from theEurope/Berlintimezone to\nUTC\n\n```questdb-sql\nSELECT to_utc(1623167145000000, 'Europe/Berlin')\n```\n\n\n| to_utc |\n| --- |\n| 2021-06-08T13:45:45.000000Z |\n\n- Unix timestamp in microseconds from PST to UTC by UTC offset\n\n```questdb-sql\nSELECT to_utc(1623167145000000, '-08:00')\n```\n\n\n| to_utc |\n| --- |\n| 2021-06-08T23:45:45.000000Z |\n\n- Timestamp as string inPSTto UTC\n\n```questdb-sql\nSELECT to_utc('2021-06-08T13:45:45.000000Z', 'PST')\n```\n\n\n| to_utc |\n| --- |\n| 2021-06-08T20:45:45.000000Z |\n\n\n## week_of_year‚Äã\n\nweek_of_year(value)\n- returns the number representing the week number in the\nyear.\nArguments:\n- valueis anytimestamp,timestamp_ns,date, or date string literal.\nReturn value:\nReturn value type is\nint\nExamples\n\n```questdb-sql\nSELECT week_of_year('2023-03-31T22:00:30.555998Z');\n```\n\n\n| week_of_year |\n| --- |\n| 13 |\n\n\n## year‚Äã\n\nyear(value)\n- returns the\nyear\nfor a given timestamp\nArguments:\n- valueis anytimestamp,timestamp_ns,date, or date string literal.\nReturn value:\nReturn value type is\nint\nExamples:\nYear\n\n```questdb-sql\nSELECT year(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))FROM long_sequence(1);\n```\n\n\n| year |\n| --- |\n| 2020 |\n\nUsing in an aggregation\n\n```questdb-sql\nSELECT year(ts), count() FROM transactions;\n```\n\n\n| year | count |\n| --- | --- |\n| 2015 | 2323 |\n| 2016 | 9876 |\n| 2017 | 2567 |\n\n\n#### See also‚Äã\n\n- extract- Extract any time unit from timestamp\n\n## Appendix: Timestamp format patterns‚Äã\n\nFormat patterns tell QuestDB how to interpret string timestamps. They are used\nin multiple contexts:\n- SQL functions:to_timestamp()andto_timestamp_ns()for parsing text into native timestamp values\n- CSV import: Thetimestampparameter inCOPYand the REST API\n- Kafka connector: Thetimestamp.string.formatconfiguration property\nA format pattern combines units (letter codes for date/time components) with\nliteral characters that match your input. For example,\nyyyy-MM-dd HH:mm:ss\nparses\n2024-03-15 14:30:45\n. Units are case-sensitive.\nSee\nWorking with time zones\nfor more on\ntimestamp handling in QuestDB.\n\n| Unit | Date or Time Component | Presentation | Examples |\n| --- | --- | --- | --- |\n| G | Era designator | Text | AD |\n| y | ysingle digit or greedy year, depending on the number of digits in input | Year | 1996; 96; 999; 3 |\n| yy | Two digit year of the current century | Year | 96 (interpreted as 2096) |\n| yyy | Three-digit year | Year | 999 |\n| yyyy | Four-digit year | Year | 1996 |\n| M | Month in year, numeric, greedy | Month | 7; 07; 007; etc. |\n| MM | Month in year, two-digit | Month | 07 |\n| MMM | Month in year, name | Month | Jul; July |\n| w | Week in year | Number | 2 |\n| ww | ISO week of year (two-digit) | Number | 02 |\n| D | Day in year | Number | 189 |\n| d | Day in month | Number | 10 |\n| F | Day of week in month | Number | 2 |\n| E | Day name in week | Text | Tuesday; Tue |\n| u | Day number of week (1 = Monday, ..., 7 = Sunday) | Number | 1 |\n| a | Am/pm marker | Text | PM |\n| H | Hour in day (0-23) | Number | 0 |\n| k | Hour in day (1-24) | Number | 24 |\n| K | Hour in am/pm (0-11) | Number | 0 |\n| h | Hour in am/pm (1-12) | Number | 12 |\n| m | Minute in hour | Number | 30 |\n| s | Second in minute | Number | 55 |\n| SSS | 3-digit millisecond (see explanation below for fraction-of-second) | Number | 978 |\n| S | Millisecond up to 3 digits (see explanation below for fraction-of-second) | Number | 900 |\n| UUU | 3-digit microsecond (see explanation below for fraction-of-second) | Number | 456 |\n| U | Microsecond up to 3 digits (see explanation below for fraction-of-second) | Number | 456 |\n| U+ | Microsecond up to 6 digits (see explanation below for fraction-of-second) | Number | 123456 |\n| N | Nanosecond up to 3 digits (see explanation below for fraction-of-second) | Number | 900 |\n| N+ | Nanosecond up to 9 digits (see explanation below for fraction-of-second) | Number | 123456789 |\n| z | Time zone | General time zone | Pacific Standard Time; PST; GMT-08:00 |\n| Z | Time zone | RFC 822 time zone | -0800 |\n| x | Time zone | ISO 8601 time zone | -08; -0800; -08:00 |\n\n\n### Common format patterns‚Äã\n\nHere are practical examples of complete format strings for common use cases:\n\n| Format pattern | Example input | Description |\n| --- | --- | --- |\n| yyyy-MM-ddTHH:mm:ss.SSSUUUZ | 2024-03-15T14:30:45.123456Z | ISO 8601 with microseconds |\n| yyyy-MM-ddTHH:mm:ss.SSSUUUNNN | 2024-03-15T14:30:45.123456789 | With nanoseconds |\n| yyyy-MM-dd HH:mm:ss | 2024-03-15 14:30:45 | Standard datetime with space separator |\n| yyyy-MM-dd | 2024-03-15 | Date only |\n| yyyy-MM-ddTHH:mm:ssZ | 2024-03-15T14:30:45Z | ISO 8601 without fractional seconds |\n| yyyy-MM-dd HH:mm:ss.SSS | 2024-03-15 14:30:45.123 | Datetime with milliseconds |\n| dd/MM/yyyy HH:mm:ss | 15/03/2024 14:30:45 | European date format |\n| MM/dd/yyyy HH:mm:ss | 03/15/2024 14:30:45 | US date format |\n| yyyyMMdd-HHmmss | 20240315-143045 | Compact format (often used in filenames) |\n| yyyy-MM-ddTHH:mm:ss.SSSz | 2024-03-15T14:30:45.123PST | With timezone abbreviation |\n\nParsing common formats\n\n```questdb-sql\nSELECT  to_timestamp('2024-03-15T14:30:45.123456Z', 'yyyy-MM-ddTHH:mm:ss.SSSUUUZ') as iso,  to_timestamp('2024-03-15 14:30:45', 'yyyy-MM-dd HH:mm:ss') as standard,  to_timestamp('15/03/2024 14:30:45', 'dd/MM/yyyy HH:mm:ss') as europeanFROM long_sequence(1);\n```\n\n\n### Variable-width year parsing withy‚Äã\n\nUse\ny\nwhen your input data has years of varying lengths. Unlike\nyyyy\nwhich\nexpects exactly 4 digits,\ny\nreads all consecutive digits until it encounters\na non-digit character (such as\n-\nor\n/\n).\nSpecial case for 2-digit years:\nWhen the input contains exactly 2 digits,\nQuestDB interprets it as a year in the current century (2000-2099). All other\nlengths are interpreted literally.\n\n| Input | Format | Result | Explanation |\n| --- | --- | --- | --- |\n| 5-03 | y-M | 0005-03-01T00:00:00.000000Z | 1 digit ‚Üí literal year 5 |\n| 05-03 | y-M | 2005-03-01T00:00:00.000000Z | 2 digits ‚Üí current century (20xx) |\n| 005-03 | y-M | 0005-03-01T00:00:00.000000Z | 3 digits ‚Üí literal year 5 |\n| 0005-03 | y-M | 0005-03-01T00:00:00.000000Z | 4 digits ‚Üí literal year 5 |\n| 2024-03 | y-M | 2024-03-01T00:00:00.000000Z | 4 digits ‚Üí literal year 2024 |\n\nFor most use cases, prefer\nyyyy\nfor explicit 4-digit year matching.\n\n### Parsing fractions of a second‚Äã\n\nSub-second precision uses three unit types, each representing 3 decimal places:\n\n| Unit | Represents | Position in fraction |\n| --- | --- | --- |\n| S | Milliseconds | Digits 1-3 (.XXX) |\n| U | Microseconds | Digits 4-6 (.___XXX) |\n| N | Nanoseconds | Digits 7-9 (.______XXX) |\n\nFixed-width formats\nuse repeated letters (\nSSS\n,\nUUU\n,\nNNN\n) and expect\nan exact number of digits:\n\n| Format | Digits | Example input | Parsed value |\n| --- | --- | --- | --- |\n| .SSS | 3 | .123 | 123 ms |\n| .SSSUUU | 6 | .123456 | 123 ms + 456 ¬µs |\n| .SSSUUUNNN | 9 | .123456789 | 123 ms + 456 ¬µs + 789 ns |\n\nVariable-width formats\nuse a single letter or\n+\nsuffix to accept varying\nlengths:\n\n| Format | Digits | Example input | Parsed value |\n| --- | --- | --- | --- |\n| .S | 1-3 | .12 | 120 ms |\n| .SSSU | 4-6 | .1234 | 123 ms + 400 ¬µs |\n| .SSSUUUN | 7-9 | .1234567 | 123 ms + 456 ¬µs + 700 ns |\n| .U+ | 1-6 | .12345 | 123 ms + 450 ¬µs |\n| .N+ | 1-9 | .12 | 120 ms (pads with zeros) |\n\nPractical recommendations:\n- For microsecond timestamps (QuestDB default): use.SSSUUUor.U+\n- For nanosecond timestamps: use.SSSUUUNNNor.N+\n- For millisecond-only data: use.SSSor.S\n\n## See also‚Äã\n\n- TICK interval syntax- Declarative time intervals for filtering\n- Timestamps and timezones- Working with time zones\n- SAMPLE BY- Time-series aggregation\n- Designated timestamp- Required for interval scan optimization",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 6684,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-b6e71a01c3d6",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/visualization/superset",
    "title": "Superset | QuestDB",
    "text": "Apache Superset\nis a popular open-source\nbusiness intelligence web application that enables users to visualize and\nexplore data through customizable dashboards and reports.\nQuestDB provides the\nQuestDB Connect\npython package that\nimplements the SQLAlchemy dialect and Superset engine specification, to\nintegrate Apache Superset with QuestDB.\n\n## Installing Apache Superset via Docker (recommended)‚Äã\n\nWe recommend the Docker-based Apache Superset installation. You will need to\ninstall the following requirements:\n- Docker, including Docker Compose\n- QuestDB 7.1.2 or later\nThen, following the steps below:\n- Clone theSuperset repo:git clone https://github.com/apache/superset.git\n- Change your directory:cd superset\n- Create a filedocker/requirements-local.txtwith the requirement toquestdb-connect:touch ./docker/requirements-local.txtecho \"questdb-connect==1.1.3\" > docker/requirements-local.txt\n- Set Superset version to 4.0.2:\nThis step is important to ensure compatibility with QuestDB Connect.export TAG=4.0.2\n- Run Apache Superset:docker compose -f docker-compose-image-tag.yml pulldocker compose -f docker-compose-image-tag.yml upThis step will initialize your Apache Superset installation, creating a\ndefault admin, users, and several other settings. The first time you start\nApache Superset it can take a few minutes until it is completely initialized.\nPlease keep an eye on the console output to see when Apache Superset is ready\nto be used.\n\n## Installing Superset via QuestDB Connect‚Äã\n\nIf you have a stand-alone installation of Apache Superset and are using Apache\nSuperset without Docker, you need to install the following requirements :\n- Python from 3.9 to 3.11\n- Superset4.0.x\n- QuestDB 7.1.2 or later\nInstall QuestDB Connect using\npip\n:\n\n```bash\npip install 'questdb-connect==1.1.3'\n```\n\n\n## Connecting QuestDB to Superset‚Äã\n\nOnce installed and initialized, Apache Superset is accessible via\nhttp://localhost:8088\n.\n- Sign in with the following details:Username: adminPassword: admin\n- From Superset UI, select Setting > Database Connections\n- Select+Databaseto add a new QuestDB database\n- In the next step usehost.docker.internalwhen running\nApache Superset from Docker andlocalhostfor outside of Docker. Port is8812by default, and the database name isQuestDB, default user isadminand password isquest.\n- Once connected, tables in QuestDB will be visible for creating Datasets in\nApache Superset.\n\n## Conclusion‚Äã\n\nThe integration of Apache Superset with QuestDB allows users to visualize and\nexplore data through customizable dashboards and reports. This guide provides\ninstructions for installing Apache Superset via Docker and QuestDB Connect, and\nconnecting QuestDB to Apache Superset.\nIf you have any questions or need help, please join our\ncommunity Slack\nor\nopen a\nGitHub issue\n.\n\n## See also‚Äã\n\n- QuestDB Connect at GitHub\n- QuestDB Connect Python module\n- Apache Superset install\n- Blog post with Superset dashboard example",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 406,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-512fd5b3d562",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/visualization/qstudio",
    "title": "qStudio | QuestDB",
    "text": "qStudio\nis a free SQL GUI. It allows to\nrun SQL scripts, browse tables easily, chart and export results.\nqStudio includes charting functionality including time-series charting which is\nparticularly useful with QuestDB. It works on every operating system and with\nevery database including QuestDB via the PostgreSQL driver.\n\n## Prerequisites‚Äã\n\n- A running QuestDB instance (SeeGetting Started)\n\n## Configure QuestDB connection‚Äã\n\n- Download qStudiofor your OS\n- Launch qStudio\n- Go toServer->Add Server\n- ClickAdd data source\n- Choose thePostgreSQLplugin and configure it with the following settings:host:localhostport:8812database:qdbuser:adminpassword:quest\n\n## Sending Queries‚Äã\n\nRun queries with:\n- Ctrl+Enterto run the current line, or\n- Ctrl+Eto run the highlighted code.\nScreenshot of the qStudio UI running QuestDB query\n\n## See also‚Äã\n\n- QuestDB Postgres wire protocol",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 122,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-1dc6cd9b4a79",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/orchestration/dagster",
    "title": "Dagster | QuestDB",
    "text": "Dagster is a modern data orchestrator that enables structured and scalable workflow automation. With Dagster, you can automate tasks such as executing SQL queries on QuestDB and managing data pipelines with built-in monitoring and logging.\nAlternatively, checkout our\nAutomating QuestDB Tasks\nguide for a scripted approach.\n\n## Prerequisites‚Äã\n\n- Python 3.9 or later\n- QuestDB running locally or remotely\n- psycopglibrary for PostgreSQL interaction\n- Dagster installed\n\n## Installation‚Äã\n\nTo install Dagster and the required dependencies, run:\n\n```bash\npip install dagster dagster-webserver psycopg\n```\n\nPlease refer to the\nDagster Docs\nfor other options.\n\n## Basic integration‚Äã\n\nOn Dagster you write your automation either using a dependency graph approach, similar to Apache Airflow, or following\na data resource model. Whichever approach you take, the automation is written in Python and the easiest way to automate\nQuestDB tasks is by using\nPsycopg\n.\n\n## Example: Running a Query on QuestDB‚Äã\n\nThe following example defines a Dagster operation (\nop\n) to execute a SQL query on QuestDB:\n\n```python\nfrom dagster import op, jobimport psycopg@opdef execute_query():    conn = psycopg.connect(\"postgresql://admin:quest@localhost:8812/qdb\")    with conn.cursor() as cursor:        cursor.execute(\"ALTER TABLE my_table DROP PARTITION WHERE timestamp < dateadd('d', -30, now());\")    conn.commit()@jobdef questdb_cleanup_job():    execute_query()\n```\n\n\n## Running the Dagster Job‚Äã\n\n- Start the Dagster UI:dagster dev\n- Openhttp://localhost:3000and trigger thequestdb_cleanup_jobmanually.\n\n## Scheduling the Job‚Äã\n\nTo schedule the job to run daily at midnight:\n\n```python\nfrom dagster import schedule@schedule(cron_schedule=\"0 0 * * *\", job=questdb_cleanup_job, execution_timezone=\"UTC\")def daily_questdb_cleanup_schedule():    return {}\n```\n\n\n## Next Steps‚Äã\n\nFor further details and resources, refer to the following links:\n- Dagster Documentation:https://docs.dagster.io/\n- Full Example Repository:https://github.com/questdb/data-orchestration-and-scheduling-samples",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 258,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-6ed0ec559c20",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/other/sqlalchemy",
    "title": "SQLAlchemy | QuestDB",
    "text": "SQLAlchemy\nis an open-source SQL toolkit and ORM\nlibrary for Python. It provides a high-level API for communicating with\nrelational databases\n, including schema\ncreation and modification, an SQL expression language, and database connection\nmanagement. The ORM layer abstracts away the complexities of the database,\nallowing developers to work with Python objects instead of raw SQL statements.\nQuestDB implements a dialect for SQLAlchemy using the\nQuestDB Connect\nPython package.\nPlease note that the SQLAlchemy ORM and metadata operations are only partially\nsupported.\n\n## Prerequisites‚Äã\n\n- Python from 3.9 to 3.11\n- Psycopg2\n- SQLAlchemy<=1.4.47\n- A QuestDB instance\n\n## Installation‚Äã\n\nYou can install this package using\npip\n:\n\n```shell\npip install questdb-connect\n```\n\n\n## Example usage‚Äã\n\n\n```python\nimport sqlalchemyfrom sqlalchemy import create_enginefrom sqlalchemy import textfrom sqlalchemy import MetaDatafrom sqlalchemy import Tablefrom pprint import pprintengine = create_engine(\"questdb://admin:quest@localhost:8812/qdb\")with engine.connect() as conn:  # SQL statements with no parameters  conn.execute(text(\"CREATE TABLE IF NOT EXISTS some_table (x int, y int)\"))  result=conn.execute(text(\"SHOW TABLES\"))  print(result.all())  # results can be iterated in many ways. Check SQLAlchemy docs for details  # passing parameters to your statements  conn.execute(      text(\"INSERT INTO some_table (x, y) VALUES (:x, :y)\"),      [{\"x\": 11, \"y\": 12}, {\"x\": 13, \"y\": 14}],      )  # basic select, no parameters  result = conn.execute(text(\"select * from some_table\"))  print(result.all())  # select with parameters  result = conn.execute(text(\"SELECT x, y FROM some_table WHERE y > :y\"), {\"y\": 2})  print(result.all())  # partial support for metadata  metadata_obj = MetaData()  some_table = Table(\"some_table\", metadata_obj, autoload_with=engine)  pprint(some_table)  # cleaning up  conn.execute(text(\"DROP TABLE some_table\"))\n```\n\n\n## See also‚Äã\n\n- TheSQLAlchemy tutorial\n- TheQuestDB ConnectGitHub",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 255,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-75c5fc6278d3",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/message-brokers/telegraf",
    "title": "Telegraf | QuestDB",
    "text": "Telegraf\nis a client for\ncollecting metrics from many inputs and has support for sending it on to various\noutputs. It is plugin-driven for the collection and delivery of data, so it is\neasily configurable and customizable. Telegraf is compiled as a standalone\nbinary, which means there are no external dependencies required to manage.\nQuestDB supports ingesting from Telegraf via the InfluxDB Line Protocol. This\npage provides examples for collecting CPU and memory usage metrics using\nTelegraf and sends these metrics to a locally-running QuestDB instance for\nquerying and visualization.\n\n## Prerequisites‚Äã\n\n- QuestDBmust be running and accessible. Checkout thequick start.\n- Telegrafcan be installed usinghomebrew,docker, or directly as a binary. For more\ndetails, refer to the official Telegrafinstallation instructions.\n\n## Configuring Telegraf‚Äã\n\nAs Telegraf is a plugin-driven agent, the configuration file provided when\nTelegraf is launched will determine which metrics to collect, if and how\nprocessing of the metrics should be performed, and the destination outputs.\nThe default location that Telegraf can pick up configuration files is\n/usr/local/etc/\non macOS and\n/etc/telegraf/\non Linux. After installation,\ndefault configuration files are in the following locations:\n- Homebrew install:/usr/local/etc/telegraf.conf\n- Linux, Deb and RPM:/etc/telegraf/telegraf.conf\nFull configuration files for writing are provided below and can be placed in\nthese directories and picked up by Telegraf. To view a comprehensive\nconfiguration file with example inputs and outputs, the following command can\ngenerate an example:\n\n```questdb-sql\ntelegraf -sample-config > example.conf\n```\n\n\n### Example Inputs‚Äã\n\nThe examples on this page will use input plugins that read CPU and memory usage\nstatistics of the host machine and send this to the outputs specified in the\nconfiguration file. The following snippet includes code comments which describe\nthe inputs in more detail:\nExample inputs sending host data to QuestDB\n\n```shell\n...# -- INPUT PLUGINS -- #[[inputs.cpu]]  # Read metrics about cpu usage  ## Whether to report per-cpu stats or not  percpu = true  ## Whether to report total system cpu stats or not  totalcpu = true  ## If true, collect raw CPU time metrics  collect_cpu_time = false  ## If true, compute and report the sum of all non-idle CPU states  report_active = false# Read metrics about memory usage[[inputs.mem]]  # no customisation\n```\n\n\n## Writing to QuestDB over HTTP‚Äã\n\nQuestDB expects Influx Line Protocol messages over HTTP on port\n9000\n. To change\nthe default port, see the\nHTTP server configuration\nsection of the server configuration page.\nCreate a new file named\nquestdb.conf\nin one of the locations Telegraf can\nload configuration files from and paste the following example:\n/path/to/telegraf/config/questdb.conf\n\n```shell\n# Configuration for Telegraf agent[agent]  ## Default data collection interval for all inputs  interval = \"5s\"  hostname = \"qdb\"# -- OUTPUT PLUGINS -- #[[outputs.influxdb_v2]]# Use InfluxDB Line Protocol to write metrics to QuestDB  urls = [\"http://localhost:9000\"]# Disable gzip compression  content_encoding = \"identity\"# -- INPUT PLUGINS -- #[[inputs.cpu]]  percpu = true  totalcpu = true  collect_cpu_time = false  report_active = false[[inputs.mem]]  # no customisation\n```\n\nOptionally, we recommend applying an aggregator plugin.\nThe InfluxDB Line Protocol default in many cases will lead to data in the form\nof multiple, fairly sparse rows.\nQuestDB prefers rows that are\n\"more dense\"\n.\nTo that end, the aggregator plugin takes all the metrics for the same tag -\nequivalent to a symbol - and the timestamp. It then outputs them into single\nrow. If metrics are arriving in the usual ILP style with a metric per tag, the\naggregator plugin will instead roll them into a more \"dense\" row as desired.\n/path/to/telegraf/config/questdb.conf - Aggregator plugin\n\n```shell\n# -- AGGREGATOR PLUGINS ------------------------------------------------- ## Merge metrics into multifield metrics by series key[[aggregators.merge]]  ## If true, the original metric will be dropped by the  ## aggregator and will not get sent to the output plugins.  drop_original = true\n```\n\nRun Telegraf and specify the configuration file with the QuestDB output:\n\n```shell\ntelegraf --config questdb.conf\n```\n\nTelegraf should report the following if configured correctly:\n\n```bash\n2021-01-29T12:11:32Z I! Loaded inputs: cpu mem2021-01-29T12:11:32Z I! Loaded aggregators:2021-01-29T12:11:32Z I! Loaded processors:2021-01-29T12:11:32Z I! Loaded outputs: influxdb_v2...\n```\n\n\n## Verifying the integration‚Äã\n\n- Navigate to the QuestDBWeb Consoleathttp://127.0.0.1:9000/. The Schema\nNavigator in the top left should display two new tables:\n- cpugenerated frominputs.cpu\n- memgenerated frominputs.mem\n- Typecpuin the query editor and clickRUN\nThe\ncpu\ntable will have a column for each metric collected by the Telegraf\nplugin for monitoring memory:\n\n### Graphing system CPU‚Äã\n\nTo create a graph that visualizes CPU usage over time, run the following example\nquery:\n\n```questdb-sql\nSELECTavg(usage_system) cpu_average,max(usage_system) cpu_max,timestampFROM cpu SAMPLE BY 1m;\n```\n\nSelect the\nChart\ntab and set the following values:\n- Chart typeline\n- Labelstimestamp\n- Seriescpu_averageandcpu_max",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 762,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-93d4e50a6acd",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/message-brokers/redpanda",
    "title": "Redpanda | QuestDB",
    "text": "Redpanda\nis an open-source, Kafka-compatible streaming\nplatform that uses C++ and Raft to replace Java and Zookeeper. Since it is Kafka\ncompatible, it can be used with the\nQuestDB Kafka connector\n,\nproviding an alternative data\nstreaming\noption.\nThis guide also covers\nRedpanda Connect\n, a stream processing\ntool that can be used to build data pipelines.\n\n### Prerequisites‚Äã\n\n- Docker\n- A local JDK installation\n- A running QuestDB instance\n\n### Configure and start Redpanda‚Äã\n\nThe Redpanda\nQuick start guide\nprovides a\ndocker-compose.yaml\nfile that can be used. Copy and paste the\ncontent into into a file named\ndocker-compose.yml\non your local filesystem:\ndocker-compose.yml\n\n```yaml\n---version: \"3.7\"name: redpanda-quickstartnetworks:  redpanda_network:    driver: bridgevolumes:  redpanda-0: nullservices:  redpanda-0:    command:      - redpanda      - start      - --kafka-addr      - internal://0.0.0.0:9092,external://0.0.0.0:19092      # use the internal addresses to connect to the Redpanda brokers'      # from inside the same Docker network.      #      # use the external addresses to connect to the Redpanda brokers'      # from outside the Docker network.      #      # address the broker advertises to clients that connect to the Kafka API.      - --advertise-kafka-addr      - internal://redpanda-0:9092,external://localhost:19092      - --pandaproxy-addr      - internal://0.0.0.0:8082,external://0.0.0.0:18082      # address the broker advertises to clients that connect to PandaProxy.      - --advertise-pandaproxy-addr      - internal://redpanda-0:8082,external://localhost:18082      - --schema-registry-addr      - internal://0.0.0.0:8081,external://0.0.0.0:18081      # Redpanda brokers use the RPC API to communicate with eachother internally.      - --rpc-addr      - redpanda-0:33145      - --advertise-rpc-addr      - redpanda-0:33145      # tells Seastar (the framework Redpanda uses under the hood) to use 1 core on the system.      - --smp 1      # the amount of memory to make available to Redpanda.      - --memory 1G      # the amount of memory that's left for the Seastar subsystem.      # For development purposes this is set to 0.      - --reserve-memory 0M      # Redpanda won't assume it has all of the provisioned CPU      # (to accommodate Docker resource limitations).      - --overprovisioned      # enable logs for debugging.      - --default-log-level=debug    image: docker.redpanda.com/vectorized/redpanda:v22.3.11    container_name: redpanda-0    volumes:      - redpanda-0:/var/lib/redpanda/data    networks:      - redpanda_network    ports:      - 18081:18081      - 18082:18082      - 19092:19092      - 19644:9644  console:    container_name: redpanda-console    image: docker.redpanda.com/vectorized/console:v2.1.1    networks:      - redpanda_network    entrypoint: /bin/sh    command: -c 'echo \"$$CONSOLE_CONFIG_FILE\" > /tmp/config.yml; /app/console'    environment:      CONFIG_FILEPATH: /tmp/config.yml      CONSOLE_CONFIG_FILE: |        kafka:          brokers: [\"redpanda-0:9092\"]          schemaRegistry:            enabled: true            urls: [\"http://redpanda-0:8081\"]        redpanda:          adminApi:            enabled: true            urls: [\"http://redpanda-0:9644\"]    ports:      - 8080:8080    depends_on:      - redpanda-0\n```\n\nOnce the file is saved, run the following command to start a single Redpanda\nbroker inside Docker and expose Redpanda to your host machine:\n\n```shell\ndocker compose up\n```\n\nIt also start the\nRedpanda web UI\n.\n\n### Download Apache Kafka‚Äã\n\nDownload\nApache Kafka\nand unzip the file.\nThis step is required as Redpanda does not have its own Kafka Connect\nequivalent.\n\n### Download the QuestDB Kafka connector‚Äã\n\nDownload\nthe QuestDB Kafka connector\n,\nunder the zip archive named\nkafka-questdb-connector-<version>-bin.zip\n.\ntip\nYou can automate downloading the latest connector package by running this\ncommand:\n\n```shell\ncurl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest |jq -r '.assets[]|select(.content_type == \"application/zip\")|.browser_download_url'|wget -qi -\n```\n\nUnzip the connector - it has a directory with 2 JARs: Copy these JARs into\n/path/to/kafka/lib:\n\n```shell\nunzip kafka-questdb-connector-*-bin.zipcd kafka-questdb-connectorcp ./*.jar /path/to/kafka/libs\n```\n\nThere should be already a lot of other JAR files. That's how you can tell you\nare in the right directory.\n\n### Configure properties‚Äã\n\nGo to /path/to/kafka/config - there should be already quite a few *.property\nfiles. Create a new file:\nquestdb-connector.properties\nwith the following\nlines:\nquestdb-connector.properties\n\n```text\nname=questdb-sinkconnector.class=io.questdb.kafka.QuestDBSinkConnectorclient.conf.string=http::addr=localhost:9000;timestamp.kafka.native=truetopics=example-topictable=example_tableinclude.key=falsevalue.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=falsekey.converter=org.apache.kafka.connect.storage.StringConverter\n```\n\nIn addition, pointing the open\nconnect-standalone.properties\nand replace:\n\n```json\nbootstrap.servers=localhost:9092\n```\n\nwith the Redpanda broker URL:\n\n```json\nbootstrap.servers=127.0.0.1:19092\n```\n\n\n### Start Kafka Connect‚Äã\n\nNavigate to the Kafka Connect folder and then run:\n\n```shell\n./bin/connect-standalone.sh config/connect-standalone.properties config/questdb-connector.properties\n```\n\nNow the Kafka Connect is initiated.\n\n### Send a message‚Äã\n\nOpen the Redpanda UI topic page at\nhttp://127.0.0.1:8080/topics\n. It should\ndisplay\nexample-topic\n:\nIf the topic is not there then refresh a few times.\nSelect\nexample-topic\nto expand more details and click\nActions\n-->\nPublish Message\n:\nPaste the following message into the message box:\n\n```json\n{ \"firstname\": \"Arthur\", \"lastname\": \"Dent\", \"age\": 42 }\n```\n\nThen, click 'Publish'.\n\n### See result from QuestDB‚Äã\n\nGo to QuestDB web console at\nhttp://localhost:9000\n. Run\na\nSELECT\nquery:\n\n```questdb-sql\nSELECT * FROM example_table;\n```\n\nThe message is delivered to QuestDB:\n\n### Summary and next steps‚Äã\n\nThe guide demonstrates how to use Redpanda with the QuestDB Kafka connector. The\nconnector implicitly creates a table in QuestDB with inferred schema from the\nKafka message.\nOur connector configuration properties includes a key\ntimestamp.kafka.native=true\nwhich tells the connector to use the timestamp\nfrom the Kafka message metadata.\nThe connector can be also configured to use a custom timestamp field from the\nKafka message. See the\nQuestDB Kafka Connector reference manual\nfor details.\nA possible improvement could be to explicitly create the target table in QuestDB\ninstead of relying on the connector to create it implicitly. This way, you can\ncontrol the schema,\npartitioning\nand data\ntypes of the table. It also enables QuestDB's native\ndeduplication feature\n. Deduplication is required\nfor\nExactly-Once\nprocessing semantics.\n\n## See also‚Äã\n\n- QuestDB Kafka Connector reference manual\n\n## Redpanda Connect‚Äã\n\nRedpanda Connect is a stream processing tool that can be used to build data pipelines.\nIt's a lightweight alternative to\nApache Kafka Connect\n.\nThis guide shows the steps to use the Redpanda Connect to write JSON data\nas rows into a QuestDB table.\n\n### Prerequisites‚Äã\n\nYou will need the following:\n- Redpanda Connect\n- A running QuestDB instance\n\n### Download Redpanda Connect‚Äã\n\nThe QuestDB output component was added to Redpanda Connect in version v4.37.0.\nTo download the latest version of Redpanda Connect, follow the\ninstallation instructions\nin the official documentation.\n\n### Configure Redpanda Connect‚Äã\n\nOne of Redpanda Connect's strengths is the ability to configure an entire data pipeline in a single\nyaml file. We will create a simple configuration to demonstrate the QuestDB connector's capabilities\nby using a straightforward input source.\nCreate this file and name it\nconfig.yaml\nin your current directory\n\n```yaml\ninput:  stdin: {}output:  questdb:    address: localhost:9000    table: redpanda_connect_demo    doubles:      - price    designated_timestamp_field: timestamp\n```\n\nThis configuration will read lines from stdin and publish them to your running QuestDB instance\n\n### Run Redpanda Connect and publish messages‚Äã\n\nRun the following command to send some messages to QuestDB through Redpanda Connect\n\n```bash\necho \\'{\"symbol\": \"AAPL\", \"price\": 225.83, \"timestamp\": 1727294094}{\"symbol\": \"MSFT\", \"price\": 431.78, \"timestamp\": 1727294142}' \\| rpk connect run config.yaml\n```\n\nThe command above sends two JSON messages to Redpanda Connect standard input, which then writes them to QuestDB.\n\n### Verify the integration‚Äã\n\nNavigate to the QuestDB Web Console at\nhttp://localhost:9000\nand run the following query to see your data:\n\n```sql\nSELECT *FROM redpanda_connect_demo\n```\n\n\n### Next steps‚Äã\n\nExplore Redpanda Connect's\nofficial documentation\nto learn more\nabout its capabilities and how to use it in your projects.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1099,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-924121b172b8",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/data-processing/pandas",
    "title": "Pandas | QuestDB",
    "text": "Pandas\nis a fast, powerful, flexible, and\neasy-to-use open-source data analysis and manipulation tool, built on top of the\nPython programming language. The\nQuestDB Python client\nprovides native support for ingesting Pandas dataframes via the InfluxDB Line\nProtocol.\n\n## Prerequisites‚Äã\n\n- QuestDB must be running and accessible. Checkout thequick start.\n- Python 3.8 or later\n- Pandas\n- pyarrow\n- NumPy\n\n## Querying vs. Ingestion‚Äã\n\nThis page focuses on ingestion, which is the process of inserting data into\nQuestDB. For querying data, see\nPGWire client guide\n.\n\n## Overview‚Äã\n\nThe QuestDB Python client implements the\ndataframe()\nmethod to transform\nPandas DataFrames into QuestDB-flavored InfluxDB Line Protocol messages.\nThe following example shows how to insert data from a Pandas DataFrame to the\ntrades\ntable:\n\n```python\nfrom questdb.ingress import Sender, IngressErrorimport sysimport pandas as pddef example(host: str = 'localhost', port: int = 9009):    df = pd.DataFrame({            'pair': ['USDGBP', 'EURJPY'],            'traded_price': [0.83, 142.62],            'qty': [100, 400],            'limit_price': [0.84, None],            'timestamp': [                pd.Timestamp('2022-08-06 07:35:23.189062', tz='UTC'),                pd.Timestamp('2022-08-06 07:35:23.189062', tz='UTC')]})    try:        with Sender(host, port) as sender:            sender.dataframe(                df,                table_name='trades',  # Table name to insert into.                symbols=['pair'],  # Columns to be inserted as SYMBOL types.                at='timestamp')  # Column containing the designated timestamps.    except IngressError as e:        sys.stderr.write(f'Got error: {e}\\n')if __name__ == '__main__':    example()\n```\n\n\n## See also‚Äã\n\nFor detailed documentation, please see:\n- Sender.dataframe()\n- Buffer.dataframe()\n- Examples usingdataframe()",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 222,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-3f686c7140f7",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/other/prometheus",
    "title": "Prometheus monitoring and alerting | QuestDB",
    "text": "Prometheus is an open-source systems monitoring and alerting toolkit. Prometheus\ncollects and stores metrics as\ntime-series data\n, i.e. metrics information is\nstored with the timestamp at which it was recorded, alongside optional key-value\npairs called labels.\nUsers can measure the internal status of a QuestDB instance via an HTTP endpoint\nexposed by QuestDB at port\n9003\n. This document describes how to enable metrics\nvia this endpoint, how to configure Prometheus to scrape metrics from a QuestDB\ninstance, and how to enable alerting from QuestDB to Prometheus Alertmanager.\nFor guidance on what metrics to monitor and alerting strategies, see\nMonitoring and alerting\n.\n\n## Prerequisites‚Äã\n\n- QuestDBmust be running and accessible. Checkout thequick start.\n- Prometheuscan be installed usinghomebrew,Docker, or directly as a binary. For more\ndetails, refer to the official Prometheusinstallation instructions.\n- Alertmanagercan be run usingDockerorQuay, or can be built\nfrom source by following thebuild instructions on GitHub.\n\n## Scraping Prometheus metrics from QuestDB‚Äã\n\nQuestDB has a\n/metrics\nHTTP endpoint on port\n9003\nto expose Prometheus\nmetrics. Before being able to query metrics, they must be enabled via the\nmetrics.enabled\nkey in server configuration:\n/path/to/server.conf\n\n```ini\nmetrics.enabled=true\n```\n\nWhen running QuestDB via Docker, port\n9003\nmust be exposed and the metrics\nconfiguration can be enabled via the\nQDB_METRICS_ENABLED\nenvironment variable:\nDocker\n\n```shell\ndocker run \\-e QDB_METRICS_ENABLED=TRUE \\-p 8812:8812 -p 9000:9000 -p 9003:9003 -p 9009:9009 \\-v \"$(pwd):/var/lib/questdb\" \\questdb/questdb:9.3.2\n```\n\nTo verify that metrics are being exposed correctly by QuestDB, navigate to\nhttp://<questdb_ip>:9003/metrics\nin a browser, where\n<questdb_ip>\nis the IP\naddress of an instance, or execute a basic curl like the following example:\nGiven QuestDB running at 127.0.0.1\n\n```bash\ncurl http://127.0.0.1:9003/metrics# TYPE questdb_json_queries_total counterquestdb_json_queries_total 0# TYPE questdb_memory_tag_MMAP_DEFAULT gaugequestdb_memory_tag_MMAP_DEFAULT 77872# TYPE questdb_memory_malloc_count gaugequestdb_memory_malloc_count 659# ...\n```\n\nTo configure Prometheus to scrape these metrics, provide the QuestDB instance IP\nand port\n9003\nas a target. The following example configuration file\nquestdb.yml\nassumes there is a running QuestDB instance on localhost\n(127.0.0.1) with port\n9003\navailable:\nquestdb.yml\n\n```shell\nglobal:  scrape_interval: 5s  external_labels:    monitor: 'questdb'scrape_configs:  - job_name: 'questdb'    scrape_interval: 5s    static_configs:      - targets: ['127.0.0.1:9003']\n```\n\nStart Prometheus and pass this configuration on launch:\n\n```bash\nprometheus --config.file=questdb.yml\n```\n\nPrometheus should be available on\n0.0.0.0:9090\nand navigating to\nhttp://0.0.0.0:9090/targets\nshould show that QuestDB is being scraped\nsuccessfully:\nIn the graphing tab of Prometheus (\nhttp://0.0.0.0:9090/graph\n), autocomplete\ncan be used to graph QuestDB-specific metrics which are all prefixed with\nquestdb_\n:\nThe following metrics are available:\n\n| Metric | Type | Description |\n| --- | --- | --- |\n| questdb_commits_total | counter | Number of total commits of all types (in-order and out-of-order) executed on the database tables. |\n| questdb_o3_commits_total | counter | Number of total out-of-order (O3) commits executed on the database tables. |\n| questdb_committed_rows_total | counter | Number of total rows committed to the database tables. |\n| questdb_physically_written_rows_total | counter | Number of total rows physically written to disk. Greater thancommitted_rowswith [out-of-order ingestion. Write amplification isquestdb_physically_written_rows_total / questdb_committed_rows_total. |\n| questdb_rollbacks_total | counter | Number of total rollbacks executed on the database tables. |\n| questdb_json_queries_total | counter | Number of total REST API queries, including retries. |\n| questdb_json_queries_completed_total | counter | Number of successfully executed REST API queries. |\n| questdb_pg_wire_queries_total | counter | Number of total PGWire queries. |\n| questdb_pg_wire_queries_completed_total | counter | Number of successfully executed PGWire queries. |\n| questdb_unhandled_errors_total | counter | Number of total unhandled errors occurred in the database. Such errors usually mean a critical service degradation in one of the database subsystems. |\n| questdb_jvm_major_gc_count_total | counter | Number of times major JVM garbage collection was triggered. |\n| questdb_jvm_major_gc_time_total | counter | Total time spent on major JVM garbage collection in milliseconds. |\n| questdb_jvm_minor_gc_count_total | counter | Number of times minor JVM garbage collection pause was triggered. |\n| questdb_jvm_minor_gc_time_total | counter | Total time spent on minor JVM garbage collection pauses in milliseconds. |\n| questdb_jvm_unknown_gc_count_total | counter | Number of times JVM garbage collection of unknown type was triggered. Non-zero values of this metric may be observed only on some, non-mainstream JVM implementations. |\n| questdb_jvm_unknown_gc_time_total | counter | Total time spent on JVM garbage collection of unknown type in milliseconds. Non-zero values of this metric may be observed only on some, non-mainstream JVM implementations. |\n| questdb_memory_tag_MMAP_DEFAULT | gauge | Amount of memory allocated for mmaped files. |\n| questdb_memory_tag_NATIVE_DEFAULT | gauge | Amount of allocated untagged native memory. |\n| questdb_memory_tag_MMAP_O3 | gauge | Amount of memory allocated for O3 mmapped files. |\n| questdb_memory_tag_NATIVE_O3 | gauge | Amount of memory allocated for O3. |\n| questdb_memory_tag_NATIVE_RECORD_CHAIN | gauge | Amount of memory allocated for SQL record chains. |\n| questdb_memory_tag_MMAP_TABLE_WRITER | gauge | Amount of memory allocated for table writer mmapped files. |\n| questdb_memory_tag_NATIVE_TREE_CHAIN | gauge | Amount of memory allocated for SQL tree chains. |\n| questdb_memory_tag_MMAP_TABLE_READER | gauge | Amount of memory allocated for table reader mmapped files. |\n| questdb_memory_tag_NATIVE_COMPACT_MAP | gauge | Amount of memory allocated for SQL compact maps. |\n| questdb_memory_tag_NATIVE_FAST_MAP | gauge | Amount of memory allocated for SQL fast maps. |\n| questdb_memory_tag_NATIVE_LONG_LIST | gauge | Amount of memory allocated for long lists. |\n| questdb_memory_tag_NATIVE_HTTP_CONN | gauge | Amount of memory allocated for HTTP connections. |\n| questdb_memory_tag_NATIVE_PGW_CONN | gauge | Amount of memory allocated for PostgreSQL Wire Protocol connections. |\n| questdb_memory_tag_MMAP_INDEX_READER | gauge | Amount of memory allocated for index reader mmapped files. |\n| questdb_memory_tag_MMAP_INDEX_WRITER | gauge | Amount of memory allocated for index writer mmapped files. |\n| questdb_memory_tag_MMAP_INDEX_SLIDER | gauge | Amount of memory allocated for indexed column view mmapped files. |\n| questdb_memory_tag_NATIVE_REPL | gauge | Amount of memory mapped for replication tasks. |\n| questdb_memory_free_count | gauge | Number of times native memory was freed. |\n| questdb_memory_mem_used | gauge | Current amount of allocated native memory. |\n| questdb_memory_malloc_count | gauge | Number of times native memory was allocated. |\n| questdb_memory_realloc_count | gauge | Number of times native memory was reallocated. |\n| questdb_memory_rss | gauge | Resident Set Size (Linux/Unix) / Working Set Size (Windows). |\n| questdb_memory_jvm_free | gauge | Current amount of free Java memory heap in bytes. |\n| questdb_memory_jvm_total | gauge | Current size of Java memory heap in bytes. |\n| questdb_memory_jvm_max | gauge | Maximum amount of Java heap memory that can be allocated in bytes. |\n| questdb_http_connections | gauge | Number of currently active HTTP connections. |\n| questdb_json_queries_cached | gauge | Number of current cached REST API queries. |\n| questdb_line_tcp_connections | gauge | Number of currently active InfluxDB Line Protocol TCP connections. |\n| questdb_pg_wire_connections | gauge | Number of currently active PostgreSQL Wire Protocol connections. |\n| questdb_pg_wire_select_queries_cached | gauge | Number of current cached PostgreSQL Wire ProtocolSELECTqueries. |\n| questdb_pg_wire_update_queries_cached | gauge | Number of current cached PostgreSQL Wire ProtocolUPDATEqueries. |\n| questdb_json_queries_cache_hits_total | counter | Number of total cache hits for JSON queries. |\n| questdb_json_queries_cache_misses_total | counter | Number of total cache misses for JSON queries. |\n| questdb_json_queries_completed_total | counter | Total number of completed JSON queries. |\n| questdb_jvm_major_gc_count_total | counter | Total number of major garbage collection events. |\n| questdb_jvm_major_gc_time_total | counter | Total time spent on major garbage collection. |\n| questdb_jvm_minor_gc_count_total | counter | Total number of minor garbage collection events. |\n| questdb_jvm_minor_gc_time_total | counter | Total time spent on minor garbage collection. |\n| questdb_jvm_unknown_gc_count_total | counter | Total number of unknown type garbage collection events. |\n| questdb_jvm_unknown_gc_time_total | counter | Total time spent on unknown type garbage collection. |\n| questdb_memory_tag_MMAP_BLOCK_WRITER | gauge | Amount of memory allocated for block writer mmapped files. |\n| questdb_memory_tag_MMAP_IMPORT | gauge | Amount of memory allocated for import operations. |\n| questdb_memory_tag_MMAP_PARALLEL_IMPORT | gauge | Amount of memory allocated for parallel import operations. |\n| questdb_memory_tag_MMAP_PARTITION_CONVERTER | gauge | Amount of memory allocated for partition converter operations. |\n| questdb_memory_tag_MMAP_SEQUENCER_METADATA | gauge | Amount of memory allocated for sequencer metadata. |\n| questdb_memory_tag_MMAP_TABLE_WAL_READER | gauge | Amount of memory allocated for table WAL reader mmapped files. |\n| questdb_memory_tag_MMAP_TABLE_WAL_WRITER | gauge | Amount of memory allocated for table WAL writer mmapped files. |\n| questdb_memory_tag_MMAP_TX_LOG | gauge | Amount of memory allocated for transaction log mmapped files. |\n| questdb_memory_tag_MMAP_TX_LOG_CURSOR | gauge | Amount of memory allocated for transaction log cursor mmapped files. |\n| questdb_memory_tag_MMAP_UPDATE | gauge | Amount of memory allocated for update operations. |\n| questdb_memory_tag_NATIVE_CB1 | gauge | Amount of memory allocated for native circular buffer 1. |\n| questdb_memory_tag_NATIVE_CB2 | gauge | Amount of memory allocated for native circular buffer 2. |\n| questdb_memory_tag_NATIVE_CB3 | gauge | Amount of memory allocated for native circular buffer 3. |\n| questdb_memory_tag_NATIVE_CB4 | gauge | Amount of memory allocated for native circular buffer 4. |\n| questdb_memory_tag_NATIVE_CB5 | gauge | Amount of memory allocated for native circular buffer 5. |\n| questdb_memory_tag_NATIVE_CIRCULAR_BUFFER | gauge | Amount of memory allocated for native circular buffers. |\n| questdb_memory_tag_NATIVE_DIRECT_BYTE_SINK | gauge | Amount of memory allocated for native direct byte sink. |\n| questdb_memory_tag_NATIVE_DIRECT_CHAR_SINK | gauge | Amount of memory allocated for native direct char sink. |\n| questdb_memory_tag_NATIVE_DIRECT_UTF8_SINK | gauge | Amount of memory allocated for native direct UTF-8 sink. |\n| questdb_memory_tag_NATIVE_FAST_MAP_INT_LIST | gauge | Amount of memory allocated for native fast map integer list. |\n| questdb_memory_tag_NATIVE_FUNC_RSS | gauge | Amount of memory allocated for native function RSS. |\n| questdb_memory_tag_NATIVE_GROUP_BY_FUNCTION | gauge | Amount of memory allocated for native group by function. |\n| questdb_memory_tag_NATIVE_ILP_RSS | gauge | Amount of memory allocated for native ILP RSS. |\n| questdb_memory_tag_NATIVE_IMPORT | gauge | Amount of memory allocated for native import operations. |\n| questdb_memory_tag_NATIVE_INDEX_READER | gauge | Amount of memory allocated for native index reader. |\n| questdb_memory_tag_NATIVE_IO_DISPATCHER_RSS | gauge | Amount of memory allocated for native IO dispatcher RSS. |\n| questdb_memory_tag_NATIVE_JIT | gauge | Amount of memory allocated for native JIT. |\n| questdb_memory_tag_NATIVE_JIT_LONG_LIST | gauge | Amount of memory allocated for native JIT long list. |\n| questdb_memory_tag_NATIVE_JOIN_MAP | gauge | Amount of memory allocated for native join map. |\n| questdb_memory_tag_NATIVE_LATEST_BY_LONG_LIST | gauge | Amount of memory allocated for native latest by long list. |\n| questdb_memory_tag_NATIVE_LOGGER | gauge | Amount of memory allocated for native logger. |\n| questdb_memory_tag_NATIVE_MIG | gauge | Amount of memory allocated for native MIG. |\n| questdb_memory_tag_NATIVE_MIG_MMAP | gauge | Amount of memory allocated for native MIG mmapped files. |\n| questdb_memory_tag_NATIVE_OFFLOAD | gauge | Amount of memory allocated for native offload. |\n| questdb_memory_tag_NATIVE_PARALLEL_IMPORT | gauge | Amount of memory allocated for native parallel import. |\n| questdb_memory_tag_NATIVE_PATH | gauge | Amount of memory allocated for native path. |\n| questdb_memory_tag_NATIVE_ROSTI | gauge | Amount of memory allocated for native rosti. |\n| questdb_memory_tag_NATIVE_SAMPLE_BY_LONG_LIST | gauge | Amount of memory allocated for native sample by long list. |\n| questdb_memory_tag_NATIVE_SQL_COMPILER | gauge | Amount of memory allocated for native SQL compiler. |\n| questdb_memory_tag_NATIVE_TABLE_READER | gauge | Amount of memory allocated for native table reader. |\n| questdb_memory_tag_NATIVE_TABLE_WAL_WRITER | gauge | Amount of memory allocated for native table WAL writer. |\n| questdb_memory_tag_NATIVE_TABLE_WRITER | gauge | Amount of memory allocated for native table writer. |\n| questdb_memory_tag_NATIVE_TEXT_PARSER_RSS | gauge | Amount of memory allocated for native text parser RSS. |\n| questdb_memory_tag_NATIVE_TLS_RSS | gauge | Amount of memory allocated for native TLS RSS. |\n| questdb_memory_tag_NATIVE_UNORDERED_MAP | gauge | Amount of memory allocated for native unordered map. |\n| questdb_pg_wire_errors_total | counter | Total number of errors in PostgreSQL wire protocol. |\n| questdb_pg_wire_select_cache_hits_total | counter | Total number of cache hits for PostgreSQL wire protocol select queries. |\n| questdb_pg_wire_select_cache_misses_total | counter | Total number of cache misses for PostgreSQL wire protocol select queries. |\n| questdb_wal_apply_physically_written_rows_total | counter | Total number of physically written rows during WAL apply. |\n| questdb_wal_apply_rows_per_second | gauge | Rate of rows applied per second during WAL apply. |\n| questdb_wal_apply_written_rows_total | counter | Total number of rows written during WAL apply. |\n| questdb_wal_written_rows_total | counter | Total number of rows written to WAL. |\n| questdb_wal_seq_txn | gauge | Sum of all committed transaction sequence numbers. Used in conjunction withquestdb_wal_writer_txn. |\n| questdb_wal_writer_txn | gauge | Sum of all transaction sequence numbers applied to tables. With no pending transactions in the WAL, equal toquestdb_wal_seq_txn. When its lag behindquestdb_wal_seq_txnis steadily growing, indicates QuestDB is unable to keep up with writes. |\n| questdb_wal_seq_txn_total |  | Removed, new name isquestdb_wal_seq_txn |\n| questdb_wal_writer_txn_total |  | Removed, new name isquestdb_writer_seq_txn |\n| questdb_workers_job_start_micros_max | gauge | Maximum time taken to start a worker job in microseconds. |\n| questdb_workers_job_start_micros_min | gauge | Minimum time taken to start a worker job in microseconds. |\n\nMost of the above metrics are volatile, i.e. they're collected since the current\ndatabase start. The exception are\nquestdb_wal_seq_txn\nand\nquestdb_wal_writer_txn\n, because transaction sequence numbers are persistent.\n\n## Configuring Prometheus Alertmanager‚Äã\n\nnote\nFull details on logging configurations can be found within the\nLogging & Metrics documentation\n.\nQuestDB includes a log writer that sends any message logged at critical level\n(by default) to Prometheus\nAlertmanager\nover a\nTCP/IP socket connection. To configure this writer, add it to the\nwriters\nconfig alongside other log writers.\nAlertmanager may be started via Docker with the following command:\n\n```shell\ndocker run -p 127.0.0.1:9093:9093 --name alertmanager quay.io/prometheus/alertmanager\n```\n\nTo discover the IP address of this container, run the following command which\nspecifies\nalertmanager\nas the container name:\n\n```bash\ndocker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' alertmanager\n```\n\nTo run QuestDB and point it towards Alertmanager for alerting, first create a\nfile\n./conf/log.conf\nwith the following contents.\n172.17.0.2\nin this case is\nthe IP address of the docker container for alertmanager that was discovered by\nrunning the\ndocker inspect\ncommand above.\n./conf/log.conf\n\n```ini\n# Which writers to enablewriters=stdout,alert# stdoutw.stdout.class=io.questdb.log.LogConsoleWriterw.stdout.level=INFO# Prometheus Alertingw.alert.class=io.questdb.log.LogAlertSocketWriterw.alert.level=CRITICALw.alert.alertTargets=172.17.0.2:9093\n```\n\nStart up QuestDB in Docker using the following command:\n\n```bash\ndocker run \\  -p 9000:9000 -p 8812:8812 -p 9009:9009 -p 9003:9003 \\  -v \"$(pwd)::/var/lib/questdb\" \\  questdb/questdb:6.1.3\n```\n\nWhen alerts are successfully triggered, QuestDB logs will indicate the sent and\nreceived status:\n\n```txt\n2021-12-14T18:42:54.222967Z I i.q.l.LogAlertSocketWriter Sending: 2021-12-14T18:42:54.122874Z I i.q.l.LogAlertSocketWriter Sending: 2021-12-14T18:42:54.073978Z I i.q.l.LogAlertSocketWriter Received [0] 172.17.0.2:9093: {\"status\":\"success\"}2021-12-14T18:42:54.223377Z I i.q.l.LogAlertSocketWriter Received [0] 172.17.0.2:9093: {\"status\":\"success\"}\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2371,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-002cf9281421",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/other/mindsdb",
    "title": "MindsDB | QuestDB",
    "text": "MindsDB\nprovides Machine\nLearning capabilities to enable predictive questions about your data. With\nMindsDB:\n- Developers can quickly add AI capabilities to their applications.\n- Data scientists can streamline MLOps by deploying ML models as AI Tables.\n- Data analysts can easily make forecasts on complex data, such as multivariate\ntime-series withhigh cardinality, and\nvisualize these in BI tools like Grafana, and Tableau.\nCombining both MindsDB and QuestDB provides unbound prediction ability\nwith\nSQL\n.\nThis guide describes how to pre-process data in QuestDB and then access these\ndata from MindsDB to produce powerful ML models.\n\n## Prerequisites‚Äã\n\n- docker: To create an image and run the container.\n- mysql: The client we use to interact with MindsDB\n(mysql -h 127.0.0.1 --port 47335 -u mindsdb -p). Alternatively, use MindsDB\nweb console athttp://localhost:47334instead.\n- Curl: To upload data to QuestDB from a local\nCSV file.\n\n## Instructions‚Äã\n\nThe following are the overall steps to connect MindsDB and QuestDB:\n- Build a Docker image and spawn a container to run MindsDB and QuestDB\ntogether.\n- Add QuestDB as a datasource to MindsDB using a SQL Statement.\n- Create a table and add data for a simple ML use case using QuestDB's web\nconsole.\n- Connect to MindsDB usingmysqlas a client and write some SQL.\nWe have put together all the files needed in\nGH\n.\n\n### Running the Docker container‚Äã\n\nClone the\nrepository for this tutorial\n. The\nDockerfile\nallows us to build an image with the following command:\n\n```shell\ndocker build -t questdb/mindsdb:latest .\n```\n\nThen, start the service container\nqmdb\nwith the following command:\n\n```shell\ndocker run --rm \\    -p 8812:8812 \\    -p 9009:9009 \\    -p 9000:9000 \\    -p 8888:8888 \\    -p 47334:47334 \\    -p 47335:47335 \\    -d \\    --name qmdb \\    questdb/mindsdb:latest\n```\n\nThe container is run as user\nquest\n. It takes about 10 seconds to become\nresponsive, logs can be followed in the terminal:\n\n```shell\ndocker logs -f qmdb...http API: starting...mysql API: starting...mongodb API: starting......mongodb API: started on 47336mysql API: started on 47335http API: started on 47334\n```\n\nThe container has these mount points:\n- /home/quest: User home directory.\n- ~/questdb/: QuestDB's root directory.\n- ~/questdb/db/: QuestDB's data root directory.\n- ~/backups/: Directory for backups.\n- ~/csv/: Directory for theCOPYoperation.\n- ~/mindsdb/storage/: MindsDB's data root directory.\nThe container is running\nDebian GNU/Linux 11 (bullseye)\nand exposes these\nports:\n- 9000: QuestDBWeb Console\n- 8812: QuestDB pg-wire\n- 9009: QuestDB InfluxDB Line Protocol ingress line protocol\n- 47334: MindsDB WebConsole\n- 47335: MindsDB mysql API\n- 47336: MindsDB mongodb API\n\n### Adding data to QuestDB‚Äã\n\nThere are different ways to\ninsert data to QuestDB\n.\n\n#### SQL‚Äã\n\nWe can access QuestDB's\nWeb Console\nat\nhttp://localhost:9000\n.\nRun the following SQL query to create a simple table:\n\n```questdb-sql\nCREATE TABLE IF NOT EXISTS house_rentals_data (    number_of_rooms INT,    number_of_bathrooms INT,    sqft INT,    location SYMBOL,    days_on_market INT,    initial_price FLOAT,    neighborhood SYMBOL,    rental_price FLOAT,    ts TIMESTAMP) TIMESTAMP(ts) PARTITION BY YEAR;\n```\n\nWe could populate table house_rentals_data with random data:\n\n```questdb-sql\nINSERT INTO house_rentals_data SELECT * FROM (    SELECT        rnd_int(1,6,0),        rnd_int(1,3,0),        rnd_int(180,2000,0),        rnd_symbol('great', 'good', 'poor'),        rnd_int(1,20,0),        rnd_float(0) * 1000,        rnd_symbol('alcatraz_ave', 'berkeley_hills', 'downtown', 'south_side', 'thowsand_oaks', 'westbrae'),        rnd_float(0) * 1000 + 500,        timestamp_sequence(            to_timestamp('2021-01-01', 'yyyy-MM-dd'),            14400000000L        )    FROM long_sequence(100));\n```\n\n\n#### CURL command‚Äã\n\nThe\ndata CSV file\ncan be downloaded to a local folder and uploaded to QuestDB using the following\ncommand:\n\n```shell\ncurl -F data=@sample_house_rentals_data.csv \"http://localhost:9000/imp?forceHeader=true&name=house_rentals_data\"\n```\n\nEither way, this gives us 100 data points, one every 4 hours, from\n2021-01-16T12:00:00.000000Z\n(QuestDB's timestamps are UTC with microsecond\nprecision).\n\n### Connect to MindsDB‚Äã\n\nWe can connect to MindsDB with a standard mysql-wire-protocol compliant client\n(no password, hit ENTER):\n\n```shell\nmysql -h 127.0.0.1 --port 47335 -u mindsdb -p\n```\n\nAlternatively, we can use MindsDB web console at\nhttp://localhost:47334\n:\nFrom the terminal or the MindsDB web console, run the following command to check\nthe available databases:\n\n```sql\nSHOW DATABASES;\n```\n\nQuestDB is not shown in the result:\n\n```shell\n+--------------------+| Database           |+--------------------+| mindsdb            || files              || information_schema |+--------------------+\n```\n\nTo see QuestDB as a database we need to add it to MindsDB:\n\n```sql\nCREATE DATABASE questdb    WITH ENGINE = \"questdb\",    PARAMETERS = {        \"user\": \"admin\",        \"password\": \"quest\",        \"host\": \"questdb\",        \"port\": \"8812\",        \"database\": \"questdb\"    };\n```\n\nThen, run\nSHOW DATABASES;\nshould display both MindsDB and QuestDB:\n\n```shell\n+--------------------+| Database           |+--------------------+| mindsdb            || files              || questdb            || information_schema |+--------------------+\n```\n\n\n#### questdb‚Äã\n\nThis is a read-only view on our QuestDB instance. We can query it leveraging the\nfull power of QuestDB's unique SQL syntax because statements are sent from\nMindsDB to QuestDB without interpreting them. It only works for SELECT\nstatements:\n\n```sql\nSELECT * FROM questdb(  SELECT        ts, neighborhood,            sum(days_on_market) DaysLive,            min(rental_price) MinRent,            max(rental_price) MaxRent,            avg(rental_price) AvgRent    FROM house_rentals_data    WHERE ts BETWEEN '2021-01-08' AND '2021-01-10'    SAMPLE BY 1d FILL (0, 0, 0, 0));\n```\n\nThe result should be something like this:\n\n```shell\n+--------------+----------------+----------+----------+----------+--------------------+| ts           | neighborhood   | DaysLive | MinRent  | MaxRent  | AvgRent            |+--------------+----------------+----------+----------+----------+--------------------+| 1610064000.0 | south_side     | 19       | 1285.338 | 1285.338 | 1285.338134765625  || 1610064000.0 | downtown       | 7        | 1047.14  | 1047.14  | 1047.1396484375    || 1610064000.0 | berkeley_hills | 17       | 727.52   | 727.52   | 727.5198974609375  || 1610064000.0 | westbrae       | 36       | 1038.358 | 1047.342 | 1042.85009765625   || 1610064000.0 | thowsand_oaks  | 5        | 1067.319 | 1067.319 | 1067.318603515625  || 1610064000.0 | alcatraz_ave   | 0        | 0.0      | 0.0      | 0.0                || 1610150400.0 | south_side     | 10       | 694.403  | 694.403  | 694.4031982421875  || 1610150400.0 | downtown       | 16       | 546.798  | 643.204  | 595.0011291503906  || 1610150400.0 | berkeley_hills | 4        | 1256.49  | 1256.49  | 1256.4903564453125 || 1610150400.0 | westbrae       | 0        | 0.0      | 0.0      | 0.0                || 1610150400.0 | thowsand_oaks  | 0        | 0.0      | 0.0      | 0.0                || 1610150400.0 | alcatraz_ave   | 14       | 653.924  | 1250.477 | 952.2005004882812  || 1610236800.0 | south_side     | 0        | 0.0      | 0.0      | 0.0                || 1610236800.0 | downtown       | 9        | 1357.916 | 1357.916 | 1357.9158935546875 || 1610236800.0 | berkeley_hills | 0        | 0.0      | 0.0      | 0.0                || 1610236800.0 | westbrae       | 0        | 0.0      | 0.0      | 0.0                || 1610236800.0 | thowsand_oaks  | 0        | 0.0      | 0.0      | 0.0                || 1610236800.0 | alcatraz_ave   | 0        | 0.0      | 0.0      | 0.0                |+--------------+----------------+----------+----------+----------+--------------------+\n```\n\nBeyond SELECT statements, for instance when we need to save the results of a\nquery into a new table, we need to use QuestDB's\nWeb Console\navailable at\nhttp://localhost:9000\n:\n\n```questdb-sql\nCREATE TABLE sample_query_results AS (    SELECT        ts,        neighborhood,        sum(days_on_market) DaysLive,        min(rental_price) MinRent,        max(rental_price) MaxRent,        avg(rental_price) AvgRent    FROM house_rentals_data    WHERE ts BETWEEN '2021-01-08' AND '2021-01-10'    SAMPLE BY 1d FILL (0, 0, 0, 0)) TIMESTAMP(ts) PARTITION BY MONTH;\n```\n\n\n#### mindsdb‚Äã\n\nContains the metadata tables necessary to create ML models:\n\n```sql\nUSE mindsdb;SHOW TABLES;\n```\n\n\n```shell\n+-------------------+| Tables_in_mindsdb |+-------------------+| models            || models_versions   |+-------------------+\n```\n\n\n### Create a predictor model‚Äã\n\nWe can create a predictor model\nmindsdb.home_rentals_model_ts\nto predict the\nrental_price\nfor a neighborhood considering the past 20 days, and no\nadditional features:\n\n```sql\nCREATE PREDICTOR mindsdb.home_rentals_model_ts FROM questdb (    SELECT        neighborhood,        rental_price,        ts    FROM house_rentals_data)PREDICT rental_price ORDER BY ts GROUP BY neighborhoodWINDOW 20 HORIZON 1;\n```\n\nThis triggers MindsDB to create/train the model based on the full data available\nfrom QuestDB's table\nhouse_rentals_data\n(100 rows) as a time series on the\ncolumn\nts\n.\nWhen status is complete, the model is ready for use; otherwise, we simply wait\nwhile we observe MindsDB's logs. Creating/training a model will take time\nproportional to the number of features, i.e. cardinality of the source table as\ndefined in the inner SELECT of the CREATE MODEL statement, and the size of the\ncorpus, i.e. number of rows. The model is a table in MindsDB:\n\n```sql\nSHOW TABLES;\n```\n\nThe new table is displayed:\n\n```shell\n+-----------------------+| Tables_in_mindsdb     |+-----------------------+| models                || models_versions       || home_rentals_model_ts |+-----------------------+\n```\n\n\n### Describe the predictor model‚Äã\n\nWe can get more information about the trained model, how was the accuracy\ncalculated or which columns are important for the model by executing the\nDESCRIBE MODEL\nstatement:\n\n```sql\nDESCRIBE MODEL mindsdb.home_rentals_model_ts;\n```\n\n\n```shell\n*************************** 1. row ***************************        accuracies: {'complementary_smape_array_accuracy':0.859}           outputs: ['rental_price']            inputs: ['neighborhood', 'ts', '__mdb_ts_previous_rental_price']        datasource: home_rentals_model_ts             model: encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> accuracy_functions\n```\n\nOr, to see how the model encoded the data prior to training we can execute:\n\n```sql\nDESCRIBE MODEL mindsdb.home_rentals_model_ts.features;\n```\n\n\n```shell\n+--------------+-------------+------------------+---------+| column       | type        | encoder          | role    |+--------------+-------------+------------------+---------+| neighborhood | categorical | OneHotEncoder    | feature || rental_price | float       | TsNumericEncoder | target  || ts           | datetime    | ArrayEncoder     | feature |+--------------+-------------+------------------+---------+\n```\n\nAdditional information about the models and how they can be customized can be\nfound on the\nLightwood docs\n.\n\n### Query MindsDB for predictions‚Äã\n\nThe latest\nrental_price\nvalue per neighborhood in table\nquestdb.house_rentals_data\ncan be obtained directly from QuestDB executing\nquery:\n\n```sql\nSELECT * FROM questdb (    SELECT        neighborhood,        rental_price,        ts    FROM house_rentals_data    LATEST BY neighborhood);\n```\n\n\n```shell\n+----------------+--------------+--------------+| neighborhood   | rental_price | ts           |+----------------+--------------+--------------+| thowsand_oaks  | 1150.427     | 1610712000.0 |   (2021-01-15 12:00:00.0)| south_side     | 726.953      | 1610784000.0 |   (2021-01-16 08:00:00.0)| downtown       | 568.73       | 1610798400.0 |   (2021-01-16 12:00:00.0)| westbrae       | 543.83       | 1610841600.0 |   (2021-01-17 00:00:00.0)| berkeley_hills | 559.928      | 1610870400.0 |   (2021-01-17 08:00:00.0)| alcatraz_ave   | 1268.529     | 1610884800.0 |   (2021-01-17 12:00:00.0)+----------------+--------------+--------------+\n```\n\nTo predict the next value:\n\n```sql\nSELECT    tb.ts,    tb.neighborhood,    tb.rental_price as predicted_rental_price,    tb.rental_price_explain as explanationFROM questdb.house_rentals_data AS taJOIN mindsdb.home_rentals_model_ts AS tbWHERE ta.ts > LATEST;\n```\n\n\n```shell\n+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| ts                  | neighborhood   | predicted_rental_price | explanation                                                                                                                                                                              |+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| 2021-01-17 00:00:00 | downtown       |      877.3007391233444 | {\"predicted_value\": 877.3007391233444, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 379.43294697022424, \"confidence_upper_bound\": 1375.1685312764646} || 2021-01-19 08:00:00 | westbrae       |      923.1387395936794 | {\"predicted_value\": 923.1387395936794, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 385.8327438509463, \"confidence_upper_bound\": 1460.4447353364124}  || 2021-01-15 16:00:00 | thowsand_oaks  |      1418.678199780345 | {\"predicted_value\": 1418.678199780345, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 1335.4600013965369, \"confidence_upper_bound\": 1501.8963981641532} || 2021-01-17 12:00:00 | berkeley_hills |      646.5979284300436 | {\"predicted_value\": 646.5979284300436, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 303.253838410034, \"confidence_upper_bound\": 989.9420184500532}    || 2021-01-18 12:00:00 | south_side     |       1422.69481363723 | {\"predicted_value\": 1422.69481363723, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 129.97617491441304, \"confidence_upper_bound\": 2715.413452360047}   || 2021-01-18 04:00:00 | alcatraz_ave   |      1305.009073065412 | {\"predicted_value\": 1305.009073065412, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 879.0232742685288, \"confidence_upper_bound\": 1730.994871862295}   |+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n\n### Stop the container‚Äã\n\nTo terminate the container, run:\n\n```shell\ndocker stop qmdb\n```\n\n- MindsDB GitHub\n- MindsDB Documentation\n- This tutorial's artefacts",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1710,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-3a5af1ad2755",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/orchestration/airflow",
    "title": "Apache Airflow | QuestDB",
    "text": "Apache Airflow is a powerful workflow automation tool that allows you to schedule and monitor tasks through directed acyclic graphs (DAGs). Airflow provides built-in operators for executing SQL queries, making it easy to automate QuestDB tasks.\nAlternatively, checkout our\nAutomating QuestDB Tasks\nguide for a scripted approach.\n\n## Prerequisites‚Äã\n\n- QuestDB running locally or remotely\n- Docker or Python 3, depending on how you want to install Airflow\n- Airflow installed and configured\n\n## Installation‚Äã\n\nWe recommended installing Airflow via Docker Compose, but any other supported method should also work. Follow the official guide:\n- Airflow Installation Documentation\n\n## QuestDB Connection‚Äã\n\nOn the Airflow UI you can find the\nAdmin > Connections\noption. You can create\na named connection to your QuestDB instance by adding a new connection of type\nPostgres\n. Just point to your host (if running Airflow inside of Docker, this\nmight be either the name of the container running QuestDB or\nhost.docker.internal\n), port (defaults to\n8812\n), database (\nqdb\n), user (\nadmin\n) and\npassword (\nquest\n).\n\n## Basic integration‚Äã\n\nOn Airflow you write a DAG, which is a graph of all the tasks you want to\nautomate, together with its dependencies and in which order they will be executed.\nDAGs are written as Python files, so you can virtually integrate with any data tool, but in the case of automating QuestDB queries, the easiest way to proceed\nis yo use the built-in\nPostgresOperator\n, which accepts a connection_id, and\na query to execute.\n\n## Example: Running a Query on QuestDB‚Äã\n\nThe following example defines an Airflow DAG to execute a SQL query on QuestDB:\n\n```python\nimport pendulumfrom airflow import DAGfrom airflow.providers.postgres.operators.postgres import PostgresOperatordefault_args = {    'owner': 'airflow',    'depends_on_past': False,    'start_date': pendulum.datetime(2025, 1, 1, tz=\"UTC\"),    'email_on_failure': False,    'email_on_retry': False,    'retries': 1,}dag = DAG(    'questdb_cleanup',    default_args=default_args,    description='Drops old partitions in QuestDB',    schedule_interval='@daily',    catchup=False,)cleanup_task = PostgresOperator(    task_id='drop_old_partitions',    postgres_conn_id='questdb',    sql=\"\"\"    ALTER TABLE my_table DROP PARTITION WHERE timestamp < dateadd('d', -30, now());    \"\"\",    dag=dag,)\n```\n\n\n## Running the Airflow DAG‚Äã\n\n- Open the Airflow UI athttp://localhost:8080.\n- Enable and trigger thequestdb_cleanupDAG manually.\n\n## Next Steps‚Äã\n\nFor further details and resources, refer to the following links:\n- Airflow Documentation:https://airflow.apache.org/docs/apache-airflow/stable/\n- Full Example Repository:https://github.com/questdb/data-orchestration-and-scheduling-samples",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 366,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-83ffb3ba3073",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/data-processing/spark",
    "title": "Apache Spark and Time-Series Analytics | QuestDB",
    "text": "High-level instructions for loading data from QuestDB to Spark and back.\n\n## What is Spark?‚Äã\n\nApache Spark\nis an analytics engine for large-scale\ndata engineering and\nstream processing\n,\nwell-known in the big data landscape. It is suitable for executing data\nengineering, data science, and machine learning on single-node machines or\nclusters.\n\n## QuestDB Spark integration‚Äã\n\nA typical Spark application processes data in the following steps:\n- Loading data from different sources\n- Transforming and analyzing the data\n- Saving the result to a data storage\nOur example demonstrates these steps using QuestDB as the data source and\nstorage. It loads data from QuestDB into a Spark Dataframe; then the data is\nenriched with new features, and eventually, it is written back into QuestDB.\n\n## Prerequisites‚Äã\n\n- Package manager: This depends on your choice of OS. The below instructions\nare for macOS using Homebrew.\n- QuestDB: An instance must be running and accessible. Not running? Checkout\nthequick start.\n\n## Installing Apache Spark‚Äã\n\nSpark can be installed and set up in many ways, depending on requirements.\nTypically, it is part of a Big Data stack, installed on multiple nodes with an\nexternal cluster manager, such as\nYarn\nor\nApache Mesos\n. In this tutorial, we will work\nwith a single-node standalone Spark installation.\nSpark has a multi-language environment. It is written in Scala, runs on the Java\nVirtual Machine, and also integrates with R and Python. Our example is written\nusing Python. By running the below commands Spark will be installed with all\nrequired dependencies:\n\n```shell\nbrew install openjdk@11brew install python@3.10brew install scalabrew install apache-spark\n```\n\nThe exact versions used for this example:\n\n```shell\nopenjdk@11 11.0.12python@3.10 3.10.10_1scala 3.2.2apache-spark 3.3.2\n```\n\n\n## Installing the JDBC driver‚Äã\n\nSpark communicates with QuestDB via JDBC, connecting to its Postgres Wire\nProtocol endpoint. This requires the Postgres JDBC driver to be present.\n- Create a working directory:\n\n```shell\nmkdir sparktestcd sparktest\n```\n\n- Download the JDBC driver fromhereinto the working directory. The exact version used for this example:\n\n```shell\npostgresql-42.5.1.jar\n```\n\n\n## Setting up database tables‚Äã\n\nFirst, start QuestDB. If you are using Docker run the following command:\n\n```shell\ndocker run -p 9000:9000 -p 8812:8812 questdb/questdb:9.3.2\n```\n\nThe port mappings allow us to connect to QuestDB's REST and PostgreSQL Wire\nProtocol endpoints. The former is required for opening the Web Console, and the\nlatter is used by Spark to connect to the database.\nOpen the\nWeb Console\nin your browser at\nhttp://localhost:9000\n.\nRun the following SQL commands using the console:\n\n```questdb-sql\nCREATE TABLE trades (  symbol SYMBOL,  side SYMBOL,  price DOUBLE,  amount DOUBLE,  timestamp TIMESTAMP) timestamp (timestamp) PARTITION BY DAY;CREATE TABLE trades_enriched (  symbol SYMBOL,  volume DOUBLE,  mid DOUBLE,  ts TIMESTAMP,  ma10 DOUBLE,  std DOUBLE) timestamp (ts) PARTITION BY DAY;INSERT INTO trades SELECT * FROM (  SELECT 'BTC-USD' symbol,  rnd_symbol('buy', 'sell') side,  rnd_double() * 10000 price,  rnd_double() amount,  timestamp_sequence(1677628800000000, 10000000) ts  FROM long_sequence(25920)) timestamp (ts);\n```\n\nThe\nINSERT\ncommand generates 3 days' worth of test data, and stores it in the\ntrades\ntable.\n\n## Feature engineering examples‚Äã\n\nSave the below Python code into a file called\nsparktest.py\ninside the working\ndirectory:\n\n```python\nfrom pyspark.sql import SparkSessionfrom pyspark.sql.window import Windowfrom pyspark.sql.functions import avg, stddev, when# create Spark sessionspark = SparkSession.builder.appName(\"questdb_test\").getOrCreate()# load 1-minute aggregated trade data into the dataframedf = spark.read.format(\"jdbc\") \\    .option(\"url\", \"jdbc:postgresql://localhost:8812/questdb\") \\    .option(\"driver\", \"org.postgresql.Driver\") \\    .option(\"user\", \"admin\").option(\"password\", \"quest\") \\    .option(\"dbtable\", \"(SELECT symbol, sum(amount) as volume, \"                       \"round((max(price)+min(price))/2, 2) as mid, \"                       \"timestamp as ts \"                       \"FROM trades WHERE symbol = 'BTC-USD' \"                       \"SAMPLE BY 1m ALIGN to CALENDAR) AS mid_prices\") \\    .option(\"partitionColumn\", \"ts\") \\    .option(\"numPartitions\", \"3\") \\    .option(\"lowerBound\", \"2023-03-01T00:00:00.000000Z\") \\    .option(\"upperBound\", \"2023-03-04T00:00:00.000000Z\") \\    .load()# extract new features, clean datawindow_10 = Window.partitionBy(df.symbol).rowsBetween(-10, Window.currentRow)df = df.withColumn(\"ma10\", avg(df.mid).over(window_10))df = df.withColumn(\"std\", stddev(df.mid).over(window_10))df = df.withColumn(\"std\", when(df.std.isNull(), 0.0).otherwise(df.std))# save the data as 'trades_enriched', overwrite if already existsdf.write.format(\"jdbc\") \\    .option(\"url\", \"jdbc:postgresql://localhost:8812/questdb\") \\    .option(\"driver\", \"org.postgresql.Driver\") \\    .option(\"user\", \"admin\").option(\"password\", \"quest\") \\    .option(\"dbtable\", \"trades_enriched\") \\    .option(\"truncate\", True) \\    .option(\"createTableColumnTypes\", \"volume DOUBLE, mid DOUBLE, ma10 DOUBLE, std DOUBLE\") \\    .save(mode=\"overwrite\")\n```\n\nThis Spark application loads aggregated data from the\ntrades\ntable into a\nDataframe, then adds two new features, a 10-minute moving average and the\nstandard deviation. Finally, it writes the enriched data back into QuestDB and\nsaves it to the\ntrades_enriched\ntable.\n\n## Run the example‚Äã\n\nSubmit the application to Spark for execution using\nspark-submit\n:\n\n```shell\nspark-submit --jars postgresql-42.5.1.jar sparktest.py\n```\n\nThe example requires the JDBC driver at runtime. This dependency is submitted to\nSpark using the\n--jars\noption.\nAfter the execution is completed, we can check the content of the\ntrades_enriched\ntable:\n\n```questdb-sql\nSELECT * FROM trades_enriched;\n```\n\nThe enriched data should be displayed in the\nWeb Console\n.\n\n## See also‚Äã\n\nFor a more detailed explanation of the QuestDB Spark integration, please also\nsee our tutorial\nIntegrate Apache Spark and QuestDB for Time-Series Analytics\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 794,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-b426c84be3c8",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/visualization/powerbi",
    "title": "PowerBI | QuestDB",
    "text": "This guide demonstrates how to connect QuestDB with Microsoft PowerBI to create\ninteractive data visualizations and dashboards.\n\n## Prerequisites‚Äã\n\n- QuestDBrunning locally or remotely\n- PowerBI Desktopinstalled\n\n## Connection Setup‚Äã\n\nQuestDB utilizes a fully featured PostgreSQL Wire Protocol (PGWire). As such,\nsetup for PowerBI mirrors the standard PostgreSQL connection setup. The benefit\nis the performance profile of QuestDB, and its powerful time-series SQL extensions,\nwith the simplicity of the PGWire protocol.\n- Open PowerBI Desktop\n- Click \"Get Data\" in the Home tab\n- Select \"Database\" ‚Üí \"PostgreSQL\"\n- Enter your QuestDB connection details:Server:localhost(or your server address)Database:qdbData Connectivity mode:ImportAdvanced options (optional):Port:8812(default QuestDB PGWire port)Command timeout: Adjust based on your query complexity\n- Select:Database authentication:User:adminPassword:quest\n- Click \"Connect\"\n\n## Working with Data‚Äã\n\n- In the Navigator window, select the tables you want to analyze\n- Click \"Transform Data\" to modify the data or \"Load\" to import it directly\n- Create visualizations by dragging fields onto the report canvas\n- Save your report and publish it to PowerBI Service if needed\n\n## Using Custom SQL‚Äã\n\nTo leverage QuestDB-specific features like\nSAMPLE BY\nand\nLATEST ON\n, you can use custom SQL:\n- In the \"Get Data\" dialog, click \"Advanced options\"\n- Enter your SQL query in the \"SQL statement\" field\n- Click \"OK\" to execute\nRemember, you must include a timestamp column when using functions like\nSAMPLE BY\n.\nHere are some useful query examples:\n\n```questdb-sql\n-- Get 1-hour samples of trade pricesSELECT    timestamp,    avg(price) as avg_price,    sum(amount) as volumeFROM tradesWHERE timestamp >= dateadd('d', -7, now())SAMPLE BY 1h;-- Get latest trade for each symbolSELECT * FROM tradesLATEST ON timestamp PARTITION BY symbol;-- Combine SAMPLE BY with multiple aggregationsSELECT     timestamp,    symbol,    max(price) max_price,    min(price) min_price,    avg(price) avg_priceFROM tradesWHERE timestamp >= dateadd('M', -1, now())SAMPLE BY 1dALIGN TO CALENDAR;\n```\n\n\n## Best Practices‚Äã\n\n- Leveragetimestampsfunctions for time-series analysis\n- Explore variousaggregation functionsto suit your data needs\n- Consider using powerfulwindow functionsto perform complex calculations\n- For large datasets, use incremental refresh in PowerBI\n\n## Caveats‚Äã\n\n\n### Date Table Limitations‚Äã\n\nQuestDB currently cannot be used as a source for PowerBI's \"Mark as Date Table\" feature. This means:\n- You cannot mark QuestDB tables as date tables in PowerBI\n- Some time intelligence functions in PowerBI may not be available\n- If you need date table functionality, consider creating it in PowerBI or using another data source\ntip\nIf you'd like QuestDB to support this feature, please add a üëç to\nthis GitHub issue\n.\n\n## Troubleshooting‚Äã\n\n- If connection fails, verify your QuestDB instance is running and accessible\n- Ensure PGWire is enabled in your QuestDB configurationpg.enabled=true- seeconfigurationfor more details\n- Check that the port8812is open and not blocked by firewalls\n- For timeout errors, adjust the command timeout in advanced options\n\n## Further Reading‚Äã\n\n- QuestDB PGWire\n- PowerBI Documentation",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 465,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-7d8ac5ad6a5c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/message-brokers/flink",
    "title": "QuestDB Flink connector | QuestDB",
    "text": "Apache Flink\nis a popular framework and\nstream processing\nengine. QuestDB ships a\nQuestDB Flink Sink connector\nfor fast ingestion from Apache Flink into QuestDB. The connector implements the\nTable API and SQL\nfor Flink.\n\n## Quick start‚Äã\n\nThis section shows the steps to use the QuestDB Flink connector to ingest data\nfrom Flink into QuestDB. The connector uses the SQL interface to interact with\nFlink. The overall steps are the followings:\n- The connector creates a table in Flink backed by QuestDB.\n- The connector inserts data into the table.\n- Finally it queries the data in QuestDB.\n\n### Prerequisites‚Äã\n\n- A local JDK version 11 installation\n- Docker for running QuestDB\n\n### Connector installation‚Äã\n\n- Start the QuestDB container image:docker run -p 9000:9000 -p 9009:9009 questdb/questdb:9.3.2\n- DownloadApache Flink distributionand\nunpack it.\n- Downloadthe QuestDB Flink connector from Maven Central and place it in thelibdirectory of your Flink installation.\n- Go to thebindirectory of your Flink installation and run the following to\nstart a Flink cluster:./start-cluster.sh\n- While still in thebindirectory, start a Flink SQL console by running:./sql-client.shThen, run the following SQL command in the Flink SQL console:CREATETABLEOrders(order_numberBIGINT,priceBIGINT,buyer        STRING)WITH('connector'='questdb','host'='localhost');Expected output:[INFO] Execute statement succeed.This command created a Flink table backed by QuestDB. The table is calledOrdersand has three columns:order_number,price, andbuyer. Theconnectoroption specifies the QuestDB Flink connector. Thehostoption\nspecifies the host and port where QuestDB is running. The default port is9009.\n- While still in the Flink SQL console execute:INSERT INTO Orders values (0, 42, 'IBM');Expected output:[INFO] SQL update statement has been successfully submitted to the cluster:Job ID: <random hexadecimal id>This command used Flink SQL to insert a row into theOrderstable in Flink.\nThe table is connected to QuestDB, so the row is also into QuestDB.\n- Go to the QuestDBWeb Consoleathttp://localhost:9000and execute this query:SELECT * FROM Orders;You should see a table with one row.\nCongratulations! You have successfully used the QuestDB Flink connector to\ningest data from Flink into QuestDB. You can now build Flink data pipelines that\nuse QuestDB as a sink.\nSee the\nQuestDB Flink connector GitHub repository\nfor more examples.\n\n## Configuration‚Äã\n\nThe QuestDB Flink connector supports the following configuration options:\n\n| Name | Type | Example | Default | Meaning |\n| --- | --- | --- | --- | --- |\n| host | string | localhost:9009 | N/A | Host and port where QuestDB server is running |\n| username | string | testUser1 | admin | Username for authentication. The default is used when alsotokenis set. |\n| token | string | GwBXoGG5c6NoUTLXnzMxw | admin | Token for authentication |\n| table | string | my_table | Same as Flink table name | Target table in QuestDB |\n| tls | boolean | true | false | Whether to use TLS/SSL for connecting to QuestDB server |\n| buffer.size.kb | integer | 32 | 64 | Size of the QuestDB client send buffer |\n| sink.parallelism | integer | 2 | Same as upstream processors | QuestDB Sink Parallelism |\n\nExample configuration for connecting to QuestDB running on localhost:\n\n```sql\nCREATE TABLE Orders (     order_number BIGINT,     price        BIGINT,     buyer        STRING ) WITH (   'connector'='questdb',   'host'='localhost',   'table' = 'orders');\n```\n\n\n## Connector Distribution‚Äã\n\nThe connector is distributed as a single jar file. The jar file is available in\nthe\nMaven Central repository\nand it's available under the following coordinates:\n\n```xml\n<dependency>  <groupId>org.questdb</groupId>  <artifactId>flink-questdb-connector</artifactId>  <version>LATEST</version></dependency>\n```\n\n\n## FAQ‚Äã\n\nQ: Why is QuestDB client not repackaged into a different Java package?\nA:\nQuestDB client uses native code, this makes repackaging hard.\nQ: I need to use QuestDB as a Flink source, what should I do?\nA: This\nconnector is Sink only. If you want to use QuestDB as a Source then your best\nchance is to use\nFlink JDBC source\nand rely on\nQuestDB Postgres compatibility\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 632,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-6d9f40dcc45e",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/backup",
    "title": "BACKUP keyword | QuestDB",
    "text": "Enterprise\n‚Äî\nObject storage backups with incremental and point-in-time recovery support.\nLearn more\nBACKUP\n- start and abort incremental backups to object storage.\nLooking for a detailed guide on backup creation and restoration? Check out ourBackup and Restoreguide!\n\n## Syntax‚Äã\n\n\n```questdb-sql\nBACKUP DATABASE;BACKUP ABORT;\n```\n\n\n### BACKUP DATABASE‚Äã\n\nStarts a new incremental backup. Returns immediately with the backup timestamp.\nThe backup runs asynchronously in the background.\n\n### BACKUP ABORT‚Äã\n\nAborts a running backup. Returns a single row:\n\n| Column | Type | Description |\n| --- | --- | --- |\n| status | VARCHAR | abortedornot running |\n| backup_id | TIMESTAMP | Timestamp of aborted backup, orNULL |\n\nExample when backup was running:\n\n| status | backup_id |\n| --- | --- |\n| aborted | 2024-01-15T10:30:00.000000Z |\n\nExample when no backup was running:\n\n| status | backup_id |\n| --- | --- |\n| not running | NULL |\n\n\n## Monitoring backups‚Äã\n\nUse the\nbackups()\ntable function to monitor backup progress and history:\n\n```questdb-sql\nSELECT * FROM backups();\n```\n\nReturns:\n\n| Column | Type | Description |\n| --- | --- | --- |\n| status | VARCHAR | Current status (see below) |\n| progress_percent | INT | Completion percentage (0-100) |\n| start_ts | TIMESTAMP | When the backup started |\n| end_ts | TIMESTAMP | When the backup completed (NULL if running) |\n| backup_error | VARCHAR | Error message if backup failed |\n| cleanup_error | VARCHAR | Error message if cleanup failed |\n\n\n### Status values‚Äã\n\nbackup in progress\n,\nbackup complete\n,\nbackup failed\n,\ncleanup in progress\n,\ncleanup complete\n,\ncleanup failed\nSee\nstatus values\nin the Backup\nguide for descriptions and recommended actions.\n\n## Examples‚Äã\n\nStart a backup:\n\n```questdb-sql\nBACKUP DATABASE;\n```\n\nResult:\n\n| backup_timestamp |\n| --- |\n| 2024-08-24T12:34:56.789123Z |\n\nCheck current backup status:\n\n```questdb-sql\nSELECT status, progress_percent FROM backups() ORDER BY start_ts DESC LIMIT 1;\n```\n\n\n## Configuration‚Äã\n\nBackups must be configured before use. At minimum:\n\n```conf\nbackup.enabled=truebackup.object.store=s3::bucket=my-bucket;region=eu-west-1;...\n```\n\nSee the\nBackup and Restore guide\nfor full\nconfiguration options.\n\n## Limitations‚Äã\n\n- Only one backup can run at a time\n- Primary and replica backups are separate (each has its ownbackup_instance_name)\n\n## See also‚Äã\n\n- Backup and Restore guide- Complete backup\nconfiguration and restore procedures\n- CHECKPOINT- Manual checkpoint mode for\nQuestDB OSS backups",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 389,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-2b98c1c1286c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/high-availability/setup",
    "title": "Replication setup guide | QuestDB",
    "text": "Enterprise\n‚Äî\nThis guide covers setting up primary-replica replication.\nLearn more\nThis guide walks you through setting up QuestDB Enterprise replication.\nPrerequisites:\nRead the\nReplication overview\nto understand how replication works.\n\n## Setup steps‚Äã\n\n- Configure object storage (AWS S3, Azure Blob, GCS, or NFS)\n- Configure theprimarynode\n- Take a snapshot of the primary\n- Configurereplicanode(s)\n\n## 1. Configure object storage‚Äã\n\nChoose your object storage provider and build the connection string for\nreplication.object.store\nin\nserver.conf\n.\n\n### AWS S3‚Äã\n\nCreate an S3 bucket following\nAWS documentation\n.\nRecommendations:\n- Select a region close to your primary node\n- Disable blob versioning\n- Set up alifecycle policyto manage WAL file retention (seeSnapshot and expiration policies)\nConnection string:\n\n```ini\nreplication.object.store=s3::bucket=${BUCKET_NAME};root=${DB_INSTANCE_NAME};region=${AWS_REGION};access_key_id=${AWS_ACCESS_KEY};secret_access_key=${AWS_SECRET_ACCESS_KEY};\n```\n\nDB_INSTANCE_NAME\ncan be any unique alphanumeric string (dashes allowed). Use\nthe same value across all nodes in your replication cluster.\nUsing IAM roles\nIf your instance has an IAM role attached (EC2 instance profile, EKS pod identity,\nor ECS task role), you can omit the credentials:\n\n```ini\nreplication.object.store=s3::bucket=${BUCKET_NAME};root=${DB_INSTANCE_NAME};region=${AWS_REGION};\n```\n\nQuestDB will automatically use the instance's IAM role for authentication.\n\n### Azure Blob Storage‚Äã\n\nCreate a Storage Account following\nAzure documentation\n,\nthen create a Blob Container.\nRecommendations:\n- Select a region close to your primary node\n- Disable blob versioning\n- Set upLifecycle Managementfor WAL file retention\nConnection string:\n\n```ini\nreplication.object.store=azblob::endpoint=https://${STORE_ACCOUNT}.blob.core.windows.net;container=${BLOB_CONTAINER};root=${DB_INSTANCE_NAME};account_name=${STORE_ACCOUNT};account_key=${STORE_KEY};\n```\n\nUsing Managed Identity\nIf your instance has a Managed Identity assigned (Azure VM, AKS pod identity,\nor Container Apps), you can omit the\naccount_key\n:\n\n```ini\nreplication.object.store=azblob::endpoint=https://${STORE_ACCOUNT}.blob.core.windows.net;container=${BLOB_CONTAINER};root=${DB_INSTANCE_NAME};account_name=${STORE_ACCOUNT};\n```\n\nQuestDB will automatically use the Managed Identity for authentication. Ensure\nthe identity has the\nStorage Blob Data Contributor\nrole on the container.\n\n### Google Cloud Storage‚Äã\n\nCreate a GCS bucket, then create a service account with\nStorage Admin\n(or\nequivalent) permissions. Download the JSON key and encode it as Base64:\n\n```bash\ncat <key>.json | base64\n```\n\nConnection string:\n\n```ini\nreplication.object.store=gcs::bucket=${BUCKET_NAME};root=/;credential=${BASE64_ENCODED_KEY};\n```\n\nAlternatively, use\ncredential_path\nto reference the key file directly.\nUsing Workload Identity\nIf your instance uses Workload Identity (GKE) or runs on a GCE VM with a service\naccount attached, you can omit the credentials entirely:\n\n```ini\nreplication.object.store=gcs::bucket=${BUCKET_NAME};root=/;\n```\n\nQuestDB will automatically use Application Default Credentials for authentication.\n\n### NFS‚Äã\n\nMount the shared filesystem on all nodes. Ensure the QuestDB user has read/write\npermissions.\nImportant:\nBoth the WAL folder and scratch folder must be on the same NFS\nmount to prevent write corruption.\nConnection string:\n\n```ini\nreplication.object.store=fs::root=/mnt/nfs_replication/final;atomic_write_dir=/mnt/nfs_replication/scratch;\n```\n\n\n## 2. Configure the primary node‚Äã\n\nAdd to\nserver.conf\n:\n\n| Setting | Value |\n| --- | --- |\n| replication.role | primary |\n| replication.object.store | Your connection string from step 1 |\n| cairo.snapshot.instance.id | Unique UUID for this node |\n\nRestart QuestDB.\n\n## 3. Take a snapshot‚Äã\n\nReplicas are initialized from a snapshot of the primary's data. This involves\ncreating a backup of the primary and preparing it for restoration on replica\nnodes.\nSee\nBackup and restore\nfor the full procedure.\ntip\nSet up regular snapshots (daily or weekly). See\nSnapshot and expiration policies\nfor\nguidance.\n\n## 4. Configure replica node(s)‚Äã\n\nCreate a new QuestDB instance. Add to\nserver.conf\n:\n\n| Setting | Value |\n| --- | --- |\n| replication.role | replica |\n| replication.object.store | Same connection string as primary |\n| cairo.snapshot.instance.id | Unique UUID for this replica |\n\nwarning\nDo not copy\nserver.conf\nfrom the primary. Two nodes configured as primary\nwith the same object store will break replication.\nRestore the\ndb\ndirectory from the primary's snapshot, then start the replica.\nIt will download and apply WAL files to catch up with the primary.\n\n## Configuration reference‚Äã\n\nAll replication settings go in\nserver.conf\n. After changes, restart QuestDB.\ntip\nUse environment variables for sensitive settings:\n\n```bash\nexport QDB_REPLICATION_OBJECT_STORE=\"azblob::...\"\n```\n\n\n| Property | Default | Reloadable | Description |\n| --- | --- | --- | --- |\n| replication.role | none | No | Defaults tononefor stand-alone instances. To enable replication set to one of:primary,replica. |\n| replication.object.store |  | No | A configuration string that allows connecting to an object store. The format isscheme::key1=value;key2=value2;‚Ä¶. The various keys and values are detailed in a later section. Ignored if replication is disabled. No default given variability. |\n| cairo.wal.segment.rollover.size | 2097152 | No | The size of the WAL segment before it is rolled over. Default is2MiB. However, defaults to0unlessreplication.role=primaryis set. |\n| cairo.writer.command.queue.capacity | 32 | No | Maximum writer ALTER TABLE and replication command capacity. Shared between all the tables. |\n| replication.primary.throttle.window.duration | 10000 | No | The millisecond duration of the sliding window used to process replication batches. Default is10000ms. |\n| replication.requests.max.concurrent | 0 | No | A limit to the number of concurrent object store requests. The default is0for unlimited. |\n| replication.requests.retry.attempts | 3 | No | Maximum number of times to retry a failed object store request before logging an error and reattempting later after a delay. Default is3. |\n| replication.requests.retry.interval | 200 | No | How long to wait before retrying a failed operation. Default is200ms. |\n| replication.primary.compression.threads | calculated | No | Max number of threads used to perform file compression operations before uploading to the object store. The default value is calculated as half the number of CPU cores. |\n| replication.primary.compression.level | 1 | No | Zstd compression level. Defaults to1. Valid values are from 1 to 22. |\n| replication.replica.poll.interval | 1000 | No | Millisecond polling rate of a replica instance to check for the availability of new changes. |\n| replication.primary.sequencer.part.txn.count | 5000 | No | Sets the txn chunking size for each compressed batch. Smaller is better for constrained networks (but more costly). |\n| replication.primary.checksum=service-dependent | service-dependent | No | Where a checksum should be calculated for each uploaded artifact. Required for some object stores. Other options: never, always |\n| replication.primary.upload.truncated | true | No | Skip trailing, empty column data inside a WAL column file. |\n| replication.requests.buffer.size | 32768 | No | Buffer size used for object-storage downloads. |\n| replication.summary.interval | 1m | No | Frequency for printing replication progress summary in the logs. |\n| replication.metrics.per.table | true | No | Enable per-table replication metrics on the prometheus metrics endpoint. |\n| replication.metrics.dropped.table.poll.count | 10 | No | How many scrapes of prometheus metrics endpoint before dropped tables will no longer appear. |\n| replication.requests.max.batch.size.fast | 64 | No | Number of parallel requests allowed during the 'fast' process (non-resource constrained). |\n| replication.requests.max.batch.size.slow | 2 | No | Number of parallel requests allowed during the 'slow' process (error/resource constrained path). |\n| replication.requests.base.timeout | 10s | No | Replication upload/download request timeout. |\n| replication.requests.min.throughput | 262144 | No | Expected minimum network speed for replication transfers. Used to expand the timeout and account for network delays. |\n| native.async.io.threads | cpuCount | No | The number of async (network) io threads used for replication (and in the future cold storage). The default should be appropriate for most use cases. |\n| native.max.blocking.threads | cpuCount * 4 | No | Maximum number of threads for parallel blocking disk IO read/write operations for replication (and other). These threads are ephemeral: They are spawned per need and shut down after a short duration if no longer in use. These are not cpu-bound threads, hence the relative large number. The default should be appropriate for most use cases. |\n\nFor tuning options, see the\nTuning guide\n.\n\n## Snapshot and expiration policies‚Äã\n\nWAL files are typically read by replicas shortly after upload. To optimize\ncosts, move files to cooler storage tiers after 1-7 days.\nRecommendations:\n- Take snapshots every 1-7 days\n- Keep WAL files for at least 30 days\n- Ensure snapshot interval is shorter than WAL expiration\nExample: Weekly snapshots + 30-day WAL retention = ability to restore up to 23\ndays back. Daily snapshots restore faster but use more storage.\n\n## Disaster recovery‚Äã\n\n\n### Failure scenarios‚Äã\n\n\n| Node | Recoverable | Unrecoverable |\n| --- | --- | --- |\n| Primary | Restart | Promote replica, create new replica |\n| Replica | Restart | Destroy and recreate |\n\n\n### Network partitions‚Äã\n\nTemporary partitions cause replicas to lag, then catch up when connectivity\nrestores. This is normal operation.\nPermanent partitions require\nemergency primary migration\n.\n\n### Instance crashes‚Äã\n\nIf a crash corrupts transactions, tables may suspend on restart. You can skip\nthe corrupted transaction and reload missing data, or follow the emergency\nmigration flow.\n\n### Disk failures‚Äã\n\nSymptoms: high latency, unmounted disk, suspended tables. Follow the emergency\nmigration flow to move to new storage.\n\n## Migration procedures‚Äã\n\n\n### Planned primary migration‚Äã\n\nUse when the current primary is healthy but you want to switch to a new one.\n- Stop the primary\n- Restart withreplication.role=primary-catchup-uploads\n- Wait for uploads to complete (exits with code 0)\n- Follow emergency migration steps below\n\n### Emergency primary migration‚Äã\n\nUse when the primary has failed.\n- Stop the failed primary (ensure it cannot restart)\n- Stop the replica\n- Setreplication.role=primaryon the replica\n- Create an empty_migrate_primaryfile in the installation directory\n- Start the replica (now the new primary)\n- Create a new replica to replace the promoted one\nwarning\nData committed to the primary but not yet replicated will be lost. Use planned\nmigration if the primary is still functional.\n\n### Point-in-time recovery‚Äã\n\nRestore the database to a specific historical timestamp.\n- Locate a snapshot from before your target timestamp\n- Create a new instance from the snapshot (do not start it)\n- Create a_recover_point_in_timefile containing:replication.object.store=<source object store>replication.recovery.timestamp=YYYY-MM-DDThh:mm:ss.mmmZ\n- If using a snapshot, create a_restorefile to trigger recovery\n- Optionally configureserver.confto replicate to anewobject store\n- Start the instance\n\n## Next steps‚Äã\n\n- Tuning guide- Optimize replication\nperformance",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1616,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-c8f4b08f6458",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/troubleshooting/error-codes",
    "title": "List of  QuestDB Error Codes | QuestDB",
    "text": "Error codes may appear during start-up if an instance is misconfigured.\nWhen a primary instance is running, QuestDB checks that other primary instances are not running.\nIt does so by keeping a rolling ID locally and in the object store in sync.\nIf these two IDs are out of sync, the primary instance will raise an error.\nFor additional information, refer to the\nreplication overview\nand\nreplication setup guide\n, especially its \"Disaster Recovery\" section.\n\n## ER001‚Äã\n\nThis code indicates that a point in time recovery completed successfully.\nIt confirms that the database is configured with\nreplication.role=primary\n, and that the associated object store is not empty.\nHowever, the configured location may contain WAL data from a different replication \"timeline\".\nAs such, this error is raised to prevent an overwrite of the existing data.\nTo resolve, shut down the database and reconfigure the\nreplication.object.store\nto point to a new empty location.\nOnce restarted, if the old location is no longer needed, you can delete it.\n\n## ER002‚Äã\n\nThe database cannot read or write its local copy of the replication sync ID stored in the\n_replication_sync_id.d\nfile.\nIf you recently recovered the database from a backup, check that the file permissions of the restored directory (and its contents recursively) are readable and writable.\nIf the error indicates a \"Could not read\" error, perform a primary migration to recreate it.\nTo do so, place an empty\n_migrate_primary\nfile into your databases installation directory - for example, the parent of\nconf\nand\ndb\ndirectories.\nThis will trigger the database instance to resync with the latest state in the object store and restart as primary.\n\n## ER003‚Äã\n\nWhen you create a replica from a snapshot that is too old, this error may occur.\nThe workflow to enable replication on the primary instance and create replicas is:\n- Reconfigure the primary instance withreplication.role=primaryand configure itsreplication.object.storeto point to the object store and start it.\n- While running, snapshot the primary instance and copy the snapshot and restore it on the replica instance.\n- Reconfigure the replica instance withreplication.role=replicaand ensure itsreplication.object.storepoints to the same object store as the primary. Also, set a new and unique value to thecairo.snapshot.instance.idconfiguration.\n- Start the replica instance.\nSee the\ncheckpointing\npage for more details\non how to create and restore snapshots.\n\n## ER004‚Äã\n\nThis error is very similar to ER003.\nIt indicates that the transactions in the object store and the replica are out of sync.\nThis can happen if you created the replica from a database that replicated on a different timeline or from a database unrelated to the primary instance.\nTo resolve this, recreate the replica using a recent primary instance snapshot.\nUse a snapshot you created after enabling replication on the primary instance.\nSee the workflow in ER003 for detailed steps.\n\n## ER005‚Äã\n\nA primary instance started which is not in sync with the configured object store.\nVerify the\nreplication.object.store\nconfiguration and ensure that the object store is not in use by another primary instance.\nAlternatively, you might have migrated the primary role to a different\ninstance which has committed more transactions than the current instance.\nIf you are certain that the\nreplication.object.store\nconfiguration is correct and that the object store is not in use by another primary instance, perform\na primary migration.\nTo do so, place an empty\n_migrate_primary\nfile in your database installation directory, the parent of\nconf\nand\ndb\ndirectories.\nThis will update the primary instance to the latest state from the object\nstore and have it take over as the new primary instance.\n\n## ER006‚Äã\n\nThis error occurs when you start a primary instance and discover another instance is already acting as the primary.\nThis typically happens after an emergency primary migration, usually due to a network partition that separated your database instances.\nBefore proceeding, check your infrastructure to determine exactly how many primary instances are currently running.\nYou have the following options:\n- Destroy the extra instance\n- Reconfigure it asreplication.role=replicaand restart it\n- Perform a planned primary migration and resume the primary role on this instance\n\n## ER007‚Äã\n\nThis error indicates a Data ID mismatch between the local database and the backup or replication object store.\nEach QuestDB database has a unique Data ID (stored in\n<install_root>/db/.data_id\n) that identifies it for backup and replication purposes. This error occurs when:\n- Attempting to back up to an object store that contains backups from a different database instance\n- A replication object store contains data from a different primary instance\nTo resolve:\n- For backup creation: Verify thebackup.object.storepoints to the correct location for this database instance. If intentionally backing up to a new location, the location must be empty.\n- For replication: Verify thereplication.object.storeconfiguration matches the primary instance that owns the data.\nNote: Restore operations check for an empty database separately. If the target\ndatabase already has a Data ID, restore fails with: \"The local database is not\nempty. It already has an associated data ID.\"\n\n### Starting fresh with a new Data ID‚Äã\n\nA Data ID is automatically generated when QuestDB first starts with an empty\ndatabase. To intentionally start fresh:\nRecommended\n: Create a new, empty database directory and configure QuestDB\nto use it.\nAlternative\n: Delete the existing\n.data_id\nfile (stop QuestDB first):\n\n```bash\n# Stop QuestDB first!rm <install_root>/db/.data_id# Restart QuestDB - a new Data ID will be generated\n```\n\nwarning\nChanging the Data ID on a database with existing data will:\n- Break any existing replication configuration\n- Make existing backups incompatible for restore\n- Cause ER007 errors when connecting to the original object store\nSee the\nBackup and Restore guide\nfor more information.\n\n# Operating system error codes\n\nRefer to the\nOS error codes\npage for any\nfile or network related errors that QuestDB may raise.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 958,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-c67682aad4a8",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/checkpoint",
    "title": "CHECKPOINT keyword | QuestDB",
    "text": "Checkpoint SQL toggles the database into and out of \"checkpoint mode\". In this\nmode the databases file system can be safely backed up using external tools,\nsuch as disk snapshots or copy utilities.\nLooking for a detailed guide backup creation and restoration? Check out ourBackup and Restoreguide!\ncaution\nQuestDB currently does not support creating checkpoints on Windows.\nIf you are a Windows user and require backup functionality, please\ncomment on this issue\n.\n\n## CHECKPOINT syntax‚Äã\n\n\n## CHECKPOINT overview‚Äã\n\nTo enable online backups, data in QuestDB is mutated via either file append or\nvia copy-on-write. Checkpoint leverages these storage methods to achieve\nreliable and consistent restorations from your database backups.\n\n### What happens during CHECKPOINT CREATE?‚Äã\n\nWhen initiatied,\nCHECKPOINT CREATE\n:\n- Disables background jobs that housekeep stale files and data blocks\n- Takes snapshot of table transactions across the whole database (all tables)\n- Creates a new on-disk data structure that captures append offsets and versions\nof files that represent data for the above transactions. Typically this data\nis stored in the/var/lib/questdb/.checkpointdirectory.Do not alter contents of this directory manually!\n- Callssync()to\nsynchronously flush filesystem caches to disk\n\n### What happens after a checkpoint has been created?‚Äã\n\nOnce a checkpoint is created, QuestDB continues taking in writes. However, it\nwill consume more disk space. How much more depends on the shape of the data\nthat is being written. Data that is written via the append method will yeild\nalmost no additional disk space consumption other that of the data itself. In\ncontrast, the copy-on-write method will make data copies, which are usually\ncopies of non-recent table partitions. This will lead to an increase in disk\nspace consumption.\nIt is strongly recommended that you minimize the time database is in\ncheckpoint mode and monitor the free disk space closely. The recommended way to\nachive this is to utilize file system SNAPSHOTS as described inour backup and restore guide.\nAlso note that QuestDB can only enter checkpoint mode once. After that period of\ntime, the next checkpoint operation must be to exit checkpoint mode. Attempts to\ncreate a new checkpoint when once exists will fail with the appropriate message.\nWhen in checkpoint mode, you can safely access the file system to take your\nsnapshot.\n\n### What happens after my snapshot is complete?‚Äã\n\nAfter your snapshot is complete, checkpoint mode must be exited via the\nCHECKPOINT RELEASE\nSQL. Once executed, QuestDB will reinstate the usual\nhousekeeping and reclaim disk space.\nThe database restore is preformed semi-automatically on the database startup.\nThis is done deliberately to avoid the restore procedure running accidentally on\nthe source database instance. The database will attempt a restore when empty an\nfile, typically\n/var/lib/questdb/_restore\nis present.\nThe restore procedure will use\n/var/lib/questdb/.checkpoint\nto adjust the\ndatabase files and remove extra data copies. After the restore is successful the\ndatabase is avaialble as normal with no extra intervantion required.\n\n## CHECKPOINT examples‚Äã\n\nTo enter checkpoint mode:\n\n```sql\nCHECKPOINT CREATE\n```\n\nTo exit checkpoint mode:\n\n```sql\nCHECKPOINT RELEASE\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 498,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-c98add0f7b7e",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/ingestion/import-csv",
    "title": "CSV Import | QuestDB",
    "text": "tip\nCSV import is for bulk/batch loading. For streaming data, use\nInfluxDB Line Protocol (ILP)\ninstead.\nThere are three methods for CSV import:\n- COPY SQL- Best for large files and migrations\n- REST API- For programmatic uploads of smaller files\n- Web Console- Interactive uploads via browser\n\n## Import CSV via COPY SQL‚Äã\n\nThe\nCOPY\nSQL command is the preferred way to import\nlarge CSV files into partitioned tables. Use it for bulk data migrations from\nother databases.\nFor partitioned tables, the best\nCOPY\nperformance can be achieved only on a\nmachine with a local, physically attached SSD. It is possible to use a network\nblock storage, such as an AWS EBS volume to perform the operation, with the\nfollowing impact:\n- Users need to configure the maximum IOPS and throughput setting values for the\nvolume.\n- The required import time is likely to be 5-10x longer.\n\n### Prepare the import‚Äã\n\nPreparation is key. Import is a multi-step process, which consists of:\n- Export the existing database as CSV files\n- Enable and configureCOPYcommand to be optimal for the system\n- Prepare target schema in QuestDB\n\n#### Export the existing database‚Äã\n\nExport data using one CSV file per table. Include a column that can be used as\nthe designated timestamp. Data in CSV is not expected to be in any particular\norder. If it is not possible to export the table as one CSV, export multiple\nfiles and concatenate these files before importing into QuestDB.\n\n##### Concatenate multiple CSV files‚Äã\n\nThe way to concatenate files depends on whether the CSV files have headers.\nFor CSV files without headers, concatenation is straightforward:\n- Linux\n- macOS\n- Windows PowerShell\n\n```shell\nls *.csv | xargs cat > singleFile.csv\n```\n\n\n```shell\nls *.csv | xargs cat > singleFile.csv\n```\n\n\n```shell\n$TextFiles = Get-Item C:\\Users\\path\\to\\csv\\*.csv# The files are moved to the same folder.$TextFiles foreach { Add-Content -Value $(Get-Content $_) -Path C:\\Users\\path\\to\\csv\\singleFile.csv}\n```\n\nFor CSV files with headers, concatenation can be tricky. You could manually\nremove the first line of the files before concatenating, or use some smart\ncommand line to concatenate and remove the headers. A good alternative is using\nthe open source tool\ncsvstack\n.\nThis is how you can concatenate multiple CSV files using\ncsvstack\n:\n\n```shell\ncsvstack *.csv > singleFile.csv\n```\n\n\n#### Things to know aboutCOPY‚Äã\n\n- COPYis disabled by default, as a security precaution. Configuration is\nrequired.\n- COPYis more efficient when source and target disks are different.\n- COPYis parallel when target table is partitioned.\n- COPYisserialwhen target table is non-partitioned. Out-of-order\ntimestamps are rejected.\n- COPYcannot import data into non-empty table.\n- COPYindexes CSV file; reading indexed CSV file benefits hugely from disk\nIOPS. We recommend using NVME.\n- COPYimports one file at a time; there is no internal queuing system yet.\n- COPY reference\n\n#### ConfigureCOPY‚Äã\n\n- EnableCOPYandconfiguretheCOPYdirectories to suit your server.\n- cairo.sql.copy.rootmust be set forCOPYto work.\n\n### Create the target table schema‚Äã\n\nIf you know the target table schema already, you can\nskip this section\n.\nQuestDB can analyze the input file and infer the schema. This happens\nautomatically when the target table does not exist.\nTo have QuestDB help with determining file schema, it is best to work with a\nsub-set of CSV. A smaller file allows us to iterate faster if iteration is\nrequired.\nLet's assume we have the following CSV:\n\n```csv\n\"locationId\",\"timestamp\",\"windDir\",\"windSpeed\",\"windGust\",\"cloudCeiling\",\"skyCover\",\"visMiles\",\"tempF\",\"dewpF\",\"rain1H\",\"rain6H\",\"rain24H\",\"snowDepth\"1,\"2010-07-05T00:23:58.981263Z\",3050,442,512,,\"OBS\",11.774906006761,-5,-31,58.228032196984,70.471606345673,77.938252342637,582,\"2017-10-10T10:13:55.246046Z\",900,63,428,5487,\"BKN\",4.958601701089,-19,-7,4.328016420894,36.020659549374,97.821114441800,413,\"2010-03-12T11:17:13.727137Z\",2880,299,889,371,\"BKN\",10.342717709226,46,81,9.149518425127,20.229637391479,20.074738007931,804,\"2018-08-21T15:42:23.107543Z\",930,457,695,4540,\"OBS\",13.359184086767,90,-47,33.346163208862,37.501996055160,58.316836760009,13...\n```\n\n- Extract the first 1000 line totest_file.csv(assuming both files are in\nthecairo.sql.copy.rootdirectory):\n\n```shell\nhead -1000 weather.csv > test_file.csv\n```\n\n- Use a simpleCOPYcommand to importtest_file.csvand define the table\nname:COPY weather from 'test_file.csv' WITH HEADER true;\nThis creates the\nweather\ntable and returns the ID of the background import\nprocess:\n\n| id |\n| --- |\n| 5179978a6d7a1772 |\n\n- In theWeb Consoleright click table and selectCopy Schema to Clipboard- this copies the schema generated by the input\nfile analysis.\n- Paste the table schema to the code editor:CREATE TABLE 'weather' (timestamp TIMESTAMP,windDir INT,windSpeed INT,windGust INT,cloudCeiling INT,skyCover VARCHAR,visMiles DOUBLE,tempF INT,dewpF INT,rain1H DOUBLE,rain6H DOUBLE,rain24H DOUBLE,snowDepth INT);\n- Identify the correct schema:5.1. The generated schema may not be completely correct. Check the log table\nand log file to resolve common errors using the id (see alsoTrack import progressandFAQ):SELECT * FROM sys.text_import_log WHERE id = '5179978a6d7a1772' ORDER BY ts DESC;\n\n| ts | id | table | file | phase | status | message | rows_handled | rows_imported | errors |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2022-08-08T16:38:06.262706Z | 5179978a6d7a1772 | weather | test_file.csvtest_file.csv |  | finished |  | 999 | 999 | 0 |\n| 2022-08-08T16:38:06.226162Z | 5179978a6d7a1772 | weather | test_file.csvtest_file.csv |  | started |  |  |  | 0 |\n\nCheck\nrows_handled\n,\nrows_imported\n, and\nmessage\nfor any errors and amend\nthe schema as required.\n5.2. Drop the table and re-import\ntest_file.csv\nusing the updated schema.\n- Repeat the steps to narrow down to a correct schema.The process may require either truncating:TRUNCATE TABLE table_name;or dropping the target table:DROP TABLE table_name;\n- Clean up: Once all the errors are resolved, copy the final schema, drop the\nsmall table.\n- Make sure table is correctly partitioned. The final schema in our example\nshould look like this:CREATE TABLE 'weather' (timestamp TIMESTAMP,windDir INT,windSpeed INT,windGust INT,cloudCeiling INT,skyCover VARCHAR,visMiles DOUBLE,tempF INT,dewpF INT,rain1H DOUBLE,rain6H DOUBLE,rain24H DOUBLE,snowDepth INT) TIMESTAMP (timestamp) partition by DAY;\n- Ready for import: Create an empty table using the final schema.\n\n### Import CSV‚Äã\n\nOnce an empty table is created in QuestDB using the correct schema, import can\nbe initiated with:\n\n```questdb-sql\nCOPY weather FROM 'weather.csv' WITH HEADER true TIMESTAMP 'timestamp' FORMAT 'yyyy-MM-ddTHH:mm:ss.SSSUUUZ';\n```\n\nIt quickly returns id of asynchronous import process running in the background:\n\n| id |\n| --- |\n| 55020329020b446a |\n\n\n### Track import progress‚Äã\n\nCOPY\nreturns an id for querying the log table (\nsys.text_import_log\n), to\nmonitor the progress of ongoing import:\n\n```questdb-sql\nSELECT * FROM sys.text_import_log WHERE id = '55020329020b446a';\n```\n\n\n| ts | id | table | file | phase | status | message | rows_handled | rows_imported | errors |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2022-08-03T14:00:40.907224Z | 55020329020b446a | weather | weather.csv | null | started | null | null | null | 0 |\n| 2022-08-03T14:00:40.910709Z | 55020329020b446a | weather | weather.csv | analyze_file_structure | started | null | null | null | 0 |\n| 2022-08-03T14:00:42.370563Z | 55020329020b446a | weather | weather.csv | analyze_file_structure | finished | null | null | null | 0 |\n| 2022-08-03T14:00:42.370793Z | 55020329020b446a | weather | weather.csv | boundary_check | started | null | null | null | 0 |\n\nLooking at the log from the newest to the oldest might be more convenient:\n\n```questdb-sql\nSELECT * FROM sys.text_import_log WHERE id = '55020329020b446a' ORDER BY ts DESC;\n```\n\nOnce import successfully ends the log table should contain a row with a 'null'\nphase and 'finished' status :\n\n| ts | id | table | file | phase | status | message | rows_handled | rows_imported | errors |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2022-08-03T14:10:59.198672Z | 55020329020b446a | weather | weather.csv | null | finished |  | 300000000 | 300000000 | 0 |\n\nImport into non-partitioned tables uses single-threaded implementation (serial\nimport) that reports only start and finish records in the status table. Given an\nordered CSV file\nweather1mil.csv\n, when importing, the log table shows:\n\n| ts | id | table | file | phase | status | message | rows_handled | rows_imported | errors |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2022-08-03T15:00:40.907224Z | 42d31603842f771a | weather | weather1mil.csv | null | started | null | null | null | 0 |\n| 2022-08-03T15:01:20.000709Z | 42d31603842f771a | weather | weather1mil.csv | null | finished | null | 999999 | 999999 | 0 |\n\nThe log table contains only coarse-grained, top-level data. Import phase run\ntimes vary a lot (e.g.\npartition_import\noften takes 80% of the whole import\nexecution time), and therefore\nthe server log\nprovides an\nalternative to follow more details of import:\nimport log\n\n```log\n2022-08-03T14:00:40.907224Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=analyze_file_structure, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]2022-08-03T14:00:40.917224Z I i.q.c.p.WriterPool >> [table=`weather`, thread=43]2022-08-03T14:00:41.440049Z I i.q.c.t.ParallelCsvFileImporter finished [importId=5502031634e923b2, phase=analyze_file_structure, file=`C:\\dev\\tmp\\weather.csv`, duration=0s, errors=0]2022-08-03T14:00:41.440196Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=boundary_check, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]2022-08-03T14:01:18.853212Z I i.q.c.t.ParallelCsvFileImporter finished [importId=5502031634e923b2, phase=boundary_check, file=`C:\\dev\\tmp\\weather.csv`, duration=6s, errors=0]2022-08-03T14:01:18.853303Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=indexing, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]2022-08-03T14:01:18.853516Z I i.q.c.t.ParallelCsvFileImporter temporary import directory [path='E:\\dev\\tmp\\weather\\]2022-08-03T14:01:42.612302Z I i.q.c.t.CsvFileIndexer finished chunk [chunkLo=23099021813, chunkHi=26948858785, lines=29999792, errors=0]2022-08-03T14:01:42.791789Z I i.q.c.t.CsvFileIndexer finished chunk [chunkLo=11549510915, chunkHi=15399347885, lines=30000011, errors=0]\n```\n\nIf the\nON ERRORoption\nis set to\nABORT\n,\nimport stops on the first error and the error is logged. Otherwise, all errors\nare listed in the log.\nThe reference to the error varies depending on the phase of an import:\n- In the indexing phase, if an error occurs, the absolute input file line is\nreferenced:\n\n```log\n2022-08-08T11:50:24.319675Z E i.q.c.t.CsvFileIndexer could not parse timestamp [line=999986, column=1]\n```\n\n- In the data import phase, if an error occurs, the log references the offset as\nrelated to the start of the file.\n\n```log\n2022-08-08T12:19:56.828792Z E i.q.c.t.TextImportTask type syntax [type=INT, offset=5823, column=0, value='CMP2']\n```\n\nThe errored rows can then be extracted for further investigation.\n\n### FAQ‚Äã\n\nCOPY on a table with symbol columns is very slow. How can I speed it up?QuestDB uses256as the default symbol capacity. If the number of distinct\nsymbol values exceeds this default significantly, theCOPYperformance will\nsuffer. Make sure that you specify symbol capacities when creating the table\nbefore running theCOPYcommand.Here is an example:CREATE TABLE table_name (ts TIMESTAMP,sym SYMBOL CAPACITY 100000) TIMESTAMP(ts) PARTITION BY DAY;Refer to thesymbol type documentationfor more information on configuring the symbol capacity.\nWhat happens in a database crash or OS reboot?If reboot/power loss happens while partitions are being attached, the table\nmay be left with incomplete data. Truncate the table before re-importing\nwith:TRUNCATE TABLE table_name;If reboot/power loss happens before any partitions being attached, the import\nshould not be affected.\nI'm getting \"COPY is disabled ['cairo.sql.copy.root' is not set?]\" error messagePlease setcairo.sql.copy.rootsetting, restart the instance and try again.\nI'm getting \"could not create temporary import work directory [path='somepath', errno=-1]\" error messagePlease make sure that thecairo.sql.copy.rootandcairo.sql.copy.work.rootare valid paths pointing to existing directories.\nI'm getting \"[2] could not open read-only [file=somepath]\" error messagePlease check that import file path is valid and accessible to QuestDB instance\nusers.If you are running QuestDB using Docker, please check if the directory mounted\nfor storing source CSV files is identical to the onecairo.sql.copy.rootproperty orQDB_CAIRO_SQL_COPY_ROOTenvironment variable points to.For example, the following command can start a QuestDB instance:docker run -p 9000:9000 \\-v \"/tmp/questdb:/var/lib/questdb\" \\-v \"/tmp/questdb/my_input_root:/tmp/questdb_import\" \\-e QDB_CAIRO_SQL_COPY_ROOT=/tmp/questdb_wrong \\questdb/questdbHowever, running:COPY weather from 'weather_example.csv' WITH HEADER true;Results in the \"[2] could not open read-only\n[file=/tmp/questdb_wrong/weather_example.csv]\" error message.\nI'm getting \"column count mismatch [textColumnCount=4, tableColumnCount=3, table=someTable]\" error messageThere are more columns in input file than in the existing target table. Please\nremove column(s) from input file or add them to the target table schema.\nI'm getting \"timestamp column 'ts2' not found in file header\" error messageEither input file is missing header or timestamp column name given inCOPYcommand is invalid. Please add file header or fix timestamp option.\nI'm getting \"column is not a timestamp [no=0, name='ts']\" error messageTimestamp column given by the user or (if header is missing) assumed based on\ntarget table schema is of a different type. Please check timestamp column name\nin input file header or make sure input file column order matches that of target\ntable.\nI'm getting \"target table must be empty [table=t]\" error messageCOPYdoesn't yet support importing into partitioned table with existing data.Please truncate table before re-importing with:TRUNCATE TABLE table_name;or import into another empty table and then useINSERT INTO SELECT:INSERT BATCH 100000 INTO table_nameSELECT * FROM other_table;to copy data into original target table.\nI'm getting \"io_uring error\" error messageIt's possible that you've hit a IO_URING-related kernel error. Please setcairo.iouring.enabledsetting to false, restart QuestDB instance, and try\nagain.\nI'm getting \"name is reserved\" error messageThe table you're trying to import into is in a bad state (incomplete metadata).Please either drop the table with:DROP TABLE table_name;and recreate the table or change the table name in theCOPYcommand.\nI'm getting \"Unable to process the import request. Another import request may be in progress.\" error messageOnly one import can be running at a time.Either cancel running import with:COPY 'paste_import_id_here' CANCEL;or wait until the current import is finished.\nImport finished but table is (almost) emptyPlease check the latest entries in log table:SELECT * FROM sys.text_import_log LIMIT -10;If \"errors\" column is close to number of records in the input file then it may\nmean:FORMAToption ofCOPYcommand or auto-detected format doesn't match\ntimestamp column data in fileOther column(s) can't be parsed andON ERROR SKIP_ROWoption was usedInput file is unordered and target table has designated timestamp but is not\npartitionedIf none of the above causes the error, please check the log file for messages\nlike:2022-08-08T11:50:24.319675Z E i.q.c.t.CsvFileIndexer could not parse timestamp [line=999986, column=1]or2022-08-08T12:19:56.828792Z E i.q.c.t.TextImportTask type syntax [type=INT, offset=5823, column=0, value='CMP2']that should explain why rows were rejected. Note that in these examples, the\nformer log message mentions the absolute input file line while the latter is\nreferencing the offset as related to the start of the file.\nImport finished but table column names aref0,f1, ...The input file has no header and the target table does not exist, so columns\nreceived synthetic names. You can rename them withALTER TABLE:ALTER TABLE table_name RENAME COLUMN f0 TO ts;\n\n## Import CSV via Rest‚Äã\n\nThe REST API provides an\n/imp\nendpoint exposed on port\n9000\nby default. This\nendpoint allows streaming tabular text data directly into a table, supporting\nCSV, TAB and pipe (\n|\n) delimited inputs with optional headers. Data types and\nstructures are detected automatically, but additional configurations can be\nprovided to improve automatic detection.\nnote\nThe REST API is better suited when the following conditions are true:\n- Regular uploads of small batches of data into the same table.\n- The file batches do not contain overlapping periods (they contain distinct\ndays/weeks/months). Otherwise, the import performance will be impacted.\nFor database migrations, or uploading one large CSV file into QuestDB, users may\nconsider using the\nCOPY\nSQL command. See\nCOPY command documentation\nand\nGuide on CSV import\nfor more\ndetails.\n\n### Importing compressed files‚Äã\n\nIt is possible to upload compressed files directly without decompression:\n\n```bash\ngzip -cd compressed_data.tsv.gz | curl -v -F data=@- 'http://localhost:9000/imp'\n```\n\nThe\ndata=@-\nvalue instructs\ncurl\nto read the file contents from\nstdin\n.\n\n### Specifying a schema during CSV import‚Äã\n\nA\nschema\nJSON object can be provided with POST requests to\n/imp\nwhile\ncreating tables via CSV import. This allows for more control over user-defined\npatterns for timestamps, or for explicitly setting types during column-creation.\nThe following example demonstrates basic usage, in this case, that the\nticker_name\ncolumn should be parsed as\nSYMBOL\ntype instead of\nVARCHAR\n:\n\n```bash\ncurl \\  -F schema='[{\"name\":\"ticker_name\", \"type\": \"SYMBOL\"}]' \\  -F data=@trades.csv 'http://localhost:9000/imp'\n```\n\nIf a timestamp column (\nts\n) in this CSV file has a custom or non-standard\ntimestamp format, this may be included with the call as follows:\n\n```bash\ncurl \\  -F schema='[ \\    {\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}, \\    {\"name\":\"ticker_name\", \"type\": \"SYMBOL\"} \\  ]' \\  -F data=@trades.csv 'http://localhost:9000/imp'\n```\n\nFor\nnanosecond-precision\ntimestamps such as\n2021-06-22T12:08:41.077338934Z\n, a pattern can be provided in the following\nway:\n\n```bash\ncurl \\  -F schema='[ \\    {\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUNNNZ\"} \\  ]' \\  -F data=@my_file.csv \\  http://localhost:9000/imp\n```\n\nMore information on the patterns for timestamps can be found on the\ndate and time functions\npage.\nnote\nThe\nschema\nobject must precede the\ndata\nobject in calls to this REST\nendpoint. For example:\n\n```bash\n# correct ordercurl -F schema='{my_schema_obj}' -F data=@my_file.csv http://localhost:9000/imp# incorrect ordercurl -F data=@my_file.csv -F schema='{my_schema_obj}' http://localhost:9000/imp\n```\n\n\n### Text loader configuration‚Äã\n\nQuestDB uses a\ntext_loader.json\nconfiguration file which can be placed in the\nserver's\nconf\ndirectory. This file does not exist by default, but has the\nfollowing implicit settings:\nconf/text_loader.json\n\n```json\n{  \"date\": [    {      \"format\": \"dd/MM/y\"    },    {      \"format\": \"yyyy-MM-dd HH:mm:ss\"    },    {      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSz\",      \"locale\": \"en-US\",      \"utf8\": false    },    {      \"format\": \"MM/dd/y\"    }  ],  \"timestamp\": [    {      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUz\",      \"utf8\": false    }  ]}\n```\n\n\n#### Example‚Äã\n\nGiven a CSV file which contains timestamps in the format\nyyyy-MM-dd - HH:mm:ss.SSSUUU\n, the following text loader configuration will\nprovide the correct timestamp parsing:\nconf/text_loader.json\n\n```json\n{  \"date\": [    {      \"format\": \"dd/MM/y\"    },    {      \"format\": \"yyyy-MM-dd HH:mm:ss\"    },    {      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSz\",      \"locale\": \"en-US\",      \"utf8\": false    },    {      \"format\": \"MM/dd/y\"    }  ],  \"timestamp\": [    {      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUz\",      \"utf8\": false    },    {      \"format\": \"yyyy-MM-dd - HH:mm:ss.SSSUUU\",      \"utf8\": false    }  ]}\n```\n\nThe CSV data can then be loaded via POST request, for example, using cURL:\n\n```curl\ncurl -F data=@weather.csv 'http://localhost:9000/imp'\n```\n\nFor more information on the\n/imp\nentry point, refer to the\nREST API documentation\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2819,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-d991d1610102",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/getting-started/web-console/import-csv",
    "title": "Import CSV Using Web Console | QuestDB",
    "text": "The\nImport CSV\nfunctionality in the Web Console provides a user-friendly interface to upload and import CSV files into QuestDB. You can create new tables or append data to existing tables with automatic schema detection and flexible configuration options.\n\n## Accessing the Import Interface‚Äã\n\nYou can access the import tab by clicking the import icon in the left-side navigation menu of the Web Console.\n\n## Import Process‚Äã\n\n\n### Upload Queue‚Äã\n\nOnce a file is added to the upload queue, the following configurations will be displayed:\n\n### Configuration Options‚Äã\n\n- File: The file name, size, and import status\n- Table name: The name of the table to be created or updated. By default, this is the name of the imported file\n- Schema: The column name and data type. The schema is automatically detected but can be set manually\n- Write mode:Append: Uploaded data will be appended to the end of the tableOverwrite: Uploaded data will override existing data in the table\n- Actions:Settings: Additional configuration for the importUpload: Start the uploadX: Delete the file from the upload queue\n\n## Table Schema Configuration‚Äã\n\n\n### For Existing Tables‚Äã\n\nTo update the schema of an existing table, select\nOverwrite\nwrite mode to replace the existing rows and partition unit with data from the CSV file.\nFor an existing table, changing the table name allows you to import the data as a new separate table.\n\n### For New Tables‚Äã\n\nThe following settings are available for configuration:\n\n| Setting | Description |\n| --- | --- |\n| Partition | Change the partition setting of the table |\n| Designated timestamp | Selecting a designated timestamp. This is mandatory if the partition unit is notNONE |\n| Data type | Define the data type. For timestamp, the timestamp format is mandatory and there is the option to select the column as the designated timestamp |\n| Delete column | Clickxto delete the column from the table |\n| Add column | At the end of the column list, select \"Add column\" to insert a new column into the table |\n\nThe following table schema details are imported based on the CSV file:\n- The column order\n- The column name\n\n## Import Settings‚Äã\n\nThe Settings panel displays the following configurations:\n\n| Setting | Description | Default value |\n| --- | --- | --- |\n| Maximum number of uncommitted rows | The size of the commit batch. A commit will be issued when this number is reached in the buffer. This setting is the same ascairo.max.uncommitted.rows. To avoid running out of memory during an import, set this value based on the RAM size of the machine | 500000 |\n| Delimiter | The delimiter character to parse the CSV file | Automatic |\n| Atomicity | Error behavior. Rejected rows or columns will be reported in the Details panel after the import is completed | Skip column |\n| Force header | Whether to interpret the first line as the header. The result will be reported in the Details panel after the import is completed | FALSE |\n| Skip line extra values | Whether the parser should ignore extra values by skipping the entire line. An extra value is something in addition to what is defined by the header | FALSE |\n\n\n## Import Results and Details‚Äã\n\n\n### Status Display‚Äã\n\nThe import status is displayed in the file column. Once the action is completed, the number of rows inserted is displayed alongside the\nDetails\ntab:\n\n### Details Panel‚Äã\n\nThe\nDetails\npanel lists rejected rows and import errors for each column:\nThe details such as forced header, table name, and rejected rows are related to the import settings you defined. For example, setting Atomicity in Settings to \"Skip row\" will result in skipped rows being reported under Rejected rows after the import.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 638,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-99d4561cc4dc",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/create-table",
    "title": "CREATE TABLE reference | QuestDB",
    "text": "To create a new table in the database, the\nCREATE TABLE\nkeywords followed by\ncolumn definitions are used.\n\n## Syntax‚Äã\n\nTo create a table by manually entering parameters and settings:\nnote\nChecking table metadata can be done via the\ntables()\nand\ntable_columns()\nfunctions which are described in the\nmeta functions\ndocumentation page.\nTo create a table by cloning the metadata of an existing table:\n\n## Examples‚Äã\n\nThe following examples demonstrate creating tables from basic statements, and\nintroduces feature such as\npartitioning\n,\ndesignated timestamps and data deduplication. For more information on the\nconcepts introduced to below, see\n- designated timestampreference on\nelecting a timestamp column\n- partitiondocumentation which describes how\npartitions work in QuestDB\n- symbolreference for using thesymboldata type\n- data deduplicationreference on discarding\nduplicates.\nThis first iteration of our example creates a table with a designated timestamp\nand also applies a partitioning strategy,\nBY DAY\n:\nBasic example, partitioned by day\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAY;\n```\n\nNow we can add a time-to-live (TTL) period. Once an entire data partition is\npast its TTL, it becomes eligible for automatic removal.\nWith TTL\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYTTL 1 WEEK;\n```\n\nNext, we enable data deduplication. This will discard exact duplicates on the\ntimestamp and ticker columns:\nWith deduplication, adding ticker as an upsert key.\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYTTL 1 WEEKDEDUP UPSERT KEYS (timestamp, symbol);\n```\n\nFinally, we add additional parameters for our SYMBOL type:\nAdding parameters for symbol type\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL CAPACITY 256 NOCACHE,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYTTL 1 WEEKDEDUP UPSERT KEYS (timestamp, symbol);\n```\n\n\n## Write-Ahead Log (WAL) Settings‚Äã\n\nBy default, created tables are\nWrite-Ahead Log enabled\n. While we recommend\nWAL-enabled tables, it is still possible to create non-WAL-enabled tables.\nCREATE TABLE\n's\nglobal configuration setting\nallows you to\nalter the default behaviour via\ncairo.wal.enabled.default\n:\n- true: Creates a WAL table (default)\n- false: Creates a non-WAL table\nAnd on an individual basis, you can also use\nBYPASS WAL\n.\n\n## Designated timestamp‚Äã\n\nThe timestamp function allows for specifying which column (which must be of\ntimestamp\ntype) should be a designated timestamp for the table. For more\ninformation, see the\ndesignated timestamp\nreference.\nThe designated timestamp column\ncannot be changed\nafter the table has been\ncreated.\n\n## Partitioning‚Äã\n\nPARTITION BY\nallows for specifying the\npartitioning strategy\nfor the table. Tables created\nvia SQL are not partitioned by default (\nNONE\n) and tables can be partitioned by\none of the following:\n- NONE: the default when partition is not defined.\n- YEAR\n- MONTH\n- WEEK\n- DAY\n- HOUR\nThe partitioning strategy\ncannot be changed\nafter the table has been\ncreated.\n\n## Time To Live (TTL)‚Äã\n\nTo store and analyze only recent data, configure a time-to-live (TTL) period on\na table using the\nTTL\nclause, placing it right after\nPARTITION BY <unit>\n.\nYou can't set TTL on a non-partitioned table.\nFollow the\nTTL\nkeyword with a number and a time unit, one of:\n- HOURS\n- DAYS\n- WEEKS\n- MONTHS\n- YEARS\nTTL units fall into two categories:\n- Fixed time periods:HOURSDAYSWEEKS\n- Calendar-based periods:MONTHSYEARS\nFixed-time periods are always exact durations:\n1 WEEK\nis always 7 days.\nCalendar-based periods may vary in length:\n1 MONTH\nfrom January 15th goes to\nFebruary 15th and could be between 28 and 31 days.\nQuestDB accepts both singular and plural forms:\n- HOURorHOURS\n- DAYorDAYS\n- WEEKorWEEKS\n- MONTHorMONTHS\n- YEARorYEARS\nIt also supports shorthand notation:\n3H\nfor 3 hours,\n2M\nfor 2 months.\nnote\nQuestDB drops data that exceeded its TTL only a whole partition at a time. For\nthis reason, the TTL period must be a whole number multiple of the table's\npartition size.\nFor example:\n- If a table is partitioned byDAY, the TTL must be a whole number of days\n(24 HOURS,2 DAYSand3 MONTHSare all accepted)\n- If a table is partitioned byMONTH, the TTL must be in months or years.\nQuestDB won't accept theHOUR,DAY, orWEEKunits\nRefer to the\nsection on TTL in Concepts\nfor detailed\ninformation on the behavior of this feature.\n\n## Deduplication‚Äã\n\nWhen\nDeduplication\nis enabled, QuestDB only\ninserts rows that do not match the existing data. When you insert a row into a\ntable with deduplication enabled, QuestDB searches for existing rows with\nmatching values in all the columns specified with\nUPSERT KEYS\n. It replaces all\nsuch matching rows with the new row.\nDeduplication only works on\nWrite-Ahead Log (WAL)\ntables.\nYou can include multiple columns of different types in the\nUPSERT KEYS\nlist.\nHowever, there are a few limitations to keep in mind:\n- You must include the designated timestamp column\n- You cannot use anARRAYcolumn\nYou can change the deduplication configuration at any time using\nALTER TABLE\n:\n- Enable deduplication and changeUPSERT KEYSwithALTER TABLE ENABLE\n- Disable deduplication with usingALTER TABLE DISABLE\n\n### Examples‚Äã\n\nCreating a table for tracking ticker prices with daily partitions and upsert deduplication\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYDEDUP UPSERT KEYS (timestamp, symbol);\n```\n\nEnabling dedup on an existing table, for timestamp and ticker columns\n\n```questdb-sql\nALTER TABLE trades DEDUP ENABLE UPSERT KEYS (timestamp, symbol);\n```\n\nDisabling dedup on the entire table\n\n```questdb-sql\nALTER TABLE trades DEDUP DISABLE;\n```\n\nChecking whether a table has dedup enabled\n\n```questdb-sql\nSELECT dedup FROM tables() WHERE table_name = '<the table name>';\n```\n\nChecking whether a column has dedup enabled\n\n```questdb-sql\nSELECT `column`, upsertKey FROM table_columns('<the table name>');\n```\n\n\n## IF NOT EXISTS‚Äã\n\nAn optional\nIF NOT EXISTS\nclause may be added directly after the\nCREATE TABLE\nkeywords to indicate that a new table should be created if one\nwith the desired table name does not already exist.\n\n```questdb-sql\nCREATE TABLE IF NOT EXISTS trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAY;\n```\n\n\n## Table name‚Äã\n\nInternally the table name is used as a directory name on the file system. It can\ncontain both ASCII and Unicode characters. The table name\nmust be unique\nand\nan error is returned if a table already exists with the requested name.\nValidation rules:\n- Length: subject to filesystem limits (typically ‚â§255).\n- Spaces:notallowed at the start or end.\n- Period.: only asingledot is allowednotat the start or end andnotnext to another dot.\n- Disallowed characters:?,,,',\",\\,/,:,),(,+,*,%,~,\\u0000,\\u0001,\\u0002,\\u0003,\\u0004,\\u0005,\\u0006,\\u0007,\\u0008,\\t,\\u000B,\\u000c,\\r,\\n,\\u000e,\\u000f,\\u007f,0xfeff(UTF-8 BOM).\nSome clients may have trouble parsing table names that contain unusual characters, even if those names are valid in\nQuestDB. For best results, we recommend using only alphanumeric characters along with\n-\n,\n_\n, or\n.\n.\nIn addition, table names are case insensitive:\nexample\n,\nexAmPlE\n,\nEXAMplE\nand\nEXAMPLE\nare all treated the same. Table names containing spaces or period\n.\ncharacter must be enclosed in\ndouble quotes\n, for example:\n\n```questdb-sql\nCREATE TABLE \"example out of.space\" (a INT);INSERT INTO \"example out of.space\" VALUES (1);\n```\n\n\n## Column name‚Äã\n\nAs with table names, the column name is used for file names internally. Although\nit does support both ASCII and Unicode characters, character restrictions\nspecific to the file system still apply.\nTables may have up to\n2,147,483,647\ncolumns. Column names are also case\ninsensitive. For example:\nexample\n,\nexAmPlE\n,\nEXAMplE\nand\nEXAMPLE\nare all\ntreated the same. However, column names\nmust be\nunique within each table and\nmust not contain\na period\n.\ncharacter.\nValidation rules:\n- Length: subject to filesystem limits (typically ‚â§255).Period.: not allowed.Hyphen-: not allowed.Other disallowed characters:?,.,,,',\",\\,/,:,),(,+,-,*,%,~,\\u0000,\\u0001,\\u0002,\\u0003,\\u0004,\\u0005,\\u0006,\\u0007,\\u0008,\\t,\\u000B,\\u000c,\\n,\\r,\\u000e,\\u000f,\\u007f,0xfeff(UTF-8 BOM).\nSome clients may have trouble parsing column names that contain unusual characters, even if those names are valid in\nQuestDB. For best results, we recommend using only alphanumeric characters along with\n-\n, or\n_\n.\n\n## Type definition‚Äã\n\nWhen specifying a column, a name and\ntype definition\nmust be provided. The\nsymbol\ntype may have additional optional parameters applied.\n\n### Symbols‚Äã\n\nOptional keywords and parameters may follow the\nsymbol\ntype which allow for\nfurther optimization on the handling of this type. For more information on the\nbenefits of using this type, see the\nsymbol\noverview.\n\n#### Symbol capacity‚Äã\n\nCAPACITY\nis an optional keyword used when defining a symbol type on table\ncreation to indicate how many distinct values this column is expected to have.\nWhen\ndistinctValueEstimate\nis not explicitly specified, a default value of\ncairo.default.symbol.capacity\nis used.\ndistinctValueEstimate\n- the value used to size data structures for\nsymbols\n.\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL CAPACITY 50,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAY;\n```\n\n\n#### Symbol caching‚Äã\n\nCACHE | NOCACHE\nis used to specify whether a symbol should be cached. The\ndefault value is\nCACHE\nunless otherwise specified.\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL CAPACITY 50 NOCACHE,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp);\n```\n\n\n### Casting types‚Äã\n\ncastDef\n- casts the type of a specific column.\ncolumnRef\nmust reference\nexisting column in the\nselectSql\n\n```questdb-sql\nCREATE TABLE test AS (  SELECT x FROM long_sequence(10)), CAST (x AS DOUBLE);\n```\n\n\n## Column indexes‚Äã\n\nIndex definitions (\nindexDef\n) are used to create an\nindex\nfor a table column. The referenced table column\nmust be of type\nsymbol\n.\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE), INDEX(symbol) TIMESTAMP(timestamp);\n```\n\nwarning\n- Theindex capacityandsymbol capacityare different\nsettings.\n- The index capacity value should not be changed, unless a user is aware of all\nthe implications. :::\nSee the\nIndex concept\nfor more\ninformation about indexes.\n\n## OWNED BY‚Äã\n\nEnterprise only.\nWhen a user creates a new table, they automatically get all table level\npermissions with the\nGRANT\noption for that table. However, if the\nOWNED BY\nclause is used, the permissions instead go to the user, group, or service\naccount named in that clause.\nThe\nOWNED BY\nclause cannot be omitted if the table is created by an external\nuser, because permissions cannot be granted to them.\n\n```questdb-sql\nCREATE GROUP analysts;CREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYOWNED BY analysts;\n```\n\n\n## CREATE TABLE AS‚Äã\n\nCreates a table, using the results from the\nSELECT\nstatement to determine the\ncolumn names and data types.\nCreate table as select\n\n```questdb-sql\nCREATE TABLE new_trades AS (  SELECT *  FROM    trades) TIMESTAMP(timestamp);\n```\n\nWe can use keywords such as\nIF NOT EXISTS\n,\nPARTITION BY\n..., as needed for\nthe new table. The data type of a column can be changed:\nClone an existing wide table and change type of cherry-picked columns\n\n```questdb-sql\nCREATE TABLE new_trades AS (  SELECT *  FROM    trades), CAST(price AS LONG) TIMESTAMP(timestamp);\n```\n\nHere we changed type of\nprice\nto\nLONG\n.\nnote\nSince QuestDB v7.4.0, the default behaviour for\nCREATE TABLE AS\nhas been\nchanged.\nPreviously, the table would be created atomically. For large tables, this\nrequires a significant amount of RAM, and can cause errors if the database runs\nout of memory.\nBy default, this will be performed in batches. If the query fails, partial data\nmay be inserted.\nIf this is a problem, it is recommended to use the ATOMIC keyword\n(\nCREATE ATOMIC TABLE\n). Alternatively, enabling deduplication on the table will\nallow you to perform an idempotent insert to re-insert any missed data.\n\n### ATOMIC‚Äã\n\nTables can be created atomically, which first loads all of the data and then\ncommits in a single transaction.\nThis requires the data to be available in memory all at once, so for large\ninserts, this may have performance issues.\nTo force this behaviour, one can use the\nATOMIC\nkeyword:\nCreate atomic table as select\n\n```questdb-sql\nCREATE ATOMIC TABLE new_trades AS (  SELECT *  FROM    trades) TIMESTAMP(timestamp);\n```\n\n\n### BATCH‚Äã\n\nBy default, tables will be created with data inserted in batches.\nThe size of the batches can be configured:\n- globally, by setting thecairo.sql.create.table.model.batch.sizeconfiguration option inserver.conf.\n- locally, by using theBATCHkeyword in theCREATE TABLEstatement.\nCreate batched table as select\n\n```questdb-sql\nCREATE BATCH 4096 TABLE new_trades AS (  SELECT *  FROM    trades) TIMESTAMP(timestamp);\n```\n\nOne can also specify the out-of-order commit lag for these batched writes, using\nthe o3MaxLag option:\nCreate table as select with batching and O3 lag\n\n```questdb-sql\nCREATE BATCH 4096 o3MaxLag 1s TABLE new_trades AS (  SELECT * FROM trades) TIMESTAMP(timestamp);\n```\n\n\n### Turning unordered data into ordered data‚Äã\n\nAs an additional example, let's assume we imported a text file into the table\ntrades_unordered\nand now we want to turn this data into time series\nthrough ordering trades by\ntimestamp\n, assign dedicated timestamp and\npartition by month:\nCreate table as select with data manipulation\n\n```questdb-sql\nCREATE TABLE trades AS (  SELECT * FROM trades_unordered ORDER BY timestamp) TIMESTAMP(timestamp)PARTITION BY MONTH;\n```\n\n\n## CREATE TABLE LIKE‚Äã\n\nThe\nLIKE\nkeyword clones the table schema of an existing table or materialized\nview without copying the data. Table settings and parameters such as designated\ntimestamp and symbol column indexes will be cloned, too.\nCreate table like\n\n```questdb-sql\nCREATE TABLE new_table (LIKE my_table);\n```\n\n\n## WITH table parameter‚Äã\n\nThe parameter influences how often commits of out-of-order data occur. It may be\nset during table creation using the\nWITH\nkeyword.\nmaxUncommittedRows\n- defines the maximum number of uncommitted rows per-table\nto keep in memory before triggering a commit for a specific table.\nThe purpose of specifying maximum uncommitted rows per table is to reduce the\noccurrences of resource-intensive commits when ingesting out-of-order data.\nThe global setting for the same parameter is\ncairo.max.uncommitted.rows\n.\nSetting out-of-order table parameters via SQL\n\n```questdb-sql\nCREATE TABLE trades (  timestamp TIMESTAMP,  symbol SYMBOL,  price DOUBLE,  amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYWITH maxUncommittedRows=250000;\n```\n\nChecking the values per-table may be done using the\ntables()\nfunction:\nList all tables\n\n```questdb-sql\nSELECT id, table_name, maxUncommittedRows FROM tables();\n```\n\n\n| id | name | maxUncommittedRows |\n| --- | --- | --- |\n| 1 | trades | 250000 |\n| 2 | sample_table | 50000 |\n\n\n## Table target volume‚Äã\n\nThe\nIN VOLUME\nclause is used to create a table in a different volume than the\nstandard. The table is created in the specified target volume, and a symbolic\nlink is created in the table's standard volume to point to it.\nThe use of the comma (\n,\n) depends on the existence of the\nWITH\nclause:\n- If theWITHclause is present, a comma is mandatory beforeIN VOLUME:CREATE TABLE trades (timestamp TIMESTAMP,symbol SYMBOL,price DOUBLE,amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYWITH maxUncommittedRows=250000,IN VOLUME SECONDARY_VOLUME;\n- If noWITHclause is used, the comma must not be added for theIN VOLUMEsegment:CREATE TABLE trades (timestamp TIMESTAMP,symbol SYMBOL,price DOUBLE,amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYIN VOLUME SECONDARY_VOLUME;\nThe use of quotation marks (\n'\n) depends on the volume alias:\n- If the alias contains spaces, the quotation marks are required:CREATE TABLE trades (timestamp TIMESTAMP,symbol SYMBOL,price DOUBLE,amount DOUBLE) TIMESTAMP(timestamp)PARTITION BY DAYIN VOLUME 'SECONDARY_VOLUME';\n- If the alias does not contain spaces, no quotation mark is necessary.\n\n### Description‚Äã\n\nThe table behaves the same way as if it had been created in the standard\n(default) volume, with the exception that\nDROP TABLE\nremoves the symbolic link from the\nstandard volume but the content pointed to is left intact in its volume. A table\nusing the same name in the same volume cannot be created again as a result, it\nrequires manual intervention to either remove or rename the table's directory in\nits volume.\n\n### Configuration‚Äã\n\nThe secondary table target volume is defined by\ncairo.volumes\nin\nserver.conf\n. The default setting contains\nan empty list, which means the feature is not enabled.\nTo enable the feature, define as many volume pairs as you need, with syntax\nalias -> volume-root-path\n, and separate different pairs with a comma. For\nexample:\n\n```questdb-sql\ncairo.volumes=SECONDARY_VOLUME -> /Users/quest/mounts/secondary, BIN -> /var/bin\n```\n\nAdditional notes about defining the alias and volume root paths:\n- Aliases are case-insensitive.\n- Volume root paths must be valid and exist at bootstrap time and at the time\nwhen the table is created.\n- Aliases and/or volume root paths can be single quoted, it is not required.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2670,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-de2aca757f08",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/ttl",
    "title": "Time To Live (TTL) | QuestDB",
    "text": "TTL (Time To Live) automatically drops old partitions based on data age. Set a\nretention period, and QuestDB removes partitions that fall entirely outside that\nwindow - no cron jobs or manual cleanup required.\n\n## Requirements‚Äã\n\nTTL requires:\n- Adesignated timestampcolumn\n- Partitioningenabled\nThese are standard for time-series tables in QuestDB.\n\n## Setting TTL‚Äã\n\n\n### At table creation‚Äã\n\n\n```questdb-sql\nCREATE TABLE trades (    ts TIMESTAMP,    symbol SYMBOL,    price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY TTL 7 DAYS;\n```\n\n\n### On existing tables‚Äã\n\n\n```questdb-sql\nALTER TABLE trades SET TTL 7 DAYS;\n```\n\nSupported units:\nHOUR\n/\nH\n,\nDAY\n/\nD\n,\nWEEK\n/\nW\n,\nMONTH\n/\nM\n,\nYEAR\n/\nY\n.\n\n```questdb-sql\n-- These are equivalentALTER TABLE trades SET TTL 2 WEEKS;ALTER TABLE trades SET TTL 2w;\n```\n\nFor full syntax, see\nALTER TABLE SET TTL\n.\n\n## How TTL works‚Äã\n\nTTL drops partitions based on the\npartition's time range\n, not individual row\ntimestamps. A partition is dropped only when its\nentire period\nfalls outside\nthe TTL window.\nKey rule\n: A partition is dropped when\npartition_end_time < reference_time - TTL\n.\n\n### Reference time‚Äã\n\nBy default, TTL uses wall-clock time as the reference, not the maximum timestamp\nin the table. This protects against accidental data loss if a row with a\nfar-future timestamp is inserted (which would otherwise cause all existing data\nto appear \"expired\").\nThe reference time is:\nmin(max_timestamp_in_table, wall_clock_time)\nTo restore legacy behavior (using only max timestamp), set in\nserver.conf\n:\n\n```ini\ncairo.ttl.use.wall.clock=false\n```\n\ncaution\nDisabling wall-clock protection means inserting a row with a future timestamp\n(e.g., year 2100) will immediately drop all partitions that fall outside the TTL\nwindow relative to that future time.\n\n### Example‚Äã\n\nTable partitioned by\nHOUR\nwith\nTTL 1 HOUR\n:\n\n| Wall-clock time | Action | Partitions remaining |\n| --- | --- | --- |\n| 08:00 | Insert row at 08:00 | 08:00-09:00 |\n| 09:00 | Insert row at 09:00 | 08:00-09:00,09:00-10:00 |\n| 09:59 | Insert row at 09:59 | 08:00-09:00,09:00-10:00 |\n| 10:00 | Insert row at 10:00 | 09:00-10:00,10:00-11:00 |\n\nThe\n08:00-09:00\npartition survives until 10:00 because its\nend time\n(09:00)\nmust be more than 1 hour behind the reference time. At 10:00, the partition end\n(09:00) is exactly 1 hour old, so it's dropped.\n\n## Checking TTL settings‚Äã\n\n\n```questdb-sql\nSELECT table_name, ttlValue, ttlUnit FROM tables();\n```\n\n\n| table_name | ttlValue | ttlUnit |\n| --- | --- | --- |\n| trades | 7 | DAY |\n| metrics | 0 | null |\n\nA\nttlValue\nof\n0\nmeans TTL is not configured.\n\n## Removing TTL‚Äã\n\nTo disable automatic retention and keep all data:\n\n```questdb-sql\nALTER TABLE trades SET TTL 0;\n```\n\n\n## Guidelines‚Äã\n\n\n| Data type | Typical TTL | Rationale |\n| --- | --- | --- |\n| Real-time metrics | 1-7 days | High volume, recent data most valuable |\n| Trading data | 30-90 days | Compliance requirements vary |\n| Aggregated data | 1-2 years | Lower volume, longer analysis windows |\n| Audit logs | Per compliance | Often legally mandated retention |\n\nTips:\n- Match TTL to your longest typical query range plus a buffer\n- TTL should be significantly larger than your partition interval\n- For manual control instead of automatic TTL, seeData Retention",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 551,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-f1a6fe7b06ad",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/deduplication",
    "title": "Deduplication | QuestDB",
    "text": "Deduplication ensures that only one row exists for a given set of key columns.\nWhen a new row matches an existing row's keys, the old row is replaced.\n\n## When to use deduplication‚Äã\n\nUse deduplication when:\n- You need idempotent writes (safe to retry or resend data)\n- You're reloading data that may have changed (e.g., third-party data feeds)\n- You want \"last write wins\" behavior for a given key\n- You're recovering from ingestion errors and need to resend a time range\nSkip deduplication when:\n- Your timestamps are always unique (no duplicates possible)\n- You're doing append-only logging where duplicates are acceptable\n- Write performance is critical and you're certain duplicates won't occur\n\n## Quick example‚Äã\n\n\n```questdb-sql\nCREATE TABLE prices (    ts TIMESTAMP,    ticker SYMBOL,    price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY WALDEDUP UPSERT KEYS(ts, ticker);\n```\n\nWith this configuration, each\n(ts, ticker)\ncombination can have only one row:\n\n```questdb-sql\nINSERT INTO prices VALUES ('2026-01-15T10:00:00', 'AAPL', 185.50);INSERT INTO prices VALUES ('2026-01-15T10:00:00', 'AAPL', 186.00);  -- replaces previousSELECT * FROM prices;\n```\n\n\n| ts | ticker | price |\n| --- | --- | --- |\n| 2026-01-15T10:00:00 | AAPL | 186.00 |\n\nOnly the last value is kept.\n\n## How it works‚Äã\n\nWhen deduplication is enabled, QuestDB:\n- Checks if incoming rows match existing rows by UPSERT KEYS\n- If keys match, compares the full row content\n- If the row is identical, skips the write entirely (no disk I/O)\n- If the row differs, replaces the old row with the new one\nThis full-row comparison significantly reduces write amplification when\nreloading large datasets where only a small portion has changed ‚Äî common\nwhen consuming third-party data feeds that provide full snapshots.\n\n## Performance‚Äã\n\nDeduplication has minimal overhead when:\n- Timestamps are mostly unique across rows\n- Data arrives in roughly time-ordered fashion\nDeduplication is more expensive when:\n- Many rows share the same timestamp\n- Deduplication keys have high cardinality\nThe full-row check optimization means that reloading unchanged data is\ncheap ‚Äî QuestDB detects identical rows and skips unnecessary writes.\n\n## Configuration‚Äã\n\n\n### Create table with deduplication‚Äã\n\n\n```questdb-sql\nCREATE TABLE prices (    ts TIMESTAMP,    ticker SYMBOL,    price DOUBLE) TIMESTAMP(ts) PARTITION BY DAY WALDEDUP UPSERT KEYS(ts, ticker);\n```\n\nThe designated timestamp must always be included in UPSERT KEYS.\n\n### Enable on existing table‚Äã\n\n\n```questdb-sql\nALTER TABLE prices DEDUP ENABLE UPSERT KEYS(ts, ticker);\n```\n\n\n### Disable deduplication‚Äã\n\n\n```questdb-sql\nALTER TABLE prices DEDUP DISABLE;\n```\n\n\n### Change UPSERT KEYS‚Äã\n\n\n```questdb-sql\nALTER TABLE prices DEDUP ENABLE UPSERT KEYS(ts, ticker, exchange);\n```\n\n\n## Checking configuration‚Äã\n\nCheck if deduplication is enabled:\n\n```questdb-sql\nSELECT dedup FROM tables() WHERE table_name = 'prices';\n```\n\nCheck which columns are UPSERT KEYS:\n\n```questdb-sql\nSELECT \"column\", upsertKey FROM table_columns('prices');\n```\n\n\n## Requirements‚Äã\n\n- Deduplication requiresWAL tables\n- The designated timestamp must be included in UPSERT KEYS\n- Enabling deduplication does not deduplicate existing data ‚Äî only new inserts\n\n## See also‚Äã\n\n- CREATE TABLE ... DEDUP\n- ALTER TABLE DEDUP ENABLE\n- ALTER TABLE DEDUP DISABLE",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 495,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-e6c20d9baeee",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/views",
    "title": "Views | QuestDB",
    "text": "A view is a\nvirtual table\ndefined by a SQL\nSELECT\nstatement. Views do not\nstore data themselves; instead, their defining query is executed as a sub-query\nwhenever the view is referenced.\n\n## What are views for?‚Äã\n\nViews provide several benefits:\n- Abstraction: Hide complex queries behind simple table-like interfaces\n- Reusability: Define queries once, use them everywhere\n- Security: Control data access without exposing underlying tables\n- Maintainability: Single source of truth for business logic\nQuick example\n\n```questdb-sql\n-- Create a viewCREATE VIEW hourly_summary AS (  SELECT ts, symbol, sum(quantity) as volume  FROM trades  SAMPLE BY 1h);-- Query the view like a tableSELECT * FROM hourly_summary WHERE symbol = 'AAPL';\n```\n\n\n## Creating views‚Äã\n\nUse\nCREATE VIEW\nto define a new view:\nBasic view\n\n```questdb-sql\nCREATE VIEW daily_prices AS (  SELECT ts, symbol, last(price) as closing_price  FROM trades  SAMPLE BY 1d)\n```\n\n\n### CREATE IF NOT EXISTS‚Äã\n\nTo avoid errors when the view already exists:\n\n```questdb-sql\nCREATE VIEW IF NOT EXISTS price_view AS (  SELECT symbol, last(price) as price FROM trades SAMPLE BY 1h)\n```\n\n\n### CREATE OR REPLACE‚Äã\n\nTo update an existing view or create it if it doesn't exist:\n\n```questdb-sql\nCREATE OR REPLACE VIEW price_view AS (  SELECT symbol, last(price) as price, ts FROM trades SAMPLE BY 1h)\n```\n\nFor full syntax details, see\nCREATE VIEW\n.\n\n## Querying views‚Äã\n\nViews are queried exactly like tables:\n\n```questdb-sql\nSELECT * FROM my_viewSELECT ts, price FROM my_view WHERE symbol = 'AAPL'SELECT v1.ts, v2.valueFROM view1 v1JOIN view2 v2 ON v1.id = v2.id\n```\n\n\n### Optimizer transparency‚Äã\n\nViews in QuestDB are fully transparent to the query optimizer. When you query a\nview, the optimizer treats it exactly as if you had written the view's query\ninline as a sub-query. This means views benefit from the complete suite of\nquery optimizations:\n- Filter push-down: WHERE conditions are pushed to base tables\n- Projection push-down: Only required columns are read from storage\n- Join optimization: Join order and strategies are optimized across view boundaries\n- ORDER BY optimization: Sorting can leverage table indexes\n- Timestamp optimizations: Time-based operations use partition pruning\n\n```questdb-sql\n-- View definitionCREATE VIEW trades_view AS (  SELECT ts, symbol, price, quantity FROM trades WHERE price > 0)-- This query is optimized as if written inlineSELECT ts, price FROM trades_view WHERE symbol = 'AAPL' ORDER BY ts-- Optimizer sees: SELECT ts, price FROM trades WHERE price > 0 AND symbol = 'AAPL' ORDER BY ts-- Only ts and price columns are read, filters applied at scan, ordering uses index\n```\n\nUse\nEXPLAIN\nto see how the optimizer processes view queries:\n\n```questdb-sql\nEXPLAIN SELECT * FROM trades_view WHERE symbol = 'AAPL'\n```\n\nThere is no performance penalty for using views compared to writing equivalent\nsub-queries directly.\n\n## Parameterized views‚Äã\n\nViews support the\nDECLARE\nstatement to define parameters with default values.\nUse\nOVERRIDABLE\nto allow users to change parameter values at query time.\n\n### Creating a parameterized view‚Äã\n\n\n```questdb-sql\nCREATE VIEW filtered_trades AS (  DECLARE OVERRIDABLE @min_price := 100  SELECT ts, symbol, price FROM trades WHERE price >= @min_price)\n```\n\n\n### Querying with default parameters‚Äã\n\n\n```questdb-sql\nSELECT * FROM filtered_trades-- Uses default @min_price = 100\n```\n\n\n### Overriding parameters‚Äã\n\n\n```questdb-sql\nDECLARE @min_price := 500 SELECT * FROM filtered_trades-- Overrides @min_price to 500\n```\n\n\n### Multiple parameters‚Äã\n\nBy default, parameters are\nnon-overridable\n. Use\nOVERRIDABLE\nto allow\noverride at query time:\n\n```questdb-sql\nCREATE VIEW price_range AS (  DECLARE OVERRIDABLE @lo := 100, OVERRIDABLE @hi := 1000  SELECT ts, symbol, price FROM trades WHERE price >= @lo AND price <= @hi)-- Query with custom rangeDECLARE @lo := 50, @hi := 200 SELECT * FROM price_range\n```\n\n\n### Non-overridable parameters‚Äã\n\nParameters without\nOVERRIDABLE\ncannot be changed at query time, providing\nsecurity for sensitive filters:\n\n```questdb-sql\nCREATE VIEW secure_view AS (  DECLARE @min_value := 0  SELECT * FROM trades WHERE value >= @min_value)-- This will fail with \"variable is not overridable: @min_value\"DECLARE @min_value := -100 SELECT * FROM secure_view\n```\n\n\n### Mixed parameters‚Äã\n\nCombine overridable and non-overridable parameters:\n\n```questdb-sql\nCREATE VIEW mixed_params AS (  DECLARE @fixed_filter := 'active', OVERRIDABLE @limit := 100  SELECT * FROM data WHERE status = @fixed_filter LIMIT @limit)-- @limit can be overridden, @fixed_filter cannotDECLARE @limit := 50 SELECT * FROM mixed_params\n```\n\n\n## View hierarchies‚Äã\n\nViews can reference other views, tables, and materialized views:\n\n```questdb-sql\n-- Level 1: Raw data filteringCREATE VIEW valid_trades AS (  SELECT * FROM trades WHERE price > 0 AND quantity > 0)-- Level 2: AggregationCREATE VIEW hourly_stats AS (  SELECT ts, symbol, sum(quantity) as volume  FROM valid_trades  SAMPLE BY 1h)-- Level 3: Derived metricsCREATE VIEW hourly_vwap AS (  SELECT ts, symbol, volume, turnover / volume as vwap  FROM hourly_stats  WHERE volume > 0)\n```\n\ntip\nKeep view hierarchies shallow (3-4 levels maximum) for better query planning\nand maintainability.\n\n## View management‚Äã\n\n\n### Listing views‚Äã\n\n\n```questdb-sql\nSELECT * FROM views()\n```\n\nReturns:\n\n| Column | Description |\n| --- | --- |\n| view_name | Name of the view |\n| view_sql | The SQL definition |\n| view_table_dir_name | Internal directory name |\n| invalidation_reason | Error message if view is invalid |\n| view_status | validorinvalid |\n| view_status_update_time | Timestamp of last status change |\n\n\n### Show view definition‚Äã\n\n\n```questdb-sql\nSHOW CREATE VIEW my_view\n```\n\nReturns the\nCREATE VIEW\nstatement that would recreate the view.\n\n### Show view columns‚Äã\n\n\n```questdb-sql\nSHOW COLUMNS FROM my_view\n```\n\n\n### Altering views‚Äã\n\nTo modify an existing view's definition:\n\n```questdb-sql\nALTER VIEW my_view AS (SELECT col1, col2 FROM my_table WHERE col1 > 0)\n```\n\nFor full syntax, see\nALTER VIEW\n.\n\n### Dropping views‚Äã\n\n\n```questdb-sql\nDROP VIEW my_view-- Or safely:DROP VIEW IF EXISTS my_view\n```\n\nFor full syntax, see\nDROP VIEW\n.\n\n## View invalidation‚Äã\n\nViews are automatically invalidated when their dependencies change:\n\n| Operation | Effect |\n| --- | --- |\n| DROP TABLE | View becomes invalid |\n| RENAME TABLE | View becomes invalid |\n| DROP COLUMN | View becomes invalid if column is referenced |\n| RENAME COLUMN | View becomes invalid if column is referenced |\n| Column type change | View metadata is updated |\n\n\n### Automatic recovery‚Äã\n\nViews are automatically revalidated when the invalidating condition is reversed:\n- If a table is dropped and later recreated, dependent views become valid again\n- If a column is renamed back to its original name, dependent views become valid\nagain\n\n### Checking view status‚Äã\n\n\n```questdb-sql\nSELECT view_name, view_status, invalidation_reasonFROM views()WHERE view_status = 'invalid'\n```\n\n\n## Views in tables() output‚Äã\n\nViews appear in the\ntables()\nfunction with\ntable_type = 'V'\n:\n\n```questdb-sql\nSELECT table_name, table_type FROM tables()\n```\n\n\n| table_type | Description |\n| --- | --- |\n| T | Regular table |\n| V | View |\n| M | Materialized view |\n\n\n## Views vs materialized views‚Äã\n\nUnderstanding when to use each type is important for performance:\n\n| Feature | View | Materialized View |\n| --- | --- | --- |\n| Data storage | None (virtual) | Physical storage |\n| Query execution | On every access | Pre-computed |\n| Data freshness | Always current | Depends on refresh |\n| Performance | Query-time cost | Read-time benefit |\n| Storage cost | Zero | Proportional to result size |\n\n\n### When to use views‚Äã\n\n- Simple transformations that execute quickly\n- Data that must always be current\n- Ad-hoc analysis where requirements change frequently\n- Parameterized queries withDECLARE\n- Low-frequency queries\n\n### When to use materialized views‚Äã\n\n- Heavy aggregations over large datasets\n- Frequently accessed summary data\n- Dashboard queries that run repeatedly\n- Historical summaries that don't need real-time accuracy\nFor detailed comparisons and examples, see\nMaterialized Views\n.\n\n## Security with views‚Äã\n\nViews provide a security boundary between users and underlying data.\n\n### Definer security model (Enterprise)‚Äã\n\nViews use a\ndefiner security model\n. When a view is created, the creator's\npermissions are captured. Users querying the view only need\nSELECT\npermission\non the view itself - they do\nnot\nneed permissions on the underlying tables.\n\n```questdb-sql\n-- Admin creates a view on sensitive tableCREATE VIEW public_summary AS (  SELECT date, region, sum(sales) as total FROM sensitive_sales GROUP BY date, region);-- Grant SELECT on the view to analystsGRANT SELECT ON public_summary TO analyst_role;-- Analysts can query the view without access to sensitive_salesSELECT * FROM public_summary;  -- Works!SELECT * FROM sensitive_sales; -- Access denied!\n```\n\nThe view's definer permissions are preserved even if the creator's account is\nlater disabled or deleted.\n\n### No column-level permissions on views‚Äã\n\nUnlike tables, views do\nnot\nsupport column-level permissions. You can only\ngrant or revoke permissions on the entire view:\n\n```questdb-sql\n-- This works: grant SELECT on entire viewGRANT SELECT ON my_view TO user1;-- Column-level permissions are NOT supported for views-- Use separate views to expose different column subsets\n```\n\nTo provide different column access to different users, create multiple views\nwith different column selections.\n\n### Security patterns‚Äã\n\nViews enable several security patterns:\n- Data subsetting: Expose only specific rows or columns\n- Column masking: Hide sensitive columns from certain users\n- Row-level security: Filter rows based on business rules\n- Aggregation-only access: Provide summaries without raw data access\n\n### Column-level security example‚Äã\n\n\n```questdb-sql\n-- Base table with sensitive dataCREATE TABLE employees (  id LONG,  name VARCHAR,  salary DOUBLE,        -- Sensitive  department VARCHAR,  hire_date TIMESTAMP);-- View exposing only non-sensitive columnsCREATE VIEW employees_public AS (  SELECT id, name, department, hire_date  FROM employees);-- Grant access to public view onlyGRANT SELECT ON employees_public TO analyst_role;\n```\n\n\n### Row-level security example‚Äã\n\n\n```questdb-sql\n-- View for specific trading deskCREATE VIEW desk_a_trades AS (  SELECT * FROM trades WHERE trader_id IN (101, 102, 103));GRANT SELECT ON desk_a_trades TO desk_a_users;\n```\n\nFor more details on permissions, see\nRole-Based Access Control (RBAC)\n.\n\n## Performance considerations‚Äã\n\n\n### Views don't cache results‚Äã\n\nEvery query against a view executes the underlying query. For expensive\naggregations accessed frequently, consider materialized views.\n\n### Optimize with indexes‚Äã\n\nCreate indexes on base table columns used in view filters:\n\n```questdb-sql\nALTER TABLE trades ALTER COLUMN symbol ADD INDEX\n```\n\n\n### Check query plans‚Äã\n\nAlways examine query plans when optimizing:\n\n```questdb-sql\nEXPLAIN SELECT * FROM my_view WHERE symbol = 'AAPL'\n```\n\n\n### Best practices‚Äã\n\n- Use indexed columns in filters for best performance\n- Use parameterized views for common filter patterns\n- Avoid deeply nested view hierarchies (>3-4 levels) for maintainability\n- Consider materialized views for expensive aggregations that run frequently\n\n## Limitations‚Äã\n\n- No data storage: Views don't store data - the query runs each time\n- No indexes: Views cannot have indexes; filtering relies on underlying\ntable indexes\n- Circular references: Views cannot reference themselves or create circular\ndependencies\n- Read-only: You cannot INSERT, UPDATE, or DELETE on views\n- No DDL operations: You cannot run DDL operations (likeRENAME TABLE) on\nviews\n- No column-level permissions: Unlike tables, you cannot grant permissions\non individual view columns (Enterprise)\n\n## Related documentation‚Äã\n\n- SQL CommandsCREATE VIEW: Create a new viewALTER VIEW: Modify a view definitionDROP VIEW: Remove a view\n- Related ConceptsMaterialized Views: Pre-computed query resultsDECLARE: Parameter declaration for views",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1833,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-eb44affe8774",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/datatypes/decimal",
    "title": "Decimal | QuestDB",
    "text": "QuestDB provides a\ndecimal\ndata type for exact numeric calculations, useful\nfor financial computations, scientific measurements, and any scenario where\nprecision matters. This page explains how to use decimals effectively, including\nsyntax, operations, and performance considerations.\n\n## What are decimals?‚Äã\n\nDecimals are fixed-point numbers that maintain exact precision during arithmetic\noperations. Unlike floating-point types (\nfloat\nand\ndouble\n), decimals avoid\nrounding errors by storing numbers as scaled integers internally. This makes\nthem ideal for monetary calculations where accuracy is critical.\n\n## Decimal type in QuestDB‚Äã\n\nQuestDB implements decimals with the syntax\nDECIMAL(precision, scale)\n:\n- Precision: Total number of significant digits (1-76)\n- Scale: Number of digits after the decimal point (0-precision)\nFor example,\nDECIMAL(10, 2)\ncan store values from -99,999,999.99 to\n99,999,999.99.\nIf neither the precision and scale are provided, the type defaults to a\nprecision of 18 and a scale of 3.\n\n### Storage‚Äã\n\nQuestDB automatically selects the optimal storage size based on the decimal's\nprecision:\n\n| Precision | Storage Size | Internal Type |\n| --- | --- | --- |\n| 1-2 digits | 1 byte | DECIMAL8 |\n| 3-4 digits | 2 bytes | DECIMAL16 |\n| 5-9 digits | 4 bytes | DECIMAL32 |\n| 10-18 digits | 8 bytes | DECIMAL64 |\n| 19-38 digits | 16 bytes | DECIMAL128 |\n| 39-76 digits | 32 bytes | DECIMAL256 |\n\n\n## Decimal literals‚Äã\n\nQuestDB requires the\nm\nsuffix to distinguish decimal literals from\nfloating-point numbers:\n\n```questdb-sql\n-- Decimal literals use the 'm' suffixSELECT 123.45m;        -- Decimal value 123.45SELECT 0.001m;         -- Decimal value 0.001SELECT 1000000.00m;    -- Decimal value 1,000,000.00-- Without 'm' suffix, numbers are treated as doubleSELECT 123.45;         -- Double value (floating-point)\n```\n\nimportant\nAlways use the\nm\nsuffix for decimal literals. QuestDB does not implicitly\nconvert doubles to decimals to prevent unintended precision loss.\n\n## Creating tables with decimals‚Äã\n\nDefine decimal columns by specifying precision and scale:\n\n```questdb-sql\nCREATE TABLE eth_fills (    fill_id LONG,    venue SYMBOL,    fill_size_eth DECIMAL(28, 18),   -- ETH quantity with wei-level precision    fill_price_usdc DECIMAL(14, 2),  -- Execution price per ETH in USDC    fee_rate DECIMAL(6, 5),          -- Exchange fee rate (e.g., 0.00050 = 5 bps)    gas_fee_eth DECIMAL(20, 18),     -- Gas paid in ETH    timestamp TIMESTAMP) timestamp(timestamp) partition by day;\n```\n\n\n## Working with decimals‚Äã\n\n\n### Basic arithmetic‚Äã\n\nDecimal arithmetic maintains precision automatically:\n\n```questdb-sql\n-- Insert ETH fill dataINSERT INTO eth_fills VALUES    (1, 'spot:coinbase', 0.842345678901234567m, 2123.45m, 0.00040m, 0.002300000000000000m, now()),    (2, 'perp:binance', 5.250000000000000000m, 2118.10m, 0.00020m, 0.001200000000000000m, now()),    (3, 'defi:uniswap', 18.750000000000000000m, 2115.05m, 0.00065m, 0.004500000000000000m, now());-- Arithmetic operations maintain precisionSELECT    venue,    fill_size_eth,    fill_price_usdc,    fill_size_eth * fill_price_usdc AS notional_usdc,    fill_size_eth * fill_price_usdc * fee_rate AS fee_usdc,    gas_fee_eth * fill_price_usdc AS gas_cost_usdc,    fill_size_eth - gas_fee_eth AS net_eth_after_gas,    fill_size_eth * fill_price_usdc        - fill_size_eth * fill_price_usdc * fee_rate        - gas_fee_eth * fill_price_usdc AS net_settlement_usdcFROM eth_fills;\n```\n\n\n### Precision and scale in operations‚Äã\n\nQuestDB automatically determines the result precision and scale for decimal\noperations based on the operands:\n\n#### Addition and subtraction‚Äã\n\n- Scale: Maximum scale of the operands\n- Precision: Maximum precision of the operands (scaled) + 1\n\n```questdb-sql\n-- Addition with different scalesSELECT 10.5m + 1.234m;  -- scale: max(1, 3) = 3, Result: 11.734-- Adding DECIMAL(10,2) + DECIMAL(8,2) ‚Üí DECIMAL(11,2)SELECT 99999999.99m + 999999.99m;  -- Result has precision 11, scale 2\n```\n\nThe additional precision digit allows the result to accommodate potential\noverflow (e.g., 99.9 + 99.9 = 199.8 requires 4 digits instead of 3).\n\n#### Multiplication‚Äã\n\n- Scale: Sum of the scales of both operands\n- Precision: Sum of the precision of both operands\n\n```questdb-sql\n-- Multiplication adds scalesSELECT 10.50m * 1.25m;  -- scale: 2 + 2 = 4, Result: 13.1250-- DECIMAL(5,2) * DECIMAL(4,2) ‚Üí DECIMAL(9,4)SELECT 100.50m * 12.34m;  -- Result: 1240.1700\n```\n\n\n#### Division‚Äã\n\n- Scale: Maximum scale of the operands\n\n```questdb-sql\n-- Division uses maximum scaleSELECT 10.50m / 2.0m;  -- scale: max(2, 1) = 2, Result: 5.25-- Division may truncate beyond the scaleSELECT 10.00m / 3.00m;  -- Result: 3.33 (limited to scale 2)\n```\n\n\n### Comparison operations‚Äã\n\nDecimals support all standard comparison operators:\n\n```questdb-sql\n-- Find whale-sized ETH fills (>= 10 ETH)SELECT * FROM eth_fills WHERE fill_size_eth >= 10.000000000000000000m;-- Find attractive fee tiersSELECT * FROM eth_fills WHERE fee_rate <= 0.00025m;-- Range queries on ETH priceSELECT * FROM eth_fills WHERE fill_price_usdc BETWEEN 2100.00m AND 2200.00m;\n```\n\n\n## Type casting‚Äã\n\n\n### Explicit casting‚Äã\n\nConvert between numeric types using\nCAST\n:\n\n```questdb-sql\n-- From integer to decimalSELECT CAST(100 AS DECIMAL(10, 2));  -- Result: 100.00-- From double to decimal (use with caution - may lose precision)SELECT CAST(123.456789 AS DECIMAL(8, 3));  -- Result: 123.457-- From decimal to other typesSELECT CAST(99.99m AS INT);    -- Result: 99 (truncate)SELECT CAST(99.99m AS DOUBLE);  -- Result: 99.99 (as floating-point)\n```\n\n\n### Important casting rules‚Äã\n\n- No implicit conversion from double/float: Must use explicitCASTor\ndecimal literals\n- Integer to decimal: Safe, no precision loss, the decimals have a scale of\n0\n- Double to decimal: May lose precision due to floating-point representation\n- Between decimal types: Automatic when precision/scale allows\n\n## Considerations‚Äã\n\n\n### Advantages‚Äã\n\n- Exact results: Perfect for financial calculations and accounting\n- Predictable behavior: No surprising rounding errors\n- Regulatory compliance: Meets requirements for exact monetary calculations\n\n### Performance‚Äã\n\nQuestDB's decimal implementation is designed for high performance:\n- Only ~2x slower than doublefor heavy computations like division\n- Faster than other databasesincluding ClickHouse and DuckDB decimal types\n- Non-allocatingduring computations (no garbage collection overhead)\n- Native implementation- unlike Java's BigDecimal, QuestDB's decimal is\npurpose-built for time-series workloads\n\n### Performance tips‚Äã\n\n- Use appropriate precision: Don't over-specify precision beyond your needs\n- Keep precision ‚â§ 18 when possible: DECIMAL64 operations are faster than\nDECIMAL128/256\n\n## Common use cases‚Äã\n\n\n### Portfolio reporting‚Äã\n\n\n```questdb-sql\n-- Portfolio valuation with exact arithmeticCREATE TABLE portfolio (    symbol SYMBOL,    position_size DECIMAL(12, 4),     -- Fractional position sizes (shares, BTC, etc.) supported    price DECIMAL(10, 2),       -- Stock price    commission DECIMAL(7, 2),   -- Trading fees    timestamp TIMESTAMP) timestamp(timestamp);-- Calculate exact portfolio valueSELECT    symbol,    position_size,    price,    position_size * price AS position_value,    position_size * price - commission AS net_value,    sum(position_size * price) OVER () AS total_portfolio_valueFROM portfolioWHERE timestamp = now();\n```\n\n\n### Cryptocurrency trading‚Äã\n\n\n```questdb-sql\n-- ETH trading with high precision (18 decimals like wei)CREATE TABLE crypto_trades (    trade_id LONG,    pair SYMBOL,    eth_amount DECIMAL(28, 18),      -- ETH with full wei precision    usdt_price DECIMAL(12, 2),        -- USDT price per ETH    fee_rate DECIMAL(5, 4),           -- Trading fee (e.g., 0.001 for 0.1%)    gas_fee_eth DECIMAL(18, 18),      -- Gas fee in ETH    timestamp TIMESTAMP) timestamp(timestamp);-- Calculate trade values with exact precisionSELECT    trade_id,    eth_amount,    usdt_price,    eth_amount * usdt_price AS trade_value_usdt,    eth_amount * usdt_price * fee_rate AS fee_usdt,    eth_amount * usdt_price * (1.0m - fee_rate) AS net_value_usdt,    eth_amount - gas_fee_eth AS net_eth_receivedFROM crypto_trades;\n```\n\n\n### Scientific measurements‚Äã\n\n\n```questdb-sql\n-- High-precision sensor dataCREATE TABLE sensor_readings (    sensor_id SYMBOL,    measurement DECIMAL(20, 10),  -- 10 decimal places of precision    calibration_factor DECIMAL(6, 5),    timestamp TIMESTAMP) timestamp(timestamp);-- Apply calibration with exact arithmeticSELECT    sensor_id,    measurement,    measurement * calibration_factor AS calibrated_value,    avg(measurement) OVER (PARTITION BY sensor_id) AS avg_readingFROM sensor_readingsSAMPLE BY 1h;\n```\n\n\n## Best practices‚Äã\n\n\n### When to use decimals‚Äã\n\nUse decimals for:\n- Financial data (prices, amounts, exchange rates)\n- Crypto trading data (fractional position sizes, token balances, fees)\n- Accounting calculations\n- Scientific measurements requiring exact precision\n- Regulatory compliance scenarios\n- Any calculation where rounding errors are unacceptable\nAvoid decimals for:\n- Scientific calculations requiring extensive math functions\n- Performance-critical analytics on large datasets\n- Approximate values where precision isn't critical\n- Coordinates or measurements where float precision suffices\n\n### Design guidelines‚Äã\n\n- Choose appropriate precision and scale-- Good: Matches business requirementsCREATE TABLE prices (amount DECIMAL(10, 2)  -- Cents precision for USD);-- Avoid: Excessive precisionCREATE TABLE prices (amount DECIMAL(30, 15)  -- Unnecessary for most use cases);\n- Use the 'm' suffix consistently-- Good: Clear decimal literalsINSERT INTO prices VALUES (99.99m);-- Error: Missing 'm' suffixINSERT INTO prices VALUES (99.99);  -- Treated as double, will fail\n- Explicit casting when mixing types-- Good: Explicit castSELECT amount + CAST(10 AS DECIMAL(10, 2)) FROM prices;-- Good: Use decimal literalSELECT amount + 10.00m FROM prices;",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1314,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-bced8d16bce1",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/security/rbac",
    "title": "Role-based Access Control (RBAC) | QuestDB",
    "text": "Enterprise\n‚Äî\nRole-based Access Control (RBAC) provides fine-grained permissions for your QuestDB instance.\nLearn more\nQuestDB Enterprise provides fine-grained access control that can restrict access\nat\ndatabase\n,\ntable\n,\ncolumn\n, and even\nrow\nlevel (using views).\n\n## Quick start‚Äã\n\nHere's a complete example to create a read-only analyst user in under a minute:\n\n```questdb-sql\n-- 1. Create the userCREATE USER analyst WITH PASSWORD 'secure_password_here';-- 2. Grant endpoint access (required to connect)GRANT PGWIRE, HTTP TO analyst;-- 3. Grant read access to specific tablesGRANT SELECT ON trades, prices TO analyst;-- Done! The analyst can now connect and query trades and prices tables\n```\n\nTo verify:\n\n```questdb-sql\nSHOW PERMISSIONS analyst;\n```\n\n\n## Access control depth‚Äã\n\nQuestDB's access control operates across two dimensions:\n\n### Data access granularity‚Äã\n\nControl\nwhat data\nusers can access:\n\n| Level | What you can control | Example |\n| --- | --- | --- |\n| Database | All tables, global operations | GRANT SELECT ON ALL TABLES TO user |\n| Table | Specific tables | GRANT SELECT ON trades TO user |\n| Column | Specific columns within a table | GRANT SELECT ON trades(ts, price) TO user |\n| Row | Specific rows via views | Create a view with WHERE clause, grant access to view |\n\n\n### Connection access granularity‚Äã\n\nControl\nhow\nusers can connect:\n\n| Permission | Protocol | Use case |\n| --- | --- | --- |\n| HTTP | REST API, Web Console, ILP/HTTP | Interactive users, web applications |\n| PGWIRE | PostgreSQL Wire Protocol | SQL clients, BI tools, programmatic access |\n| ILP | InfluxDB Line Protocol (TCP) | High-throughput data ingestion |\n\n\n```questdb-sql\n-- User can connect via PostgreSQL protocol only (not web console)GRANT PGWIRE TO analyst;-- Service can only ingest via ILP, cannot queryGRANT ILP TO ingest_service;-- Full interactive accessGRANT HTTP, PGWIRE TO developer;\n```\n\nThese dimensions are independent: a user might have\nSELECT\non all tables but\nonly be allowed to connect via\nPGWIRE\n, or have\nINSERT\npermission but only\nvia\nILP\n.\n\n### Column-level access‚Äã\n\nRestrict users to see only certain columns:\n\n```questdb-sql\n-- User can only see timestamp and price, not quantity or trader_idGRANT SELECT ON trades(ts, price) TO analyst;\n```\n\n\n### Row-level access with views‚Äã\n\nFor row-level security, create a\nview\nthat filters rows,\nthen grant access to the view instead of the underlying table:\n\n```questdb-sql\n-- Create a view that only shows AAPL tradesCREATE VIEW aapl_trades AS (  SELECT * FROM trades WHERE symbol = 'AAPL');-- Grant access to the view, not the base tableGRANT SELECT ON aapl_trades TO aapl_analyst;-- No GRANT on trades table = user cannot see other symbols\n```\n\nThe user\naapl_analyst\ncan only see AAPL trades. They have no access to the\nunderlying\ntrades\ntable.\n\n## Common scenarios‚Äã\n\n\n### Read-only analyst‚Äã\n\nA user who can query data but cannot modify anything:\n\n```questdb-sql\nCREATE USER analyst WITH PASSWORD 'pwd';GRANT HTTP, PGWIRE TO analyst;GRANT SELECT ON ALL TABLES TO analyst;\n```\n\n\n### Application service account‚Äã\n\nA service account for an application that ingests data into specific tables:\n\n```questdb-sql\nCREATE SERVICE ACCOUNT ingest_app WITH PASSWORD 'pwd';GRANT ILP TO ingest_app;                    -- InfluxDB Line Protocol accessGRANT INSERT ON sensor_data TO ingest_app;  -- Can only insert into sensor_data\n```\n\n\n### Team-based access with groups‚Äã\n\nMultiple users sharing the same permissions:\n\n```questdb-sql\n-- Create a groupCREATE GROUP trading_team;-- Grant permissions to the groupGRANT HTTP, PGWIRE TO trading_team;GRANT SELECT ON trades, positions TO trading_team;GRANT INSERT ON trades TO trading_team;-- Add users to the group - they inherit all permissionsCREATE USER alice WITH PASSWORD 'pwd1';CREATE USER bob WITH PASSWORD 'pwd2';ADD USER alice TO trading_team;ADD USER bob TO trading_team;\n```\n\n\n### Column-level restrictions (hide sensitive data)‚Äã\n\nAllow access to a table but hide sensitive columns:\n\n```questdb-sql\nCREATE USER auditor WITH PASSWORD 'pwd';GRANT HTTP, PGWIRE TO auditor;-- Grant access to non-sensitive columns onlyGRANT SELECT ON employees(id, name, department, hire_date) TO auditor;-- Columns salary and ssn are not granted = invisible to auditor\n```\n\n\n### Row-level security (multi-tenant)‚Äã\n\nDifferent users see different subsets of data:\n\n```questdb-sql\n-- Base table has data for all regionsCREATE TABLE sales (ts TIMESTAMP, region SYMBOL, amount DOUBLE) TIMESTAMP(ts);-- Create region-specific viewsCREATE VIEW sales_emea AS (SELECT * FROM sales WHERE region = 'EMEA');CREATE VIEW sales_apac AS (SELECT * FROM sales WHERE region = 'APAC');-- Grant users access to their region onlyCREATE USER emea_manager WITH PASSWORD 'pwd';GRANT HTTP, PGWIRE TO emea_manager;GRANT SELECT ON sales_emea TO emea_manager;CREATE USER apac_manager WITH PASSWORD 'pwd';GRANT HTTP, PGWIRE TO apac_manager;GRANT SELECT ON sales_apac TO apac_manager;\n```\n\n\n### Database administrator‚Äã\n\nA user with full control (but not the built-in admin):\n\n```questdb-sql\nCREATE USER dba WITH PASSWORD 'pwd';GRANT DATABASE ADMIN TO dba;\n```\n\nwarning\nDATABASE ADMIN\ngrants all current and future permissions. Use sparingly.\n\n## Core concepts‚Äã\n\nUsers, service accounts and groups\n\n### Users and service accounts‚Äã\n\nQuestDB has two types of principals:\n- Users: For human individuals. Can belong to multiple groups and inherit\npermissions from them. Cannot be assumed by others.\n- Service accounts: For applications. Cannot belong to groups - all\npermissions must be granted directly. Can be assumed by authorized users for\ntesting.\n\n```questdb-sql\nCREATE USER human_user WITH PASSWORD 'pwd';CREATE SERVICE ACCOUNT app_account WITH PASSWORD 'pwd';\n```\n\nNames must be unique across all users, service accounts, and groups.\n\n#### Why service accounts?‚Äã\n\nService accounts provide\nclean, testable application access\n:\n\n| Aspect | User | Service Account |\n| --- | --- | --- |\n| Permission source | Direct + inherited from groups | Direct only |\n| Can belong to groups | Yes | No |\n| Can be assumed (SU) | No | Yes |\n| Typical use | Human individuals | Applications, services |\n\nBecause service accounts have no inherited permissions, their access is fully\nexplicit and predictable. Combined with the ability to assume them, this makes\nit easy to verify exactly what an application can and cannot do:\n\n```questdb-sql\n-- Create service account with specific permissionsCREATE SERVICE ACCOUNT trading_app WITH PASSWORD 'pwd';GRANT ILP TO trading_app;GRANT INSERT ON trades TO trading_app;GRANT SELECT ON positions TO trading_app;-- Developer can assume the service account to test its accessGRANT ASSUME SERVICE ACCOUNT trading_app TO developer;-- Developer switches to service account contextASSUME SERVICE ACCOUNT trading_app;-- Now operating with trading_app's exact permissions-- Test what works and what doesn't...EXIT SERVICE ACCOUNT;\n```\n\nThis makes service accounts ideal for applications where you need predictable,\nauditable, and testable access control.\n\n### Groups‚Äã\n\nGroups simplify permission management when multiple users need the same access:\n\n```questdb-sql\nCREATE GROUP analysts;GRANT SELECT ON ALL TABLES TO analysts;-- All users added to this group can read all tablesADD USER alice TO analysts;ADD USER bob TO analysts;\n```\n\nUsers inherit permissions from their groups. Inherited permissions cannot be\nrevoked directly from the user - revoke from the group instead. When a group is\ndropped, all members lose the permissions they inherited from that group.\n\n### Authentication methods‚Äã\n\nAuthentication and authorization flow\nQuestDB supports three authentication methods:\n\n| Method | Use case | Endpoints |\n| --- | --- | --- |\n| Password | Interactive users | REST API, PostgreSQL Wire |\n| JWK Token | ILP ingestion | InfluxDB Line Protocol |\n| REST API Token | Programmatic REST access | REST API |\n\nUsers can have multiple authentication methods enabled simultaneously:\n\n```questdb-sql\n-- Add JWK token for ILP accessALTER USER sensor_writer CREATE TOKEN TYPE JWK;-- Add REST API token (with 30-day expiry)ALTER USER api_user CREATE TOKEN TYPE REST WITH TTL '30d';\n```\n\nwarning\nQuestDB does not store private keys or tokens after creation. Save them\nimmediately - they cannot be recovered.\ntip\nAuthentication should happen via a\nsecure TLS connection\nto protect credentials in transit.\n\n### Endpoint permissions‚Äã\n\nBefore a user can connect, they need endpoint permissions:\n\n| Permission | Allows access to |\n| --- | --- |\n| HTTP | REST API, Web Console, ILP over HTTP |\n| PGWIRE | PostgreSQL Wire Protocol (port 8812) |\n| ILP | InfluxDB Line Protocol TCP (port 9009) |\n\n\n```questdb-sql\n-- Typical setup for an interactive userGRANT HTTP, PGWIRE TO analyst;-- Typical setup for an ingestion serviceGRANT ILP TO ingest_service;\n```\n\n\n### Built-in admin‚Äã\n\nEvery QuestDB instance starts with a built-in admin account:\n- Default username:admin\n- Default password:quest\nChange these immediately in production\nvia\nserver.conf\n:\n\n```ini\nacl.admin.user=your_admin_nameacl.admin.password=your_secure_password\n```\n\nThe built-in admin has irrevocable root access. After creating other admin\nusers, disable it:\n\n```ini\nacl.admin.user.enabled=false\n```\n\n\n## Permission levels‚Äã\n\nPermissions have different granularities determining where they can be applied:\n\n| Granularity | Can be granted at |\n| --- | --- |\n| Database | Database only |\n| Table | Database or specific tables |\n| Column | Database, tables, or specific columns |\n\nExamples:\n\n```questdb-sql\n-- Database-level: applies to all tablesGRANT SELECT ON ALL TABLES TO user;-- Table-level: applies to specific tablesGRANT SELECT ON trades, prices TO user;-- Column-level: applies to specific columnsGRANT SELECT ON trades(ts, symbol, price) TO user;\n```\n\n\n### The GRANT option‚Äã\n\nWhen granting permissions, you can allow the recipient to grant that permission\nto others:\n\n```questdb-sql\nGRANT SELECT ON trades TO team_lead WITH GRANT OPTION;-- team_lead can now grant SELECT on trades to others\n```\n\n\n### Owner permissions‚Äã\n\nWhen a user creates a table, they automatically receive all permissions on it\nwith the GRANT option. This ownership does not persist - if revoked, they cannot\nget it back without someone re-granting it.\n\n## Advanced topics‚Äã\n\n\n### Permission re-adjustment‚Äã\n\nDatabase-level permissions include access to future tables. If you revoke access\nto one table, QuestDB automatically converts the database-level grant to\nindividual table-level grants:\n\n```questdb-sql\nGRANT SELECT ON ALL TABLES TO user;  -- Database levelREVOKE SELECT ON secret_table FROM user;-- Result: user now has table-level SELECT on all tables EXCEPT secret_table-- Future tables will NOT be accessible\n```\n\nThe same applies from table to column level:\n\n```questdb-sql\nGRANT SELECT ON trades TO user;           -- Table levelREVOKE SELECT ON trades(ssn) FROM user;   -- Revoke one column-- Result: user has column-level SELECT on all columns EXCEPT ssn-- Future columns will NOT be accessible\n```\n\nnote\nWhen dropping a table, permissions on it are preserved by default (useful if\nthe table is recreated). Use\nDROP TABLE ... CASCADE PERMISSIONS\nto also\nremove all associated permissions.\n\n### Implicit timestamp permissions‚Äã\n\nIf a user has SELECT or UPDATE on any column of a table, they automatically get\nthe same permission on the designated timestamp column. This ensures time-series\noperations (SAMPLE BY, LATEST ON, etc.) work correctly.\n\n### Granting on non-existent objects‚Äã\n\nYou can grant permissions on tables/columns that don't exist yet:\n\n```questdb-sql\nGRANT INSERT ON future_table TO app;-- Permission activates when future_table is created\n```\n\nUse\nWITH VERIFICATION\nto catch typos:\n\n```questdb-sql\nGRANT SELECT ON trdaes TO user WITH VERIFICATION;-- Fails immediately because 'trdaes' doesn't exist\n```\n\n\n### Service account assumption‚Äã\n\nUsers can temporarily assume a service account's permissions for debugging:\n\n```questdb-sql\n-- Grant ability to assumeGRANT ASSUME SERVICE ACCOUNT ingest_app TO developer;-- Developer can now switch contextASSUME SERVICE ACCOUNT ingest_app;-- ... debug with app's permissions ...EXIT SERVICE ACCOUNT;\n```\n\n\n## User management reference‚Äã\n\n\n### Creating and removing principals‚Äã\n\n\n```questdb-sql\n-- UsersCREATE USER username WITH PASSWORD 'pwd';DROP USER username;-- Service accountsCREATE SERVICE ACCOUNT appname WITH PASSWORD 'pwd';DROP SERVICE ACCOUNT appname;-- GroupsCREATE GROUP groupname;DROP GROUP groupname;\n```\n\n\n### Managing group membership‚Äã\n\n\n```questdb-sql\nADD USER username TO group1, group2;REMOVE USER username FROM group1;\n```\n\n\n### Managing authentication‚Äã\n\n\n```questdb-sql\n-- Change passwordALTER USER username WITH PASSWORD 'new_pwd';-- Remove password (disables password auth)ALTER USER username WITH NO PASSWORD;-- Create tokensALTER USER username CREATE TOKEN TYPE JWK;ALTER USER username CREATE TOKEN TYPE REST WITH TTL '30d';ALTER USER username CREATE TOKEN TYPE REST WITH TTL '1d' REFRESH;  -- Auto-refresh-- Remove tokensALTER USER username DROP TOKEN TYPE JWK;ALTER USER username DROP TOKEN TYPE REST;  -- Drops all REST tokensALTER USER username DROP TOKEN TYPE REST 'token_value_here';  -- Drop specific token\n```\n\nRemoving all authentication methods (password and tokens) effectively disables\nthe user - they can no longer connect to the database.\n\n### Viewing information‚Äã\n\n\n```questdb-sql\nSHOW USERS;                    -- List all usersSHOW SERVICE ACCOUNTS;         -- List all service accountsSHOW GROUPS;                   -- List all groupsSHOW GROUPS username;          -- List groups for a userSHOW USER username;            -- Show auth methods for userSHOW PERMISSIONS username;     -- Show permissions for user\n```\n\nExample output from\nSHOW USER\n:\n\n```questdb-sql\nauth_type    enabled---------    -------Password     trueJWK Token    falseREST Token   true\n```\n\nnote\nViewing other users' information requires\nLIST USERS\n(to list all) or\nUSER DETAILS\n(to see details) permissions. Users can always view their own\ninformation without these permissions.\n\n## Permissions reference‚Äã\n\nUse\nall_permissions()\nto see all available permissions:\n\n```questdb-sql\nSELECT * FROM all_permissions();\n```\n\nFull permissions table (click to expand)Database permissions‚ÄãPermissionLevelDescriptionADD COLUMNDatabase | TableAdd columns to tablesADD INDEXDatabase | Table | ColumnAdd index on symbol columnsALTER COLUMN CACHEDatabase | Table | ColumnEnable/disable symbol cachingALTER COLUMN TYPEDatabase | Table | ColumnChange column typesATTACH PARTITIONDatabase | TableAttach partitionsBACKUP DATABASEDatabaseCreate database backupsBACKUP TABLEDatabase | TableCreate table backupsCANCEL ANY COPYDatabaseCancel COPY operationsCREATE TABLEDatabaseCreate tablesCREATE MATERIALIZED VIEWDatabaseCreate materialized viewsDEDUP ENABLEDatabase | TableEnable deduplicationDEDUP DISABLEDatabase | TableDisable deduplicationDETACH PARTITIONDatabase | TableDetach partitionsDROP COLUMNDatabase | Table | ColumnDrop columnsDROP INDEXDatabase | Table | ColumnDrop indexesDROP PARTITIONDatabase | TableDrop partitionsDROP TABLEDatabase | TableDrop tablesDROP MATERIALIZED VIEWDatabase | TableDrop materialized viewsINSERTDatabase | TableInsert dataREFRESH MATERIALIZED VIEWDatabase | TableRefresh materialized viewsREINDEXDatabase | Table | ColumnReindex columnsRENAME COLUMNDatabase | Table | ColumnRename columnsRENAME TABLEDatabase | TableRename tablesRESUME WALDatabase | TableResume WAL processingSELECTDatabase | Table | ColumnRead dataSET TABLE PARAMDatabase | TableSet table parametersSET TABLE TYPEDatabase | TableChange table typeSETTINGSDatabaseChange instance settings in Web ConsoleSNAPSHOTDatabaseCreate snapshotsSQL ENGINE ADMINDatabaseList/cancel running queriesSYSTEM ADMINDatabaseSystem functions (reload_tls, etc.)TRUNCATE TABLEDatabase | TableTruncate tablesUPDATEDatabase | Table | ColumnUpdate dataVACUUM TABLEDatabase | TableReclaim storageUser management permissions‚ÄãPermissionDescriptionADD EXTERNAL ALIASCreate external group mappingsADD PASSWORDSet user passwordsADD USERAdd users to groupsCREATE GROUPCreate groupsCREATE JWKCreate JWK tokensCREATE REST TOKENCreate REST API tokensCREATE SERVICE ACCOUNTCreate service accountsCREATE USERCreate usersDISABLE USERDisable usersDROP GROUPDrop groupsDROP JWKDrop JWK tokensDROP REST TOKENDrop REST API tokensDROP SERVICE ACCOUNTDrop service accountsDROP USERDrop usersENABLE USEREnable usersLIST USERSList users/groups/service accountsREMOVE EXTERNAL ALIASRemove external group mappingsREMOVE PASSWORDRemove passwordsREMOVE USERRemove users from groupsUSER DETAILSView user/group/service account detailsSpecial permissions‚ÄãPermissionDescriptionALLAll permissions at the granted level (database/table/column)DATABASE ADMINAll permissions including future ones; can assume any service account\n\n## SQL commands reference‚Äã\n\n- ADD USER\n- ALTER USER\n- ALTER SERVICE ACCOUNT\n- ASSUME SERVICE ACCOUNT\n- CREATE GROUP\n- CREATE SERVICE ACCOUNT\n- CREATE USER\n- DROP GROUP\n- DROP SERVICE ACCOUNT\n- DROP USER\n- EXIT SERVICE ACCOUNT\n- GRANT\n- GRANT ASSUME SERVICE ACCOUNT\n- REMOVE USER\n- REVOKE\n- REVOKE ASSUME SERVICE ACCOUNT\n- SHOW USER\n- SHOW USERS\n- SHOW GROUPS\n- SHOW SERVICE ACCOUNT\n- SHOW SERVICE ACCOUNTS\n- SHOW PERMISSIONS",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2421,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-0fb78ed6c8ed",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/symbol",
    "title": "Symbol | QuestDB",
    "text": "SYMBOL\nis a data type designed for columns with repetitive string values.\nInternally, symbols use dictionary encoding‚Äîeach unique string is stored once\nin a lookup table, and rows store integer references to that table. This is\nthe same approach used by columnar formats like Parquet and Arrow. The result\nis much faster filtering and grouping compared to regular strings.\n\n## When to use SYMBOL‚Äã\n\nUse\nSYMBOL\nfor categorical data with a limited set of repeated values:\n- Stock tickers (AAPL,GOOGL,MSFT)\n- Country or region codes (US,EU,APAC)\n- Status values (pending,completed,failed)\n- Device or sensor IDs\n- Any column frequently used inWHEREorGROUP BY\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL,        -- Good: limited set of tickers    side SYMBOL,          -- Good: just BUY/SELL    price DOUBLE,    quantity DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\n\n## When to use VARCHAR instead‚Äã\n\nUse\nVARCHAR\nwhen values are unique or very high cardinality:\n- User-generated text (comments, descriptions)\n- Log messages\n- UUIDs or unique identifiers (consider theUUIDtype instead)\n- Columns with hundreds of millions of distinct values\n\n## Why SYMBOL is fast‚Äã\n\n\n| Operation | VARCHAR | SYMBOL |\n| --- | --- | --- |\n| Storage | Full string per row | Integer + shared dictionary |\n| Filtering (WHERE symbol = 'X') | String comparison | Integer comparison |\n| Grouping (GROUP BY) | String hashing | Integer grouping |\n| Disk usage | Higher | Lower |\n\nSymbols provide:\n- Faster queries‚Äî integer comparisons instead of string operations\n- Lower storage‚Äî strings stored once in a dictionary, rows store integers\n- Index support‚Äî symbol columns can be indexed for even faster lookups\n\n## Creating SYMBOL columns‚Äã\n\n\n```questdb-sql\nCREATE TABLE orders (    timestamp TIMESTAMP,    symbol SYMBOL,    side SYMBOL,    order_type SYMBOL,    price DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nSymbol capacity scales automatically as new values are added. No manual\nconfiguration is needed.\nNote for users upgrading from versions before 9.0.0Prior to QuestDB 9.0.0, symbol capacity required manual configuration. You had\nto estimate the number of distinct values upfront and set the capacity\nexplicitly. Undersizing caused performance issues; oversizing wasted memory.From 9.0.0 onwards, symbol capacity is fully automatic. TheCAPACITYsetting\nis now obsolete and can be removed from your table definitions.\n\n## NOCACHE option‚Äã\n\nBy default, QuestDB caches the symbol dictionary in memory for fast lookups.\nFor columns with very high cardinality (10 million+ distinct values), this\ncache can consume significant memory.\nUse\nNOCACHE\nto disable dictionary caching:\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    client_id SYMBOL NOCACHE,    symbol SYMBOL) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nTrade-off:\nNOCACHE\nreduces memory usage but makes dictionary lookups\nslower. Only use it for symbols with millions of distinct values where memory\nis a concern.\nTo toggle caching on an existing column:\n\n```questdb-sql\n-- Disable cacheALTER TABLE trades ALTER COLUMN client_id NOCACHE;-- Re-enable cacheALTER TABLE trades ALTER COLUMN client_id CACHE;\n```\n\n\n## Indexing symbols‚Äã\n\nFor columns frequently used in\nWHERE\nclauses, add an index:\n\n```questdb-sql\nCREATE TABLE trades (    timestamp TIMESTAMP,    symbol SYMBOL INDEX,    price DOUBLE) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nOr add an index later:\n\n```questdb-sql\nALTER TABLE trades ALTER COLUMN symbol ADD INDEX;\n```\n\nSee\nIndexes\nfor more information.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 522,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-a258aa1da6c7",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/partitions",
    "title": "Time Partitions | QuestDB",
    "text": "QuestDB partitions tables by time intervals, storing each interval's data in a\nseparate directory. This physical separation is fundamental to time-series\nperformance - it allows the database to skip irrelevant time ranges entirely\nduring queries and enables efficient data lifecycle management.\n\n## Why partition‚Äã\n\nPartitioning provides significant benefits for time-series workloads:\n- Query performance: The SQL optimizer skips partitions outside your query's\ntime range. A query for \"last hour\" on a table with years of data reads only\none partition, not the entire table.\n- Data lifecycle: Drop old data instantly withDROP PARTITION- no expensive\nDELETE operations. Detach partitions to cold storage, reattach when needed.\n- Write efficiency: Out-of-order data only rewrites affected partitions, not\nthe entire table. Smaller partitions mean less write amplification.\n- Concurrent access: Different partitions can be written and read\nsimultaneously without contention.\n\n## How partitions work‚Äã\n\nPartitioning requires a\ndesignated timestamp\ncolumn. QuestDB uses this timestamp to determine which partition stores each row.\nEach partition is a directory on disk named by its time interval. Inside, each\ncolumn is stored as a separate file (\n.d\nfor data, plus index files for\nSYMBOL\ncolumns).\n\n## Choosing a partition interval‚Äã\n\nAvailable intervals:\nHOUR\n,\nDAY\n,\nWEEK\n,\nMONTH\n,\nYEAR\n, or\nNONE\n.\nTarget 30-80 million rows per partition\nfor tables with average-sized rows.\nTables with many columns should aim for the lower end; tables with few columns can\ngo higher.\nChoose your interval based on how much data you ingest:\n\n| Your data volume | Recommended interval |\n| --- | --- |\n| >1 billion rows/day | HOUR |\n| 30-500 million rows/day | DAY |\n| 5-30 million rows/day | WEEK |\n| 1-5 million rows/day | MONTH |\n| <1 million rows/day | YEAR |\n\nWhy this matters:\n- Too many small partitionsincreases syscall overhead. Each partition is a\ndirectory, and operations like queries and compaction must interact with many\nfilesystem objects.\n- Too few large partitionscan hurt out-of-order write performance. When late\ndata arrives, QuestDB may need to rewrite portions of the partition. Smaller\npartitions limit how much data gets rewritten in worst-case scenarios.\nOther considerations:\n- Match your most common query patterns (if you typically query by day,DAYpartitions align well)\n- You can change partitioning later, but it requires recreating the table\nFor ILP (InfluxDB Line Protocol) ingestion, the default is\nDAY\n. Change it via\nline.default.partition.by\nin\nserver.conf\n.\n\n## Creating partitioned tables‚Äã\n\nSpecify partitioning at table creation:\n\n```questdb-sql\nCREATE TABLE trades (    ts TIMESTAMP,    symbol SYMBOL,    price DOUBLE,    amount DOUBLE) TIMESTAMP(ts) PARTITION BY DAY;\n```\n\n\n### Default behavior by creation method‚Äã\n\n\n| Creation method | Default partition |\n| --- | --- |\n| SQLCREATE TABLE(noPARTITION BY) | NONE |\n| SQLCREATE TABLE(withPARTITION BY) | As specified |\n| ILP auto-created tables | DAY |\n\n\n### Partition directory naming‚Äã\n\n\n| Interval | Directory format | Example |\n| --- | --- | --- |\n| HOUR | YYYY-MM-DDTHH | 2026-01-15T09 |\n| DAY | YYYY-MM-DD | 2026-01-15 |\n| WEEK | YYYY-Www | 2026-W03 |\n| MONTH | YYYY-MM | 2026-01 |\n| YEAR | YYYY | 2026 |\n\n\n## Inspecting partitions‚Äã\n\nUse\nSHOW PARTITIONS\nor the\ntable_partitions()\nfunction:\n\n```questdb-sql\nSHOW PARTITIONS FROM trades;\n```\n\n\n| index | partitionBy | name | minTimestamp | maxTimestamp | numRows | diskSizeHuman |\n| --- | --- | --- | --- | --- | --- | --- |\n| 0 | DAY | 2026-01-15 | 2026-01-15T00:00:00Z | 2026-01-15T23:59:59Z | 1440000 | 68.0 MiB |\n| 1 | DAY | 2026-01-16 | 2026-01-16T00:00:00Z | 2026-01-16T12:30:00Z | 750000 | 35.2 MiB |\n\nThe\ntable_partitions()\nfunction returns the same data and can be used in\nqueries with\nWHERE\n,\nJOIN\n, or\nUNION\n:\n\n```questdb-sql\nSELECT name, numRows, diskSizeHumanFROM table_partitions('trades')WHERE numRows > 1000000;\n```\n\n\n## Storage on disk‚Äã\n\nA partitioned table's directory structure:\n\n```questdb-sql\ndb/trades/‚îú‚îÄ‚îÄ 2026-01-15/           # Partition directory‚îÇ   ‚îú‚îÄ‚îÄ ts.d              # Timestamp column data‚îÇ   ‚îú‚îÄ‚îÄ symbol.d          # Symbol column data‚îÇ   ‚îú‚îÄ‚îÄ symbol.k          # Symbol column index‚îÇ   ‚îú‚îÄ‚îÄ symbol.v          # Symbol column values‚îÇ   ‚îú‚îÄ‚îÄ price.d           # Price column data‚îÇ   ‚îî‚îÄ‚îÄ amount.d          # Amount column data‚îú‚îÄ‚îÄ 2026-01-16/‚îÇ   ‚îú‚îÄ‚îÄ ts.d‚îÇ   ‚îú‚îÄ‚îÄ ...‚îî‚îÄ‚îÄ _txn                  # Transaction metadata\n```\n\n\n## Partition splitting and squashing‚Äã\n\nWhen out-of-order data arrives for an existing partition, QuestDB may split that\npartition to avoid rewriting all its data. This is an optimization for write\nperformance.\nA split occurs when:\n- The existing partition prefix is larger than the new data plus suffix\n- The prefix exceedscairo.o3.partition.split.min.size(default: 50MB)\nSplit partitions appear with timestamp suffixes in\nSHOW PARTITIONS\n:\n\n| name | numRows |\n| --- | --- |\n| 2026-01-15 | 1259999 |\n| 2026-01-15T205959-880001 | 60002 |\n\nQuestDB automatically squashes splits:\n- Non-active partitions: squashed at end of each commit\n- Active (latest) partition: squashed when splits exceedcairo.o3.last.partition.max.splits(default: 20)\nTo manually squash all splits:\n\n```questdb-sql\nALTER TABLE trades SQUASH PARTITIONS;\n```\n\nPartition operations (\nATTACH\n,\nDETACH\n,\nDROP\n) treat all splits of a\npartition as a single unit.\n\n## See also‚Äã\n\n- Designated timestamp‚Äî Required for partitioning\n- DROP PARTITION‚Äî Remove old partitions\n- DETACH PARTITION‚Äî Move to cold storage\n- ATTACH PARTITION‚Äî Restore detached data\n- TTL‚Äî Automatic partition cleanup by age",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 864,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-165186c08447",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/concepts/materialized-views",
    "title": "Materialized views | QuestDB",
    "text": "A materialized view is a special QuestDB table that stores the pre-computed\nresults of a query. Unlike\nregular views\n, which compute\ntheir results at query time, materialized views persist their data to disk,\nmaking them particularly efficient for expensive aggregate queries that are run\nfrequently.\n\n## What are materialized views for?‚Äã\n\nLet's say your application ingests trade data into a table like this:\ntrades table\n\n```questdb-sql\nCREATE TABLE trades (  symbol SYMBOL,  side SYMBOL,  price DOUBLE,  amount DOUBLE,  timestamp TIMESTAMP) TIMESTAMP(timestamp) PARTITION BY DAY;\n```\n\nAs your QuestDB instance grows from gigabytes to terabytes, aggregation queries\nbecome a bottleneck. A common pattern is using\nSAMPLE BY\nto bucket data by\ntime - for example, calculating notional value (price √ó amount) by the minute:\nSAMPLE BY query\nDemo this query\n\n```questdb-sql\nSELECT  timestamp,  symbol,  side,  sum(price * amount) AS notionalFROM tradesWHERE timestamp IN today()SAMPLE BY 1m;\n```\n\nThanks to partition pruning, this query only scans today's data. But even so,\naggregating millions of rows takes time - and dashboards or applications may run\nthis query repeatedly.\nMaterialized views solve this by pre-computing and storing the aggregated\nresults. When new data arrives, only the new rows are processed incrementally.\nQuerying the materialized view becomes a simple lookup rather than a\nre-aggregation, making dashboard refreshes near-instant.\nWhen you create a materialized view you register your time-based grouping\nquery with the QuestDB database against a base table.\nConceptually a materialized view is an on-disk table tied to a query:\nAs you add new data to the base table, the materialized view will efficiently\nupdate itself. You can then query the materialized view as a regular table\nwithout the impact of a full table scan of the base table.\n\n## Quick example‚Äã\n\nCreate a materialized view that calculates 15-minute OHLC bars:\nCreate a materialized view\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_ohlc_15m ASSELECT  timestamp,  symbol,  first(price) AS open,  max(price) AS high,  min(price) AS low,  last(price) AS close,  sum(amount) AS volumeFROM tradesSAMPLE BY 15m;\n```\n\nQuery it like any table:\nQuery the materialized view\nDemo this query\n\n```questdb-sql\nSELECT * FROM trades_ohlc_15mWHERE timestamp IN today();\n```\n\nThat's it. The view refreshes incrementally as new data arrives in\ntrades\n.\nDetails on customization and options follow below.\n\n## When to use materialized views‚Äã\n\nMaterialized views are ideal for:\n- Heavy aggregations over large datasets: Queries that scan millions of rows\n- Frequently accessed summaries: Dashboard queries that run repeatedly\n- Historical summaries: Data that doesn't need real-time accuracy\n- OHLC calculations: Candlestick charts, time-bucketed analytics\nUse regular\nviews\ninstead when:\n- Query execution cost is acceptable for your workload\n- You need parameterized queries withDECLARE\n- You need patterns not supported by materialized views (e.g., data enrichment)\n- Storage cost is a concern (materialized views consume disk space)\nThe key tradeoff: views execute the full query each time (multi-threaded, can\nbe resource-intensive), while materialized views pre-compute results so queries\nbecome simple lookups. For dashboards with many concurrent users, running\nparallel aggregations doesn't scale - materialized views reduce this to O(1)\nreads on a smaller, pre-aggregated dataset.\n\n### Not suited for: data enrichment‚Äã\n\nMaterialized views support JOINs, but\nSAMPLE BY\n(aggregation) is mandatory.\nThis means you can enrich aggregated results with data from other tables, but\nyou cannot keep raw (non-aggregated) rows while adding enrichment columns.\nFor example, joining aggregated trades with instrument metadata works:\nSupported: aggregation with JOIN\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_with_metadata ASSELECT  t.timestamp,  t.symbol,  m.description,  sum(t.amount) AS volumeFROM trades tJOIN instruments m ON t.symbol = m.symbolSAMPLE BY 1h;\n```\n\nBut this pattern does not work:\nNot supported: enrichment without aggregation\n\n```questdb-sql\n-- Users try this but it won't workCREATE MATERIALIZED VIEW enriched_trades ASSELECT  t.timestamp,  t.symbol,  t.price,  t.amount,  h.hourly_vwap    -- aggregated value from another tableFROM trades tASOF JOIN hourly_stats h ON t.symbol = h.symbol;\n```\n\nThe view cannot maintain a 1:1 row mapping with the base table.\nAlso note: only changes to the base table (the one in\nSAMPLE BY\n) trigger a\nrefresh. Changes to joined tables do not trigger updates.\nComing soon\n: We are actively developing a new type of materialized view that\nwill support data enrichment use cases. Stay tuned for updates.\n\n## Creating a materialized view‚Äã\n\n\n### Basic syntax‚Äã\n\nThe simplest form requires only a\nSAMPLE BY\nquery:\nBasic materialized view\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_hourly ASSELECT  timestamp,  symbol,  avg(price) AS avg_price,  sum(amount) AS volumeFROM tradesSAMPLE BY 1h;\n```\n\nFor full syntax, see\nCREATE MATERIALIZED VIEW\n.\n\n### Extended syntax‚Äã\n\nFor more control, use the extended syntax with parentheses:\nExtended syntax\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_ohlc_15mWITH BASE trades REFRESH IMMEDIATE AS (  SELECT    timestamp,    symbol,    first(price) AS open,    max(price) AS high,    min(price) AS low,    last(price) AS close,    sum(amount) AS volume  FROM trades  SAMPLE BY 15m) PARTITION BY MONTH;\n```\n\nThis allows specifying:\n- WITH BASE: Explicit base table (required for JOINs)\n- REFRESH: Refresh strategy\n- PARTITION BY: Partitioning scheme\n- TTL: Data retention policy\n\n### Naming conventions‚Äã\n\nWe recommend naming views with reference to the base table, purpose, and sample\ninterval:\n- trades_ohlc_15m- trades table, OHLC purpose, 15-minute buckets\n- sensors_avg_1h- sensors table, averages, hourly buckets\n\n### The query‚Äã\n\nMaterialized views require a\nSAMPLE BY\nor time-based\nGROUP BY\nquery.\nSupported:\n- Aggregate functions:sum,avg,min,max,first,last,count\n- JOINwith other tables (only the base table triggers refresh)\n- WHEREclauses\nNot supported:\n- FILLclause\n- FROM-TOclause\n- ALIGN TO FIRST OBSERVATION\n- Non-deterministic functions likenow()orrnd_uuid4()\nKeep queries simple. Move complex transformations to queries that run on the\nmaterialized view.\n\n### Refresh strategies‚Äã\n\n\n#### IMMEDIATE (default)‚Äã\n\nIncrementally updates the view when new data is inserted into the base table:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW my_viewREFRESH IMMEDIATE ASSELECT ... FROM base_table SAMPLE BY 1h;\n```\n\nThis is the recommended strategy for most use cases. Only new data is processed,\nminimizing write overhead.\n\n#### MANUAL‚Äã\n\nRequires explicit refresh via SQL:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW my_viewREFRESH MANUAL ASSELECT ... FROM base_table SAMPLE BY 1h;\n```\n\nRefresh manually with:\n\n```questdb-sql\nREFRESH MATERIALIZED VIEW my_view;\n```\n\n\n#### EVERY interval‚Äã\n\nRefreshes on a timer:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW my_viewREFRESH EVERY 5m ASSELECT ... FROM base_table SAMPLE BY 1h;\n```\n\n\n#### PERIOD refresh‚Äã\n\nFor data that arrives at fixed intervals (e.g., end-of-day prices):\nPeriod refresh\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_dailyREFRESH PERIOD (LENGTH 1d TIME ZONE 'Europe/London' DELAY 2h) ASSELECT  timestamp,  symbol,  avg(price) AS avg_priceFROM tradesSAMPLE BY 1d;\n```\n\nOr use compact syntax to match the\nSAMPLE BY\ninterval:\nPeriod refresh matching SAMPLE BY\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_dailyREFRESH PERIOD (SAMPLE BY INTERVAL) ASSELECT timestamp, symbol, avg(price) AS avg_priceFROM tradesSAMPLE BY 1d;\n```\n\nPeriod refresh reduces transaction overhead during intensive real-time\ningestion.\nChange refresh strategy anytime with\nALTER MATERIALIZED VIEW SET REFRESH\n.\n\n### Partitioning‚Äã\n\nSpecify a partitioning scheme larger than the sampling interval:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW my_view AS (  SELECT timestamp, symbol, sum(amount) AS total_amount FROM trades SAMPLE BY 8h) PARTITION BY DAY;\n```\n\nAn\n8h\nsample fits nicely with\nDAY\npartitioning (3 buckets per partition).\n\n#### Default partitioning‚Äã\n\nIf omitted, partitioning is inferred from\nSAMPLE BY\n:\n\n| Interval | Default partitioning |\n| --- | --- |\n| > 1 hour | PARTITION BY YEAR |\n| > 1 minute | PARTITION BY MONTH |\n| <= 1 minute | PARTITION BY DAY |\n\n\n### TTL (Time-To-Live)‚Äã\n\nLimit how much history the materialized view retains:\nMaterialized view with TTL\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_hourly AS (  SELECT timestamp, symbol, avg(price) AS avg_price  FROM trades  SAMPLE BY 1h) PARTITION BY WEEK TTL 8 WEEKS;\n```\n\nThe view's TTL is independent of the base table's TTL.\n\n### Initial refresh‚Äã\n\nWhen created, materialized views start an\nasynchronous full refresh\n:\n- CREATE MATERIALIZED VIEWreturns immediately\n- The view is queryable right away butreturns no datauntil refresh\ncompletes\n- For large base tables, this may take significant time\nCheck if the initial refresh is complete:\n\n```questdb-sql\nSELECT view_name, view_status, refresh_base_table_txn, base_table_txnFROM materialized_views()WHERE view_name = 'your_view';\n```\n\nWhen\nrefresh_base_table_txn\nequals\nbase_table_txn\n, the view is fully\npopulated.\nTo defer initial refresh, use\nDEFERRED\n:\n\n```questdb-sql\nCREATE MATERIALIZED VIEW my_viewREFRESH MANUAL DEFERRED ASSELECT ... FROM trades SAMPLE BY 1h;\n```\n\n\n## Querying materialized views‚Äã\n\nnote\nThe example\ntrades_ohlc_15m\nview is available on our\ndemo\n, and contains realtime crypto data - try it out!\nMaterialized views support\nall the same queries\nas regular QuestDB tables:\nQuery today's data\nDemo this query\n\n```questdb-sql\nSELECT * FROM trades_ohlc_15mWHERE timestamp IN today();\n```\n\n\n| timestamp | symbol | open | high | low | close | volume |\n| --- | --- | --- | --- | --- | --- | --- |\n| 2025-03-31T00:00:00.000000Z | ETH-USD | 1807.94 | 1813.32 | 1804.69 | 1808.58 | 1784.144071999995 |\n| 2025-03-31T00:00:00.000000Z | BTC-USD | 82398.4 | 82456.5 | 82177.6 | 82284.5 | 34.47331241 |\n| ... | ... | ... | ... | ... | ... | ... |\n\n\n### Performance comparison‚Äã\n\nWithout a materialized view, aggregating 1 month of data:\nDirect query - slow\nDemo this query\n\n```questdb-sql\nSELECT  timestamp, symbol,  first(price) AS open, max(price) AS high,  min(price) AS low, last(price) AS close,  sum(amount) AS volumeFROM tradesWHERE timestamp > dateadd('M', -1, now())SAMPLE BY 15m;\n```\n\nThis takes hundreds of milliseconds, scanning tens of millions of rows.\nWith the materialized view:\nMaterialized view - fast\nDemo this query\n\n```questdb-sql\nSELECT * FROM trades_ohlc_15mWHERE timestamp > dateadd('M', -1, now());\n```\n\nThis returns in single-digit milliseconds. The data is pre-aggregated, so no\naggregation work is needed at query time.\n\n## Managing materialized views‚Äã\n\n\n### Listing views‚Äã\n\nList all materialized views\nDemo this query\n\n```questdb-sql\nSELECT  view_name,  base_table_name,  view_status,  last_refresh_finish_timestampFROM materialized_views();\n```\n\n\n### Monitoring refresh status‚Äã\n\nCheck refresh lag\n\n```questdb-sql\nSELECT  view_name,  refresh_base_table_txn,  base_table_txn,  base_table_txn - refresh_base_table_txn AS lagFROM materialized_views();\n```\n\nWhen\nrefresh_base_table_txn\nequals\nbase_table_txn\n, the view is fully\nup-to-date.\n\n### View invalidation‚Äã\n\nMaterialized views become invalid when their base table schema or data is\nmodified in incompatible ways:\n- Dropping columns referenced by the view\n- Dropping partitions\n- Renaming the base table\n- TRUNCATEorUPDATEoperations\nCheck for invalid views:\nFind invalid views\n\n```questdb-sql\nSELECT view_name, view_status, invalidation_reasonFROM materialized_views()WHERE view_status = 'invalid';\n```\n\n\n### Refreshing an invalid view‚Äã\n\nTo restore an invalid view with a full refresh:\n\n```questdb-sql\nREFRESH MATERIALIZED VIEW view_name FULL;\n```\n\nThis deletes existing data and rebuilds from the base table. For large tables,\nthis may take significant time. Cancel with\nCANCEL QUERY\nif needed.\n\n## Advanced: LATEST ON optimization‚Äã\n\nLATEST ON\nqueries can be slow when some symbols are infrequently updated,\nrequiring scans across large amounts of data:\nSlow LATEST ON\nDemo this query\n\n```questdb-sql\nSELECT * FROM trades LATEST ON timestamp PARTITION BY symbol;\n```\n\nThis might scan billions of rows to find the latest entry for rarely-updated\nsymbols.\n\n### Solution: Pre-aggregate with a materialized view‚Äã\n\nCreate a view that stores one row per symbol per day:\nLATEST ON materialized view\n\n```questdb-sql\nCREATE MATERIALIZED VIEW trades_latest_1d ASSELECT  timestamp,  symbol,  side,  last(price) AS price,  last(amount) AS amount,  last(timestamp) AS latestFROM tradesSAMPLE BY 1d;\n```\n\nThen query the view:\nFast LATEST ON\nDemo this query\n\n```questdb-sql\nSELECT symbol, side, price, amount, latest AS timestampFROM (  trades_latest_1d  LATEST ON timestamp  PARTITION BY symbol, side)ORDER BY timestamp DESC;\n```\n\nResult\n: Seconds down to milliseconds - 100x to 1000x faster.\nInstead of scanning ~1.3 billion rows, the database scans ~25,000 pre-aggregated\nrows.\n\n## Technical reference‚Äã\n\n\n### Query constraints‚Äã\n\nMaterialized view queries:\n- Must useSAMPLE BYorGROUP BYwith a designated timestamp column\n- Must not useFROM-TO,FILL, orALIGN TO FIRST OBSERVATION\n- Must not use non-deterministic functions (now(),rnd_uuid4())\n- Must use join conditions compatible with incremental refresh\n- When the base table usesdeduplication, non-aggregate\ncolumns must be a subset of theDEDUPkeys\n\n### Base table relationship‚Äã\n\nEvery materialized view is tied to a base table:\n- For single-table queries, the base table is automatically determined\n- For JOINs, specify the base table withWITH BASE\nOnly inserts to the base table trigger\nIMMEDIATE\nrefresh. Changes to joined\ntables do not trigger refresh.\n\n### Storage model‚Äã\n\nMaterialized views use the same storage engine as regular tables:\n- Columnar storage\n- Partitioning\n- Independent TTL management\n\n### Refresh mechanism‚Äã\n\nIncremental refresh process:\n- New data is inserted into the base table\n- The time-range of new data is identified\n- Only affected time slices are recomputed\nThis happens asynchronously, minimizing write performance impact.\n\n## Enterprise features‚Äã\n\n\n### Replicated views‚Äã\n\nReplication of the base table is independent of materialized view maintenance.\nPromoting a replica to primary may trigger a full materialized view refresh if\nthe replica's view was not fully up-to-date.\n\n## Related documentation‚Äã\n\n- Related ConceptsViews: Virtual tables that compute results at query\ntime\n- SQL CommandsCREATE MATERIALIZED VIEW: Create a\nnew materialized viewDROP MATERIALIZED VIEW: Remove a\nmaterialized viewREFRESH MATERIALIZED VIEW:\nManually refresh a materialized viewALTER MATERIALIZED VIEW ADD INDEX:\nAdds an index to a materialized viewALTER MATERIALIZED VIEW DROP INDEX:\nRemoves an index from a materialized viewALTER MATERIALIZED VIEW RESUME WAL:\nResume WAL for a materialized viewALTER MATERIALIZED VIEW SET REFRESH:\nChanges a materialized view's refresh strategy and parametersALTER MATERIALIZED VIEW SET REFRESH LIMIT:\nSets the time limit for incremental refresh on a materialized viewALTER MATERIALIZED VIEW SET TTL:\nSets the time-to-live (TTL) period on a materialized view\n- ConfigurationMaterialized views configs:\nServer configuration options for materialized views fromserver.conf",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2182,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-13693c01775c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/datatypes/overview",
    "title": "Data Types Overview | QuestDB",
    "text": "\n| Type Name | Storage bits | Nullable | Description |\n| --- | --- | --- | --- |\n| boolean | 1 | No | Booleantrueorfalse. |\n| ipv4 | 32 | Yes | 0.0.0.1to255.255.255.255 |\n| byte | 8 | No | Signed integer,-128to127. |\n| short | 16 | No | Signed integer,-32,768to32,767. |\n| char | 16 | Yes | unicodecharacter. |\n| int | 32 | Yes | Signed integer,-2,147,483,648to2,147,483,647. |\n| float | 32 | Yes | Single precision IEEE 754 floating point value. |\n| symbol | 32 | Yes | A symbol, stored as a 32-bit signed index into the symbol table. Each index corresponds to astringvalue. The index is transparently translated to the string value. Symbol table is stored separately from the column data. |\n| varchar | 128 + utf8Len | Yes | Length-prefixed sequence of UTF-8 encoded characters, stored using a 128-bit header and UTF-8 encoded data. Sequences shorter than 9 bytes are fully inlined within the header and do not occupy any additional data space. |\n| string | 96+n*16 | Yes | Length-prefixed sequence of UTF-16 encoded characters whose length is stored as signed 32-bit integer with maximum value of0x7fffffff. |\n| long | 64 | Yes | Signed integer,-9,223,372,036,854,775,808to9,223,372,036,854,775,807. |\n| date | 64 | Yes | Signed offset inmillisecondsfromUnix Epoch. |\n| timestamp | 64 | Yes | Signed offset inmicrosecondsfromUnix Epoch. |\n| timestamp_ns | 64 | Yes | Signed offset innanosecondsfromUnix Epoch. |\n| double | 64 | Yes | Double precision IEEE 754 floating point value. |\n| uuid | 128 | Yes | UUIDvalues. See alsothe UUID type. |\n| binary | 64+n*8 | Yes | Length-prefixed sequence of bytes whose length is stored as signed 64-bit integer with maximum value of0x7fffffffffffffffL. |\n| long256 | 256 | Yes | Unsigned 256-bit integer. Does not support arithmetic operations, only equality checks. Suitable for storing a hash code, such as crypto public addresses. |\n| geohash(<size>) | 8-64 | Yes | Geohash with precision specified as a number followed bybfor bits,cfor chars. Seethe geohashes documentationfor details on use and storage. |\n| array | See description | Yes | Header: 20 + 4 *nDimsbytes. Payload: dense array of values. Example:DOUBLE[3][4]: header 28 bytes, payload 3*4*8 = 96 bytes. |\n| interval | 128 | Yes | Pair of timestamps representing a time interval. Not a persisted type: you can use it in expressions, but can't have a database column of this type. |\n| decimal(<precision>, <scale>) | 8-256 | Yes | Decimal floating point with user-specified precision and scale. |\n\n\n## N-dimensional array‚Äã\n\nIn addition to the scalar types above, QuestDB also supports\nN-dimensional arrays\n, currently only for the\nDOUBLE\ntype.\n\n## Decimal‚Äã\n\nThe\nDECIMAL\ntype provides exact decimal arithmetic with user-specified\nprecision and scale. Use it when floating point approximation is unacceptable,\nsuch as financial calculations.\nQuestDB's decimal is high-performance: only ~2x slower than double, faster than\nClickHouse and DuckDB decimals, and non-allocating during computations.\n\n```questdb-sql\nCREATE TABLE prices (    ts TIMESTAMP,    amount DECIMAL(18, 6)  -- 18 total digits, 6 after decimal point) TIMESTAMP(ts);\n```\n\nFor detailed information on precision, scale, storage, and arithmetic behavior,\nsee\nDecimal\n.\n\n## VARCHAR and STRING considerations‚Äã\n\nQuestDB supports two types for storing strings:\nVARCHAR\nand\nSTRING\n.\nMost users should use\nVARCHAR\n. It uses the UTF-8 encoding, whereas\nSTRING\nuses UTF-16, which is less space-efficient for strings containing mostly ASCII\ncharacters. QuestDB keeps supporting it only to maintain backward compatibility.\nAdditionally,\nVARCHAR\nincludes several optimizations for fast access and\nstorage.\n\n## TIMESTAMP and DATE considerations‚Äã\n\nWhile the\ndate\ntype is available, we highly recommend using the\ntimestamp\ninstead. The only material advantage of\ndate\nis a wider time range, but\ntimestamp\nis adequate in virtually all cases. It has microsecond resolution\n(vs. milliseconds for\ndate\n), and is fully supported by all date/time\nfunctions, while support for\ndate\nis limited. If nanosecond precision is\nrequired, we recommend using the\ntimestamp_ns\ndata type.\n\n## Limitations for variable-sized types‚Äã\n\nThe maximum size of a single\nVARCHAR\nfield is 268 MB, and the maximum total\nsize of a\nVARCHAR\ncolumn in a single partition is 218 TB.\nThe maximum size of a\nBINARY\nfield is defined by the limits of the 64-bit\nsigned integer (8,388,608 petabytes).\nThe maximum size of a\nSTRING\nfield is defined by the limits of the 32-bit\nsigned integer (1,073,741,824 characters).\nThe maximum number of dimensions an array can have is 32. The hard limit on the\ntotal number of elements in an array (lengths of all dimensions multiplied\ntogether) is\n2^31 - 1\ndivided by the byte size of array element. For a\nDOUBLE[]\n, this is\n2^28 - 1\nor 268,435,455. The actual limit QuestDB will\nenforce is configurable via\ncairo.max.array.element.count\n, with the default of\n10,000,000. The length of each individual dimension has a limit of\n2^28 - 1\nor\n268,435,455, regardless of element size.\n\n## Type nullability‚Äã\n\nMany nullable types reserve a value that marks them\nNULL\n:\n\n| Type Name | Null value | Description |\n| --- | --- | --- |\n| float | NaN,+Infinity,-Infinity | As defined by IEEE 754 (java.lang.Float.NaNetc.) |\n| double | NaN,+Infinity,-Infinity | As defined by IEEE 754 (java.lang.Double.NaN, etc.) |\n| long256 | 0x8000000000000000800000000000000080000000000000008000000000000000 | The value equals four consecutivelongnull literals. |\n| long | 0x8000000000000000L | Minimum possible value alongcan take, -2^63. |\n| date | 0x8000000000000000L | Minimum possible value alongcan take, -2^63. |\n| timestamp | 0x8000000000000000L | Minimum possible value alongcan take, -2^63. |\n| timestamp_ns | 0x8000000000000000L | Minimum possible value alongcan take, -2^63. |\n| int | 0x80000000 | Minimum possible value anintcan take, -2^31. |\n| uuid | 80000000-0000-0000-8000-000000000000 | Both 64 highest bits and 64 lowest bits set to -2^63. |\n| char | 0x0000 | The zero char (NULin ASCII). |\n| geohash(byte) | 0xff | Valid for geohashes of 1 to 7 bits (inclusive). |\n| geohash(short) | 0xffff | Valid for geohashes of 8 to 15 bits (inclusive). |\n| geohash(int) | 0xffffffff | Valid for geohashes of 16 to 31 bits (inclusive). |\n| geohash(long) | 0xffffffffffffffff | Valid for geohashes of 32 to 60 bits (inclusive). |\n| symbol | 0x80000000 | Symbol is stored as anintoffset into a lookup file. The value-1marks itNULL. |\n| ipv4 | 0.0.0.0(0x00000000) | IPv4 address is stored as a 32-bit integer and the zero value representsNULL. |\n| varchar | N/A | Varchar column has an explicitNULLmarker in the header. |\n| string | N/A | String column is length-prefixed, the length is anintand-1marks itNULL. |\n| binary | N/A | Binary column is length prefixed, the length is alongand-1marks itNULL. |\n| array | N/A | Array column marks aNULLvalue with a zero in thesizefield of the header. |\n| decimal | N/A | Minimal value of the underlying decimal type, impossible to reach through arithmetic as it is always out-of-range. |\n\nTo filter columns that contain, or don't contain,\nNULL\nvalues use a filter\nlike:\n\n```questdb-sql\nSELECT * FROM <table> WHERE <column> = NULL;SELECT * FROM <table> WHERE <column> != NULL;\n```\n\nAlternatively, from version 6.3 use the NULL equality operator aliases:\n\n```questdb-sql\nSELECT * FROM <table> WHERE <column> IS NULL;SELECT * FROM <table> WHERE <column> IS NOT NULL;\n```\n\nnote\nNULL\nvalues still occupy disk space.\n\n## The UUID type‚Äã\n\nQuestDB natively supports the\nUUID\ntype, which should be used for\nUUID\ncolumns instead of storing\nUUIDs\nas\nstrings\n.\nUUID\ncolumns are internally\nstored as 128-bit integers, allowing more efficient performance particularly in\nfiltering and sorting. Strings inserted into a\nUUID\ncolumn is permitted but\nthe data will be converted to the\nUUID\ntype.\nInserting strings into a UUID column\n\n```questdb-sql\nCREATE TABLE my_table (    id UUID);[...]INSERT INTO my_table VALUES ('a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11');[...]SELECT * FROM my_table WHERE id = 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11';\n```\n\nIf you use the\nPostgreSQL Wire Protocol\nthen\nyou can use the\nuuid\ntype in your queries. The JDBC API does not distinguish\nthe UUID type, but the Postgres JDBC driver supports it in prepared statements:\n\n```java\nUUID uuid = UUID.randomUUID();PreparedStatement ps = connection.prepareStatement(\"INSERT INTO my_table VALUES (?)\");ps.setObject(1, uuid);\n```\n\nQuestDB Client Libraries\ncan\nsend\nUUIDs\nas\nstrings\nto be converted to UUIDs by the server.\n\n## IPv4‚Äã\n\nQuestDB supports the IPv4 data type. It has validity checks and some\nIPv4-specific functions.\nIPv4 addresses exist within the range of\n0.0.0.1\n-\n255.255.255.255\n.\nAn all-zero address -\n0.0.0.0\n- is interpreted as\nNULL\n.\nCreate a column with the IPv4 data type like this:\n\n```sql\n-- Creating a table named traffic with two ipv4 columns: src and dst.CREATE TABLE traffic (ts timestamp, src ipv4, dst ipv4) timestamp(ts) PARTITION BY DAY;\n```\n\nIPv4 addresses support a wide range of existing SQL functions, and there are\nsome operators specifically for them. For a full list, see\nIPv4 Operators\n.\n\n### Limitations‚Äã\n\nYou cannot auto-create an IPv4 column using the InfluxDB Line Protocol, since it\ndoesn't support this type explicitly. The QuestDB server cannot distinguish\nbetween string and IPv4 data. However, you can insert IPv4 data into a\npre-existing IPv4 column by sending IPs as strings.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1529,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-01bae16cda7c",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/operators/date-time",
    "title": "Date and Time Operators | QuestDB",
    "text": "This page covers operators for filtering data by timestamp in\nWHERE\nclauses.\nRecommended: TICK syntax\nFor most timestamp filtering, use\nIN\nwith\nTICK syntax\n.\nIt handles simple ranges, multiple intervals, business days, timezones, and more\nin a single unified syntax:\n\n```questdb-sql\nWHERE ts IN '2024-01-[01..31]T09:30@EST#wd;6h30m'\n```\n\nThe\ninterval()\nfunction and\nBETWEEN\noperator described below are alternatives\nfor specific use cases, but TICK syntax covers most needs.\nFor date/time manipulation functions (\ndateadd()\n,\nnow()\n,\nextract()\n, etc.),\nsee\nDate/time functions\n.\n\n## INwith timestamp intervals‚Äã\n\nThe\nIN\noperator with a string argument queries timestamp intervals. QuestDB\nuses\nTICK syntax\nfor all timestamp interval\nexpressions.\n\n```questdb-sql\n-- Simple: all data from a specific daySELECT * FROM trades WHERE ts IN '2024-01-15';-- With duration: 1-hour window starting at 09:30SELECT * FROM trades WHERE ts IN '2024-01-15T09:30;1h';-- Multiple dates with bracket expansionSELECT * FROM trades WHERE ts IN '2024-01-[15,16,17]';-- Workdays only with timezoneSELECT * FROM trades WHERE ts IN '2024-01-[01..31]T09:30@EST#wd;6h30m';-- Dynamic: last 5 business daysSELECT * FROM trades WHERE ts IN '[$today-5bd..$today-1bd]';\n```\n\nFor complete documentation of all patterns including bracket expansion,\ndate variables, timezones, and day filters, see\nTICK interval syntax\n.\nInterval scan optimization\nWhen timestamp predicates are used on a\ndesignated timestamp\ncolumn, QuestDB performs an\ninterval scan\nusing binary search instead of a full table scan.\nThis optimization works with:\n- INwith TICK syntax orinterval()function\n- BETWEENranges\n- Comparison operators (>,<,>=,<=)\n- ANDcombinations (intersects intervals)\n- ORcombinations (unions intervals)\n\n```questdb-sql\n-- AND: intersects intervals (both conditions must match)WHERE ts IN '2024-01' AND ts > '2024-01-15'-- Results in: 2024-01-15 to 2024-01-31-- OR: unions intervals (either condition matches)WHERE ts IN '2024-01-10' OR ts IN '2024-01-20'-- Results in: two separate interval scans\n```\n\n\n## INwithinterval()function‚Äã\n\nThe\ninterval()\nfunction creates an interval from two explicit bounds. This is\nuseful when bounds come from variables or subqueries.\ntip\nFor static bounds, prefer TICK syntax:\nIN '2024-01-01;30d'\ninstead of\nIN interval('2024-01-01', '2024-01-31')\n.\nInterval from explicit bounds\n\n```questdb-sql\nSELECT * FROM tradesWHERE ts IN interval('2024-01-01', '2024-01-31');\n```\n\nInterval with bound parameters (prepared statements)\n\n```questdb-sql\nSELECT * FROM tradesWHERE ts IN interval($1, $2);\n```\n\n\n## BETWEEN...AND‚Äã\n\nThe\nBETWEEN\noperator specifies an inclusive range. Useful when working with\ndynamic bounds from functions.\ntip\nFor static ranges, prefer TICK syntax:\nIN '2024-01'\ninstead of\nBETWEEN '2024-01-01' AND '2024-01-31'\n.\nExplicit timestamp range\n\n```questdb-sql\nSELECT * FROM tradesWHERE ts BETWEEN '2024-01-01T00:00:00Z' AND '2024-01-31T23:59:59Z';\n```\n\nDynamic range using functions\n\n```questdb-sql\nSELECT * FROM tradesWHERE ts BETWEEN dateadd('d', -7, now()) AND now();\n```\n\nBETWEEN\nproduces the same\ninterval scan\noptimization as\nIN\nwhen used on a designated timestamp column.\n\n### When to use each‚Äã\n\n\n| Use case | Recommended |\n| --- | --- |\n| Any static range | INwith TICK ‚Äî'2024-01','2024-01-15T09:00;1h' |\n| Multiple intervals | INwith TICK ‚Äî'2024-01-[15,16,17]' |\n| Schedules, business days | INwith TICK ‚Äî'[$today-5bd..$today]#workday' |\n| Dynamic bounds from functions | BETWEEN‚ÄîBETWEEN dateadd('d', -7, now()) AND now() |\n| Prepared statement parameters | IN interval()‚ÄîIN interval($1, $2) |\n\n\n## See also‚Äã\n\n- TICK interval syntax‚Äî Full reference forINpatterns\n- Interval scan‚Äî How timestamp queries are optimized\n- Designated timestamp‚Äî Required for interval scan\n- Date/time functions‚Äîdateadd(),now(), etc.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 524,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-bd44a8dcdb3e",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/asof-join",
    "title": "ASOF JOIN keyword | QuestDB",
    "text": "ASOF JOIN is a powerful SQL keyword that allows you to join two time-series\ntables.\nIt is a variant of the\nJOINkeyword\nand shares\nmany of its execution traits.\nThis document will demonstrate how to utilize it, and link to other relevant\nJOIN context.\n\n## JOIN overview‚Äã\n\nThe JOIN operation is broken into three components:\n- Select clause\n- Join clause\n- Where clause\nThis document will demonstrate the JOIN clause, while the other keywords\ndemonstrate their respective clauses.\nVisually, a JOIN operation looks like this:\n- selectClause- see theSELECTreference docs\nfor more information.\n- joinClauseASOF JOINwith an optionalONclause which allows only the=predicate and an optionalTOLERANCEclause:\n- whereClause- see theWHEREreference docs for\nmore information.\nIn addition, the following are items of importance:\n- Columns from joined tables are combined in a single row.\n- Columns with the same name originating from different tables will be\nautomatically aliased into a unique column namespace of the result set.\n- Though it is usually preferable to explicitly specify join conditions, QuestDB\nwill analyzeWHEREclauses for implicit join conditions and will derive\ntransient join conditions where necessary.\n\n### Execution order‚Äã\n\nJoin operations are performed in order of their appearance in a SQL query.\nRead more about execution order in the\nJOIN reference documentation\n.\n\n## ASOF JOIN‚Äã\n\nASOF JOIN\njoins two time-series on their timestamp, using the following\nlogic: for each row in the first time-series...\n- consider all timestamps in the second time-seriesearlier or equal tothe first one\n- choosethe latestsuch timestamp\n- If the optionalTOLERANCEclause is specified, an additional condition\napplies: the chosen record from t2 must satisfyt1.ts - t2.ts <= tolerance_value. If no record from t2 meets this condition\n(along witht2.ts <= t1.ts), then the row from t1 will not have a match.\n\n### Example‚Äã\n\nLet's use an example with two tables:\n- market_data: Multi-level L2 FX order book snapshots per symbol\n- core_price: Quote streamer per symbol and ECN\nmarket_data\ndata: For the purposes of these examples, we will focus only on\nthe best bid price.\nBest Bid Price per Symbol from Market Data\nDemo this query\n\n```questdb-sql\nSELECT timestamp, symbol, bids[1,1] as best_bid_priceFROM market_data limit 20;\n```\n\n\n| timestamp | symbol | best_bid_price |\n| --- | --- | --- |\n| 2025-09-16T14:00:00.006068Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.008934Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.014362Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.016543Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.017379Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.020635Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.021059Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.032753Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.035691Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.038910Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.041939Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.042338Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.053509Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.060495Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.065560Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.068744Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.073389Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.073536Z | EURUSD | 1.1869 |\n| 2025-09-16T14:00:00.077558Z | GBPUSD | 1.3719 |\n| 2025-09-16T14:00:00.078433Z | GBPUSD | 1.3719 |\n\ncore_price\ndata: We will focus only on the bid_price\nBid Price per Symbol from Core Prices\nDemo this query\n\n```questdb-sql\nselect timestamp, symbol, bid_price fromcore_price limit 20;\n```\n\n\n| timestamp | symbol | bid_price |\n| --- | --- | --- |\n| 2025-09-16T14:00:00.009328Z | USDJPY | 145.39 |\n| 2025-09-16T14:00:00.043761Z | USDJPY | 145.67 |\n| 2025-09-16T14:00:00.056230Z | EURUSD | 1.1863 |\n| 2025-09-16T14:00:00.057539Z | USDJPY | 145.57 |\n| 2025-09-16T14:00:00.069197Z | GBPUSD | 1.3682 |\n| 2025-09-16T14:00:00.083291Z | EURUSD | 1.1835 |\n| 2025-09-16T14:00:00.098121Z | GBPUSD | 1.3691 |\n| 2025-09-16T14:00:00.105339Z | EURUSD | 1.185 |\n| 2025-09-16T14:00:00.111114Z | EURUSD | 1.1863 |\n| 2025-09-16T14:00:00.129785Z | GBPUSD | 1.3709 |\n| 2025-09-16T14:00:00.145194Z | GBPUSD | 1.3689 |\n| 2025-09-16T14:00:00.148178Z | GBPUSD | 1.3694 |\n| 2025-09-16T14:00:00.155810Z | USDJPY | 145.51 |\n| 2025-09-16T14:00:00.178333Z | USDJPY | 145.48 |\n| 2025-09-16T14:00:00.185806Z | GBPUSD | 1.3687 |\n| 2025-09-16T14:00:00.191322Z | EURUSD | 1.185 |\n| 2025-09-16T14:00:00.220899Z | GBPUSD | 1.3697 |\n| 2025-09-16T14:00:00.222574Z | USDJPY | 145.65 |\n| 2025-09-16T14:00:00.249440Z | EURUSD | 1.1853 |\n| 2025-09-16T14:00:00.274688Z | EURUSD | 1.184 |\n\nWe want to join each market data snapshot to the relevant core price. All\nwe have to write is\nA basic ASOF JOIN example\nDemo this query\n\n```questdb-sql\nSELECT  m.timestamp, m.symbol, bids[1,1] AS best_bid_price,  p.timestamp, p.symbol, p.bid_priceFROM  market_data m ASOF JOIN core_price pLIMIT 20;\n```\n\nand we get this result:\n\n| timestamp | symbol | best_bid_price | timestamp_2 | symbol_2 | bid_price |\n| --- | --- | --- | --- | --- | --- |\n| 2025-09-16T14:00:00.006068Z | USDJPY | 145.67 | 2025-09-16T14:00:00.004409Z | CADJPY | 106.49 |\n| 2025-09-16T14:00:00.008934Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.008094Z | NZDUSD | 0.5926 |\n| 2025-09-16T14:00:00.014362Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.013547Z | CADJPY | 106.41 |\n| 2025-09-16T14:00:00.016543Z | USDJPY | 145.67 | 2025-09-16T14:00:00.015730Z | CADJPY | 106.6 |\n| 2025-09-16T14:00:00.017379Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.017359Z | EURGBP | 0.8726 |\n| 2025-09-16T14:00:00.020635Z | USDJPY | 145.67 | 2025-09-16T14:00:00.017813Z | EURCHF | 0.9363 |\n| 2025-09-16T14:00:00.021059Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.017813Z | EURCHF | 0.9363 |\n| 2025-09-16T14:00:00.032753Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.031278Z | USDSGD | 1.2865 |\n| 2025-09-16T14:00:00.035691Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.034997Z | GBPJPY | 200.45 |\n| 2025-09-16T14:00:00.038910Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.037147Z | EURNZD | 1.9588 |\n| 2025-09-16T14:00:00.041939Z | USDJPY | 145.67 | 2025-09-16T14:00:00.039227Z | USDTRY | 41.133 |\n| 2025-09-16T14:00:00.042338Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.042233Z | EURGBP | 0.8726 |\n| 2025-09-16T14:00:00.053509Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.052584Z | USDSEK | 9.221 |\n| 2025-09-16T14:00:00.060495Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.059674Z | NZDCAD | 0.8171 |\n| 2025-09-16T14:00:00.065560Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.061656Z | EURGBP | 0.8733 |\n| 2025-09-16T14:00:00.068744Z | USDJPY | 145.67 | 2025-09-16T14:00:00.068729Z | GBPCHF | 1.0722 |\n| 2025-09-16T14:00:00.073389Z | USDJPY | 145.67 | 2025-09-16T14:00:00.072195Z | EURGBP | 0.8737 |\n| 2025-09-16T14:00:00.073536Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.072195Z | EURGBP | 0.8737 |\n| 2025-09-16T14:00:00.077558Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.077447Z | NZDUSD | 0.5936 |\n| 2025-09-16T14:00:00.078433Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.077447Z | NZDUSD | 0.5936 |\n\nNote the result doesn't really make sense, as we are joining each row in\nmarket_data\nwith the row in\ncore_price\nwith\nexact or immediately before timestamp, regardless of the symbol. If our join does not depend only on timestamp, but also\non matching columns, we need to add extra keywords.\n\n### UsingONfor matching column value‚Äã\n\nBy using the\nON\nclause, you can point out the key (\nsymbol\nin our example)\ncolumn and get results separate for each key.\nHere's the ASOF JOIN query with the\nON\nclause added:\nASOF JOIN with symbol matching\nDemo this query\n\n```questdb-sql\nSELECT  m.timestamp, m.symbol, bids[1,1] AS best_bid_price,  p.timestamp, p.symbol, p.bid_priceFROM  market_data m ASOF JOIN core_price pON (symbol)LIMIT 20;\n```\n\nResult:\n\n| timestamp | symbol | best_bid_price | timestamp_2 | symbol_2 | bid_price |\n| --- | --- | --- | --- | --- | --- |\n| 2025-09-16T14:00:00.006068Z | USDJPY | 145.67 | null | null | null |\n| 2025-09-16T14:00:00.008934Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.014362Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.016543Z | USDJPY | 145.67 | 2025-09-16T14:00:00.009328Z | USDJPY | 145.39 |\n| 2025-09-16T14:00:00.017379Z | EURUSD | 1.1869 | null | null | null |\n| 2025-09-16T14:00:00.020635Z | USDJPY | 145.67 | 2025-09-16T14:00:00.009328Z | USDJPY | 145.39 |\n| 2025-09-16T14:00:00.021059Z | EURUSD | 1.1869 | null | null | null |\n| 2025-09-16T14:00:00.032753Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.035691Z | EURUSD | 1.1869 | null | null | null |\n| 2025-09-16T14:00:00.038910Z | EURUSD | 1.1869 | null | null | null |\n| 2025-09-16T14:00:00.041939Z | USDJPY | 145.67 | 2025-09-16T14:00:00.009328Z | USDJPY | 145.39 |\n| 2025-09-16T14:00:00.042338Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.053509Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.060495Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.056230Z | EURUSD | 1.1863 |\n| 2025-09-16T14:00:00.065560Z | GBPUSD | 1.3719 | null | null | null |\n| 2025-09-16T14:00:00.068744Z | USDJPY | 145.67 | 2025-09-16T14:00:00.057539Z | USDJPY | 145.57 |\n| 2025-09-16T14:00:00.073389Z | USDJPY | 145.67 | 2025-09-16T14:00:00.057539Z | USDJPY | 145.57 |\n| 2025-09-16T14:00:00.073536Z | EURUSD | 1.1869 | 2025-09-16T14:00:00.056230Z | EURUSD | 1.1863 |\n| 2025-09-16T14:00:00.077558Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.069197Z | GBPUSD | 1.3682 |\n| 2025-09-16T14:00:00.078433Z | GBPUSD | 1.3719 | 2025-09-16T14:00:00.069197Z | GBPUSD | 1.3682 |\n\nNote how the first few rows for each symbol don't match anything on the\ncore_price\ntable, as there are no rows with timestamps equal or earlier than\nthe timestamp on the\nmarket_data\ntable for those first rows.\n\n### How ASOF JOIN uses timestamps‚Äã\n\nASOF JOIN\nrequires tables or subqueries to be ordered by time. The best way to\nmeet this requirement is to use a\ndesignated timestamp\n, which is set when\nyou create a table. This not only enforces the chronological order of your data\nbut also tells QuestDB which column to use for time-series operations\nautomatically.\n\n#### Default behavior‚Äã\n\nBy default, an\nASOF JOIN\nwill always use the designated timestamp of the\ntables involved.\nThis behavior is so fundamental that it extends to subqueries in a unique way:\neven if you do not explicitly SELECT the designated timestamp column in a\nsubquery, QuestDB implicitly propagates it. The join is performed correctly\nunder the hood using this hidden timestamp, which is then omitted from the final\nresult set.\nThis makes most\nASOF JOIN\nqueries simple and intuitive.\nASOF JOIN with designated timestamp\nDemo this query\n\n```questdb-sql\n-- The 'market_data' table has 'timestamp' as its designated timestamp.-- Even though 'timestamp' is not selected in the subquery,-- it is used implicitly for the ASOF JOIN.WITH market_subset AS (  SELECT symbol,bids  FROM market_data  WHERE timestamp in today())SELECT *FROM market_subset ASOF JOIN core_price ON (symbol);\n```\n\nIn more complicated subqueries, the implicit propagation of the designated\ntimestamp may not work QuestDB responds with an error\nleft side of time series join has no timestamp\n. In such cases, your subquery\nshould explicitly include the designated timestamp column in the\nSELECT\nclause\nto ensure it is used for the join.\n\n#### The standard override method: Using ORDER BY‚Äã\n\nThe easiest and safest way to join on a different timestamp column is to use an\nORDER BY ... ASC\nclause in your subquery.\nWhen you sort a subquery by a\nTIMESTAMP\ncolumn, QuestDB makes that column the\nnew designated timestamp for the subquery's results. The subsequent\nASOF JOIN\nwill automatically detect and use this new timestamp.\nExample: Joining on\ningestion_time\ninstead of the default\ntrade_ts\nASOF JOIN with custom timestamp\n\n```questdb-sql\nWITH trades_ordered_by_ingestion AS (  SELECT symbol, price, ingestion_time  FROM trades  WHERE timestamp in today()  -- This ORDER BY clause tells QuestDB to use 'ingestion_time'  -- as the new designated timestamp for this subquery.  ORDER BY ingestion_time ASC)-- No extra syntax is needed here. The ASOF JOIN automatically uses-- the new designated timestamp from the subquery.SELECT *FROM trades_ordered_by_ingestionASOF JOIN quotes ON (symbol);\n```\n\n\n#### Using the timestamp() syntax‚Äã\n\nThe\ntimestamp()\nsyntax is an expert-level hint for the query engine. It should\nonly be used to manually assign a timestamp to a dataset that does not have one,\nwithout forcing a sort.\nYou should only use this when you can guarantee that your data is already sorted\nby that timestamp column. Using\ntimestamp()\nincorrectly on unsorted data will\nlead to incorrect join results.\nThe primary use case is performance optimization on a table that has no\ndesignated timestamp in its schema, but where you know the data is physically\nstored in chronological order. Using the\ntimestamp()\nhint avoids a costly\nORDER BY operation. This can be the case, for example, with external Parquet\nfiles where you know data is already sorted by timestamp.\nASOF JOIN with timestamp()\nDemo this query\n\n```questdb-sql\n-- Use this ONLY IF the left-side table has NO designated timestamp,-- but you can guarantee its data is already physically ordered by the-- column you declare.SELECT *FROM (      (SELECT * from read_parquet('trades.parquet') )      timestamp(timestamp)      )ASOF JOIN trades ON (symbol);\n```\n\nTo summarize:\n- By default, the table's designated timestamp is used.\n- To join on a different column, the standard method is toORDER BYthat\ncolumn in a subquery.\n- Use thetimestamp()syntax as an expert-level hint to avoid a sort on a\ntable with no designated timestamp, if and only if you are certain the data\nis already sorted.\n\n### TOLERANCE clause‚Äã\n\nThe\nTOLERANCE\nclause enhances ASOF and LT JOINs by limiting how far back in\ntime the join should look for a match in the right table. The\nTOLERANCE\nparameter accepts a time interval value (e.g.,\n2s\n,\n100ms\n,\n1d\n).\nWhen specified, a record from the left table t1 at t1.ts will only be joined\nwith a record from the right table t2 at t2.ts if both conditions are met:\nt2.ts <= t1.ts\nand\nt1.ts - t2.ts <= tolerance_value\nThis ensures that the matched record from the right table is not only the latest\none on or before t1.ts, but also within the specified time window.\nTOLERANCE works both with or without the ON clause:\nASOF JOIN with keys and 50 milliseconds of TOLERANCE\nDemo this query\n\n```questdb-sql\nSELECT market_data.timestamp, market_data.symbol, bids, core_price.*FROM market_dataASOF JOIN core_price ON (symbol) TOLERANCE 50TWHERE market_data.timestamp IN today();\n```\n\nThe interval_literal must be a valid QuestDB interval string, like '5s' (5\nseconds), '100T' (100 milliseconds), '2m' (2 minutes), '3h' (3 hours), or '1d'\n(1 day).\n\n#### Supported Units for interval_literal‚Äã\n\nThe\nTOLERANCE\ninterval literal supports the following time unit qualifiers:\n- n: Nanoseconds\n- U: Microseconds\n- T: Milliseconds\n- s: Seconds\n- m: Minutes\n- h: Hours\n- d: Days\n- w: Weeks\nFor example, '500n' is 500 nanoseconds, '100U' is 100 microseconds, '50T' is 50\nmilliseconds, '2s' is 2 seconds, '30m' is 30 minutes, '1h' is 1 hour, '7d' is 7\ndays, and '2w' is 2 weeks. Please note that months (M) and years (Y) are not\nsupported as units for the\nTOLERANCE\nclause.\nThe effective precision of the\nTOLERANCE\nclause depends on the\ndesignated timestamp resolution\nof the tables involved. For example, if a table uses microsecond resolution, specifying nanosecond\ntolerance (e.g.,\n500n\n) will not provide nanosecond-level matching precision.\n\n#### Performance impact of TOLERANCE‚Äã\n\nSpecifying\nTOLERANCE\ncan also improve performance.\nASOF JOIN\nexecution plans\noften scan backward in time on the right table to find a matching entry for each\nleft-table row.\nTOLERANCE\nallows these scans to terminate early - once a\nright-table record is older than the left-table record by more than the\nspecified tolerance - thus avoiding unnecessary processing of more distant\nrecords.\n\n### Choose the optimal algorithm with an SQL Hint‚Äã\n\nQuestDB has several different algorithms that fit different queries and data\ndistributions. If you query is performing poorly, consult the\nSQL optimizer hints\npage and try out the\nnon-default algorithms.\n\n## SPLICE JOIN‚Äã\n\nWant to join all records from both tables?\nSPLICE JOIN\nis a full\nASOF JOIN\n.\nRead the\nJOIN reference\nfor more\ninformation on SPLICE JOIN.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 2532,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-32cb7ad695f0",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/integrations/data-processing/polars",
    "title": "Polars | QuestDB",
    "text": "Polars\nis a fast DataFrame library implemented in Rust and\nPython. It is designed to process large datasets efficiently and is particularly\nwell-suited for time-series data. Polars provides a DataFrame API similar to\nPandas, but with a focus on performance and parallel execution.\n\n## Overview‚Äã\n\nConnectorX is a Rust library that provides fast data transfer between Python\nand various databases, including QuestDB. It includes a connector for PostgreSQL\nwhich is compatible with QuestDB's PGWire protocol. This allows you to use\nConnectorX to read data from QuestDB into a Polars DataFrame.\ncaution\nNote\n: By default ConnectorX for PostgreSQL uses features not supported by QuestDB.\nIf you instruct ConnectorX to use the Redshift protocol, it will work with QuestDB.\n\n## Prerequisites‚Äã\n\n- QuestDB must be running and accessible. Checkout thequick start.\n- Python 3.8 or later\n- Polars\n- pyarrow\n- ConnectorX\n\n```pip\npip install polars pyarrow connectorx\n```\n\n\n## Example‚Äã\n\n\n```python\nimport polars as plQUESTDB_URI = \"redshift://admin:quest@localhost:8812/qdb\"QUERY = \"SELECT * FROM tables() LIMIT 5;\"df = pl.read_database_uri(query=QUERY, uri=QUESTDB_URI)print(\"Received DataFrame:\")print(df)\n```\n\nNote that the URL uses the\nredshift\nschema. This is important because\nit makes ConnectorX to avoid using features not supported by QuestDB.\n\n## Ingestion vs Querying‚Äã\n\nThis guides deals with querying data from QuestDB using Polars. For ingestion to QuestDB we recommend using the\nQuestDB Python client\n.\n\n## Additional Resources‚Äã\n\n- Integration with Pandas\n- QuestDB Client for fast ingestion\n- Python clients guide",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 235,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-28f4e029e39e",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/where",
    "title": "WHERE keyword | QuestDB",
    "text": "WHERE\nclause filters data. Filter expressions are required to return boolean\nresult.\nQuestDB includes a\nJIT compiler\nfor SQL queries\nwhich contain\nWHERE\nclauses.\n\n## Syntax‚Äã\n\nThe general syntax is as follows. Specific filters have distinct syntaxes\ndetailed thereafter.\n\n### Logical operators‚Äã\n\nQuestDB supports\nAND\n,\nOR\n,\nNOT\nas logical operators and can assemble\nconditions using brackets\n()\n.\nExample\n\n```questdb-sql\nSELECT * FROM tableWHEREa = 1 AND (b = 2 OR c = 3 AND NOT d);\n```\n\n\n## Symbol and string‚Äã\n\nQuestDB can filter strings and symbols based on equality, inequality, and\nregular expression patterns.\n\n### Exact match‚Äã\n\nEvaluates match of a string or symbol.\nExample\n\n```questdb-sql\nSELECT * FROM usersWHERE name = 'John';\n```\n\n\n| name | age |\n| --- | --- |\n| John | 31 |\n| John | 45 |\n| ... | ... |\n\n\n### Does NOT match‚Äã\n\nEvaluates mismatch of a string or symbol.\nExample\n\n```questdb-sql\nSELECT * FROM usersWHERE name != 'John';\n```\n\n\n| name | age |\n| --- | --- |\n| Tim | 31 |\n| Tom | 45 |\n| ... | ... |\n\n\n### Regular expression match‚Äã\n\nEvaluates match against a regular expression defined using\njava.util.regex\npatterns.\nRegex example\n\n```questdb-sql\nSELECT * FROM users WHERE name ~ 'Jo';\n```\n\n\n| name | age |\n| --- | --- |\n| Joe | 31 |\n| Jonathan | 45 |\n| ... | ... |\n\n\n### Regular expression does NOT match‚Äã\n\nEvaluates mismatch against a regular expression defined using\njava.util.regex\npatterns.\nExample\n\n```questdb-sql\nSELECT * FROM users WHERE name !~ 'Jo';\n```\n\n\n| name | age |\n| --- | --- |\n| Tim | 31 |\n| Tom | 45 |\n| ... | ... |\n\n\n### List search‚Äã\n\nEvaluates match or mismatch against a list of elements.\nList match\n\n```questdb-sql\nSELECT * FROM users WHERE name in('Tim', 'Tom');\n```\n\n\n| name | age |\n| --- | --- |\n| Tim | 31 |\n| Tom | 45 |\n| ... | ... |\n\nList mismatch\n\n```questdb-sql\nSELECT * FROM users WHERE NOT name in('Tim', 'Tom');\n```\n\n\n| name | age |\n| --- | --- |\n| Aaron | 31 |\n| Amelie | 45 |\n| ... | ... |\n\n\n## Numeric‚Äã\n\nQuestDB can filter numeric values based on equality, inequality, comparison, and\nproximity.\nnote\nFor timestamp filters, we recommend the\ntimestamp search notation\nwhich is faster and less\nverbose.\n\n### Equality, inequality and comparison‚Äã\n\nSuperior or equal to 23\n\n```questdb-sql\nSELECT * FROM users WHERE age >= 23;\n```\n\nEqual to 23\n\n```questdb-sql\nSELECT * FROM users WHERE age = 23;\n```\n\nNOT Equal to 23\n\n```questdb-sql\nSELECT * FROM users WHERE age != 23;\n```\n\n\n## Boolean‚Äã\n\nUsing the columnName will return\ntrue\nvalues. To return\nfalse\nvalues,\nprecede the column name with the\nNOT\noperator.\nExample - true\n\n```questdb-sql\nSELECT * FROM users WHERE isActive;\n```\n\n\n| userId | isActive |\n| --- | --- |\n| 12532 | true |\n| 38572 | true |\n| ... | ... |\n\nExample - false\n\n```questdb-sql\nSELECT * FROM users WHERE NOT isActive;\n```\n\n\n| userId | isActive |\n| --- | --- |\n| 876534 | false |\n| 43234 | false |\n| ... | ... |\n\n\n## Timestamp and date‚Äã\n\nQuestDB supports both its own timestamp search notation and standard search\nbased on inequality. This section describes the use of the\ntimestamp search\nnotation\nwhich is efficient and fast but requires a\ndesignated timestamp\n.\nIf a table does not have a designated timestamp applied during table creation,\none may be applied dynamically\nduring a select operation\n.\n\n### Native timestamp format‚Äã\n\nQuestDB automatically recognizes strings formatted as ISO timestamp as a\ntimestamp\ntype. The following are valid examples of strings parsed as\ntimestamp\ntypes:\n\n| Valid STRING Format | Resulting Timestamp |\n| --- | --- |\n| 2010-01-12T12:35:26.123456+01:30 | 2010-01-12T11:05:26.123456Z |\n| 2010-01-12T12:35:26.123456+01 | 2010-01-12T11:35:26.123456Z |\n| 2010-01-12T12:35:26.123456Z | 2010-01-12T12:35:26.123456Z |\n| 2010-01-12T12:35:26.12345 | 2010-01-12T12:35:26.123450Z |\n| 2010-01-12T12:35:26.1234 | 2010-01-12T12:35:26.123400Z |\n| 2010-01-12T12:35:26.123 | 2010-01-12T12:35:26.123000Z |\n| 2010-01-12T12:35:26.12 | 2010-01-12T12:35:26.120000Z |\n| 2010-01-12T12:35:26.1 | 2010-01-12T12:35:26.100000Z |\n| 2010-01-12T12:35:26 | 2010-01-12T12:35:26.000000Z |\n| 2010-01-12T12:35 | 2010-01-12T12:35:00.000000Z |\n| 2010-01-12T12 | 2010-01-12T12:00:00.000000Z |\n| 2010-01-12 | 2010-01-12T00:00:00.000000Z |\n| 2010-01 | 2010-01-01T00:00:00.000000Z |\n| 2010 | 2010-01-01T00:00:00.000000Z |\n| 2010-01-12 12:35:26.123456-02:00 | 2010-01-12T14:35:26.123456Z |\n| 2010-01-12 12:35:26.123456Z | 2010-01-12T12:35:26.123456Z |\n| 2010-01-12 12:35:26.123 | 2010-01-12T12:35:26.123000Z |\n| 2010-01-12 12:35:26.12 | 2010-01-12T12:35:26.120000Z |\n| 2010-01-12 12:35:26.1 | 2010-01-12T12:35:26.100000Z |\n| 2010-01-12 12:35:26 | 2010-01-12T12:35:26.000000Z |\n| 2010-01-12 12:35 | 2010-01-12T12:35:00.000000Z |\n\n\n### Exact timestamp‚Äã\n\n\n#### Syntax‚Äã\n\nTimestamp equals date\n\n```questdb-sql\nSELECT * FROM scores WHERE ts = '2010-01-12T00:02:26.000Z';\n```\n\n\n| ts | score |\n| --- | --- |\n| 2010-01-12T00:02:26.000Z | 2.4 |\n| 2010-01-12T00:02:26.000Z | 3.1 |\n| ... | ... |\n\nTimestamp equals timestamp\n\n```questdb-sql\nSELECT * FROM scores WHERE ts = '2010-01-12T00:02:26.000000Z';\n```\n\n\n| ts | score |\n| --- | --- |\n| 2010-01-12T00:02:26.000000Z | 2.4 |\n| 2010-01-12T00:02:26.000000Z | 3.1 |\n| ... | ... |\n\n\n### Time range (WHERE IN)‚Äã\n\nReturns results within a defined range.\nRecommended: TICK syntax\nFor complex timestamp filtering, use\nTICK interval syntax\n.\nTICK handles date ranges, business days, timezones, and schedules in a single\nexpression:\n\n```questdb-sql\n-- Last 5 business days, 9:30 AM New York time, 6.5 hour windowsWHERE ts IN '[$today-5bd..$today-1bd]T09:30@America/New_York#workday;6h30m'\n```\n\nWith\nexchange calendars\n(Enterprise),\nyou can filter by real exchange schedules including holidays and early closes:\n\n```questdb-sql\n-- NYSE trading hours for January, holidays excluded automaticallyWHERE ts IN '2025-01-[01..31]#XNYS'\n```\n\n\n#### Syntax‚Äã\n\nResults in a given year\n\n```questdb-sql\nSELECT * FROM scores WHERE ts IN '2018';\n```\n\n\n| ts | score |\n| --- | --- |\n| 2018-01-01T00:0000.000000Z | 123.4 |\n| ... | ... |\n| 2018-12-31T23:59:59.999999Z | 115.8 |\n\nResults in a given minute\n\n```questdb-sql\nSELECT * FROM scores WHERE ts IN '2018-05-23T12:15';\n```\n\n\n| ts | score |\n| --- | --- |\n| 2018-05-23T12:15:00.000000Z | 123.4 |\n| ... | ... |\n| 2018-05-23T12:15:59.999999Z | 115.8 |\n\n\n### Time range with interval modifier‚Äã\n\nYou can apply a modifier to further customize the range. The modifier extends\nthe upper bound of the original timestamp based on the modifier parameter. An\noptional interval with occurrence can be set, to apply the search in the given\ntime range repeatedly, for a set number of times.\n\n#### Syntax‚Äã\n\n- timestampis the original time range for the query.\n- modifieris a signed integer modifying the upper bound applying to thetimestamp:Apositivevalue extends the selected period.Anegativevalue reduces the selected period.\n- intervalis an unsigned integer indicating the desired interval period for\nthe time range.\n- repetitionis an unsigned integer indicating the number of times the\ninterval should be applied.\n\n#### Examples‚Äã\n\nModifying the range:\nResults in a given year and the first month of the next year\n\n```questdb-sql\nSELECT * FROM scores WHERE ts IN '2018;1M';\n```\n\nThe range is 2018. The modifier extends the upper bound (originally 31 Dec 2018)\nby one month.\n\n| ts | score |\n| --- | --- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ... | ... |\n| 2019-01-31T23:59:59.999999Z | 115.8 |\n\nResults in a given month excluding the last 3 days\n\n```questdb-sql\nSELECT * FROM scores WHERE ts IN '2018-01;-3d';\n```\n\nThe range is Jan 2018. The modifier reduces the upper bound (originally 31\nJan 2018) by 3 days.\n\n| ts | score |\n| --- | --- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ... | ... |\n| 2018-01-28T23:59:59.999999Z | 113.8 |\n\nModifying the interval:\nResults on a given date with an interval\n\n```questdb-sql\nSELECT * FROM scores WHERE ts IN '2018-01-01;1d;1y;2';\n```\n\nThe range is extended by one day from Jan 1 2018, with a one-year interval,\nrepeated twice. This means that the query searches for results on Jan 1-2 in\n2018 and in 2019:\n\n| ts | score |\n| --- | --- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ... | ... |\n| 2018-01-02T23:59:59.999999Z | 110.3 |\n| 2019-01-01T00:00:00.000000Z | 128.7 |\n| ... | ... |\n| 2019-01-02T23:59:59.999999Z | 103.8 |\n\nA more complete query breakdown would appear as such:\n\n```questdb-sql\n-- IN extension for time-intervalsSELECT * FROM trades WHERE timestamp in '2023'; -- whole yearSELECT * FROM trades WHERE timestamp in '2023-12'; -- whole monthSELECT * FROM trades WHERE timestamp in '2023-12-20'; -- whole day-- The whole day, extending 15s into the next daySELECT * FROM trades WHERE timestamp in '2023-12-20;15s';-- For the past 7 days, 2 seconds before and after midnightSELECT * from trades WHERE timestamp in '2023-09-20T23:59:58;4s;-1d;7'\n```\n\n\n### IN with multiple arguments‚Äã\n\n\n#### Syntax‚Äã\n\nIN\nwith more than 1 argument is treated as standard SQL\nIN\n. It is a\nshorthand of multiple\nOR\nconditions, i.e. the following query:\nIN list\n\n```questdb-sql\nSELECT * FROM scoresWHERE ts IN ('2018-01-01', '2018-01-01T12:00', '2018-01-02');\n```\n\nis equivalent to:\nIN list equivalent OR\n\n```questdb-sql\nSELECT * FROM scoresWHERE ts = '2018-01-01' or ts = '2018-01-01T12:00' or ts = '2018-01-02';\n```\n\n\n| ts | value |\n| --- | --- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| 2018-01-01T12:00:00.000000Z | 589.1 |\n| 2018-01-02T00:00:00.000000Z | 131.5 |\n\n\n### BETWEEN‚Äã\n\n\n#### Syntax‚Äã\n\nFor non-standard ranges, users can explicitly specify the target range using the\nBETWEEN\noperator. As with standard SQL, both upper and lower bounds of\nBETWEEN\nare inclusive, and the order of lower and upper bounds is not\nimportant so that\nBETWEEN X AND Y\nis equivalent to\nBETWEEN Y AND X\n.\nExplicit range\n\n```questdb-sql\nSELECT * FROM scoresWHERE ts BETWEEN '2018-01-01T00:00:23.000000Z' AND '2018-01-01T00:00:23.500000Z';\n```\n\n\n| ts | value |\n| --- | --- |\n| 2018-01-01T00:00:23.000000Z | 123.4 |\n| ... | ... |\n| 2018-01-01T00:00:23.500000Z | 131.5 |\n\nBETWEEN\ncan accept non-constant bounds, for example, the following query will\nreturn all records older than one year before the current date:\nOne year before current date\n\n```questdb-sql\nSELECT * FROM scoresWHERE ts BETWEEN to_str(now(), 'yyyy-MM-dd')AND dateadd('y', -1, to_str(now(), 'yyyy-MM-dd'));\n```\n\n\n##### Inclusivity example‚Äã\n\nInclusivity is precise, and may be more granular than the provided dates appear.\nIf a timestamp in the format YYYY-MM-DD is passed forward, it is computed as YYYY-MM-DDThh:mm\n:ss\n.sss.\nTo demonstrate, note the behaviour of the following example queries:\nDemonstrating inclusivity\n\n```questdb-sql\nSELECT *FROM tradesWHERE timestamp BETWEEN '2024-04-01' AND '2024-04-03'LIMIT -1;\n```\n\n\n| symbol | side | price | amount | timestamp |\n| --- | --- | --- | --- | --- |\n| BTC-USD | sell | 65,464.14 | 0.05100764 | 2024-04-02T23:59:59.9947212 |\n\nThe query pushes to the boundaries as far as is possible, all the way to:\n2024-04-02T23:59:59.9947212\n.\nIf there was an event at precisely\n2024-04-03T00:00:00.00000\n, it would also be included.\nNow let us look at:\n\n```title=\"demonstrating\nSELECT *FROM tradesWHERE timestamp BETWEEN '2024-04-01' AND '2024-04-03T00:00:00.99'LIMIT -1;\n```\n\n\n| symbol | side | price | amount | timestamp |\n| --- | --- | --- | --- | --- |\n| ETH-USD | sell | 3,279.11 | 0.00881686 | 2024-04-03T00:00:00.988858Z |\n\nEven with fractional seconds, the boundary is inclusive.\nA row with timestamp 2024-04-03T00:00:00.990000Z would also return in boundary.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1849,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-ffcec101c52f",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql/latest-on",
    "title": "LATEST ON keyword | QuestDB",
    "text": "Retrieves the latest entry by timestamp for a given key or combination of keys,\nfor scenarios where multiple time series are stored in the same table.\n\n## Syntax‚Äã\n\nwhere:\n- columnNameused in theLATEST ONpart of the clause is aTIMESTAMPcolumn.\n- columnNamelist used in thePARTITION BYpart of the clause is a list of\ncolumns of one of the following types:SYMBOL,STRING,BOOLEAN,SHORT,INT,LONG,LONG256,CHAR,DECIMAL.\n\n## Description‚Äã\n\nLATEST ON\nis used as part of a\nSELECT statement\nfor returning the most recent records per unique time series identified by the\nPARTITION BY\ncolumn values.\nLATEST ON\nrequires a\ndesignated timestamp\ncolumn. Use\nsub-queries\nfor tables without the designated\ntimestamp.\nThe query syntax has an impact on the\nexecution order\nof the\nLATEST ON\nclause and the\nWHERE\nclause.\nTo illustrate how\nLATEST ON\nis intended to be used, consider the\ntrades\ntable\nin the QuestDB demo instance\n. This table has a\nsymbol\ncolumn as\nSYMBOL\ntype which specifies the traded instrument. We can\nfind the most recent trade for each symbol with the following query:\n\n```questdb-sql\nSELECT symbol, timestamp, priceFROM tradesLATEST ON timestamp PARTITION BY symbol;\n```\n\n\n| symbol | timestamp | price |\n| --- | --- | --- |\n| BTC-USD | 2024-06-30T23:59:56.000000Z | 61432.5 |\n| ETH-USD | 2024-06-30T23:59:54.000000Z | 3421.8 |\n| SOL-USD | 2024-06-30T23:59:42.000000Z | 142.3 |\n\nThe above query returns the latest value within each time series stored in the\ntable. Those time series are determined based on the values in the column(s)\nspecified in the\nLATEST ON\nclause. In our example those time series are\nrepresented by different symbols. Then the column used in the\nLATEST ON\npart of the clause stands for the designated timestamp column for the table.\nThis allows the database to find the latest value within each time series.\n\n## Examples‚Äã\n\nFor the next examples, we can create a table called\nbalances\nwith the\nfollowing SQL:\n\n```questdb-sql\nCREATE TABLE balances (    cust_id SYMBOL,    balance_ccy SYMBOL,    balance DOUBLE,    ts TIMESTAMP) TIMESTAMP(ts) PARTITION BY DAY;insert into balances values ('1', 'USD', 600.5, '2020-04-21T16:03:43.504432Z');insert into balances values ('2', 'USD', 950, '2020-04-21T16:08:34.404665Z');insert into balances values ('2', 'EUR', 780.2, '2020-04-21T16:11:22.704665Z');insert into balances values ('1', 'USD', 1500, '2020-04-21T16:11:32.904234Z');insert into balances values ('1', 'EUR', 650.5, '2020-04-22T16:11:32.904234Z');insert into balances values ('2', 'USD', 900.75, '2020-04-22T16:12:43.504432Z');insert into balances values ('2', 'EUR', 880.2, '2020-04-22T16:18:34.404665Z');insert into balances values ('1', 'USD', 330.5, '2020-04-22T16:20:14.404997Z');\n```\n\nThis provides us with a table with the following content:\n\n| cust_id | balance_ccy | balance | ts |\n| --- | --- | --- | --- |\n| 1 | USD | 600.5 | 2020-04-21T16:01:22.104234Z |\n| 2 | USD | 950 | 2020-04-21T16:03:43.504432Z |\n| 2 | EUR | 780.2 | 2020-04-21T16:08:34.404665Z |\n| 1 | USD | 1500 | 2020-04-21T16:11:22.704665Z |\n| 1 | EUR | 650.5 | 2020-04-22T16:11:32.904234Z |\n| 2 | USD | 900.75 | 2020-04-22T16:12:43.504432Z |\n| 2 | EUR | 880.2 | 2020-04-22T16:18:34.404665Z |\n| 1 | USD | 330.5 | 2020-04-22T16:20:14.404997Z |\n\n\n### Single column‚Äã\n\nWhen a single\nsymbol\ncolumn is specified in\nLATEST ON\nqueries, the query\nwill end after all distinct symbol values are found.\nLatest records by customer ID\n\n```questdb-sql\nSELECT * FROM balancesLATEST ON ts PARTITION BY cust_id;\n```\n\nThe query returns two rows with the most recent records per unique\ncust_id\nvalue:\n\n| cust_id | balance_ccy | balance | ts |\n| --- | --- | --- | --- |\n| 2 | EUR | 880.2 | 2020-04-22T16:18:34.404665Z |\n| 1 | USD | 330.5 | 2020-04-22T16:20:14.404997Z |\n\n\n### Multiple columns‚Äã\n\nWhen multiple columns are specified in\nLATEST ON\nqueries, the returned results\nare the most recent\nunique combinations\nof the column values. This example\nquery returns\nLATEST ON\ncustomer ID and balance currency:\nLatest balance by customer and currency\n\n```questdb-sql\nSELECT cust_id, balance_ccy, balanceFROM balancesLATEST ON ts PARTITION BY cust_id, balance_ccy;\n```\n\nThe results return the most recent records for each unique combination of\ncust_id\nand\nbalance_ccy\n.\n\n| cust_id | balance_ccy | balance | inactive | ts |\n| --- | --- | --- | --- | --- |\n| 1 | EUR | 650.5 | FALSE | 2020-04-22T16:11:32.904234Z |\n| 2 | USD | 900.75 | FALSE | 2020-04-22T16:12:43.504432Z |\n| 2 | EUR | 880.2 | FALSE | 2020-04-22T16:18:34.404665Z |\n| 1 | USD | 330.5 | FALSE | 2020-04-22T16:20:14.404997Z |\n\n\n#### Performance considerations‚Äã\n\nWhen the\nLATEST ON\nclause contains a single\nsymbol\ncolumn, QuestDB will know\nall distinct values upfront and stop scanning table contents once the latest\nentry has been found for each distinct symbol value.\nWhen the\nLATEST ON\nclause contains multiple columns, QuestDB has to scan the\nentire table to find distinct combinations of column values.\nAlthough scanning is fast, performance will degrade on hundreds of millions of\nrecords. If there are multiple columns in the\nLATEST ON\nclause, this will\nresult in a full table scan.\n\n### LATEST ON over sub-query‚Äã\n\nFor this example, we can create another table called\nunordered_balances\nwith\nthe following SQL:\n\n```questdb-sql\nCREATE TABLE unordered_balances (    cust_id SYMBOL,    balance_ccy SYMBOL,    balance DOUBLE,    ts TIMESTAMP);insert into unordered_balances values ('2', 'USD', 950, '2020-04-21T16:08:34.404665Z');insert into unordered_balances values ('1', 'USD', 330.5, '2020-04-22T16:20:14.404997Z');insert into unordered_balances values ('2', 'USD', 900.75, '2020-04-22T16:12:43.504432Z');insert into unordered_balances values ('1', 'USD', 1500, '2020-04-21T16:11:32.904234Z');insert into unordered_balances values ('1', 'USD', 600.5, '2020-04-21T16:03:43.504432Z');insert into unordered_balances values ('1', 'EUR', 650.5, '2020-04-22T16:11:32.904234Z');insert into unordered_balances values ('2', 'EUR', 880.2, '2020-04-22T16:18:34.404665Z');insert into unordered_balances values ('2', 'EUR', 780.2, '2020-04-21T16:11:22.704665Z');\n```\n\nNote that this table doesn't have a designated timestamp column and also\ncontains time series that are unordered by\nts\ncolumn.\nDue to the absent designated timestamp column, we can't use\nLATEST ON\ndirectly\non this table, but it's possible to use\nLATEST ON\nover a sub-query:\nLatest balance by customer over unordered data\n\n```questdb-sql\n(SELECT * FROM unordered_balances)LATEST ON ts PARTITION BY cust_id;\n```\n\nJust like with the\nbalances\ntable, the query returns two rows with the most\nrecent records per unique\ncust_id\nvalue:\n\n| cust_id | balance_ccy | balance | ts |\n| --- | --- | --- | --- |\n| 2 | EUR | 880.2 | 2020-04-22T16:18:34.404665Z |\n| 1 | USD | 330.5 | 2020-04-22T16:20:14.404997Z |\n\n\n### Execution order‚Äã\n\nThe following queries illustrate how to change the execution order in a query by\nusing brackets.\n\n#### WHERE first‚Äã\n\n\n```questdb-sql\nSELECT * FROM balancesWHERE balance > 800LATEST ON ts PARTITION BY cust_id;\n```\n\nThis query executes\nWHERE\nbefore\nLATEST ON\nand returns the most recent\nbalance which is above 800. The execution order is as follows:\n- filter out all balances below 800\n- find the latest balance bycust_id\n\n| cust_id | balance_ccy | balance | ts |\n| --- | --- | --- | --- |\n| 1 | USD | 1500 | 2020-04-22T16:11:22.704665Z |\n| 2 | EUR | 880.2 | 2020-04-22T16:18:34.404665Z |\n\n\n#### LATEST ON first‚Äã\n\n\n```questdb-sql\n(SELECT * FROM balances LATEST ON ts PARTITION BY cust_id) --note the bracketsWHERE balance > 800;\n```\n\nThis query executes\nLATEST ON\nbefore\nWHERE\nand returns the most recent\nrecords, then filters out those below 800. The steps are:\n- Find the latest balances by customer ID.\n- Filter out balances below 800. Since the latest balance for customer 1 is\nequal to 330.5, it is filtered out in this step.\n\n| cust_id | balance_ccy | balance | inactive | ts |\n| --- | --- | --- | --- | --- |\n| 2 | EUR | 880.2 | FALSE | 2020-04-22T16:18:34.404665Z |\n\n\n#### Combination‚Äã\n\nIt's possible to combine a time-based filter with the balance filter from the\nprevious example to query the latest values for the\n2020-04-21\ndate and filter\nout those below 800.\n\n```questdb-sql\n(balances WHERE ts in '2020-04-21' LATEST ON ts PARTITION BY cust_id)WHERE balance > 800;\n```\n\nSince QuestDB allows you to omit the\nSELECT * FROM\npart of the query, we\nomitted it to keep the query compact.\nSuch a combination is very powerful since it allows you to find the latest\nvalues for a time slice of the data and then apply a filter to them in a single\nquery.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1334,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-73fbb9026afa",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/functions/window-functions/overview",
    "title": "Window Functions Overview | QuestDB",
    "text": "Window functions perform calculations across sets of table rows related to the current row. Unlike aggregate functions that return a single result for a group of rows, window functions return a value for\nevery row\nwhile considering a \"window\" of related rows defined by the\nOVER\nclause.\n\n## Syntax‚Äã\n\n\n```sql\nfunction_name(arguments) OVER (    [PARTITION BY column [, ...]]    [ORDER BY column [ASC | DESC] [, ...]]    [frame_clause])\n```\n\n- PARTITION BY: Divides rows into groups; the function resets for each group\n- ORDER BY: Defines the order of rows within each partition\n- frame_clause: Specifies which rows relative to the current row to include (e.g.,ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)\nSome functions (\nfirst_value\n,\nlast_value\n,\nlag\n,\nlead\n) also support\nIGNORE NULLS\nor\nRESPECT NULLS\nbefore the\nOVER\nkeyword to control null handling.\nFor complete syntax details including frame specifications and exclusion options, see\nOVER Clause Syntax\n.\nWindow function arithmetic (9.3.1+)\nArithmetic operations on window functions (e.g.,\nsum(...) OVER (...) / sum(...) OVER (...)\n) are supported from version 9.3.1. Earlier versions require wrapping window functions in CTEs or subqueries.\n\n## Quick reference‚Äã\n\n\n| Function | Description | Respects Frame |\n| --- | --- | --- |\n| avg() | Average value in window (also supports EMA and VWEMA) | Yes (standard) / No (EMA/VWEMA) |\n| count() | Count rows or non-null values | Yes |\n| sum() | Sum of values in window | Yes |\n| ksum() | Sum with Kahan precision | Yes |\n| min() | Minimum value in window | Yes |\n| max() | Maximum value in window | Yes |\n| first_value() | First value in window | Yes |\n| last_value() | Last value in window | Yes |\n| row_number() | Sequential row number | No |\n| rank() | Rank with gaps for ties | No |\n| dense_rank() | Rank without gaps | No |\n| percent_rank() | Relative rank (0 to 1) | No |\n| lag() | Value from previous row | No |\n| lead() | Value from following row | No |\n\nRespects Frame\n: Functions marked \"Yes\" use the frame clause (\nROWS\n/\nRANGE BETWEEN\n). Functions marked \"No\" operate on the entire partition regardless of frame specification.\n\n## When to use window functions‚Äã\n\nWindow functions are essential for analytics tasks where you need to:\n- Calculaterunning totalsorcumulative sums\n- Computemoving averagesover time periods\n- Find themaximum or minimumvalue within a sequence\n- Rankitems within categories\n- Accessprevious or next rowvalues without self-joins\n- Compare each row to anaggregateof related rows\n\n### Example: Moving average‚Äã\n\n4-row moving average of price\nDemo this query\n\n```questdb-sql\nSELECT    symbol,    price,    timestamp,    avg(price) OVER (        PARTITION BY symbol        ORDER BY timestamp        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW    ) AS moving_avgFROM tradesWHERE timestamp IN '[$today]'LIMIT 100;\n```\n\nThis calculates a moving average over the current row plus three preceding rows, grouped by symbol.\n\n## How window functions work‚Äã\n\nA window function has three key components:\n\n```questdb-sql\nfunction_name(arguments) OVER (    [PARTITION BY column]      -- Divide into groups    [ORDER BY column]          -- Order within groups    [frame_specification]      -- Define which rows to include)\n```\n\n\n### 1. Partitioning‚Äã\n\nPARTITION BY\ndivides rows into independent groups. The window function\nresets\nfor each partition‚Äîcalculations start fresh, as if each group were a separate table.\nWhen to use it:\nWhen storing multiple instruments in the same table, you typically want calculations isolated per symbol. For example:\n- Cumulative volumeper symbol(not across all instruments)\n- Moving average priceper symbol(not mixing BTC-USDT with ETH-USDT)\n- Intraday high/lowper symbol\n\n```questdb-sql\n-- Without PARTITION BY: cumulative volume across ALL symbols (mixing instruments)sum(amount) OVER (ORDER BY timestamp)-- With PARTITION BY: cumulative volume resets for each symbolsum(amount) OVER (PARTITION BY symbol ORDER BY timestamp)\n```\n\n\n| timestamp | symbol | amount | cumulative (no partition) | cumulative (by symbol) |\n| --- | --- | --- | --- | --- |\n| 09:00 | BTC-USDT | 100 | 100 | 100 |\n| 09:01 | ETH-USDT | 200 | 300 | 200 |\n| 09:02 | BTC-USDT | 150 | 450 | 250 |\n| 09:03 | ETH-USDT | 100 | 550 | 300 |\n\nWithout\nPARTITION BY\n, all rows are treated as a single partition.\n\n### 2. Ordering‚Äã\n\nORDER BY\nwithin the\nOVER\nclause determines the logical order for calculations:\n\n```questdb-sql\n-- Row numbers ordered by timestamprow_number() OVER (ORDER BY timestamp)\n```\n\nThis is independent of the query-level\nORDER BY\n.\nTime-series optimization\nFor tables with a designated timestamp, data is already ordered by time. When your window\nORDER BY\nmatches the designated timestamp, QuestDB skips redundant sorting‚Äîno performance penalty.\n\n### 3. Frame specification‚Äã\n\nThe frame defines which rows relative to the current row are included in the calculation:\n\n```questdb-sql\n-- Sum of current row plus 2 preceding rowssum(price) OVER (    ORDER BY timestamp    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)\n```\n\nFor complete frame syntax details, see\nOVER Clause Syntax\n.\n\n## Aggregate vs window functions‚Äã\n\nThe key difference: aggregate functions collapse rows into one result, while window functions keep all rows and add a computed column.\nSource data:\n\n| timestamp | symbol | price |\n| --- | --- | --- |\n| 09:00 | BTC-USDT | 100 |\n| 09:01 | BTC-USDT | 102 |\n| 09:02 | BTC-USDT | 101 |\n\nAggregate function\n‚Äî returns one row:\n\n```questdb-sql\nSELECT symbol, avg(price) AS avg_priceFROM tradesGROUP BY symbol;\n```\n\n\n| symbol | avg_price |\n| --- | --- |\n| BTC-USDT | 101 |\n\nWindow function\n‚Äî returns all rows with computed column:\n\n```questdb-sql\nSELECT timestamp, symbol, price,       avg(price) OVER (PARTITION BY symbol) AS avg_priceFROM trades;\n```\n\n\n| timestamp | symbol | price | avg_price |\n| --- | --- | --- | --- |\n| 09:00 | BTC-USDT | 100 | 101 |\n| 09:01 | BTC-USDT | 102 | 101 |\n| 09:02 | BTC-USDT | 101 | 101 |\n\nEach row keeps its original data\nplus\nthe average‚Äîuseful for comparing each price to the mean, calculating deviations, or adding running totals alongside the raw values.\n\n## ROWS vs RANGE frames‚Äã\n\nQuestDB supports two frame types:\n\n### ROWS frame‚Äã\n\nBased on physical row count:\n\n```questdb-sql\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n```\n\nIncludes exactly 4 rows: current row plus 3 before it.\n\n### RANGE frame‚Äã\n\nBased on values in the\nORDER BY\ncolumn (must be a timestamp):\n\n```questdb-sql\nRANGE BETWEEN '1' MINUTE PRECEDING AND CURRENT ROW\n```\n\nIncludes all rows within 1 minute of the current row's timestamp.\nnote\nRANGE frames have a known limitation: rows with the same ORDER BY value (\"peers\") do not produce identical results as required by the SQL standard. QuestDB currently processes peers as distinct rows rather than treating them as a group. See\nGitHub issue #5177\nfor details.\nFor complete frame syntax, see\nOVER Clause Syntax\n.\n\n## Common patterns‚Äã\n\n\n### Running total‚Äã\n\nUse the\nCUMULATIVE\nshorthand for running totals:\nCumulative sum\nDemo this query\n\n```questdb-sql\nSELECT    timestamp,    amount,    sum(amount) OVER (        ORDER BY timestamp        CUMULATIVE    ) AS running_totalFROM tradesWHERE timestamp IN '[$today]';\n```\n\nThis is equivalent to\nROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n.\n\n### VWAP (Volume-Weighted Average Price)‚Äã\n\nFor high-frequency market data, VWAP is typically calculated over OHLC time series using the typical price\n(high + low + close) / 3\n:\nVWAP over OHLC data\nDemo this query\n\n```questdb-sql\nDECLARE @symbol := 'BTC-USDT'WITH ohlc AS (    SELECT        timestamp AS ts,        symbol,        first(price) AS open,        max(price) AS high,        min(price) AS low,        last(price) AS close,        sum(amount) AS volume    FROM trades    WHERE timestamp IN '2024-05-22' AND symbol = @symbol    SAMPLE BY 1m ALIGN TO CALENDAR)SELECT    ts,    symbol,    open, high, low, close, volume,    sum((high + low + close) / 3 * volume) OVER (ORDER BY ts CUMULATIVE)        / sum(volume) OVER (ORDER BY ts CUMULATIVE) AS vwapFROM ohlcORDER BY ts;\n```\n\n\n### Compare to group average‚Äã\n\nPrice vs symbol average\nDemo this query\n\n```questdb-sql\nSELECT    symbol,    price,    timestamp,    avg(price) OVER (PARTITION BY symbol) AS symbol_avg,    price - avg(price) OVER (PARTITION BY symbol) AS diff_from_avgFROM tradesWHERE timestamp IN '[$today]';\n```\n\n\n### Rank within category‚Äã\n\nRank prices per symbol\nDemo this query\n\n```questdb-sql\nSELECT    symbol,    price,    timestamp,    rank() OVER (        PARTITION BY symbol        ORDER BY price DESC    ) AS price_rankFROM tradesWHERE timestamp IN '[$today]';\n```\n\n\n### Access previous row‚Äã\n\nCalculate price change\nDemo this query\n\n```questdb-sql\nSELECT    timestamp,    price,    lag(price) OVER (ORDER BY timestamp) AS prev_price,    price - lag(price) OVER (ORDER BY timestamp) AS price_changeFROM tradesWHERE timestamp IN '[$today]'    AND symbol = 'BTC-USDT';\n```\n\n\n## Next steps‚Äã\n\n- Function Reference: Detailed documentation for each window function\n- OVER Clause Syntax: Complete syntax for partitioning, ordering, and frame specifications\nLooking for WINDOW JOIN?\nWINDOW JOIN\nis a separate feature for aggregating data from a\ndifferent table\nwithin a time window. Use window functions (this page) for calculations within a single table; use WINDOW JOIN to correlate two time-series tables.\n\n## Common mistakes‚Äã\n\n\n### Using window functions in WHERE‚Äã\n\nWindow functions cannot be used directly in\nWHERE\nclauses:\nIncorrect - will not work\n\n```questdb-sql\nSELECT symbol, priceFROM tradesWHERE avg(price) OVER (ORDER BY timestamp) > 100;\n```\n\nUse a CTE or subquery instead:\nCorrect approach\nDemo this query\n\n```questdb-sql\nWITH prices AS (    SELECT        symbol,        price,        avg(price) OVER (ORDER BY timestamp) AS moving_avg    FROM trades    WHERE timestamp IN '[$today]')SELECT * FROM pricesWHERE moving_avg > 100;\n```\n\n\n### Missing ORDER BY‚Äã\n\nWithout\nORDER BY\n, the window includes all rows in the partition, which may not be the intended behavior:\nAll rows show same average\nDemo this query\n\n```questdb-sql\nSELECT    symbol,    price,    timestamp,    avg(price) OVER (PARTITION BY symbol) AS avg_price  -- Same value for all rows in partitionFROM tradesWHERE timestamp IN '[$today]';\n```\n\nAdd\nORDER BY\nfor cumulative/moving calculations:\nRunning average\nDemo this query\n\n```questdb-sql\nSELECT    symbol,    price,    timestamp,    avg(price) OVER (        PARTITION BY symbol        ORDER BY timestamp    ) AS running_avgFROM tradesWHERE timestamp IN '[$today]';\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 1649,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-dffc51dc80ad",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/functions/parquet",
    "title": "Parquet functions | QuestDB",
    "text": "QuestDB can read and query external\nApache Parquet\nfiles using SQL.\nTo export data as Parquet, see\nParquet Export\n.\ninfo\nApache Parquet support is in\nbeta\n. Please report issues via\nemail\n,\nSlack\n, or\nDiscourse\n.\n\n## read_parquet‚Äã\n\nReads a parquet file as a table.\nread_parquet(parquet_file_path)\n\n### Usage‚Äã\n\nThe file path must be within the\nconfigured root directory\n. It can be specified as a relative path (resolved under the root) or as an absolute path (which must still start with the root directory). Path traversal (\n../\n) is not allowed.\nRelative path\n\n```questdb-sql\nSELECT * FROM read_parquet('trades.parquet')WHERE side = 'buy'LIMIT 1;\n```\n\n\n| symbol | side | price | amount | timestamp |\n| --- | --- | --- | --- | --- |\n| BTC-USD | buy | 62755.6 | 0.00043367 | 2024-07-01T00:46:39.754075Z |\n\nAbsolute path (must be within the configured root)\n\n```questdb-sql\nSELECT * FROM read_parquet('/var/lib/questdb/import/trades.parquet');\n```\n\nJoin a Parquet file with a QuestDB table\n\n```questdb-sql\nSELECT t.symbol, t.price, r.labelFROM read_parquet('trades.parquet') tJOIN ref_data r ON t.symbol = r.symbol;\n```\n\n\n### Configuration‚Äã\n\nFor security reasons, reading is only allowed from a configured directory. By default, this is the\nimport\ndirectory\ninside the QuestDB root directory (e.g.\n/var/lib/questdb/import/\n). To change it, set\ncairo.sql.copy.root\n:\n- Inserver.conf:cairo.sql.copy.root=/path/to/dir\n- Or via the environment variableQDB_CAIRO_SQL_COPY_ROOT\n\n### Limitations‚Äã\n\nParquet format supports a rich set of data types, including structural types. QuestDB can only read Parquet columns whose types map to QuestDB types:\n- Boolean\n- Byte\n- Short\n- Char\n- Int\n- Long\n- Long128\n- Long256\n- Float\n- Double\n- Varchar (also reads Symbol columns as Varchar)\n- Timestamp\n- Date\n- UUID\n- IPv4\n- GeoHash (Byte, Short, Int, Long)\n- Binary\n- Array (Double)\nParquet columns with unsupported data types are ignored.\nOnly a single file can be read per\nread_parquet\ncall.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 309,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-cfc5a9b0cb06",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/query/sql-execution-order",
    "title": "SQL execution order | QuestDB",
    "text": "QuestDB attempts to implement standard ANSI SQL. We also try to be compatible\nwith PostgreSQL, although parts of this are a work in progress. QuestDB\nimplements these clauses which have the following execution order:\n- FROM\n- ON\n- JOIN\n- WHERE\n- LATEST ON\n- GROUP BY(optional)\n- WITH\n- HAVING(implicit)\n- SELECT\n- DISTINCT\n- ORDER BY\n- LIMIT\nWe have also implemented sub-queries that users may execute at any part of a\nquery that mentions a table name. The sub-query implementation adds almost zero\nexecution cost to SQL. We encourage the use of sub-queries as they add flavors\nof functional language features to traditional SQL.\nFor more information on the SQL extensions in QuestDB which deviate from ANSI\nSQL and PostgreSQL, see the\nSQL extensions documentation\n.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 130,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-a71855782103",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/troubleshooting/os-error-codes",
    "title": "List of OS error codes | QuestDB",
    "text": "The following document contains a partial list of Operating System (OS) error\ncodes that can be reported when running QuestDB and brief descriptions for them.\nIf instead you come across a QuestDB error code (e.g.\nER001\n), refer to the\nQuestDB Error Codes\npage.\n\n## Where to find error codes‚Äã\n\nQuestDB includes OS error codes into the\n[<code>]\npart of the exception\nmessage written to the error logs:\n\n```questdb-sql\nio.questdb.cairo.CairoException: [24] could not open read-only [file=/root/.questdb/db/cpu/service.k]\n```\n\nThe above message reports error code 24 which is \"Too many open files\" on Linux.\nSome error log messages may also include\nerrno=<code>\nkey/value pair:\n\n```questdb-sql\n2022-02-01T13:40:10.636014Z E i.q.c.l.t.LineTcpConnectionContext [8655] could not process line data [table=test_table, msg=could not mmap  [size=248, offset=0, fd=1766, memUsed=314809894008, fileLen=8192], errno=12]\n```\n\nThe above message reports error code 12 which is \"Out of memory\" on Linux.\n\n## Linux error codes‚Äã\n\n\n| Error number | Error name | Description |\n| --- | --- | --- |\n| 1 | EPERM | Operation not permitted. |\n| 2 | ENOENT | No such file or directory. |\n| 3 | ESRCH | No such process. |\n| 4 | EINTR | Interrupted system call. |\n| 5 | EIO | I/O error. |\n| 6 | ENXIO | No such device or address. |\n| 7 | E2BIG | Argument list too long. |\n| 8 | ENOEXEC | Exec format error. |\n| 9 | EBADF | Bad file number. |\n| 10 | ECHILD | No child processes. |\n| 11 | EAGAIN | Try again. |\n| 12 | ENOMEM | Out of memory. |\n| 13 | EACCES | Permission denied. |\n| 14 | EFAULT | Bad address. |\n| 15 | ENOTBLK | Block device required. |\n| 16 | EBUSY | Device or resource busy. |\n| 17 | EEXIST | File exists. |\n| 18 | EXDEV | Cross-device link. |\n| 19 | ENODEV | No such device. |\n| 20 | ENOTDIR | Not a directory. |\n| 21 | EISDIR | Is a directory. |\n| 22 | EINVAL | Invalid argument. |\n| 23 | ENFILE | File table overflow. |\n| 24 | EMFILE | Too many open files. |\n| 25 | ENOTTY | Not a typewriter. |\n| 26 | ETXTBSY | Text file busy. |\n| 27 | EFBIG | File too large. |\n| 28 | ENOSPC | No space left on device. |\n| 29 | ESPIPE | Illegal seek. |\n| 30 | EROFS | Read-only file system. |\n| 31 | EMLINK | Too many links. |\n| 32 | EPIPE | Broken pipe. |\n| 33 | EDOM | Math argument out of domain of func. |\n| 34 | ERANGE | Math result not representable. |\n| 35 | EDEADLK | Resource deadlock would occur. |\n| 36 | ENAMETOOLONG | File name too long. |\n| 37 | ENOLCK | No record locks available. |\n| 38 | ENOSYS | Function not implemented. |\n| 39 | ENOTEMPTY | Directory not empty. |\n| 40 | ELOOP | Too many symbolic links encountered. |\n| 42 | ENOMSG | No message of desired type. |\n| 43 | EIDRM | Identifier removed. |\n| 44 | ECHRNG | Channel number out of range. |\n| 45 | EL2NSYNC | Level 2 not synchronized. |\n| 46 | EL3HLT | Level 3 halted. |\n| 47 | EL3RST | Level 3 reset. |\n| 48 | ELNRNG | Link number out of range. |\n| 49 | EUNATCH | Protocol driver not attached. |\n| 50 | ENOCSI | No CSI structure available. |\n| 51 | EL2HLT | Level 2 halted. |\n| 52 | EBADE | Invalid exchange. |\n| 53 | EBADR | Invalid request descriptor. |\n| 54 | EXFULL | Exchange full. |\n| 55 | ENOANO | No anode. |\n| 56 | EBADRQC | Invalid request code. |\n| 57 | EBADSLT | Invalid slot. |\n| 59 | EBFONT | Bad font file format. |\n| 60 | ENOSTR | Device not a stream. |\n| 61 | ENODATA | No data available. |\n| 62 | ETIME | Timer expired. |\n| 63 | ENOSR | Out of streams resources. |\n| 64 | ENONET | Machine is not on the network. |\n| 65 | ENOPKG | Package not installed. |\n| 66 | EREMOTE | Object is remote. |\n| 67 | ENOLINK | Link has been severed. |\n| 68 | EADV | Advertise error. |\n| 69 | ESRMNT | Srmount error. |\n| 70 | ECOMM | Communication error on send. |\n| 71 | EPROTO | Protocol error. |\n| 72 | EMULTIHOP | Multihop attempted. |\n| 73 | EDOTDOT | RFS specific error. |\n| 74 | EBADMSG | Not a data message. |\n| 75 | EOVERFLOW | Value too large for defined data type. |\n| 76 | ENOTUNIQ | Name not unique on network. |\n| 77 | EBADFD | File descriptor in bad state. |\n| 78 | EREMCHG | Remote address changed. |\n| 79 | ELIBACC | Can not access a needed shared library. |\n| 80 | ELIBBAD | Accessing a corrupted shared library. |\n| 81 | ELIBSCN | .lib section in a.out corrupted. |\n| 82 | ELIBMAX | Attempting to link in too many shared libraries. |\n| 83 | ELIBEXEC | Cannot exec a shared library directly. |\n| 84 | EILSEQ | Illegal byte sequence. |\n| 85 | ERESTART | Interrupted system call should be restarted. |\n| 86 | ESTRPIPE | Streams pipe error. |\n| 87 | EUSERS | Too many users. |\n| 88 | ENOTSOCK | Socket operation on non-socket. |\n| 89 | EDESTADDRREQ | Destination address required. |\n| 90 | EMSGSIZE | Message too long. |\n| 91 | EPROTOTYPE | Protocol wrong type for socket. |\n| 92 | ENOPROTOOPT | Protocol not available. |\n| 93 | EPROTONOSUPPORT | Protocol not supported. |\n| 94 | ESOCKTNOSUPPORT | Socket type not supported. |\n| 95 | EOPNOTSUPP | Operation not supported on transport endpoint. |\n| 96 | EPFNOSUPPORT | Protocol family not supported. |\n| 97 | EAFNOSUPPORT | Address family not supported by protocol. |\n| 98 | EADDRINUSE | Address already in use. |\n| 99 | EADDRNOTAVAIL | Cannot assign requested address. |\n| 100 | ENETDOWN | Network is down. |\n| 101 | ENETUNREACH | Network is unreachable. |\n| 102 | ENETRESET | Network dropped connection because of reset. |\n| 103 | ECONNABORTED | Software caused connection abort. |\n| 104 | ECONNRESET | Connection reset by peer. |\n| 105 | ENOBUFS | No buffer space available. |\n| 106 | EISCONN | Transport endpoint is already connected. |\n| 107 | ENOTCONN | Transport endpoint is not connected. |\n| 108 | ESHUTDOWN | Cannot send after transport endpoint shutdown. |\n| 109 | ETOOMANYREFS | Too many references: cannot splice. |\n| 110 | ETIMEDOUT | Connection timed out. |\n| 111 | ECONNREFUSED | Connection refused. |\n| 112 | EHOSTDOWN | Host is down. |\n| 113 | EHOSTUNREACH | No route to host. |\n| 114 | EALREADY | Operation already in progress. |\n| 115 | EINPROGRESS | Operation now in progress. |\n| 116 | ESTALE | Stale NFS file handle. |\n| 117 | EUCLEAN | Structure needs cleaning. |\n| 118 | ENOTNAM | Not a XENIX named type file. |\n| 119 | ENAVAIL | No XENIX semaphores available. |\n| 120 | EISNAM | Is a named type file. |\n| 121 | EREMOTEIO | Remote I/O error. |\n| 122 | EDQUOT | Quota exceeded. |\n| 123 | ENOMEDIUM | No medium found. |\n| 124 | EMEDIUMTYPE | Wrong medium type. |\n| 125 | ECANCELED | Operation Canceled. |\n| 126 | ENOKEY | Required key not available. |\n| 127 | EKEYEXPIRED | Key has expired. |\n| 128 | EKEYREVOKED | Key has been revoked. |\n| 129 | EKEYREJECTED | Key was rejected by service. |\n| 130 | EOWNERDEAD | Owner died. |\n| 131 | ENOTRECOVERABLE | State not recoverable. |\n\n\n## Windows error codes‚Äã\n\nA complete list of Windows error codes may be found\nhere\n.\n\n| Error number | Error name | Description |\n| --- | --- | --- |\n| 1 | ERROR_INVALID_FUNCTION | Incorrect function. |\n| 2 | ERROR_FILE_NOT_FOUND | The system cannot find the file specified. |\n| 3 | ERROR_PATH_NOT_FOUND | The system cannot find the path specified. |\n| 4 | ERROR_TOO_MANY_OPEN_FILES | The system cannot open the file. |\n| 5 | ERROR_ACCESS_DENIED | Access is denied. |\n| 6 | ERROR_INVALID_HANDLE | The handle is invalid. |\n| 7 | ERROR_ARENA_TRASHED | The storage control blocks were destroyed. |\n| 8 | ERROR_NOT_ENOUGH_MEMORY | Not enough memory is available to process this command. |\n| 9 | ERROR_INVALID_BLOCK | The storage control block address is invalid. |\n| 10 | ERROR_BAD_ENVIRONMENT | The environment is incorrect. |\n| 11 | ERROR_BAD_FORMAT | An attempt was made to load a program with an incorrect format. |\n| 12 | ERROR_INVALID_ACCESS | The access code is invalid. |\n| 13 | ERROR_INVALID_DATA | The data is invalid. |\n| 14 | ERROR_OUTOFMEMORY | Not enough storage is available to complete this operation. |\n| 15 | ERROR_INVALID_DRIVE | The system cannot find the drive specified. |\n| 16 | ERROR_CURRENT_DIRECTORY | The directory cannot be removed. |\n| 17 | ERROR_NOT_SAME_DEVICE | The system cannot move the file to a different disk drive. |\n| 18 | ERROR_NO_MORE_FILES | There are no more files. |\n| 19 | ERROR_WRITE_PROTECT | The media is write protected. |\n| 20 | ERROR_BAD_UNIT | The system cannot find the device specified. |\n| 21 | ERROR_NOT_READY | The device is not ready. |\n| 22 | ERROR_BAD_COMMAND | The device does not recognize the command. |\n| 23 | ERROR_CRC | Data error (cyclic redundancy check). |\n| 24 | ERROR_BAD_LENGTH | The program issued a command but the command length is incorrect. |\n| 25 | ERROR_SEEK | The drive cannot locate a specific area or track on the disk. |\n| 26 | ERROR_NOT_DOS_DISK | The specified disk or diskette cannot be accessed. |\n| 27 | ERROR_SECTOR_NOT_FOUND | The drive cannot find the sector requested. |\n| 28 | ERROR_OUT_OF_PAPER | The printer is out of paper. |\n| 29 | ERROR_WRITE_FAULT | The system cannot write to the specified device. |\n| 30 | ERROR_READ_FAULT | The system cannot read from the specified device. |\n| 31 | ERROR_GEN_FAILURE | A device attached to the system is not functioning. |\n| 32 | ERROR_SHARING_VIOLATION | The process cannot access the file because it is being used by another process. |\n| 33 | ERROR_LOCK_VIOLATION | The process cannot access the file because another process has locked a portion of the file. |\n| 34 | ERROR_WRONG_DISK | The wrong diskette is in the drive. Insert %2 (Volume Serial Number: %3) into drive %1. |\n| 36 | ERROR_SHARING_BUFFER_EXCEEDED | Too many files opened for sharing. |\n| 38 | ERROR_HANDLE_EOF | Reached the end of the file. |\n| 39 | ERROR_HANDLE_DISK_FULL | The disk is full. |\n| 87 | ERROR_INVALID_PARAMETER | The parameter is incorrect. |\n| 112 | ERROR_DISK_FULL | The disk is full. |\n| 123 | ERROR_INVALID_NAME | The file name, directory name, or volume label syntax is incorrect. |\n| 1450 | ERROR_NO_SYSTEM_RESOURCES | Insufficient system resources exist to complete the requested service. |\n\n\n## MacOS error codes‚Äã\n\n\n| Error number | Error name | Description |\n| --- | --- | --- |\n| 0 | Base | Undefined error: 0 |\n| 1 | EPERM | Operation not permitted |\n| 2 | ENOENT | No such file or directory |\n| 3 | ESRCH | No such process |\n| 4 | EINTR | Interrupted system call |\n| 5 | EIO | Input/output error |\n| 6 | ENXIO | Device not configured |\n| 7 | E2BIG | Argument list too long |\n| 8 | ENOEXEC | Exec format error |\n| 9 | EBADF | Bad file descriptor |\n| 10 | ECHILD | No child processes |\n| 11 | EDEADLK | Resource deadlock avoided |\n| 12 | ENOMEM | Cannot allocate memory |\n| 13 | EACCES | Permission denied |\n| 14 | EFAULT | Bad address |\n| 15 | ENOTBLK | Block device required |\n| 16 | EBUSY | Device busy |\n| 17 | EEXIST | File exists |\n| 18 | EXDEV | Cross-device link |\n| 19 | ENODEV | Operation not supported by device |\n| 20 | ENOTDIR | Not a directory |\n| 21 | EISDIR | Is a directory |\n| 22 | EINVAL | Invalid argument |\n| 23 | ENFILE | Too many open files in system |\n| 24 | EMFILE | Too many open files |\n| 25 | ENOTTY | Inappropriate ioctl for device |\n| 26 | ETXTBSY | Text file busy |\n| 27 | EFBIG | File too large |\n| 28 | ENOSPC | No space left on device |\n| 29 | ESPIPE | Illegal seek |\n| 30 | EROFS | Read-only file system |\n| 31 | EMLINK | Too many links |\n| 32 | EPIPE | Broken pipe |\n| 33 | EDOM | Numerical argument out of domain |\n| 34 | ERANGE | Result too large |\n| 35 | EAGAIN | Resource temporarily unavailable |\n| 36 | EINPROGRESS | Operation now in progress |\n| 37 | EALREADY | Operation already in progress |\n| 38 | ENOTSOCK | Socket operation on non-socket |\n| 39 | EDESTADDRREQ | Destination address required |\n| 40 | EMSGSIZE | Message too long |\n| 41 | EPROTOTYPE | Protocol wrong type for socket |\n| 42 | ENOPROTOOPT | Protocol not available |\n| 43 | EPROTONOSUPPORT | Protocol not supported |\n| 44 | ESOCKTNOSUPPORT | Socket type not supported |\n| 45 | ENOTSUP | Operation not supported |\n| 46 | EPFNOSUPPORT | Protocol family not supported |\n| 47 | EAFNOSUPPORT | Address family not supported by protocol family |\n| 48 | EADDRINUSE | Address already in use |\n| 49 | EADDRNOTAVAIL | Can‚Äôt assign requested address |\n| 50 | ENETDOWN | Network is down |\n| 51 | ENETUNREACH | Network is unreachable |\n| 52 | ENETRESET | Network dropped connection on reset |\n| 53 | ECONNABORTED | Software caused connection abort |\n| 54 | ECONNRESET | Connection reset by peer |\n| 55 | ENOBUFS | No buffer space available |\n| 56 | EISCONN | Socket is already connected |\n| 57 | ENOTCONN | Socket is not connected |\n| 58 | ESHUTDOWN | Can‚Äôt send after socket shutdown |\n| 59 | ETOOMANYREFS | Too many references: can‚Äôt splice |\n| 60 | ETIMEDOUT | Operation timed out |\n| 61 | ECONNREFUSED | Connection refused |\n| 62 | ELOOP | Too many levels of symbolic links |\n| 63 | ENAMETOOLONG | File name too long |\n| 64 | EHOSTDOWN | Host is down |\n| 65 | EHOSTUNREACH | No route to host |\n| 66 | ENOTEMPTY | Directory not empty |\n| 67 | EPROCLIM | Too many processes |\n| 68 | EUSERS | Too many users |\n| 69 | EDQUOT | Disc quota exceeded |\n| 70 | ESTALE | Stale NFS file handle |\n| 71 | EREMOTE | Too many levels of remote in path |\n| 72 | EBADRPC | RPC struct is bad |\n| 73 | ERPCMISMATCH | RPC version wrong |\n| 74 | EPROGUNAVAIL | RPC prog. not avail |\n| 75 | EPROGMISMATCH | Program version wrong |\n| 76 | EPROCUNAVAIL | Bad procedure for program |\n| 77 | ENOLCK | No locks available |\n| 78 | ENOSYS | Function not implemented |\n| 79 | EFTYPE | Inappropriate file type or format |\n| 80 | EAUTH | Authentication error |\n| 81 | ENEEDAUTH | Need authenticator |\n| 82 | EPWROFF | Device power is off |\n| 83 | EDEVERR | Device error |\n| 84 | EOVERFLOW | Value too large to be stored in data type |\n| 85 | EBADEXEC | Bad executable |\n| 86 | EBADARCH | Bad CPU type in executable |\n| 87 | ESHLIBVERS | Shared library version mismatch |\n| 88 | EBADMACHO | Malformed Macho file |\n| 89 | ECANCELED | Operation canceled |\n| 90 | EIDRM | Identifier removed |\n| 91 | ENOMSG | No message of desired type |\n| 92 | EILSEQ | Illegal byte sequence |\n| 93 | ENOATTR | Attribute not found |\n| 94 | EBADMSG | Bad message |\n| 95 | EMULTIHOP | EMULTIHOP (Reserved) |\n| 96 | ENODATA | No message available on STREAM |\n| 97 | ENOLINK | ENOLINK (Reserved) |\n| 98 | ENOSR | No STREAM resources |\n| 99 | ENOSTR | Not a STREAM |\n| 100 | EPROTO | Protocol error |\n| 101 | ETIME | STREAM ioctl timeout |\n| 102 | EOPNOTSUPP | Operation not supported on socket |\n| 103 | ENOPOLICY | Policy not found |\n| 104 | ENOTRECOVERABLE | State not recoverable |\n| 105 | EOWNERDEAD | Previous owner died |\n| 106 | EQFULL | Interface output queue is full |\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 3014,
    "metadata": {}
  },
  {
    "id": "questdb-official_docs-da82cea9eb59",
    "origin": "questdb",
    "source_type": "official_docs",
    "url": "https://questdb.io/docs/deployment/systemd",
    "title": "Launch QuestDB with systemd | QuestDB",
    "text": "Use systemd to run QuestDB as a system or user service. This guide will\ndemonstrate an initial configuration which you can use as the basis for your\ninstallation scripts. It will also demonstrate how to setup and start a QuestDB\nsystemd service.\n\n## Prerequisites‚Äã\n\nThe prerequisites for deploying QuestDB with systemd are:\n- A Unix machine supporting systemd\n\n## Initial system configuration‚Äã\n\nThe following commands inform a basis for your systemd service. Prior to running\nsystemd, you will require some directory structure and a binary for QuestDB.\nDepending on your specific needs and operational preferences, your commands may\ndiffer. The goal is to give you a helpful starting point for the example\nservice. The example presumes that you have used a privileged user to create a\nuser with appropriately scoped permissions.\n\n```bash\n#!/bin/bash# Download and install the JDKcurl -s https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz -o jdk.tar.gzmkdir -p ~/jdktar -xzf jdk.tar.gz -C ~/jdk --strip-components=1export JAVA_HOME=~/jdkexport PATH=$JAVA_HOME/bin:$PATH# Download and set up QuestDBcurl -s https://dl.questdb.io/snapshots/questdb-latest-no-jre-bin.tar.gz -o questdb.tar.gzmkdir -p ~/questdb/binarytar -xzf questdb.tar.gz -C ~/questdb/binary --strip-components 1mv ~/questdb/binary/questdb.jar ~/bin/\n```\n\n\n### Using a QuestDB server.conf‚Äã\n\nYour QuestDB configuration is done in a\nserver.conf\nfile. The\nserver.conf\nfile is populated with safe defaults on first startup if it does not exist. It\nis common for user's of QuestDB to stick with the default configuration.\nHowever, should you choose to update your own and serve it via a scripted method\nor similar, you may do so.\nRead more about the available options in our\nConfiguration reference page\n.\n\n## Example questdb.service‚Äã\n\nCreate a new file called\nquestdb.service\n:\n\n```shell\ntouch questdb.service\n```\n\nThe example below is a recommended starting point. Note the default QuestDB\nservice configuration and system paths in line with the above example. Next,\nopen the\nquestdb.service\nfile and add the following:\n\n```shell\n[Unit]Description=QuestDBDocumentation=https://www.questdb.com/docs/After=network.target[Service]Type=simpleRestart=alwaysRestartSec=2# Adjust java path to match requirements of a given distroExecStart=/home/[USER_NAME]/jdk/bin/java \\--add-exports java.base/jdk.internal.math=io.questdb \\-p /home/[USER_NAME]/bin/questdb.jar \\-m io.questdb/io.questdb.ServerMain \\-DQuestDB-Runtime-66535 \\-ea -Dnoebug \\-XX:+UnlockExperimentalVMOptions \\-XX:+AlwaysPreTouch \\-XX:+UseParallelOldGC \\-d /home/[USER_NAME]/var/lib/questdbExecReload=/bin/kill -s HUP $MAINPID# Prevent writes to /usr, /boot, and /etcProtectSystem=fullStandardError=syslogSyslogIdentifier=questdb[Install]WantedBy=multi-user.target\n```\n\nNext, move your\nquestdb.service\nfile into your user's\nsystemd\nconfig:\n\n```shell\nmv questdb.service  ~/.config/systemd/user/questdb.service\n```\n\nEnable the service:\n\n```shell\nsystemctl --user enable questdb.service\n```\n\nStart the service:\n\n```shell\nsystemctl --user start questdb\n```\n\nCheck out the service status:\n\n```shell\nsystemctl --user status questdb.service\n```\n\nYour QuestDB instance should now be accessible at localhost, with services\navailable at the following default ports:\n- Web Console& REST API is available on port9000\n- PostgreSQL wire protocol available on8812\n- InfluxDB line protocol9009(TCP and UDP)\n- Health monitoring & Prometheus/metrics9003\n\n## User vs. System‚Äã\n\nAs an operator, you can decide whether to run systemd as the \"system\" from root\npermissions, or a user with its own privileges. At the system level, root is\nrequired to make changes to the\nsystemctl\napplication. Services created this\nway will start and stop when the system is restarted.\nUnlike at the system level, user services will start & stop as the user session\nis activated or de-activated. You also do not need to apply\nsudo\nto make\nchanges to the services.\nConsistent with the examples on this page, we recommend scoped users.\n\n## Daily timers‚Äã\n\nIf running QuestDB on a\nsystemd\nbased Linux (for example,\nUbuntu\n) you may find that, by default, there are a number of daily upgrade timers enabled.\nWhen executed, these tasks restart\nsystemd\nservices, which can cause interruptions to QuestDB. It will appear\nthat QuestDB restarted with no errors or apparent trigger.\nTo resolve it, either:\n- Force services to be listed for restart, but not restarted automatically.Modify/etc/needrestart/needrestart.confto contain$nrconf{restart} = 'l'.\n- Disable the auto-upgrade services entirely:\n\n```bash\nsudo systemctl disable --now apt-daily-upgrade.timersudo systemctl disable --now apt-daily.timersudo systemctl disable --now unattended-upgrades.service\n```\n\nYou can check the status of the timers using:\n\n```bash\nsystemctl list-timers --all | grep apt\n```\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "official",
    "sentiment": "neutral",
    "word_count": 627,
    "metadata": {}
  }
]