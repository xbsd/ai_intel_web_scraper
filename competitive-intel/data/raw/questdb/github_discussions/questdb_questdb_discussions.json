[
  {
    "id": "questdb-github_discussion-0cad1e900796",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/6646",
    "title": "Keep Sender Object Alive",
    "text": "# Keep Sender Object Alive\n\nDear community, I think it would be great to have the functionality to keep connected/alive a sender object in real time, so that users could save new rows into questdb at high frequency, without having to instantiate a new sender each cycle. \r\n\r\nref: https://py-questdb-client.readthedocs.io/en/stable/sender.html",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 49,
    "metadata": {
      "discussion_number": 6646,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2026-01-14T13:22:31Z",
      "author": "rdiok3"
    }
  },
  {
    "id": "questdb-github_discussion-a928a83b591c",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/6447",
    "title": "Proposal: Security Assessment from Team Atlanta (DARPA AIxCC)",
    "text": "# Proposal: Security Assessment from Team Atlanta (DARPA AIxCC)\n\nHi questdb developers,\r\n\r\nWe ([LeeSinLiang](https://github.com/LeeSinLiang), and [Cen Zhang](https://github.com/occia), and a lot of our team members) are Team Atlanta from Georgia Institute of Technology, winners of DARPA's AI Cyber Challenge (AIxCC). We're reaching out to propose a security assessment collaboration with your project. This effort is recommended by DARPA's initiative to apply competition technologies to real-world open source projects.\r\n\r\n#### Background\r\n\r\nWe have built an AI-enhanced CRS (Cyber Reasoning System) for automatic vulnerability detection and repair.\r\n\r\n- AIxCC Competition: https://aicyberchallenge.com/\r\n- Our Team: https://team-atlanta.github.io/\r\n\r\n#### What we plan to provide\r\n\r\n- OSS-Fuzz Integration: \r\n  - If your project isn't yet supported by [OSS-Fuzz](https://github.com/google/oss-fuzz), we'll develop compatible fuzzing harnesses to enable its integration. This can make our system applicable to your project.\r\n- Security Assessment: \r\n  - We'll run assessments locally on our infrastructure (no changes/efforts from your side) to identify potential vulnerabilities and synthesize corresponding patches.\r\n- Detailed Reports: \r\n  - For any findings, we'll provide reports including: 1) identified vulnerabilities and explanations, 2) the proof-of-concept (PoC) to trigger those vulnerabilities, and 3) corresponding patches.\r\n- Responsible Disclosure: \r\n  - We'll follow your preferred reporting channels (private email, OSS-Fuzz bug report system, or whatever channel you prefer) and coordinate disclosure timelines with your team. Note that **all findings will be further manually validated by our researchers before reporting to ensure quality and accuracy**.\r\n\r\n#### What we need\r\n\r\nA brief acknowledgment confirming your willingness to collaborate. This will serve as approval for our assessment plans.\r\n\r\n\r\nLooking forward to your response and please let me know for any further issues/concerns!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 262,
    "metadata": {
      "discussion_number": 6447,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 3,
      "created_at": "2025-11-25T04:21:40Z",
      "author": "occia"
    }
  },
  {
    "id": "questdb-github_discussion-e08807116595",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5956",
    "title": "Production Usage in Aviation & Border Systems: Clarification on Replication / HA Roadmap",
    "text": "# Production Usage in Aviation & Border Systems: Clarification on Replication / HA Roadmap\n\n\r\nHello QuestDB team,\r\n\r\nWe are currently using QuestDB in production within a project for a world-leading company in the aviation and border security domain. QuestDBâ€™s performance has been excellent for our high-ingestion workloads, and we appreciate the engineering efforts behind it.\r\n\r\nHowever, our operations and leadership teams are increasingly concerned about the lack of HA or replication support.\r\n\r\nIâ€™ve been following issue #4957 and noticed the continuous improvements to WAL mode, which I understand are prerequisites for any future replication implementation.\r\n\r\nCould you please clarify:\r\n\r\n1. Is replication (WAL shipping, streaming, or multi-node clustering) actively under development at the moment, or is it still in design consideration?\r\n\r\n\r\n2. Do you have a tentative timeframe (e.g. within the next 6-12 months, or longer-term) for introducing any replication or HA features, even in an experimental or enterprise-only release?\r\n\r\n\r\n3. If there is no near-term plan, would you recommend any best practices or architectural patterns to achieve active/passive resilience today beyond backups and dual ingestion pipelines?\r\n\r\n\r\n\r\nThis is crucial for us to determine our long-term strategy with QuestDB, as replication and HA are operational requirements for production systems in aviation and border security.\r\n\r\nThank you for your outstanding work and clarity on this topic.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 215,
    "metadata": {
      "discussion_number": 5956,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2025-07-17T22:25:12Z",
      "author": "Javier-Godon"
    }
  },
  {
    "id": "questdb-github_discussion-e706d9c6af35",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5635",
    "title": "QuestDB as a Railway.app template",
    "text": "# QuestDB as a Railway.app template\n\nhttps://railway.com/template/GEyNgl?referralCode=PfZ6Y8\r\n\r\nJust made a Railway template for Quest DB for anyone who wants to test out or work with Quest DB for their hobby projects ðŸš€\r\n\r\n<img width=\"1582\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c0e59e04-4889-4158-a007-cef6b79c689d\" />\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 37,
    "metadata": {
      "discussion_number": 5635,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2025-04-29T05:35:48Z",
      "author": "monotykamary"
    }
  },
  {
    "id": "questdb-github_discussion-4d5ef819a2ec",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5511",
    "title": "Data Ingest via curl / PHP?",
    "text": "# Data Ingest via curl / PHP?\n\nHello,\r\n\r\nI have a PHP application which creates data in ILP format. As far as I can see, there is no PHP client available? As the data is already in ILP format, is there a writing REST endpoint which I can use to ingest the ILP data from PHP / curl?\r\n\r\nThanks a lot!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 61,
    "metadata": {
      "discussion_number": 5511,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2025-03-25T10:26:29Z",
      "author": "Tegomena"
    }
  },
  {
    "id": "questdb-github_discussion-f82b33b48c51",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5468",
    "title": "SAMPLE BY months FROM-TO for multiple unique regions",
    "text": "# SAMPLE BY months FROM-TO for multiple unique regions\n\nHello,\r\n\r\nI have a table with weather data for multiple regions.\r\n\r\nI would like to do 3 monthly aggregations for the weather data for each region like this:\r\n```\r\nSELECT\r\n  ts,\r\n  region,\r\n  avg(total_sun) AS average\r\nFROM\r\n  ukmo_monthly_data\r\nWHERE\r\n  ts >= '2610-02-01' AND ts <= '2650-02-01'\r\nSAMPLE BY 3M\r\nFROM '2610-02-01' TO '2650-02-01'\r\n```\r\n(Please excuse the dates - I believe these were needed to index dates older than epoch time, otherwise I couldn't use `SAMPLE BY`)\r\n\r\nHowever when using `FROM-TO` I get the \"`FROM-TO` intervals are not supported for keyed `SAMPLE BY` queries\" error as explained in the limitations of `FROM-TO` here (https://questdb.com/docs/reference/sql/sample-by/#from-to)\r\n\r\nIf I drop the `FROM TO` query, then my returned ts columns are indexed by the January, even though I have requested to sample from February. By using an `ALIGN TO CALENDAR WITH OFFSET` only allows HH:mm, and if you put in 1 months worth of hours (744), it gives an invalid offset amount.\r\n\r\nFinally, by removing the regions from my `SELECT` clause, the data is indexed correctly, obviously missing out on my regions data.\r\n\r\nRealistically I am looking for a work around to the aggregation limitation of `FROM-TO` so that I can get the values for each region. Has anyone found a solution for something similar? And it would be great to know why there is this limitation in place, since it feels like it would be a pretty common problem in most timeseries data...\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 248,
    "metadata": {
      "discussion_number": 5468,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2025-03-14T11:52:02Z",
      "author": "WeatherNet-Benedict"
    }
  },
  {
    "id": "questdb-github_discussion-e60b8bf56fa5",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5267",
    "title": "Can we have the UUIDv7 as the rnd_uuid7?",
    "text": "# Can we have the UUIDv7 as the rnd_uuid7?\n\nWe have this -> https://questdb.io/docs/reference/function/random-value-generator/#rnd_uuid4\r\nwould it be possible to have uuidv7, one use case is to have it as primary key which is also sortable",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 35,
    "metadata": {
      "discussion_number": 5267,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-12-19T15:15:35Z",
      "author": "woss"
    }
  },
  {
    "id": "questdb-github_discussion-fc7ef5a04a7a",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5254",
    "title": "Remove duplicate rows",
    "text": "# Remove duplicate rows\n\nWe have a situation where a table that captures daily state of assets got duplicate information for a specific day (process that captures the state ran twice).  From documentation I see there is a `DEDUP` feature, but that only applies to new data imported into table.  I there a way to delete (we have a deleted column to handle special cases where we need to ignore some rows) the duplicate rows that currently exist in the table?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 81,
    "metadata": {
      "discussion_number": 5254,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2024-12-16T19:25:31Z",
      "author": "sptrakesh"
    }
  },
  {
    "id": "questdb-github_discussion-adaf1d070d90",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5185",
    "title": "Error when querying over longer time periods",
    "text": "# Error when querying over longer time periods\n\nHello,\r\n\r\nWe are frequently running into errors trying to retrieve aggregated information over longer time periods (more than 3 months for instance).  The data set is not very large, generally under 1m rows in table (usually a lot lower).  We very rarely perform out-of-order writes.  Usual error looks like the following:\r\n\r\n```Status: 500. Message: db query error: pq: could not open read-only [file=/var/lib/questdb/db/reelHistory~55/_cv]```\r\n\r\nThese error increase in frequency the longer the database has been up.  Restart definitely helps as a short term fix.\r\n\r\nWe have not experimented much with configuration parameters, so wondering if this is just a case of us needing to tweak the settings.   Running via docker, the following environment variables are specified:\r\n\r\n```\r\n    -e QDB_PG_READONLY_USER_ENABLED=true \\\r\n    -e QDB_PG_SELECT_CACHE_ENABLED=true \\\r\n    -e QDB_PG_NET_CONNECTION_LIMIT=128 \\\r\n    -e QDB_CAIRO_SNAPSHOT_INSTANCE_ID=98cf4d56-d759-440a-be03-edea3e4cb47d \\\r\n    -e QDB_CAIRO_WRITER_DATA_APPEND_PAGE_SIZE=1M \\\r\n    -e QDB_CAIRO_O3_COLUMN_MEMORY_SIZE=1M \\\r\n```\r\n\r\nWe are not specifying any memory parameters when running the container.  Generally the container consumes about 1G of RAM, but we have more that could be pre-allocated if that helps.  Thanks for any suggestions on working around this issue.\r\n\r\nRakesh",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 183,
    "metadata": {
      "discussion_number": 5185,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-11-22T12:52:54Z",
      "author": "sptrakesh"
    }
  },
  {
    "id": "questdb-github_discussion-dfdcab9737d7",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/5011",
    "title": "How to configure for a testing suite where data needs to be inserted before run",
    "text": "# How to configure for a testing suite where data needs to be inserted before run\n\nHi! \r\n\r\nWe're migrating from postgres to questdb in our timeseries tables, but we're struggling to configure the tables to **insert the data immediately**. \r\n\r\nWe know that Questdb does asynchronous inserting/batching etc..  that's amazing for our production usage but for testing we need the data to be inserted immediately or the promise of the insert wait until it's completely inserted, since we test that with these prices/values inserted our calculations are ok.\r\n\r\nI've tried to alter the `o3MaxLag`, `uncommitedRows` or changing the set type wall to bypass, but none worked.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 105,
    "metadata": {
      "discussion_number": 5011,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-10-03T10:16:41Z",
      "author": "semoal"
    }
  },
  {
    "id": "questdb-github_discussion-3106f16102cb",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4988",
    "title": "Migration from influxdb2: large increase in storage",
    "text": "# Migration from influxdb2: large increase in storage\n\nHello,\r\n\r\nI've been using influxdb2 for IoT sensors data storage for several years and I wanted to try questdb that sounds promising!\r\n\r\nI've used a python script to read my influxdb2 database and write (using the official influxdb python client), to questdb. I did migrate half of my db. It has 55M rows and 122 columns. The problem is that it uses 39 Go on my hard drive, vs 500 Mo with influxdb!\r\n\r\nI only have 6 tags on influx db and around 100 fields. Most points only have 1 field. But it seems that questdb stores the all columns for each point?\r\n\r\nI'm not sure how I can optimize it!\r\n\r\nThank!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 120,
    "metadata": {
      "discussion_number": 4988,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 4,
      "created_at": "2024-09-26T12:25:23Z",
      "author": "azertylr"
    }
  },
  {
    "id": "questdb-github_discussion-fe9e5db33d74",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4970",
    "title": "Querying large number of rows in C++.",
    "text": "# Querying large number of rows in C++.\n\nHello,\r\n\r\nI am using C++ in my project. I need to process hundreds of millions of rows from a single query.\r\n\r\nI read the documentation about cursors but it seems that `libpqxx` implements cursors using `DECLARE` which is not supported.\r\n\r\nThe only other solution I came up with is to use `LIMIT` with lower and upper bounds. Is there any better way I am missing? It seems weird that C++ is the language with which I cannot natively process large amounts of data from QuestDB.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 93,
    "metadata": {
      "discussion_number": 4970,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-09-20T09:11:40Z",
      "author": "peter-facko"
    }
  },
  {
    "id": "questdb-github_discussion-545c71dccd11",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4958",
    "title": "PG JDBC binary data transfer",
    "text": "# PG JDBC binary data transfer\n\nI was profiling application that queries a lot of data from QuestDB using postgres JDBC driver and noticed that all the data is returned as  Strings and then converted to long or double by JDBC driver. It creates a lot of garbage on the client side to parse from strings. It would be great to avoid binary -> String -> binary transformation. I am sure it will have positive impact on speed of queries.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 80,
    "metadata": {
      "discussion_number": 4958,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-09-16T14:12:34Z",
      "author": "asinitsyn"
    }
  },
  {
    "id": "questdb-github_discussion-beb9fdb66b36",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4955",
    "title": "Aggregate function with SAMPLE BY for multiple symbols.",
    "text": "# Aggregate function with SAMPLE BY for multiple symbols.\n\nHello,\r\n\r\nI have a table with price changes of multiple symbols.\r\nTherefore the columns are `TIMESTAMP`, `SYMBOL`, and `VALUE`.\r\nI need OHLC.\r\n\r\nIf I only had `TIMESTAMP` and `VALUE`, I would use `first(VALUE)` and `SAMPLE BY` for open prices.\r\nBut when I also have the `SYMBOL` column, how do I get the open price _for each symbol_?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 66,
    "metadata": {
      "discussion_number": 4955,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2024-09-15T20:09:13Z",
      "author": "peter-facko"
    }
  },
  {
    "id": "questdb-github_discussion-57b84056d2de",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4834",
    "title": "Are non-WAL tables really 'legacy'?",
    "text": "# Are non-WAL tables really 'legacy'?\n\nIn some of our projects, we make use of non-WAL tables. These are either tables without a designated timestamp or tables for which we require updates. In these cases, WAL is either not applicable or does not make sense.\r\n\r\nIs there any incentive to remove non-WAL tables in the future? If not, I think they should not be considered legacy.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 66,
    "metadata": {
      "discussion_number": 4834,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2024-07-31T12:28:27Z",
      "author": "peeterburger"
    }
  },
  {
    "id": "questdb-github_discussion-0b115a0d4f5e",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/4363",
    "title": "QuestDB driver for Dbeaver",
    "text": "# QuestDB driver for Dbeaver\n\nI have tried using the native Postgresql drivers, but they throw errors related to tables that are created by default on Postgres databases but are missing in QuestDB. Dbeaver is a very popular RDBMS gui tool. It will help adoption of questdb to have a native driver for this application.\r\n\r\nhttps://dbeaver.io/\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 56,
    "metadata": {
      "discussion_number": 4363,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2024-04-02T15:57:04Z",
      "author": "rodryquintero"
    }
  },
  {
    "id": "questdb-github_discussion-d98076cdf32a",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3973",
    "title": "better SQL syntax to find the latest data of different symbols?",
    "text": "# better SQL syntax to find the latest data of different symbols?\n\nTo find the latest one data of different symbols\r\nIs there better SQL syntax? I use below\r\n```sql\r\nSELECT\r\n  symbol,\r\n  timestamp,\r\n  value\r\nFROM\r\n  (\r\n    SELECT\r\n      symbol,\r\n      timestamp,\r\n      value,\r\n      ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY timestamp DESC) AS row_number\r\n    FROM\r\n      [table_name]\r\n  ) AS t\r\nWHERE\r\n  row_number = 1\r\n```\r\nIt is necessary to number each row and then take the row numbered 1, which may not be efficient.\r\n\r\nThe structure of the table is as follows:\r\n symbol | timestamp | value \r\n------- | -------- | -------\r\nGOOG | 2023-07-20 | 100 \r\nGOOG | 2023-07-21  | 110 \r\nGOOG | 2023-07-22 | 120 \r\nAAPL | 2023-07-20 | 200 \r\nAAPL | 2023-07-21 | 210 \r\nAAPL | 2023-07-22 | 220\r\n\r\n Using the above SQL syntax to query this table, we will get the following  Result: \r\nsymbol | timestamp | value \r\n------- | -------- | -------\r\nGOOG | 2023-07-22 | 120 \r\nAAPL | 2023-07-22 | 220",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 163,
    "metadata": {
      "discussion_number": 3973,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2023-11-17T06:53:29Z",
      "author": "sevenjay"
    }
  },
  {
    "id": "questdb-github_discussion-54b0ee920ec1",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3837",
    "title": "I'm having a hard time",
    "text": "# I'm having a hard time\n\nI need to change the database storage path, but I can't find a configuration file in the app folder that can change the storage path, my system is Windows, I only know that the database file is stored on C drive, but I can't find it",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 52,
    "metadata": {
      "discussion_number": 3837,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2023-10-11T09:03:47Z",
      "author": "1845062639"
    }
  },
  {
    "id": "questdb-github_discussion-ffe324307fc5",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3795",
    "title": "Some Questions about embedded mode QuestDB.",
    "text": "# Some Questions about embedded mode QuestDB.\n\nHello, I am utilizing QuestDB in embedded mode within my JDK 8 Spring Boot application, and I have a few questions:\r\n\r\nI've noticed that in the latest versions available on Maven Central Repository, such as 7.3.x and 7.2.x, there isn't a dedicated artifact for JDK 8. Could you provide some insights into this? Are there plans for future support for JDK 8?\r\n\r\nCould you recommend any ORM framework or provide examples for using QuestDB in SQL mode?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 84,
    "metadata": {
      "discussion_number": 3795,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2023-10-03T05:03:13Z",
      "author": "johnluoyx"
    }
  },
  {
    "id": "questdb-github_discussion-295f43ebcf8c",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3232",
    "title": "Programming Langauges ?",
    "text": "# Programming Langauges ?\n\n-_-",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 5,
    "metadata": {
      "discussion_number": 3232,
      "category": "Polls",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2023-04-18T11:10:51Z",
      "author": "SyedImtiyaz-1"
    }
  },
  {
    "id": "questdb-github_discussion-94be0dc26b26",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3136",
    "title": "Add inverted index for full-text search?",
    "text": "# Add inverted index for full-text search?\n\nFor datasets with large text, inverted index will search very fast.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 18,
    "metadata": {
      "discussion_number": 3136,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2023-03-30T01:16:11Z",
      "author": "GrassInWind2019"
    }
  },
  {
    "id": "questdb-github_discussion-5fdb68b26a68",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/3122",
    "title": "Issue or Query regarding multiple database in QuestDB",
    "text": "# Issue or Query regarding multiple database in QuestDB\n\nAdd support for multiple databases within a single QuestDB instance",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 19,
    "metadata": {
      "discussion_number": 3122,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2023-03-26T12:09:14Z",
      "author": "arnav398"
    }
  },
  {
    "id": "questdb-github_discussion-6db1ef0f6d38",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2876",
    "title": "Some questions about Demo official website",
    "text": "# Some questions about Demo official website\n\nMay I ask, in the Demo of the official website, does 1.6 billion data use a single node server or a multi-node server? If it is multi-node, what is the maximum data volume of a single node to ensure the speed of ms class",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 51,
    "metadata": {
      "discussion_number": 2876,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2022-12-21T05:22:07Z",
      "author": "muyu1285"
    }
  },
  {
    "id": "questdb-github_discussion-af456acf761a",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2811",
    "title": "Best practice for ingesting RSS feeds?",
    "text": "# Best practice for ingesting RSS feeds?\n\nHello,\r\n\r\nWe have a data source that publishes events to an RSS feed. \r\n\r\nIs there a best practice recommendation for ingesting that data and transforming for QuestDB?\r\n\r\nThere are ways to handle this via Flink or possibly Kafka, but if we could go: RSS -> QuestDB  it would simplify our solution.\r\n\r\nThanks for your help!",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 62,
    "metadata": {
      "discussion_number": 2811,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2022-11-24T11:08:09Z",
      "author": "tomtom215"
    }
  },
  {
    "id": "questdb-github_discussion-1c1cfb6f87c9",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2728",
    "title": "Thoughts on implementation of many separate multivariate time series?",
    "text": "# Thoughts on implementation of many separate multivariate time series?\n\nIn my application, we have multivariate time series data (some signals at a consistent interval, others not) associated with each human user.  What is the best configuration of these data in questdb if \r\n\r\n1. Data from multiple users is rarely accessed at the same time\r\n2. Application should scale as the number of users grow and as the number of samples for each user grows\r\n\r\nI'm still learning about questdb and any ideas about this would be much appreciated",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 89,
    "metadata": {
      "discussion_number": 2728,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2022-11-03T18:05:19Z",
      "author": "PorkShoulderHolder"
    }
  },
  {
    "id": "questdb-github_discussion-b884b04e0970",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2568",
    "title": "When the active partition is b0rked ...",
    "text": "# When the active partition is b0rked ...\n\nWe had a server crash yesterday. When the server was rebooted and questdb restarted, we found the active partition on one of the tables was corrupted.  As a result, all `INSERT`s were failing.\r\n\r\nWe could generally`SELECT` data prior to the current day but there were statements that would cause the questdb worker to crash. Examples:\r\n```\r\nSELECT * FROM tablename ORDER BY ts DESC LIMIT 1;\r\n\r\nINSERT INTO TABLE tablename2 SELECT * FROM tablename;\r\n```\r\n> Note: I didn't keep a copy of the corrupted partition folder but LMK if you're interested in the crash logs.\r\n\r\nSince we `...PARTITION BY DAY...`, I thought we'd only have to wait until 00:00 UTC, expecting QuestDB to create a new partition but it didn't. `INSERT`s continued to fail and I could not `ALTER TABLE DROP PARTITION` because the active partition was still the corrupted one.\r\n\r\nMy restoration process became:\r\n```\r\n# stop processes/cron/supervisor workers that write to table\r\n...\r\n\r\nCREATE TABLE tablename2 ( ... );\r\n\r\nINSERT INTO TABLE tablename2 SELECT * FROM tablename WHERE ts < '2022-09-29';\r\n\r\nDROP TABLE tablename;\r\n\r\nRENAME TABLE tablename2 TO tablename;\r\n\r\n...\r\n# re-import log data for 2022-09-29\r\n\r\n# restart writers\r\n```\r\n\r\n----\r\n\r\nWhat's the rationale behind not allowing the active partiton to be detached or dropped, given that there's no similar limitation to drop the entire table? (Being able to drop the active partition could have saved me a few steps.)\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 241,
    "metadata": {
      "discussion_number": 2568,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-09-30T13:30:22Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-9e5603cb8e89",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2525",
    "title": "Timestamps before start of Unix epoch",
    "text": "# Timestamps before start of Unix epoch\n\nI have been trying to import some data with timestamps before 1970-01-01, and it seems this does not work even though the documentation mentions that timestamps are stored as signed offset from start of the Unix epoch. Do I understand it correctly that storing that kind of data is not currently supported, and that this is the expected behaviour?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 66,
    "metadata": {
      "discussion_number": 2525,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 6,
      "created_at": "2022-09-14T19:21:24Z",
      "author": "this-user"
    }
  },
  {
    "id": "questdb-github_discussion-91ff435c25a9",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2503",
    "title": "Late arriving data - by design - an IoT use case",
    "text": "# Late arriving data - by design - an IoT use case\n\nHi everyone,\r\n\r\nNot really a show and tell, but more a show and ask. I toyed with putting this discussion in slack, but I think what I'm going to ask might be useful to others and things tend to get buried in slack.\r\n\r\n### My Use Case.\r\n\r\n- IoT\r\n- Temperature and humidity sensors deployed in residential properties.\r\n- Sensors communicate to an in-home hub via RF 868.\r\n- In-home hub has a GSM backhaul/connection to the internet.\r\n- Sensors take a reading every 15mins and communicate this reading to the in-home hub instantly. \r\n- In-home hub caches the readings for 2 hours to reduce data costs over the cellular network due to overhead of communication i.e. fewer transmits with larger payloads (more readings) is more data efficient.\r\n- The 2 hour delay is not synchronised i.e. an in-home hub can and does send it's data every second of every day (we have a lot of hubs!)\r\n\r\nSo, in summary, delivery of data across the network is \"laggy\" by design. This lag is obviously _significantly_ longer than the type of lag you would expect to see with typical TCP based delivery due to network jitter and out-of-order (OOO) async processing. \r\n\r\n### My Use Case - In QuestDb\r\n\r\nI see a few possible approaches.\r\n\r\n#### Option 1 - long commit lag\r\n\r\nSet a commit lag in QuestDb to cover the 2 hour window + a little extra to account for the standard jitter once the message arrives on a TCP network.\r\n\r\nThis feels like the best approach for optimal data ingest but feels like it might have some potential pain points.\r\n\r\n1. My server would need enough RAM to store a two hour block of readings + overhead for normal operation. Whilst this _might_ be possible today, it doesn't feel very scalable as the number of sensors grows overtime.\r\n2. Risk of data loss - the more data we have in-memory and uncommitted the greater the chance we might lose that data in the event of an unexpected shutdown - power loss etc.\r\n\r\n#### Option 2 - slower ingest\r\n\r\nAccept that all my ingests will incur the penalty of data file resorts\r\n\r\n1. Feels safer but not very satisfying. We're taking something which is amazingly good (the super fast ingest) and throwing it away. I guess the importance of this depends entirely on just how big a penalty we pay from the resorts.\r\n2. Do these resorts result in degraded read performance?\r\n\r\n#### Option 3 - I'm missing something\r\n\r\nI hope this is the case! Is there a third option which I've not considered?\r\n\r\nWould love to get everyones thoughts.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 451,
    "metadata": {
      "discussion_number": 2503,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-09-06T04:08:36Z",
      "author": "Recodify"
    }
  },
  {
    "id": "questdb-github_discussion-526f506d2130",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2030",
    "title": "Select table of indexes",
    "text": "# Select table of indexes\n\nAccording to https://questdb.io/docs/concept/indexes/, whenever you make a column indexed, it creates a second table of the symbols you are indexing, one row for each unique symbol. What I am hoping to do is just to simply select all of the rows in this second table and get the symbols. Is there a way to do this?\r\n\r\nWhat I am doing right now is `SELECT DISTINCT sym FROM 'mytable';`. I am not sure if this is the best way to do this, but it does execute very fast... is this the proper way? Would it be a good idea to make it possible for users to select from this index table? \r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 115,
    "metadata": {
      "discussion_number": 2030,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-04-13T15:33:11Z",
      "author": "xav-ie"
    }
  },
  {
    "id": "questdb-github_discussion-5f203abbd6ff",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/2029",
    "title": "Rating curves / value transformations",
    "text": "# Rating curves / value transformations\n\nI'm looking at QuestDB for applications in hydrology where [rating curves](https://en.wikipedia.org/wiki/Rating_curve) are commonly used.\r\nIt would be **very** useful if they were '_built in_', and I think such functionality could be broadly useful outside just hydrology.\r\n\r\nI'll give some examples below, but foremost I'm wondering if this would be considered for inclusion?\r\n(I'm willing to contribute if it makes sense).\r\n\r\n---\r\n\r\nGiven some data:\r\n\r\n| ts         | val |\r\n|------------|-----|\r\n| 1| 2   |\r\n| 2 | 65  |\r\n\r\nI specify a rating curve as a relationship between `domain` and `range` as a series of fixed points, e.g:\r\n\r\n\r\n| domain | range |\r\n|-----|------|\r\n| 0   | 10   |\r\n| 50  | 20   |\r\n| 60  | 20   |\r\n| 100 | 40   |\r\n\r\nThis is then applied by linearly interpolating between the closest `domain` for each `val`, e.g:\r\n\r\n* `2` is `0.04` between `0` and `50` and so `val'` is  `10 + ((20 - 10) * 0.04) = 10.4`.\r\n* `65` is `0.125` between `60` and `100` and so `val'` is  `20 + ((40 - 20) * 0.125) = 22.5`.\r\n\r\nAny `val` outside the limits given in the rating curve's `domain` are undefined.\r\n\r\n---\r\n\r\nImplementing this currently with `JOIN` would be awful,\r\nbut I think if there were some special e.g. `TRANSFORM` function then it could be very elegant.\r\n\r\nOne other fact:\r\nThese rating tables are updated intermittently, so there also needs to be logic to select the correct table.\r\n`ASOF JOIN` would be perfect for this ðŸ’« \r\n\r\n---\r\n\r\nHere's what I envisage, I can select `data`, join a `rating` table, and use some new `TRANSFORM` function to do the math:\r\n\r\n```sql\r\nCREATE TABLE data (val FLOAT, ts TIMESTAMP) timestamp(ts);\r\nCREATE TABLE rating (id LONG, domain FLOAT, range FLOAT, effective TIMESTAMP) timestamp(effective);\r\n\r\nSELECT ts, val, TRANSFORM(val, domain, range)\r\nFROM data\r\nASOF JOIN rating\r\nWHERE id = 123\r\n```\r\n\r\n| ts         | val | transform\r\n|------------|-----|---|\r\n| 1| 2   | 10.4 |\r\n| 2 | 65  | 22.5 |",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 335,
    "metadata": {
      "discussion_number": 2029,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-04-12T21:36:58Z",
      "author": "davetapley"
    }
  },
  {
    "id": "questdb-github_discussion-43928112341a",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1973",
    "title": "Speed of querying data",
    "text": "# Speed of querying data\n\nI want to get 2.000.000 records from table via simple SQL request like \r\n**select * from table**\r\n\r\nQuery took several milliseconds and it is OK but  when i use REST API /exec command for it i get result in 3 or 4 seconds. So getting result takes 3 or 4 seconds. \r\n\r\n**questdb and client are located on same computer**\r\n\r\nIs there any way to get 2.000.000 records in 0.1 seconds ?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 76,
    "metadata": {
      "discussion_number": 1973,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-03-22T09:01:05Z",
      "author": "balexus"
    }
  },
  {
    "id": "questdb-github_discussion-2b694e3fca4f",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1877",
    "title": "Python DB API 2.0 Driver and SQLAlchemy Dialect for QuestDB & Superset Integration",
    "text": "# Python DB API 2.0 Driver and SQLAlchemy Dialect for QuestDB & Superset Integration\n\nHello QuestDB community! My name's Srini and I'm involved with the Apache Superset (https://github.com/apache/superset) project. Superset was originally designed to work well with Apache Druid and has since expanded to support nearly any SQL speaking database. \r\n\r\nSuperset especially shines in time-series visualization, which is why QuestDB came to mind. I'd love to advocate for the creation of a a Python DB API 2.0 driver + SQLAlchemy dialect (as a single library) for QuestDB so Quest and Superset can start to jam together.\r\n\r\nBesides this driver, there is a tiny amount of integration work needed on the Superset side. I wrote about that here: https://preset.io/blog/building-database-connector/\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 118,
    "metadata": {
      "discussion_number": 1877,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2022-02-12T15:07:43Z",
      "author": "srinify"
    }
  },
  {
    "id": "questdb-github_discussion-daf41268da67",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1802",
    "title": "How to implement high availability?",
    "text": "# How to implement high availability?\n\nhi, folks,\r\nI found no description about high availability from official document, so, how to achieve high availability? \r\nthanks",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 25,
    "metadata": {
      "discussion_number": 1802,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2022-01-19T03:39:07Z",
      "author": "YYXXYYZZ"
    }
  },
  {
    "id": "questdb-github_discussion-c0aaf888afbc",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1737",
    "title": "How to store unique timestamp",
    "text": "# How to store unique timestamp\n\nI have to store stock market data. When I receive update on candle, I should rewrite it, Insert creates duplicated timestamp. How to insert in not exist, but if exist to update it? ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 39,
    "metadata": {
      "discussion_number": 1737,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-12-28T09:45:39Z",
      "author": "rnurislom"
    }
  },
  {
    "id": "questdb-github_discussion-ff5444dbdaee",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1753",
    "title": "Does QuestDB support User Defined Function?",
    "text": "# Does QuestDB support User Defined Function?\n\n\r\nI am currently working on a benchmark of time series database systems for analytical (complex) queries. These queries are algorithms that cannot (easily) be implemented in SQL language so they require a third-party programming language such as C, C++, Python, Java, etc.\r\n\r\nI am writing to you to ask if your system supports such a feature of a third-party programming language for User Defined Functions? \r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 72,
    "metadata": {
      "discussion_number": 1753,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-12-17T15:17:57Z",
      "author": "AKheli"
    }
  },
  {
    "id": "questdb-github_discussion-dcbd82dc97ce",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1688",
    "title": "questDB as a service",
    "text": "# questDB as a service\n\nI would like to achieve something like yugabyte Cloud service, but using questDB.\r\n\r\nQuestDb have plans for this or in the meantime any ideas on how to achieve a simillar experience?\r\n\r\nThe ideas is to autoscale quesdb by demand and pay as you go and don't need to have a headcount managaing this infrastruture",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 59,
    "metadata": {
      "discussion_number": 1688,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-12-14T14:50:49Z",
      "author": "unknown"
    }
  },
  {
    "id": "questdb-github_discussion-43fb1462a1af",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1697",
    "title": "Use Tradingview's open source charting",
    "text": "# Use Tradingview's open source charting\n\n### Is your feature request related to a problem?\n\nThe current charts in the web console are not interactive, and especially given QuestDB's heavy usage with financial data, I think it would be a useful feature to have, so that one could ex. zoom in on a stock's price or something. \n\n### Describe the solution you'd like.\n\nTradingview has an [open source](https://github.com/tradingview/lightweight-charts) HTML5 charting library that is really amazing. See the demos [here](https://www.tradingview.com/lightweight-charts/). They are clean and configurable, load fast, can be actively updated, etc. \n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context.\n\n_No response_",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 103,
    "metadata": {
      "discussion_number": 1697,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-12-14T04:01:27Z",
      "author": "rdong8"
    }
  },
  {
    "id": "questdb-github_discussion-bfcf01f53eb1",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1580",
    "title": "Kubernetes cluster with multiple nodes",
    "text": "# Kubernetes cluster with multiple nodes\n\nHi, i want to know if I can set up a kubernetes cluster with many questdb nodes and make it open source.\r\nCan I do it or do I need to pay a subscription?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 40,
    "metadata": {
      "discussion_number": 1580,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-11-16T18:18:53Z",
      "author": "PriscillaMingo"
    }
  },
  {
    "id": "questdb-github_discussion-5c3cb3f2c524",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1545",
    "title": "`latest by` syntax",
    "text": "# `latest by` syntax\n\n## The status quo\r\n\r\nWe had quite a few questions on how `latest by` works and it has always been a confusing subject for pretty much everybody. This includes ourselves where various implementations of `latest by` do not agree in results. We do [have a PR to fix this](https://github.com/questdb/questdb/pull/1478) but I thought we should also address the syntax.\r\n\r\nThe goal is to make `latest by` syntax intuitive and easy to understand. Consider this SQL:\r\n\r\n```sql\r\npos latest by id where geo6 within(#wtq) and time < '2021-09-19T00:00:00.000000Z';\r\n```\r\n\r\nHere the \"rule\" dictates that `where` is executed before `latest by`. However it is not intuitive because of the actual order of clauses in the SQL. That's one problem.\r\n\r\nAnother problem is that, if we follow the execution \"rule\", the SQL will actually execute as follows:\r\n\r\n```sql\r\n(pos latest by id where time < '2021-09-19T00:00:00.000000Z') where geo6 within(#wtq);\r\n```\r\n\r\nso it is breaking the \"rule\" in effect making things even harder to comprehend.\r\n\r\n## The proposal\r\n\r\nThere are proposals actually, one we discusses offline with @marregui \r\n\r\n```sql\r\n(pos where x > 0 latest on ts partition by id end at '2021-09-19T00:00:00.000000Z') where geo6 within(#wtq);\r\n```\r\n\r\nThe changes are:\r\n- `where` is now syntactically preceding `latest` keyword\r\n- `latest` keyword now has `on` to specify timestamp column explicitly\r\n- `latest` has its own interval definition `start at` and `end at`. Here there are keywords `end` and `start` to open the interval bounds\r\n- `by` is replaced with `partition by` to be more explicit what this column means\r\n- `where` clauses that execute __after__ `latest by` should be outside of the sub-query\r\n\r\nI also discussed this with @ideoma and he pointed out what interval could also be specified like so:\r\n\r\n```sql\r\n(pos where x > 0 and ts < '2021-09-19T00:00:00.000000Z' latest on ts partition by id) where geo6 within(#wtq);\r\n```\r\n\r\nin this case we do not need to program against a case where timestamp interval is in two places at the same time. I tend to side with @ideoma here.\r\n\r\n## The ask\r\n\r\nPlease let us know if these syntax changes make `latest` keywork more intuitive, ask questions if you have any and propose changes and alternatives.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 366,
    "metadata": {
      "discussion_number": 1545,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-11-09T15:30:45Z",
      "author": "bluestreak01"
    }
  },
  {
    "id": "questdb-github_discussion-9a06a12504b7",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1477",
    "title": "QuestDB AUR package for Archlinux",
    "text": "# QuestDB AUR package for Archlinux\n\nI created an [AUR package](https://aur.archlinux.org/packages/questdb/) for questdb on archlinux. This package will install questdb as a systemd service so you can `systemctl start questdb`. It will be convenience for local development on archlinux. Let me know if you have further ideas or suggestions on this package.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 52,
    "metadata": {
      "discussion_number": 1477,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-10-26T07:12:44Z",
      "author": "sunng87"
    }
  },
  {
    "id": "questdb-github_discussion-1826482ccc51",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1476",
    "title": "Data retention configuration and auto drop expired partitions",
    "text": "# Data retention configuration and auto drop expired partitions\n\nAccording to [data retention](https://questdb.io/docs/operations/data-retention) section of the doc, the data retention policy on questdb is implemented as drop-able partitions. While this is flexible to implement custom policy of expiry, I still miss influxdb's straight-forward way: defining data retention policy, delete expired data automatically.  It requires no additional cronjob or operation script to keep the database sustaining. \r\n\r\nThe idea is to add a `MAX_PARTITIONS` option to `CREATE TABLE` command:\r\n\r\n```sql\r\nCREATE TABLE my_table (timestamp TIMESTAMP, x LONG) timestamp(timestamp)\r\nPARTITION BY DAY MAX_PARTITIONS 30;\r\n```\r\n\r\nSo that partitions are deleted in a FIFO manner when its count exceeds 30. \r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 106,
    "metadata": {
      "discussion_number": 1476,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 5,
      "created_at": "2021-10-26T06:59:48Z",
      "author": "sunng87"
    }
  },
  {
    "id": "questdb-github_discussion-d1c5e1f353de",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1443",
    "title": "Date/Time functions and SAMPLE BY",
    "text": "# Date/Time functions and SAMPLE BY\n\nThis [example](https://questdb.io/docs/reference/sql/sample-by):\r\n```\r\nSELECT ts, count() FROM trades\r\nSAMPLE BY 1h\r\n```\r\nappears to lose the SAMPLE BY aggregation when a date/time function is applied to `ts`, e.g.,\r\n```\r\nSELECT to_timezone(ts, 'America/Toronto'), count() FROM trades\r\nSAMPLE BY 1h\r\n```\r\n\r\nIs this intentional?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 48,
    "metadata": {
      "discussion_number": 1443,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-10-18T15:59:06Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-fd3cd4af46ed",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1410",
    "title": "systemd questdb.service config",
    "text": "# systemd questdb.service config\n\nDoes this look like a reasonable config?\r\n```\r\n[Unit]\r\nDescription=QuestDB\r\nDocumentation=https://www.questdb.io/docs/introduction\r\nAfter=network.target\r\n\r\n[Service]\r\nType=forking\r\nRestart=always\r\nRestartSec=2\r\nExecStart=/opt/questdb/bin/questdb.sh start\r\nExecStop=/opt/questdb/bin/questdb.sh stop\r\n# Prevent writes to /usr, /boot, and /etc\r\nProtectSystem=full\r\nStandardOutput=syslog\r\nStandardError=syslog\r\nSyslogIdentifier=questdb\r\n\r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\nI adapted it from https://raw.githubusercontent.com/questdb/questdb-packer-ami/master/src/assets/systemd.service",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 44,
    "metadata": {
      "discussion_number": 1410,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-10-05T22:22:16Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-49881873a784",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1390",
    "title": "[Academic study] how do OSS developers annotate Technical Debt?",
    "text": "# [Academic study] how do OSS developers annotate Technical Debt?\n\nHello,\r\n\r\nWe're a team of Software Engineering researchers investigating technical debt. Technical debt refers to \"\"not quite right code which we postpone making it rightâ€ (Cunningham)  and this may often introduce a cost. More specifically, the aim of this study is to gather information on how open source developers respond to specific source code phenomena such as partially or wrongly implemented functionality. With the results of this study we aim to create a better understanding of how developers annotate issues in source code. These results will be used to support prioritisation of maintenance activities in software systems. The survey data will be collected, analyzed and stored so that research questions can be answered and the results can be published. Since the survey is anonymous, no personal information is collected. For the same reason we cannot modify or remove answers upon request.\r\n\r\nIf you contribute code to this project you can help us out by filling out the following survey: https://forms.office.com/r/QmrtwiA296. The survey should take you at most 15 minutes. \r\n\r\nThanks in advance,\r\n\r\nNathan",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 183,
    "metadata": {
      "discussion_number": 1390,
      "category": "News",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-10-05T13:23:55Z",
      "author": "TheDutchDevil"
    }
  },
  {
    "id": "questdb-github_discussion-52e3f1ce22d4",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1378",
    "title": "Postgresql parameterized ~ query fails",
    "text": "# Postgresql parameterized ~ query fails\n\nQuestDB: 6.0.7.1\r\nJava: openjdk-11-jre-headless:amd64  11.0.11+9-0ubuntu2~20.04  \r\nOS: Ubuntu 20.04 LTS\r\nPostgresql client: 12.8-0ubuntu0.20.04.1\r\nPHP: 8.0.11 with PDO, pdo_pgsql extensions\r\n\r\nExample table:\r\n\r\n```sql\r\nCREATE TABLE foo (\r\n    ts TIMESTAMP,\r\n    code SYMBOL INDEX\r\n) timestamp(ts) PARTITION BY DAY;\r\n```\r\n\r\nQuery via psql (works):\r\n```sql\r\nSELECT code, count() FROM foo WHERE ts IN '2021-10-03' AND code ~ '154';\r\n```\r\n\r\nParameterized query  via php (errors out):\r\n```php\r\n<?php\r\n$tsn = '2021-10-03';\r\n$pattern = '154';\r\n$sql = <<<END_SQL\r\nSELECT code, count() FROM foo WHERE ts IN ? AND code ~ ?\r\nEND_SQL\r\n;\r\n$dsn = 'pgsql:host=localhost;port=8812;dbname=qdb;user=admin;password=quest';\r\n$dbh = new PDO($dsn);\r\n$sth = $dbh->prepare($sql);\r\n$sth->execute([$tsn, $pattern]);\r\n```\r\n\r\nError:\r\n```\r\n2021-10-04T00:50:29.942524Z E i.q.c.p.PGConnectionContext error [pos=109, msg=`unexpected argument for function: ~. expected args: (STRING,STRING constant). actual args: (SYMBOL,unknown)`, errno=`0]\r\n```\r\n\r\nDo you think it's a questdb bug in the postresql wire protocol, or a php pdo_pgsql bug?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 144,
    "metadata": {
      "discussion_number": 1378,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2021-10-04T02:04:14Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-3a627c4a6d71",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1337",
    "title": "What is the timestamp unit?",
    "text": "# What is the timestamp unit?\n\nHello:\r\n\r\nI have been going through the documentation, but I cannot find the units of the timestamp when I use questDB embedded.\r\nThe method putTimeStamp(int index, long ts) has no javadoc.  Question is: ts should be in milliseconds? nanoseconds? microseconds?  I assume is epoch-based.  \r\n\r\nThanks,\r\n\r\nJuan",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 52,
    "metadata": {
      "discussion_number": 1337,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2021-09-19T14:28:55Z",
      "author": "jfarjona"
    }
  },
  {
    "id": "questdb-github_discussion-6c8e6a927ffa",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1303",
    "title": "Schema for REST /imp",
    "text": "# Schema for REST /imp\n\nHow does one specify that the column has an index? Something along the lines of\r\n```\r\n {\"name\":\"currency\", \"type\":\"SYMBOL\", \"index\":true},\r\n```",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 25,
    "metadata": {
      "discussion_number": 1303,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2021-09-09T22:16:01Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-7a35f429bc7f",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1298",
    "title": "QuestDB as Grafana Data source?",
    "text": "# QuestDB as Grafana Data source?\n\nIs this plugin dead?\r\n\r\nhttps://github.com/questdb/grafana-datasource",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 11,
    "metadata": {
      "discussion_number": 1298,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-09-07T22:44:22Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-71c1cc51ca8d",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1291",
    "title": "ILP TCP randomly dropping rows",
    "text": "# ILP TCP randomly dropping rows\n\nI'm testing out QuestDB 6.0.4 (default config). Using the same data each time (~1M rows), I'm finding different rows are randomly dropped each time. The errors are always of the variety:\r\n\r\n```\r\nTIMESTAMP E i.q.c.l.t.LineTcpConnectionContext [29] could not parse measurement, code INVALID_FIELD_SEPERATOR at XXX line (may be mangled due to partial parsing) is table,symbolList fieldValueList timestamp\r\n```\r\n\r\n(The line number reported also varies, e.g., 149, 155, 306, 312, 322, ...)\r\n\r\nThere's nothing wrong with the rejected rows. I can re-ingest them without error. Is this considered acceptable behaviour, or is it a bug?",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 99,
    "metadata": {
      "discussion_number": 1291,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 10,
      "created_at": "2021-09-05T23:34:29Z",
      "author": "robocoder"
    }
  },
  {
    "id": "questdb-github_discussion-e0dac5e68164",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1238",
    "title": "Add a new TAB for notifications, log messages",
    "text": "# Add a new TAB for notifications, log messages\n\nThe actual time limited notification is not user friendly when the user makes mistakes that are reported back.\r\n\r\nWould be nice if there is a `TAB` dedicated exclusivelly for notifications, log messages and when there is an error that tab would be activated showing the error.\r\n\r\nCould be on the same row as we have now `Grid`, `Chart` and the will have `Grid`, `Chart`, `Notifications or Log messages` with let's say the last X notifications.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 84,
    "metadata": {
      "discussion_number": 1238,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-08-11T09:00:47Z",
      "author": "mingodad"
    }
  },
  {
    "id": "questdb-github_discussion-18706000dae0",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1272",
    "title": "timestamp column should be first in schema auto-created from Influx Line Protocol",
    "text": "# timestamp column should be first in schema auto-created from Influx Line Protocol\n\n**Describe the bug**\r\nIt is rather uncommon and inconvenient to have timestamp as last column in auto-created schemas.\r\nFor tables with many columns, the timestamp not even visible from console without scrolling.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Write a new measurement through ILP port 9009 on a table that does not exist (yet)\r\n2. Notice timestamp is last column in autocreated schema\r\n\r\n**Expected behavior**\r\nTimestamp is the first column of autocreated schema.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 88,
    "metadata": {
      "discussion_number": 1272,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 3,
      "created_at": "2021-08-09T17:42:09Z",
      "author": "comunidadio"
    }
  },
  {
    "id": "questdb-github_discussion-f007429779e5",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1221",
    "title": "Instructions to deploy multi-node cluster w/o K8s",
    "text": "# Instructions to deploy multi-node cluster w/o K8s\n\nWondering if there's a guide to deploy multiple OSS QuestDB containers via [Auto Scaling Groups](https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html) with Multi-Attach ESB volume to form a QuestDB cluster.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 32,
    "metadata": {
      "discussion_number": 1221,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2021-07-24T19:06:05Z",
      "author": "ericsun2"
    }
  },
  {
    "id": "questdb-github_discussion-9a6c65968ca8",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1184",
    "title": "Provide server response for ILP over TCP",
    "text": "# Provide server response for ILP over TCP\n\nQuestDB does not currently provide a server response for ILP messages sent over TCP, which means the sender would not know of any errors QuestDB encounters e.g. INVALID_FIELD_VALUE, rows out of order etc., nor will it know if the messages were successfully ingested.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 51,
    "metadata": {
      "discussion_number": 1184,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-07-06T11:58:38Z",
      "author": "spxie85"
    }
  },
  {
    "id": "questdb-github_discussion-987ac1682f25",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1180",
    "title": "Questitto: Your out-of-the-box IoT Prototype Stack with QuestDB",
    "text": "# Questitto: Your out-of-the-box IoT Prototype Stack with QuestDB\n\nBack in 2020, I created an IoT prototyping stack that proved to be very helpful and saved a lot of time.\r\n\r\nPresenting [Questitto](https://github.com/shantanoo-desai/questitto) an *out-of-the-box* stack for IoT applications that relies on standard software like:\r\n\r\n- `docker`, `docker-compose`\r\n- `questdb`\r\n- `mosquitto` MQTT broker\r\n- `telegraf`\r\n\r\n## Benefits\r\n\r\nIt is a **zero-code, only-config** stack where there is less effort on bringing vital IoT data from sensors / actuators into QuestDB. The heavy lifting of data ingestion is done by `telegraf`.\r\n\r\n## Roadmap\r\nI am looking forward to updating the repository with following features:\r\n\r\n- update components to stable image versions \r\n- Introduce a reverse-proxy like `traefik`\r\n- Introduce local TLS based secured stack to make deploying safe IoT prototypes easier\r\n- Any other features the users want to introduce",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 138,
    "metadata": {
      "discussion_number": 1180,
      "category": "Show and tell",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-07-06T09:42:53Z",
      "author": "shantanoo-desai"
    }
  },
  {
    "id": "questdb-github_discussion-ed17c01ec1ae",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1173",
    "title": "How can I put data into a binary type column?",
    "text": "# How can I put data into a binary type column?\n\nHi,\r\n\r\nI'm trying to manually put data into a binary type column using web console, InfluxDB line protocol and REST.\r\nBut I couldn't find any reference on this method. It seems that it can't simply be inserted as a hex string.\r\nIs there any way to put data into a binary column?\r\n\r\nThanks.\r\n\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 64,
    "metadata": {
      "discussion_number": 1173,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-07-01T06:57:42Z",
      "author": "lsw8724"
    }
  },
  {
    "id": "questdb-github_discussion-67dbfa2e8182",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1171",
    "title": "Develop QDB with lightness of resource and speed in mind",
    "text": "# Develop QDB with lightness of resource and speed in mind\n\nAs more and more ideas and resources are added / shared with community and the QDB team, the product is expected to grow in functionality - which is awesome!\r\nSome of this functionality however will naturally lead to more resources being consumed/needed when running QDB (that's my assumption - I may be wrong about this), but not all of the functionality will be used by everyone. \r\nWith this in mind - if possible - I believe it will be cool if QDB can still run a light version with core functionality; or ideally have it very modular where each additional functionality which weighs-in on resources is made optional and can be added / removed at runtime.\r\n\r\nA bit of an abstract thought/idea, but I hope I described it clearly. ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 140,
    "metadata": {
      "discussion_number": 1171,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-06-30T12:06:46Z",
      "author": "newskooler"
    }
  },
  {
    "id": "questdb-github_discussion-e8d13c0bbf92",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1170",
    "title": "Add native scheduler which executes SQL queries (similar to InfluxDB tasks).",
    "text": "# Add native scheduler which executes SQL queries (similar to InfluxDB tasks).\n\n**The problem**\r\nUsually QuestDB users would have very large time-series table. Often these tables need to be re-sampled in some fashion in order to be easier to read / use. One way to do this is via cron (on Linux) or via other time-based schedulers (Airflow, Luigi, Prefect, etc.). All of the other scheduler would require additional setup on the user end - some much more than others. \r\nAlso, some would be slower to run and quite heavy - thus if your job is time sensitive, execution will be slower.\r\n\r\n**Proposed simple solution**\r\nInfluxDB offers Tasks - a native time-based scheduler. I propose that QuestDB adds such a native scheduler for SQL code to run, thus enabling such re-samping from table A to table B to be done (as well as anything else doable via SQL). This will make such a time-based SQL code quick to setup and maintain.\r\n\r\n**Proposed advanced solution features**\r\n- Ability to schedule based on DB event (count / sum / etc.) vs time.\r\n- Ability to re-trigger failed execution attempts\r\n- Run sub 1min (cron is limited to 1 min schedules, so this would be quite valuable)\r\n- Be super fast and low\r\n- Ability to switch it off (so that ppl who don't use it still run QDB in a lighter way)\r\n\r\n\r\nI believe many of you will find this extremely useful, so I thought I'd open a discussion on the subject here as well. You can find the original issue [here](https://github.com/questdb/questdb/issues/979).\r\n\r\nIf you have proposals on how to improve this idea - please share. ðŸ‘ \r\nIf you like the idea - it's important you give it a thumbs up, so the QDB team can prioritize accordingly. ðŸ¥‡ ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 296,
    "metadata": {
      "discussion_number": 1170,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 3,
      "created_at": "2021-06-30T08:25:15Z",
      "author": "newskooler"
    }
  },
  {
    "id": "questdb-github_discussion-3bed8424c715",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1273",
    "title": "Add QuestDB support to Liquibase",
    "text": "# Add QuestDB support to Liquibase\n\n**Is your feature request related to a problem? Please describe.**\r\nSince QuestDB is projected to be used at an enterprise level, some level of integration with migration tools will come in handy when its being integrated into an enterprise stack. I would like to have QuestDB supported in, LiquiBase (https://www.liquibase.org/)\r\n\r\n**Describe the solution you'd like**\r\nLiquibase Community is an open source project that helps millions of developers rapidly manage database schema changes. It would be great to have QuestDB under their supported integrations\r\n\r\n**Describe alternatives you've considered**\r\nManually moving schema definitions and keeping them in sync between our various test and prod environments\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 109,
    "metadata": {
      "discussion_number": 1273,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-06-29T13:03:09Z",
      "author": "defunctostrich"
    }
  },
  {
    "id": "questdb-github_discussion-183f1b0e45fe",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1162",
    "title": "Support for sliding window / moving averages",
    "text": "# Support for sliding window / moving averages\n\nA Sliding window or Moving average is pretty important in many use cases. It would be nice if questdb has this built in.\r\n\r\nFor reference, here's clickhouse's:\r\nhttps://clickhouse.tech/docs/en/sql-reference/aggregate-functions/reference/grouparraymovingavg/",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 36,
    "metadata": {
      "discussion_number": 1162,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 6,
      "created_at": "2021-06-28T17:35:28Z",
      "author": "mationai"
    }
  },
  {
    "id": "questdb-github_discussion-ae218ff2f80f",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1155",
    "title": "Support for update / delete",
    "text": "# Support for update / delete\n\nStandard use case for QuestDB is to store billions of data and currently if you make a mistake, you accidentally store data in wrong format, you store duplicate data or any other mistake the only option you have to fix the table is to export the data, polish them and create a new table. Such operation is quite challenging and time consuming on such large datasets. It is even more tricky if you have platform that is running 24x7 and storing data into QuestDB in realtime and you don't want to miss any updates .... then fixing such data without update/delete operation is currently almost impossible.\r\n\r\nI believe that this request applies to almost any use case as it is only matter of time when some unwanted data gets stored.\r\n\r\nAlso now that QuestDB has O3 support implementing update/delete on top of it shouldn't be that difficult and will make QuestDB even more feature complete product.\r\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 162,
    "metadata": {
      "discussion_number": 1155,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2021-06-24T11:25:56Z",
      "author": "postol"
    }
  },
  {
    "id": "questdb-github_discussion-cc26d2373f53",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1151",
    "title": "Supporting Unit of Measurement (UoM) as built in",
    "text": "# Supporting Unit of Measurement (UoM) as built in\n\nHi All, \r\n\r\nOne thing that missing i think from all of the timeseries database is UoM. In my previous works on [O&G](https://www.petrolink.com/) and [Energistics](https://www.energistics.org/) we have that as core concept in the domain. \r\n\r\nIt's have it's own [standard](https://www.energistics.org/energistics-unit-of-measure-standard/).\r\n\r\nI think all of the sensors or IoT etc have some kind of measurement? As we need to align that first to do computation..\r\n\r\nThought? ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 72,
    "metadata": {
      "discussion_number": 1151,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 1,
      "created_at": "2021-06-24T02:27:00Z",
      "author": "welly87"
    }
  },
  {
    "id": "questdb-github_discussion-14ce9f843203",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1149",
    "title": "Integration with Data Lake Technology",
    "text": "# Integration with Data Lake Technology\n\nHi All, \r\n\r\nJust wondering if there are a possibility to include something from big data technology like \r\n\r\n1. [open data](https://www.dremio.com/its-wise-to-choose-open) columnar format like parquet/arrow so it can be accessed from other big data tools in s3? or maybe like intermediary etc?\r\n2. transport using [Apache Arrow Flight](https://www.dremio.com/understanding-apache-arrow-flight) as i think this one is the future of data transport? that way we can integrate easily with dremio and [analytics other tools](https://www.dremio.com/subsurface/high-performance-big-data-analytics-processing-using-hardware-acceleration)?\r\n3. streaming processing support like Apache Flink SQL etc?\r\n\r\nI'm just thinking out loud here. Hopefully the ideas is worth your time :)\r\n\r\nCheers",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 100,
    "metadata": {
      "discussion_number": 1149,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-06-24T02:23:56Z",
      "author": "welly87"
    }
  },
  {
    "id": "questdb-github_discussion-a3a7d3462382",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1147",
    "title": "Ability to save queries in web console",
    "text": "# Ability to save queries in web console\n\nMany db admin apps has the ability to save queries and I find it very useful.\r\n\r\nOne idea of how the UI can look is to a button that toggles the past queries view, maybe to the left of the Run button. When off, there's no difference from the console today. When on, a vertical scroll list of past queries appears between the schema view and the query editor. The list can be sorted by last modified or by alphabet order. Ability to pin or favorite a query would be nice too.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 100,
    "metadata": {
      "discussion_number": 1147,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-06-23T18:35:57Z",
      "author": "mationai"
    }
  },
  {
    "id": "questdb-github_discussion-7e36181b5f00",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1134",
    "title": "Welcome to QuestDB discussions",
    "text": "# Welcome to QuestDB discussions\n\n### Welcome! ðŸš€ \r\n\r\nDear QuestDB Community, weâ€™re using Discussions as a place to connect with other members of our community. \r\n\r\n* **News:** We'll share news or blog posts about QuestDB regularly! \r\n* **Ideas:** You can share your ideas and suggestions, discuss potential features, or vote for features that you like!\r\n* **Show & Tell:** Feel free to demonstrate your projects that used QuestDB! \r\n\r\n\r\n### Learn more:\r\n\r\n#### â“ Get support\r\n\r\n- [Community Slack:](https://slack.questdb.io) Join technical discussions, ask questions, and meet other users!\r\n- [GitHub issues:](https://github.com/questdb/questdb/issues) Report bugs or issues with QuestDB.\r\n- [GitHub discussions:](https://github.com/questdb/questdb/discussions) Propose new features or show what you've built.\r\n- [Stack Overflow:](https://stackoverflow.com/questions/tagged/questdb) Look for common troubleshooting solutions.\r\n\r\n#### ðŸ“š Read the docs\r\n\r\n- [QuestDB documentation:](https://questdb.io/docs/introduction/) Understand how to run and configure QuestDB.\r\n- [Tutorials:](https://questdb.io/tutorial/) Learn what's possible with QuestDB step by step.\r\n- [Product roadmap:](https://github.com/questdb/questdb/projects) Check out our plan for upcoming releases.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 150,
    "metadata": {
      "discussion_number": 1134,
      "category": "News",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2021-06-21T11:02:39Z",
      "author": "bsmth"
    }
  },
  {
    "id": "questdb-github_discussion-fa652e4cd9ab",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1136",
    "title": "Query speed of Postgres vs REST API vs InfluxDB line protocol",
    "text": "# Query speed of Postgres vs REST API vs InfluxDB line protocol\n\nHi there, as mentioned above is there any speed difference when querying OR inserting rows using these 3 different methods. Does the /exec method go through PostGRES anyway?\r\n\r\nThanks for your time.",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 44,
    "metadata": {
      "discussion_number": 1136,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 4,
      "created_at": "2021-01-21T10:02:32Z",
      "author": "huzaifahj"
    }
  },
  {
    "id": "questdb-github_discussion-8365bd49a535",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1146",
    "title": "Submitting feedback / bug report via web console",
    "text": "# Submitting feedback / bug report via web console\n\nTo simplify feedback, we could add UI components in the web console for bug reports. A simple solution is to link to GitHub issues where users can report bugs, but there is the benefit in the web console of being able to make a snapshot of the database state at this point which is easier to debug, troubleshoot or reproduce. ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 69,
    "metadata": {
      "discussion_number": 1146,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2020-12-18T13:14:51Z",
      "author": "bsmth"
    }
  },
  {
    "id": "questdb-github_discussion-4d18b8f22aa4",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1135",
    "title": "Does QuestDB support percentile aggregation?",
    "text": "# Does QuestDB support percentile aggregation?\n\nI've looked in the docs and wasn't able to find anything aside from the regular aggregations (min, avg, etc), and playing with Postgres syntax for it on `demo.questdb.io` suggests that it isn't supported.\r\n\r\nAre there plans for this? Could look into something like https://github.com/tdunning/t-digest where intermediate results can be partitioned and merged",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 58,
    "metadata": {
      "discussion_number": 1135,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 3,
      "created_at": "2020-11-19T16:56:55Z",
      "author": "pickledish"
    }
  },
  {
    "id": "questdb-github_discussion-0e2211dcc788",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1141",
    "title": "Add GitHub event log dataset for demo.questdb.io",
    "text": "# Add GitHub event log dataset for demo.questdb.io\n\nGitHub event log is a perfect data set as time series data example. GHTorrent data set available as [collection of CSV files](https://ghtorrent.org/downloads.html)\r\n\r\nSee import:\r\n![Screenshot from 2020-06-27 12-21-35](https://user-images.githubusercontent.com/10332206/85919071-d81f4f80-b870-11ea-88a3-9c2d2467f4d8.png)\r\n\r\n\r\nOther alternatives are parse json data from [gharchive](https://www.gharchive.org/) ",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 44,
    "metadata": {
      "discussion_number": 1141,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 0,
      "created_at": "2020-06-27T09:23:19Z",
      "author": "igor-suhorukov"
    }
  },
  {
    "id": "questdb-github_discussion-795bb216d3cc",
    "origin": "questdb",
    "source_type": "github_discussion",
    "url": "https://github.com/questdb/questdb/discussions/1137",
    "title": "What's the difference between commit() and commitDurable()",
    "text": "# What's the difference between commit() and commitDurable()\n\nHi,\n\nThis is not really an issue per se, but I was wondering what the implications of using `commit()` vs `commitDurable()` are. I've noticed that the number of writes per second drops to a very low figure when using the durable mode. Is this because the database doesn't really flush to disk synchronously when using simply `commit()`? In case of power failure or similar issues, would that cause any data to be lost?\n",
    "scraped_date": "2026-02-22",
    "content_date": null,
    "topics": [],
    "subtopics": [],
    "credibility": "community",
    "sentiment": "neutral",
    "word_count": 81,
    "metadata": {
      "discussion_number": 1137,
      "category": "Ideas",
      "is_answered": false,
      "answer_body": null,
      "comments_count": 2,
      "created_at": "2015-04-04T05:21:11Z",
      "author": "unknown"
    }
  }
]