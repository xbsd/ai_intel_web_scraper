"""ChromaDB vector store for competitive intelligence chunks.

Manages two collections:
  1. 'competitive_intel' — All chunked source data (blog, docs, GitHub, community)
     with rich metadata for filtering by competitor, topic, source_type, credibility, etc.
  2. 'competitive_comparisons' — Cross-competitor comparison chunks generated by the
     LLM pipeline (assessments, differentiators, objection handlers, pitches).

ChromaDB runs embedded (no server process) with persistent storage to disk.
"""

import logging
from datetime import date
from pathlib import Path
from typing import Optional

import chromadb

from vectorstore.chunker import RawChunk

logger = logging.getLogger(__name__)

DEFAULT_DB_PATH = Path(__file__).parent.parent / "data" / "vectordb"
COLLECTION_SOURCE = "competitive_intel"
COLLECTION_COMPARISONS = "competitive_comparisons"

# ChromaDB metadata supports: str, int, float, bool — not lists.
# We serialize list fields (topic_ids) as comma-separated strings.


class VectorStore:
    """ChromaDB-backed vector store with metadata filtering."""

    def __init__(self, db_path: Optional[str] = None):
        """Initialize the persistent ChromaDB client.

        Args:
            db_path: Path to the ChromaDB storage directory.
                     Defaults to data/vectordb/ within the project.
        """
        self.db_path = Path(db_path) if db_path else DEFAULT_DB_PATH
        self.db_path.mkdir(parents=True, exist_ok=True)

        self.client = chromadb.PersistentClient(path=str(self.db_path))
        logger.info("ChromaDB initialized at %s", self.db_path)

    # -------------------------------------------------------------------
    # Collection management
    # -------------------------------------------------------------------

    def get_source_collection(self) -> chromadb.Collection:
        """Get or create the source data collection."""
        return self.client.get_or_create_collection(
            name=COLLECTION_SOURCE,
            metadata={"description": "Chunked competitive intelligence source data"},
        )

    def get_comparison_collection(self) -> chromadb.Collection:
        """Get or create the comparison/generated content collection."""
        return self.client.get_or_create_collection(
            name=COLLECTION_COMPARISONS,
            metadata={"description": "LLM-generated competitive comparisons and analysis"},
        )

    # -------------------------------------------------------------------
    # Upsert source chunks
    # -------------------------------------------------------------------

    def upsert_source_chunks(
        self,
        chunks: list[RawChunk],
        embeddings: list[list[float]],
    ) -> int:
        """Upsert chunked source data into the competitive_intel collection.

        Args:
            chunks: List of RawChunk objects from the chunker.
            embeddings: Corresponding embedding vectors.

        Returns:
            Number of chunks upserted.
        """
        if len(chunks) != len(embeddings):
            raise ValueError(
                f"Chunk count ({len(chunks)}) != embedding count ({len(embeddings)})"
            )

        collection = self.get_source_collection()

        # ChromaDB accepts batches of up to 5461 items; we use 500 for safety
        batch_size = 500
        total_upserted = 0

        for start in range(0, len(chunks), batch_size):
            end = min(start + batch_size, len(chunks))
            batch_chunks = chunks[start:end]
            batch_embeddings = embeddings[start:end]

            ids = []
            documents = []
            metadatas = []

            for chunk in batch_chunks:
                ids.append(chunk.id)
                documents.append(chunk.text)
                metadatas.append(self._chunk_to_metadata(chunk))

            collection.upsert(
                ids=ids,
                documents=documents,
                embeddings=batch_embeddings,
                metadatas=metadatas,
            )

            total_upserted += len(batch_chunks)
            logger.info(
                "Upserted batch %d-%d (%d chunks)",
                start, end, len(batch_chunks),
            )

        logger.info(
            "Total upserted to '%s': %d chunks",
            COLLECTION_SOURCE, total_upserted,
        )
        return total_upserted

    # -------------------------------------------------------------------
    # Upsert comparison/generated chunks
    # -------------------------------------------------------------------

    def upsert_comparison_chunks(
        self,
        ids: list[str],
        texts: list[str],
        embeddings: list[list[float]],
        metadatas: list[dict],
    ) -> int:
        """Upsert LLM-generated comparison chunks.

        Args:
            ids: Unique IDs for each comparison chunk.
            texts: The comparison text content.
            embeddings: Embedding vectors.
            metadatas: Metadata dicts (competitor, topic_id, content_type, etc.).

        Returns:
            Number of chunks upserted.
        """
        collection = self.get_comparison_collection()

        batch_size = 500
        total = 0
        for start in range(0, len(ids), batch_size):
            end = min(start + batch_size, len(ids))
            # Sanitize metadata values for ChromaDB
            sanitized_metas = [self._sanitize_metadata(m) for m in metadatas[start:end]]
            collection.upsert(
                ids=ids[start:end],
                documents=texts[start:end],
                embeddings=embeddings[start:end],
                metadatas=sanitized_metas,
            )
            total += end - start

        logger.info("Upserted %d comparison chunks to '%s'", total, COLLECTION_COMPARISONS)
        return total

    # -------------------------------------------------------------------
    # Query
    # -------------------------------------------------------------------

    def query(
        self,
        query_embedding: list[float],
        collection_name: str = COLLECTION_SOURCE,
        n_results: int = 8,
        where: Optional[dict] = None,
        where_document: Optional[dict] = None,
    ) -> dict:
        """Query the vector store.

        Args:
            query_embedding: The query vector.
            collection_name: Which collection to search.
            n_results: Number of results to return.
            where: Metadata filter (ChromaDB where clause).
            where_document: Document content filter.

        Returns:
            ChromaDB query result dict with ids, documents, metadatas, distances.
        """
        collection = self.client.get_collection(collection_name)

        kwargs = {
            "query_embeddings": [query_embedding],
            "n_results": n_results,
        }
        if where:
            kwargs["where"] = where
        if where_document:
            kwargs["where_document"] = where_document

        return collection.query(**kwargs)

    def query_by_text(
        self,
        query_text: str,
        embedder,
        collection_name: str = COLLECTION_SOURCE,
        n_results: int = 8,
        where: Optional[dict] = None,
    ) -> dict:
        """Query using text (embeds the query first).

        Args:
            query_text: Natural language query.
            embedder: An Embedder instance to generate the query embedding.
            collection_name: Which collection to search.
            n_results: Number of results.
            where: Metadata filter.

        Returns:
            ChromaDB query result dict.
        """
        query_embedding = embedder.embed_single(query_text)
        return self.query(
            query_embedding=query_embedding,
            collection_name=collection_name,
            n_results=n_results,
            where=where,
        )

    # -------------------------------------------------------------------
    # Status / info
    # -------------------------------------------------------------------

    def get_stats(self) -> dict:
        """Get statistics about all collections."""
        stats = {}
        for name in [COLLECTION_SOURCE, COLLECTION_COMPARISONS]:
            try:
                col = self.client.get_collection(name)
                count = col.count()
                stats[name] = {"count": count}

                # Sample metadata to show what's stored
                if count > 0:
                    sample = col.peek(limit=1)
                    if sample and sample.get("metadatas"):
                        stats[name]["sample_metadata_keys"] = list(
                            sample["metadatas"][0].keys()
                        )
            except Exception:
                stats[name] = {"count": 0}

        return stats

    def get_detailed_stats(
        self,
        filter_field: Optional[str] = None,
        filter_value: Optional[str] = None,
    ) -> dict:
        """Get detailed statistics with per-field breakdowns.

        Args:
            filter_field: Optional metadata field to filter by (e.g. 'competitor').
            filter_value: Value to filter on (e.g. 'clickhouse').

        Returns aggregated counts by competitor, source_type, primary_topic,
        and credibility for the source collection.
        """
        result = {"collections": {}}

        for name in [COLLECTION_SOURCE, COLLECTION_COMPARISONS]:
            try:
                col = self.client.get_collection(name)
                count = col.count()
                col_stats = {"count": count, "breakdowns": {}}

                if count > 0:
                    # Fetch all metadata (no documents or embeddings for speed)
                    all_data = col.get(include=["metadatas"])
                    metadatas = all_data.get("metadatas", [])

                    # Apply optional drill-down filter
                    if filter_field and filter_value:
                        metadatas = [
                            m for m in metadatas
                            if m.get(filter_field, "") == filter_value
                        ]
                        col_stats["filtered_count"] = len(metadatas)

                    # Aggregate by key fields
                    fields = ["competitor", "source_type", "primary_topic", "credibility"]
                    for field in fields:
                        counts = {}
                        for meta in metadatas:
                            val = meta.get(field, "unknown")
                            if not val:
                                val = "unknown"
                            counts[val] = counts.get(val, 0) + 1
                        # Sort by count descending
                        col_stats["breakdowns"][field] = dict(
                            sorted(counts.items(), key=lambda x: -x[1])
                        )

                    # Metadata key list from first record
                    if metadatas:
                        col_stats["metadata_keys"] = sorted(metadatas[0].keys())

                result["collections"][name] = col_stats
            except Exception as e:
                logger.warning("Failed to get detailed stats for %s: %s", name, e)
                result["collections"][name] = {"count": 0, "breakdowns": {}}

        return result

    def delete_collection(self, name: str):
        """Delete a collection (for re-ingestion)."""
        try:
            self.client.delete_collection(name)
            logger.info("Deleted collection '%s'", name)
        except Exception:
            logger.info("Collection '%s' does not exist, nothing to delete", name)

    def reset(self):
        """Delete both collections for a fresh start."""
        self.delete_collection(COLLECTION_SOURCE)
        self.delete_collection(COLLECTION_COMPARISONS)

    # -------------------------------------------------------------------
    # Metadata helpers
    # -------------------------------------------------------------------

    def _chunk_to_metadata(self, chunk: RawChunk) -> dict:
        """Convert a RawChunk to a ChromaDB-compatible metadata dict.

        ChromaDB metadata values must be str, int, float, or bool.
        Lists are serialized as comma-separated strings.
        None values are converted to empty strings.
        """
        meta = {
            "competitor": chunk.competitor,
            "source_type": chunk.source_type,
            "source_url": chunk.source_url,
            "source_title": chunk.source_title,
            "topic_ids": ",".join(chunk.topic_ids),
            "primary_topic": chunk.topic_ids[0] if chunk.topic_ids else "unclassified",
            "credibility": chunk.credibility,
            "content_date": str(chunk.content_date) if chunk.content_date else "",
            "scraped_date": str(chunk.scraped_date),
            "chunk_index": chunk.chunk_index,
            "parent_doc_id": chunk.parent_doc_id,
            "token_count": chunk.token_count,
        }

        # Merge source-specific metadata (already ChromaDB-safe types from chunker)
        for k, v in chunk.metadata.items():
            if isinstance(v, (str, int, float, bool)):
                meta[k] = v
            elif v is None:
                meta[k] = ""
            else:
                meta[k] = str(v)

        return meta

    def _sanitize_metadata(self, meta: dict) -> dict:
        """Ensure all metadata values are ChromaDB-compatible types."""
        sanitized = {}
        for k, v in meta.items():
            if isinstance(v, (str, int, float, bool)):
                sanitized[k] = v
            elif isinstance(v, date):
                sanitized[k] = str(v)
            elif isinstance(v, list):
                sanitized[k] = ",".join(str(x) for x in v)
            elif v is None:
                sanitized[k] = ""
            else:
                sanitized[k] = str(v)
        return sanitized
